{"2024-06-10T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2402.13254v3","updated":"2024-06-10T17:59:55Z","published":"2024-02-20T18:59:55Z","title":"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples","summary":"  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V. To facilitate future research, we release our code, dataset, benchmark,\nand checkpoints at https://countercurate.github.io.\n","authors":["Jianrui Zhang","Mu Cai","Tengyang Xie","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2402.13254v3.pdf","comment":"15 pages, 6 figures, 12 tables, Project Page:\n  https://countercurate.github.io/"},{"id":"http://arxiv.org/abs/2310.14558v4","updated":"2024-06-10T17:52:31Z","published":"2023-10-23T04:22:50Z","title":"AlpaCare:Instruction-tuned Large Language Models for Medical Application","summary":"  Instruction-finetuning (IFT) has become crucial in aligning Large Language\nModels (LLMs) with diverse human needs and has shown great potential in medical\napplications. However, previous studies mainly fine-tune LLMs on biomedical\ndatasets with limited diversity, which often rely on benchmarks or narrow task\nscopes, and hence significantly limit the effectiveness on their medical\ninstruction-following ability and generalizability. To bridge this gap, we\npropose creating a diverse, machine-generated medical IFT dataset,\nMedInstruct-52k, using GPT-4 and ChatGPT with a high-quality expert-curated\nseed set. We then fine-tune LLaMA-series models on the dataset to develop\nAlpaCare. Despite using a smaller domain-specific dataset than previous medical\nLLMs, AlpaCare not only demonstrates superior performance on medical\napplications, with up to 38.1% absolute gain over best baselines in medical\nfree-form instruction evaluations, but also achieves 6.7% absolute gains\naveraged over multiple general domain benchmarks. Human evaluation further\nshows that AlpaCare consistently outperforms best baselines in terms of both\ncorrectness and helpfulness. We offer public access to our data, model, and\ncodebase in https://github.com/XZhang97666/AlpaCare.\n","authors":["Xinlu Zhang","Chenxin Tian","Xianjun Yang","Lichang Chen","Zekun Li","Linda Ruth Petzold"],"pdf_url":"https://arxiv.org/pdf/2310.14558v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07453v4","updated":"2024-06-10T17:50:14Z","published":"2024-01-15T03:57:15Z","title":"Model Editing at Scale leads to Gradual and Catastrophic Forgetting","summary":"  Editing knowledge in large language models is an attractive capability to\nhave which allows us to correct incorrectly learnt facts during pre-training,\nas well as update the model with an ever-growing list of new facts. While\nexisting model editing techniques have shown promise, they are usually\nevaluated using metrics for reliability, specificity and generalization over\none or few edits. We argue that for model editing to have practical utility, we\nmust be able to make multiple edits to the same model. With this in mind, we\nevaluate the current model editing methods at scale, focusing on two state of\nthe art methods: ROME and MEMIT. We find that as the model is edited\nsequentially with multiple facts, it continually forgets previously edited\nfacts and the ability to perform downstream tasks. This forgetting happens in\ntwo phases -- an initial gradual but progressive forgetting phase followed by\nabrupt or catastrophic forgetting phase. Both gradual and catastrophic\nforgetting limit the usefulness of model editing methods at scale -- the former\nmaking model editing less effective as multiple edits are made to the model\nwhile the latter caps the scalability of such model editing methods. Our\nanalysis also highlights other key limitations of ROME and MEMIT at scale. With\nour work, we push for the development and evaluation of model editing methods\nkeeping scalability in mind.\n","authors":["Akshat Gupta","Anurag Rao","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2401.07453v4.pdf","comment":"ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2402.17944v3","updated":"2024-06-10T17:41:32Z","published":"2024-02-27T23:59:01Z","title":"Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and\n  Understanding -- A Survey","summary":"  Recent breakthroughs in large language modeling have facilitated rigorous\nexploration of their application in diverse tasks related to tabular data\nmodeling, such as prediction, tabular data synthesis, question answering, and\ntable understanding. Each task presents unique challenges and opportunities.\nHowever, there is currently a lack of comprehensive review that summarizes and\ncompares the key techniques, metrics, datasets, models, and optimization\napproaches in this research domain. This survey aims to address this gap by\nconsolidating recent progress in these areas, offering a thorough survey and\ntaxonomy of the datasets, metrics, and methodologies utilized. It identifies\nstrengths, limitations, unexplored territories, and gaps in the existing\nliterature, while providing some insights for future research directions in\nthis vital and rapidly evolving field. It also provides relevant code and\ndatasets references. Through this comprehensive review, we hope to provide\ninterested readers with pertinent references and insightful perspectives,\nempowering them with the necessary tools and knowledge to effectively navigate\nand address the prevailing challenges in the field.\n","authors":["Xi Fang","Weijie Xu","Fiona Anting Tan","Jiani Zhang","Ziqing Hu","Yanjun Qi","Scott Nickleach","Diego Socolinsky","Srinivasan Sengamedu","Christos Faloutsos"],"pdf_url":"https://arxiv.org/pdf/2402.17944v3.pdf","comment":"41 pages, 4 figures, 8 tables"},{"id":"http://arxiv.org/abs/2406.04313v2","updated":"2024-06-10T17:40:19Z","published":"2024-06-06T17:57:04Z","title":"Improving Alignment and Robustness with Circuit Breakers","summary":"  AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that interrupts the models as they respond with harmful outputs\nwith \"circuit breakers.\" Existing techniques aimed at improving alignment, such\nas refusal training, are often bypassed. Techniques such as adversarial\ntraining try to plug these holes by countering specific attacks. As an\nalternative to refusal training and adversarial training, circuit-breaking\ndirectly controls the representations that are responsible for harmful outputs\nin the first place. Our technique can be applied to both text-only and\nmultimodal language models to prevent the generation of harmful outputs without\nsacrificing utility -- even in the presence of powerful unseen attacks.\nNotably, while adversarial robustness in standalone image recognition remains\nan open challenge, circuit breakers allow the larger multimodal system to\nreliably withstand image \"hijacks\" that aim to produce harmful content.\nFinally, we extend our approach to AI agents, demonstrating considerable\nreductions in the rate of harmful actions when they are under attack. Our\napproach represents a significant step forward in the development of reliable\nsafeguards to harmful behavior and adversarial attacks.\n","authors":["Andy Zou","Long Phan","Justin Wang","Derek Duenas","Maxwell Lin","Maksym Andriushchenko","Rowan Wang","Zico Kolter","Matt Fredrikson","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2406.04313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12716v3","updated":"2024-06-10T17:39:04Z","published":"2023-12-20T02:22:49Z","title":"BloomVQA: Assessing Hierarchical Multi-modal Comprehension","summary":"  We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive\nevaluation of large vision-language models on comprehension tasks. Unlike\ncurrent benchmarks that often focus on fact-based memorization and simple\nreasoning tasks without theoretical grounding, we collect multiple-choice\nsamples based on picture stories that reflect different levels of\ncomprehension, as laid out in Bloom's Taxonomy, a classic framework for\nlearning assessment widely adopted in education research. Our data maps to a\nnovel hierarchical graph representation which enables automatic data\naugmentation and novel measures characterizing model consistency. We perform\ngraded evaluation and reliability analysis on recent multi-modal models. In\ncomparison to low-level tasks, we observe decreased performance on tasks\nrequiring advanced comprehension and cognitive skills with up to 38.0\\% drop in\nVQA accuracy. In comparison to earlier models, GPT-4V demonstrates improved\naccuracy over all comprehension levels and shows a tendency of bypassing visual\ninputs especially for higher-level tasks. Current models also show consistency\npatterns misaligned with human comprehension in various scenarios,\ndemonstrating the need for improvement based on theoretically-grounded\ncriteria.\n","authors":["Yunye Gong","Robik Shrestha","Jared Claypoole","Michael Cogswell","Arijit Ray","Christopher Kanan","Ajay Divakaran"],"pdf_url":"https://arxiv.org/pdf/2312.12716v3.pdf","comment":"Accepted by ACL Findings (2024). Dataset available at\n  https://huggingface.co/datasets/ygong/BloomVQA"},{"id":"http://arxiv.org/abs/2406.06496v1","updated":"2024-06-10T17:31:36Z","published":"2024-06-10T17:31:36Z","title":"Direct Preference Optimization for Suppressing Hallucinated Prior Exams\n  in Radiology Report Generation","summary":"  Recent advances in generative vision-language models (VLMs) have exciting\npotential implications for AI in radiology, yet VLMs are also known to produce\nhallucinations, nonsensical text, and other unwanted behaviors that can waste\nclinicians' time and cause patient harm. Drawing on recent work on direct\npreference optimization (DPO), we propose a simple method for modifying the\nbehavior of pretrained VLMs performing radiology report generation by\nsuppressing unwanted types of generations. We apply our method to the\nprevention of hallucinations of prior exams, addressing a long-established\nproblem behavior in models performing chest X-ray report generation. Across our\nexperiments, we find that DPO fine-tuning achieves a 3.2-4.8x reduction in\nlines hallucinating prior exams while maintaining model performance on clinical\naccuracy metrics. Our work is, to the best of our knowledge, the first work to\napply DPO to medical VLMs, providing a data- and compute- efficient way to\nsuppress problem behaviors while maintaining overall clinical accuracy.\n","authors":["Oishi Banerjee","Hong-Yu Zhou","Subathra Adithan","Stephen Kwak","Kay Wu","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2406.06496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06485v1","updated":"2024-06-10T17:24:44Z","published":"2024-06-10T17:24:44Z","title":"Can Language Models Serve as Text-Based World Simulators?","summary":"  Virtual environments play a key role in benchmarking advances in complex\nplanning and decision-making tasks but are expensive and complicated to build\nby hand. Can current language models themselves serve as world simulators,\ncorrectly predicting how actions change different world states, thus bypassing\nthe need for extensive manual coding? Our goal is to answer this question in\nthe context of text-based simulators. Our approach is to build and use a new\nbenchmark, called ByteSized32-State-Prediction, containing a dataset of text\ngame state transitions and accompanying game tasks. We use this to directly\nquantify, for the first time, how well LLMs can serve as text-based world\nsimulators. We test GPT-4 on this dataset and find that, despite its impressive\nperformance, it is still an unreliable world simulator without further\ninnovations. This work thus contributes both new insights into current LLM's\ncapabilities and weaknesses, as well as a novel benchmark to track future\nprogress as new models appear.\n","authors":["Ruoyao Wang","Graham Todd","Ziang Xiao","Xingdi Yuan","Marc-Alexandre Côté","Peter Clark","Peter Jansen"],"pdf_url":"https://arxiv.org/pdf/2406.06485v1.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2406.06484v1","updated":"2024-06-10T17:24:42Z","published":"2024-06-10T17:24:42Z","title":"Parallelizing Linear Transformers with the Delta Rule over Sequence\n  Length","summary":"  Transformers with linear attention (i.e., linear transformers) and\nstate-space models have recently been suggested as a viable linear-time\nalternative to transformers with softmax attention. However, these models still\nunderperform transformers especially on tasks that require in-context\nretrieval. While more expressive variants of linear transformers which replace\nthe additive outer-product update in linear transformers with the delta rule\nhave been found to be more effective at associative recall, existing algorithms\nfor training such models do not parallelize over sequence length and are thus\ninefficient to train on modern hardware. This work describes a\nhardware-efficient algorithm for training linear transformers with the delta\nrule, which exploits a memory-efficient representation for computing products\nof Householder matrices. This algorithm allows us to scale up DeltaNet to\nstandard language modeling settings. We train a 1.3B model for 100B tokens and\nfind that it outperforms recent linear-time baselines such as Mamba and GLA in\nterms of perplexity and zero-shot performance on downstream tasks (including on\ntasks that focus on recall). We also experiment with two hybrid models which\ncombine DeltaNet layers with (1) sliding-window attention layers every other\nlayer or (2) two global attention layers, and find that these hybrid models\noutperform strong transformer baselines.\n","authors":["Songlin Yang","Bailin Wang","Yu Zhang","Yikang Shen","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2406.06484v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.06474v1","updated":"2024-06-10T17:16:49Z","published":"2024-06-10T17:16:49Z","title":"Towards a Personal Health Large Language Model","summary":"  In health, most large language model (LLM) research has focused on clinical\ntasks. However, mobile and wearable devices, which are rarely integrated into\nsuch tasks, provide rich, longitudinal data for personal health monitoring.\nHere we present Personal Health Large Language Model (PH-LLM), fine-tuned from\nGemini for understanding and reasoning over numerical time-series personal\nhealth data. We created and curated three datasets that test 1) production of\npersonalized insights and recommendations from sleep patterns, physical\nactivity, and physiological responses, 2) expert domain knowledge, and 3)\nprediction of self-reported sleep outcomes. For the first task we designed 857\ncase studies in collaboration with domain experts to assess real-world\nscenarios in sleep and fitness. Through comprehensive evaluation of\ndomain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not\nstatistically different from expert performance in fitness and, while experts\nremain superior for sleep, fine-tuning PH-LLM provided significant improvements\nin using relevant domain knowledge and personalizing information for sleep\ninsights. We evaluated PH-LLM domain knowledge using multiple choice sleep\nmedicine and fitness examinations. PH-LLM achieved 79% on sleep and 88% on\nfitness, exceeding average scores from a sample of human experts. Finally, we\ntrained PH-LLM to predict self-reported sleep quality outcomes from textual and\nmultimodal encoding representations of wearable data, and demonstrate that\nmultimodal encoding is required to match performance of specialized\ndiscriminative models. Although further development and evaluation are\nnecessary in the safety-critical personal health domain, these results\ndemonstrate both the broad knowledge and capabilities of Gemini models and the\nbenefit of contextualizing physiological data for personal health applications\nas done with PH-LLM.\n","authors":["Justin Cosentino","Anastasiya Belyaeva","Xin Liu","Nicholas A. Furlotte","Zhun Yang","Chace Lee","Erik Schenck","Yojan Patel","Jian Cui","Logan Douglas Schneider","Robby Bryant","Ryan G. Gomes","Allen Jiang","Roy Lee","Yun Liu","Javier Perez","Jameson K. Rogers","Cathy Speed","Shyam Tailor","Megan Walker","Jeffrey Yu","Tim Althoff","Conor Heneghan","John Hernandez","Mark Malhotra","Leor Stern","Yossi Matias","Greg S. Corrado","Shwetak Patel","Shravya Shetty","Jiening Zhan","Shruthi Prabhakara","Daniel McDuff","Cory Y. McLean"],"pdf_url":"https://arxiv.org/pdf/2406.06474v1.pdf","comment":"72 pages"},{"id":"http://arxiv.org/abs/2402.12348v2","updated":"2024-06-10T17:14:09Z","published":"2024-02-19T18:23:36Z","title":"GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via\n  Game-Theoretic Evaluations","summary":"  As Large Language Models (LLMs) are integrated into critical real-world\napplications, their strategic and logical reasoning abilities are increasingly\ncrucial. This paper evaluates LLMs' reasoning abilities in competitive\nenvironments through game-theoretic tasks, e.g., board and card games that\nrequire pure logic and strategic reasoning to compete with opponents. We first\npropose GTBench, a language-driven environment composing 10 widely recognized\ntasks, across a comprehensive game taxonomy: complete versus incomplete\ninformation, dynamic versus static, and probabilistic versus deterministic\nscenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and\n(2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that\n(1) LLMs have distinct behaviors regarding various gaming scenarios; for\nexample, LLMs fail in complete and deterministic games yet they are competitive\nin probabilistic gaming scenarios; (2) Most open-source LLMs, e.g.,\nCodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than\ncommercial LLMs, e.g., GPT-4, in complex games, yet the recently released\nLlama-3-70b-Instruct makes up for this shortcoming. In addition,\ncode-pretraining greatly benefits strategic reasoning, while advanced reasoning\nmethods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always\nhelp. We further characterize the game-theoretic properties of LLMs, such as\nequilibrium and Pareto Efficiency in repeated games. Detailed error profiles\nare provided for a better understanding of LLMs' behavior. We hope our research\nprovides standardized protocols and serves as a foundation to spur further\nexplorations in the strategic reasoning of LLMs.\n","authors":["Jinhao Duan","Renming Zhang","James Diffenderfer","Bhavya Kailkhura","Lichao Sun","Elias Stengel-Eskin","Mohit Bansal","Tianlong Chen","Kaidi Xu"],"pdf_url":"https://arxiv.org/pdf/2402.12348v2.pdf","comment":"26 pages; the first two authors contributed equally; GTBench HF\n  Leaderboard: https://huggingface.co/spaces/GTBench/GTBench"},{"id":"http://arxiv.org/abs/2406.06469v1","updated":"2024-06-10T17:07:25Z","published":"2024-06-10T17:07:25Z","title":"Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning","summary":"  Language agents perform complex tasks by using tools to execute each step\nprecisely. However, most existing agents are based on proprietary models or\ndesigned to target specific tasks, such as mathematics or multi-hop question\nanswering. We introduce Husky, a holistic, open-source language agent that\nlearns to reason over a unified action space to address a diverse set of\ncomplex tasks involving numerical, tabular, and knowledge-based reasoning.\nHusky iterates between two stages: 1) generating the next action to take\ntowards solving a given task and 2) executing the action using expert models\nand updating the current solution state. We identify a thorough ontology of\nactions for addressing complex tasks and curate high-quality data to train\nexpert models for executing these actions. Our experiments show that Husky\noutperforms prior language agents across 14 evaluation datasets. Moreover, we\nintroduce HuskyQA, a new evaluation set which stress tests language agents for\nmixed-tool reasoning, with a focus on retrieving missing knowledge and\nperforming numerical reasoning. Despite using 7B models, Husky matches or even\nexceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of\nour holistic approach in addressing complex reasoning problems. Our code and\nmodels are available at https://github.com/agent-husky/Husky-v1.\n","authors":["Joongwon Kim","Bhargavi Paranjape","Tushar Khot","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2406.06469v1.pdf","comment":"50 pages, 42 figures. Project webpage available\n  [here](https://agent-husky.github.io/)"},{"id":"http://arxiv.org/abs/2406.06465v1","updated":"2024-06-10T17:02:08Z","published":"2024-06-10T17:02:08Z","title":"AID: Adapting Image2Video Diffusion Models for Instruction-guided Video\n  Prediction","summary":"  Text-guided video prediction (TVP) involves predicting the motion of future\nframes from the initial frame according to an instruction, which has wide\napplications in virtual reality, robotics, and content creation. Previous TVP\nmethods make significant breakthroughs by adapting Stable Diffusion for this\ntask. However, they struggle with frame consistency and temporal stability\nprimarily due to the limited scale of video datasets. We observe that\npretrained Image2Video diffusion models possess good priors for video dynamics\nbut they lack textual control. Hence, transferring Image2Video models to\nleverage their video dynamic priors while injecting instruction control to\ngenerate controllable videos is both a meaningful and challenging task. To\nachieve this, we introduce the Multi-Modal Large Language Model (MLLM) to\npredict future video states based on initial frames and text instructions. More\nspecifically, we design a dual query transformer (DQFormer) architecture, which\nintegrates the instructions and frames into the conditional embeddings for\nfuture frame prediction. Additionally, we develop Long-Short Term Temporal\nAdapters and Spatial Adapters that can quickly transfer general video diffusion\nmodels to specific scenarios with minimal training costs. Experimental results\nshow that our method significantly outperforms state-of-the-art techniques on\nfour datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and\nUCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and\nSSv2 respectively, demonstrating its effectiveness in various domains. More\nexamples can be found at our website https://chenhsing.github.io/AID.\n","authors":["Zhen Xing","Qi Dai","Zejia Weng","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.06465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06464v1","updated":"2024-06-10T17:00:54Z","published":"2024-06-10T17:00:54Z","title":"Transforming Wearable Data into Health Insights using Large Language\n  Model Agents","summary":"  Despite the proliferation of wearable health trackers and the importance of\nsleep and exercise to health, deriving actionable personalized insights from\nwearable data remains a challenge because doing so requires non-trivial\nopen-ended analysis of these data. The recent rise of large language model\n(LLM) agents, which can use tools to reason about and interact with the world,\npresents a promising opportunity to enable such personalized analysis at scale.\nYet, the application of LLM agents in analyzing personal health is still\nlargely untapped. In this paper, we introduce the Personal Health Insights\nAgent (PHIA), an agent system that leverages state-of-the-art code generation\nand information retrieval tools to analyze and interpret behavioral health data\nfrom wearables. We curate two benchmark question-answering datasets of over\n4000 health insights questions. Based on 650 hours of human and expert\nevaluation we find that PHIA can accurately address over 84% of factual\nnumerical questions and more than 83% of crowd-sourced open-ended questions.\nThis work has implications for advancing behavioral health across the\npopulation, potentially enabling individuals to interpret their own wearable\ndata, and paving the way for a new era of accessible, personalized wellness\nregimens that are informed by data-driven insights.\n","authors":["Mike A. Merrill","Akshay Paruchuri","Naghmeh Rezaei","Geza Kovacs","Javier Perez","Yun Liu","Erik Schenck","Nova Hammerquist","Jake Sunshine","Shyam Tailor","Kumar Ayush","Hao-Wei Su","Qian He","Cory McLean","Mark Malhotra","Shwetak Patel","Jiening Zhan","Tim Althoff","Daniel McDuff","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2406.06464v1.pdf","comment":"38 pages"},{"id":"http://arxiv.org/abs/2406.06461v1","updated":"2024-06-10T16:55:08Z","published":"2024-06-10T16:55:08Z","title":"Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning\n  Strategies","summary":"  A diverse array of reasoning strategies has been proposed to elicit the\ncapabilities of large language models. However, in this paper, we point out\nthat traditional evaluations which focus solely on performance metrics miss a\nkey factor: the increased effectiveness due to additional compute. By\noverlooking this aspect, a skewed view of strategy efficiency is often\npresented. This paper introduces a framework that incorporates the compute\nbudget into the evaluation, providing a more informative comparison that takes\ninto account both performance metrics and computational cost. In this\nbudget-aware perspective, we find that complex reasoning strategies often don't\nsurpass simpler baselines purely due to algorithmic ingenuity, but rather due\nto the larger computational resources allocated. When we provide a simple\nbaseline like chain-of-thought self-consistency with comparable compute\nresources, it frequently outperforms reasoning strategies proposed in the\nliterature. In this scale-aware perspective, we find that unlike\nself-consistency, certain strategies such as multi-agent debate or Reflexion\ncan become worse if more compute budget is utilized.\n","authors":["Junlin Wang","Siddhartha Jain","Dejiao Zhang","Baishakhi Ray","Varun Kumar","Ben Athiwaratkun"],"pdf_url":"https://arxiv.org/pdf/2406.06461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06458v1","updated":"2024-06-10T16:46:22Z","published":"2024-06-10T16:46:22Z","title":"Evaluating the Retrieval Component in LLM-Based Question Answering\n  Systems","summary":"  Question answering systems (QA) utilizing Large Language Models (LLMs)\nheavily depend on the retrieval component to provide them with domain-specific\ninformation and reduce the risk of generating inaccurate responses or\nhallucinations. Although the evaluation of retrievers dates back to the early\nresearch in Information Retrieval, assessing their performance within LLM-based\nchatbots remains a challenge.\n  This study proposes a straightforward baseline for evaluating retrievers in\nRetrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstrate\nthat this evaluation framework provides a better image of how the retriever\nperforms and is more aligned with the overall performance of the QA system.\nAlthough conventional metrics such as precision, recall, and F1 score may not\nfully capture LLMs' capabilities - as they can yield accurate responses despite\nimperfect retrievers - our method considers LLMs' strengths to ignore\nirrelevant contexts, as well as potential errors and hallucinations in their\nresponses.\n","authors":["Ashkan Alinejad","Krtin Kumar","Ali Vahdat"],"pdf_url":"https://arxiv.org/pdf/2406.06458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06455v1","updated":"2024-06-10T16:44:48Z","published":"2024-06-10T16:44:48Z","title":"A Large Language Model Pipeline for Breast Cancer Oncology","summary":"  Large language models (LLMs) have demonstrated potential in the innovation of\nmany disciplines. However, how they can best be developed for oncology remains\nunderdeveloped. State-of-the-art OpenAI models were fine-tuned on a clinical\ndataset and clinical guidelines text corpus for two important cancer treatment\nfactors, adjuvant radiation therapy and chemotherapy, using a novel Langchain\nprompt engineering pipeline. A high accuracy (0.85+) was achieved in the\nclassification of adjuvant radiation therapy and chemotherapy for breast cancer\npatients. Furthermore, a confidence interval was formed from observational data\non the quality of treatment from human oncologists to estimate the proportion\nof scenarios in which the model must outperform the original oncologist in its\ntreatment prediction to be a better solution overall as 8.2% to 13.3%. Due to\nindeterminacy in the outcomes of cancer treatment decisions, future\ninvestigation, potentially a clinical trial, would be required to determine if\nthis threshold was met by the models. Nevertheless, with 85% of U.S. cancer\npatients receiving treatment at local community facilities, these kinds of\nmodels could play an important part in expanding access to quality care with\noutcomes that lie, at minimum, close to a human oncologist.\n","authors":["Tristen Pool","Dennis Trujillo"],"pdf_url":"https://arxiv.org/pdf/2406.06455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06443v1","updated":"2024-06-10T16:34:43Z","published":"2024-06-10T16:34:43Z","title":"LLM Dataset Inference: Did you train on my dataset?","summary":"  The proliferation of large language models (LLMs) in the real world has come\nwith a rise in copyright cases against companies for training their models on\nunlicensed data from the internet. Recent works have presented methods to\nidentify if individual text sequences were members of the model's training\ndata, known as membership inference attacks (MIAs). We demonstrate that the\napparent success of these MIAs is confounded by selecting non-members (text\nsequences not used for training) belonging to a different distribution from the\nmembers (e.g., temporally shifted recent Wikipedia articles compared with ones\nused to train the model). This distribution shift makes membership inference\nappear successful. However, most MIA methods perform no better than random\nguessing when discriminating between members and non-members from the same\ndistribution (e.g., in this case, the same period of time). Even when MIAs\nwork, we find that different MIAs succeed at inferring membership of samples\nfrom different distributions. Instead, we propose a new dataset inference\nmethod to accurately identify the datasets used to train large language models.\nThis paradigm sits realistically in the modern-day copyright landscape, where\nauthors claim that an LLM is trained over multiple documents (such as a book)\nwritten by them, rather than one particular paragraph. While dataset inference\nshares many of the challenges of membership inference, we solve it by\nselectively combining the MIAs that provide positive signal for a given\ndistribution, and aggregating them to perform a statistical test on a given\ndataset. Our approach successfully distinguishes the train and test sets of\ndifferent subsets of the Pile with statistically significant p-values < 0.1,\nwithout any false positives.\n","authors":["Pratyush Maini","Hengrui Jia","Nicolas Papernot","Adam Dziedzic"],"pdf_url":"https://arxiv.org/pdf/2406.06443v1.pdf","comment":"Code is available at\n  \\href{https://github.com/pratyushmaini/llm_dataset_inference/"},{"id":"http://arxiv.org/abs/2406.06441v1","updated":"2024-06-10T16:34:30Z","published":"2024-06-10T16:34:30Z","title":"Interpretability of Language Models via Task Spaces","summary":"  The usual way to interpret language models (LMs) is to test their performance\non different benchmarks and subsequently infer their internal processes. In\nthis paper, we present an alternative approach, concentrating on the quality of\nLM processing, with a focus on their language abilities. To this end, we\nconstruct 'linguistic task spaces' -- representations of an LM's language\nconceptualisation -- that shed light on the connections LMs draw between\nlanguage phenomena. Task spaces are based on the interactions of the learning\nsignals from different linguistic phenomena, which we assess via a method we\ncall 'similarity probing'. To disentangle the learning signals of linguistic\nphenomena, we further introduce a method called 'fine-tuning via gradient\ndifferentials' (FTGD). We apply our methods to language models of three\ndifferent scales and find that larger models generalise better to overarching\ngeneral concepts for linguistic tasks, making better use of their shared\nstructure. Further, the distributedness of linguistic processing increases with\npre-training through increased parameter sharing between related linguistic\ntasks. The overall generalisation patterns are mostly stable throughout\ntraining and not marked by incisive stages, potentially explaining the lack of\nsuccessful curriculum strategies for LMs.\n","authors":["Lucas Weber","Jaap Jumelet","Elia Bruni","Dieuwke Hupkes"],"pdf_url":"https://arxiv.org/pdf/2406.06441v1.pdf","comment":"To be published at ACL 2024 (main)"},{"id":"http://arxiv.org/abs/2406.06438v1","updated":"2024-06-10T16:31:34Z","published":"2024-06-10T16:31:34Z","title":"Multimodal Contextualized Semantic Parsing from Speech","summary":"  We introduce Semantic Parsing in Contextual Environments (SPICE), a task\ndesigned to enhance artificial agents' contextual awareness by integrating\nmultimodal inputs with prior contexts. SPICE goes beyond traditional semantic\nparsing by offering a structured, interpretable framework for dynamically\nupdating an agent's knowledge with new information, mirroring the complexity of\nhuman communication. We develop the VG-SPICE dataset, crafted to challenge\nagents with visual scene graph construction from spoken conversational\nexchanges, highlighting speech and visual data integration. We also present the\nAudio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE.\nThese innovations aim to improve multimodal information processing and\nintegration. Both the VG-SPICE dataset and the AViD-SP model are publicly\navailable.\n","authors":["Jordan Voas","Raymond Mooney","David Harwath"],"pdf_url":"https://arxiv.org/pdf/2406.06438v1.pdf","comment":"10 Pages, 3 figures, ACL 2024 Main"},{"id":"http://arxiv.org/abs/2402.14860v4","updated":"2024-06-10T16:25:30Z","published":"2024-02-21T00:49:43Z","title":"Ranking Large Language Models without Ground Truth","summary":"  Evaluation and ranking of large language models (LLMs) has become an\nimportant problem with the proliferation of these models and their impact.\nEvaluation methods either require human responses which are expensive to\nacquire or use pairs of LLMs to evaluate each other which can be unreliable. In\nthis paper, we provide a novel perspective where, given a dataset of prompts\n(viz. questions, instructions, etc.) and a set of LLMs, we rank them without\naccess to any ground truth or reference responses. Inspired by real life where\nboth an expert and a knowledgeable person can identify a novice our main idea\nis to consider triplets of models, where each one of them evaluates the other\ntwo, correctly identifying the worst model in the triplet with high\nprobability. We also analyze our idea and provide sufficient conditions for it\nto succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.\nIn experiments on different generative tasks (summarization, multiple-choice,\nand dialog), our methods reliably recover close to true rankings without\nreference data. This points to a viable low-resource mechanism for practical\nuse.\n","authors":["Amit Dhurandhar","Rahul Nair","Moninder Singh","Elizabeth Daly","Karthikeyan Natesan Ramamurthy"],"pdf_url":"https://arxiv.org/pdf/2402.14860v4.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.06435v1","updated":"2024-06-10T16:25:23Z","published":"2024-06-10T16:25:23Z","title":"Language Models are Alignable Decision-Makers: Dataset and Application\n  to the Medical Triage Domain","summary":"  In difficult decision-making scenarios, it is common to have conflicting\nopinions among expert human decision-makers as there may not be a single right\nanswer. Such decisions may be guided by different attributes that can be used\nto characterize an individual's decision. We introduce a novel dataset for\nmedical triage decision-making, labeled with a set of decision-maker attributes\n(DMAs). This dataset consists of 62 scenarios, covering six different DMAs,\nincluding ethical principles such as fairness and moral desert. We present a\nnovel software framework for human-aligned decision-making by utilizing these\nDMAs, paving the way for trustworthy AI with better guardrails. Specifically,\nwe demonstrate how large language models (LLMs) can serve as ethical\ndecision-makers, and how their decisions can be aligned to different DMAs using\nzero-shot prompting. Our experiments focus on different open-source models with\nvarying sizes and training techniques, such as Falcon, Mistral, and Llama 2.\nFinally, we also introduce a new form of weighted self-consistency that\nimproves the overall quantified performance. Our results provide new research\ndirections in the use of LLMs as alignable decision-makers. The dataset and\nopen-source software are publicly available at:\nhttps://github.com/ITM-Kitware/llm-alignable-dm.\n","authors":["Brian Hu","Bill Ray","Alice Leung","Amy Summerville","David Joy","Christopher Funk","Arslan Basharat"],"pdf_url":"https://arxiv.org/pdf/2406.06435v1.pdf","comment":"15 pages total (including appendix), NAACL 2024 Industry Track"},{"id":"http://arxiv.org/abs/2309.07773v3","updated":"2024-06-10T16:08:27Z","published":"2023-09-14T15:02:05Z","title":"Spoken Humanoid Embodied Conversational Agents in Mobile Serious Games:\n  A Usability Assessment","summary":"  This paper presents an empirical investigation of the extent to which spoken\nHumanoid Embodied Conversational Agents (HECAs) can foster usability in mobile\nserious game (MSG) applications. The aim of the research is to assess the\nimpact of multiple agents and illusion of humanness on the quality of the\ninteraction. The experiment investigates two styles of agent presentation: an\nagent of high human-likeness (HECA) and an agent of low human-likeness (text).\nThe purpose of the experiment is to assess whether and how agents of high\nhumanlikeness can evoke the illusion of humanness and affect usability. Agents\nof high human-likeness were designed by following the ECA design model that is\na proposed guide for ECA development. The results of the experiment with 90\nparticipants show that users prefer to interact with the HECAs. The difference\nbetween the two versions is statistically significant with a large effect size\n(d=1.01), with many of the participants justifying their choice by saying that\nthe human-like characteristics of the HECA made the version more appealing.\nThis research provides key information on the potential effect of HECAs on\nserious games, which can provide insight into the design of future mobile\nserious games.\n","authors":["Danai Korre","Judy Robertson"],"pdf_url":"https://arxiv.org/pdf/2309.07773v3.pdf","comment":"46 pages, 9 figures, 14 tables"},{"id":"http://arxiv.org/abs/2406.06406v1","updated":"2024-06-10T15:58:42Z","published":"2024-06-10T15:58:42Z","title":"Controlling Emotion in Text-to-Speech with Natural Language Prompts","summary":"  In recent years, prompting has quickly become one of the standard ways of\nsteering the outputs of generative machine learning models, due to its\nintuitive use of natural language. In this work, we propose a system\nconditioned on embeddings derived from an emotionally rich text that serves as\nprompt. Thereby, a joint representation of speaker and prompt embeddings is\nintegrated at several points within a transformer-based architecture. Our\napproach is trained on merged emotional speech and text datasets and varies\nprompts in each training iteration to increase the generalization capabilities\nof the model. Objective and subjective evaluation results demonstrate the\nability of the conditioned synthesis system to accurately transfer the emotions\npresent in a prompt to speech. At the same time, precise tractability of\nspeaker identities as well as overall high speech quality and intelligibility\nare maintained.\n","authors":["Thomas Bott","Florian Lux","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2406.06406v1.pdf","comment":"accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.06403v1","updated":"2024-06-10T15:56:52Z","published":"2024-06-10T15:56:52Z","title":"Meta Learning Text-to-Speech Synthesis in over 7000 Languages","summary":"  In this work, we take on the challenging task of building a single\ntext-to-speech synthesis system that is capable of generating speech in over\n7000 languages, many of which lack sufficient data for traditional TTS\ndevelopment. By leveraging a novel integration of massively multilingual\npretraining and meta learning to approximate language representations, our\napproach enables zero-shot speech synthesis in languages without any available\ndata. We validate our system's performance through objective measures and human\nevaluation across a diverse linguistic landscape. By releasing our code and\nmodels publicly, we aim to empower communities with limited linguistic\nresources and foster further innovation in the field of speech technology.\n","authors":["Florian Lux","Sarina Meyer","Lyonel Behringer","Frank Zalkow","Phat Do","Matt Coler","Emanuël A. P. Habets","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2406.06403v1.pdf","comment":"accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.06401v1","updated":"2024-06-10T15:55:06Z","published":"2024-06-10T15:55:06Z","title":"INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of\n  Progress in Speech Emotion Recognition","summary":"  We revisit the INTERSPEECH 2009 Emotion Challenge -- the first ever speech\nemotion recognition (SER) challenge -- and evaluate a series of deep learning\nmodels that are representative of the major advances in SER research in the\ntime since then. We start by training each model using a fixed set of\nhyperparameters, and further fine-tune the best-performing models of that\ninitial setup with a grid search. Results are always reported on the official\ntest set with a separate validation set only used for early stopping. Most\nmodels score below or close to the official baseline, while they marginally\noutperform the original challenge winners after hyperparameter tuning. Our work\nillustrates that, despite recent progress, FAU-AIBO remains a very challenging\nbenchmark. An interesting corollary is that newer methods do not consistently\noutperform older ones, showing that progress towards `solving' SER is not\nnecessarily monotonic.\n","authors":["Andreas Triantafyllopoulos","Anton Batliner","Simon Rampp","Manuel Milling","Björn Schuller"],"pdf_url":"https://arxiv.org/pdf/2406.06401v1.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.06399v1","updated":"2024-06-10T15:52:49Z","published":"2024-06-10T15:52:49Z","title":"Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt\n  LLMs for Dialogue","summary":"  We study the limitations of Large Language Models (LLMs) for the task of\nresponse generation in human-machine dialogue. Several techniques have been\nproposed in the literature for different dialogue types (e.g., Open-Domain).\nHowever, the evaluations of these techniques have been limited in terms of base\nLLMs, dialogue types and evaluation metrics. In this work, we extensively\nanalyze different LLM adaptation techniques when applied to different dialogue\ntypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue\ntypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.\nWe evaluate the performance of in-context learning and fine-tuning techniques\nacross datasets selected for each dialogue type. We assess the impact of\nincorporating external knowledge to ground the generation in both scenarios of\nRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent\nevaluation and explainability criteria for automatic metrics and human\nevaluation protocols. Our analysis shows that there is no universal\nbest-technique for adapting large language models as the efficacy of each\ntechnique depends on both the base LLM and the specific type of dialogue. Last\nbut not least, the assessment of the best adaptation technique should include\nhuman evaluation to avoid false expectations and outcomes derived from\nautomatic metrics.\n","authors":["Simone Alghisi","Massimo Rizzoli","Gabriel Roccabruna","Seyed Mahed Mousavi","Giuseppe Riccardi"],"pdf_url":"https://arxiv.org/pdf/2406.06399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07940v2","updated":"2024-06-10T15:51:16Z","published":"2024-05-13T17:15:14Z","title":"RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text\n  Detectors","summary":"  Many commercial and open-source models claim to detect machine-generated text\nwith extremely high accuracy (99% or more). However, very few of these\ndetectors are evaluated on shared benchmark datasets and even when they are,\nthe datasets used for evaluation are insufficiently challenging-lacking\nvariations in sampling strategy, adversarial attacks, and open-source\ngenerative models. In this work we present RAID: the largest and most\nchallenging benchmark dataset for machine-generated text detection. RAID\nincludes over 6 million generations spanning 11 models, 8 domains, 11\nadversarial attacks and 4 decoding strategies. Using RAID, we evaluate the\nout-of-domain and adversarial robustness of 8 open- and 4 closed-source\ndetectors and find that current detectors are easily fooled by adversarial\nattacks, variations in sampling strategies, repetition penalties, and unseen\ngenerative models. We release our data along with a leaderboard to encourage\nfuture research.\n","authors":["Liam Dugan","Alyssa Hwang","Filip Trhlik","Josh Magnus Ludan","Andrew Zhu","Hainiu Xu","Daphne Ippolito","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2405.07940v2.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2406.06393v1","updated":"2024-06-10T15:48:07Z","published":"2024-06-10T15:48:07Z","title":"STimage-1K4M: A histopathology image-gene expression dataset for spatial\n  transcriptomics","summary":"  Recent advances in multi-modal algorithms have driven and been driven by the\nincreasing availability of large image-text datasets, leading to significant\nstrides in various fields, including computational pathology. However, in most\nexisting medical image-text datasets, the text typically provides high-level\nsummaries that may not sufficiently describe sub-tile regions within a large\npathology image. For example, an image might cover an extensive tissue area\ncontaining cancerous and healthy regions, but the accompanying text might only\nspecify that this image is a cancer slide, lacking the nuanced details needed\nfor in-depth analysis. In this study, we introduce STimage-1K4M, a novel\ndataset designed to bridge this gap by providing genomic features for sub-tile\nimages. STimage-1K4M contains 1,149 images derived from spatial transcriptomics\ndata, which captures gene expression information at the level of individual\nspatial spots within a pathology image. Specifically, each image in the dataset\nis broken down into smaller sub-image tiles, with each tile paired with\n15,000-30,000 dimensional gene expressions. With 4,293,195 pairs of sub-tile\nimages and gene expressions, STimage-1K4M offers unprecedented granularity,\npaving the way for a wide range of advanced research in multi-modal data\nanalysis an innovative applications in computational pathology, and beyond.\n","authors":["Jiawen Chen","Muqing Zhou","Wenrong Wu","Jinwei Zhang","Yun Li","Didong Li"],"pdf_url":"https://arxiv.org/pdf/2406.06393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06391v1","updated":"2024-06-10T15:46:25Z","published":"2024-06-10T15:46:25Z","title":"Towards Lifelong Learning of Large Language Models: A Survey","summary":"  As the applications of large language models (LLMs) expand across diverse\nfields, the ability of these models to adapt to ongoing changes in data, tasks,\nand user preferences becomes crucial. Traditional training methods, relying on\nstatic datasets, are increasingly inadequate for coping with the dynamic nature\nof real-world information. Lifelong learning, also known as continual or\nincremental learning, addresses this challenge by enabling LLMs to learn\ncontinuously and adaptively over their operational lifetime, integrating new\nknowledge while retaining previously learned information and preventing\ncatastrophic forgetting. This survey delves into the sophisticated landscape of\nlifelong learning, categorizing strategies into two primary groups: Internal\nKnowledge and External Knowledge. Internal Knowledge includes continual\npretraining and continual finetuning, each enhancing the adaptability of LLMs\nin various scenarios. External Knowledge encompasses retrieval-based and\ntool-based lifelong learning, leveraging external data sources and\ncomputational tools to extend the model's capabilities without modifying core\nparameters. The key contributions of our survey are: (1) Introducing a novel\ntaxonomy categorizing the extensive literature of lifelong learning into 12\nscenarios; (2) Identifying common techniques across all lifelong learning\nscenarios and classifying existing literature into various technique groups\nwithin each scenario; (3) Highlighting emerging techniques such as model\nexpansion and data selection, which were less explored in the pre-LLM era.\nThrough a detailed examination of these groups and their respective categories,\nthis survey aims to enhance the adaptability, reliability, and overall\nperformance of LLMs in real-world applications.\n","authors":["Junhao Zheng","Shengjie Qiu","Chengming Shi","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2406.06391v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2406.06385v1","updated":"2024-06-10T15:44:22Z","published":"2024-06-10T15:44:22Z","title":"Low-Rank Quantization-Aware Training for LLMs","summary":"  Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to the LLaMA-2/3 and Mistral model\nfamilies and validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory.\n","authors":["Yelysei Bondarenko","Riccardo Del Chiaro","Markus Nagel"],"pdf_url":"https://arxiv.org/pdf/2406.06385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06382v1","updated":"2024-06-10T15:42:03Z","published":"2024-06-10T15:42:03Z","title":"Diffusion-RPO: Aligning Diffusion Models through Relative Preference\n  Optimization","summary":"  Aligning large language models with human preferences has emerged as a\ncritical focus in language modeling research. Yet, integrating preference\nlearning into Text-to-Image (T2I) generative models is still relatively\nuncharted territory. The Diffusion-DPO technique made initial strides by\nemploying pairwise preference learning in diffusion models tailored for\nspecific text prompts. We introduce Diffusion-RPO, a new method designed to\nalign diffusion-based T2I models with human preferences more effectively. This\napproach leverages both prompt-image pairs with identical prompts and those\nwith semantically related content across various modalities. Furthermore, we\nhave developed a new evaluation metric, style alignment, aimed at overcoming\nthe challenges of high costs, low reproducibility, and limited interpretability\nprevalent in current evaluations of human preference alignment. Our findings\ndemonstrate that Diffusion-RPO outperforms established methods such as\nSupervised Fine-Tuning and Diffusion-DPO in tuning Stable Diffusion versions\n1.5 and XL-1.0, achieving superior results in both automated evaluations of\nhuman preferences and style alignment. Our code is available at\nhttps://github.com/yigu1008/Diffusion-RPO\n","authors":["Yi Gu","Zhendong Wang","Yueqin Yin","Yujia Xie","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.06382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00799v2","updated":"2024-06-10T15:39:56Z","published":"2024-06-02T16:53:21Z","title":"Are you still on track!? Catching LLM Task Drift with Activations","summary":"  Large Language Models (LLMs) are routinely used in retrieval-augmented\napplications to orchestrate tasks and process inputs from users and other\nsources. These inputs, even in a single LLM interaction, can come from a\nvariety of sources, of varying trustworthiness and provenance. This opens the\ndoor to prompt injection attacks, where the LLM receives and acts upon\ninstructions from supposedly data-only sources, thus deviating from the user's\noriginal instructions. We define this as task drift, and we propose to catch it\nby scanning and analyzing the LLM's activations. We compare the LLM's\nactivations before and after processing the external input in order to detect\nwhether this input caused instruction drift. We develop two probing methods and\nfind that simply using a linear classifier can detect drift with near perfect\nROC AUC on an out-of-distribution test set. We show that this approach\ngeneralizes surprisingly well to unseen task domains, such as prompt\ninjections, jailbreaks, and malicious instructions, without being trained on\nany of these attacks. Our setup does not require any modification of the LLM\n(e.g., fine-tuning) or any text generation, thus maximizing deployability and\ncost efficiency and avoiding reliance on unreliable model output. To foster\nfuture research on activation-based task inspection, decoding, and\ninterpretability, we will release our large-scale TaskTracker toolkit,\ncomprising a dataset of over 500K instances, representations from 4 SoTA\nlanguage models, and inspection tools.\n","authors":["Sahar Abdelnabi","Aideen Fay","Giovanni Cherubin","Ahmed Salem","Mario Fritz","Andrew Paverd"],"pdf_url":"https://arxiv.org/pdf/2406.00799v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06371v1","updated":"2024-06-10T15:32:42Z","published":"2024-06-10T15:32:42Z","title":"mHuBERT-147: A Compact Multilingual HuBERT Model","summary":"  We present mHuBERT-147, the first general-purpose massively multilingual\nHuBERT speech representation model trained on 90K hours of clean, open-license\ndata. To scale up the multi-iteration HuBERT approach, we use faiss-based\nclustering, achieving 5.2x faster label assignment over the original method. We\nalso apply a new multilingual batching up-sampling strategy, leveraging both\nlanguage and dataset diversity. After 3 training iterations and with only 95M\nparameters, mHuBERT-147 outperforms larger models trained on substantially more\ndata. We rank second and first on the ML-SUPERB 10min/1h leaderboards\nrespectively, with SOTA scores for all LID tasks. Across ASR/LID tasks, our\nmodel consistently surpasses XLS-R (300M params; 436K hours) and demonstrates\nstrong competitiveness against the much larger MMS (1B params; 491K hours). Our\nfindings suggest that mHuBERT-147 is a promising model for multilingual speech\nprocessing tasks, offering an unprecedented balance between high performance\nand parameter efficiency.\n","authors":["Marcely Zanon Boito","Vivek Iyer","Nikolaos Lagos","Laurent Besacier","Ioan Calapodescu"],"pdf_url":"https://arxiv.org/pdf/2406.06371v1.pdf","comment":"Extended version of the Interspeech 2024 paper of same name"},{"id":"http://arxiv.org/abs/2406.06369v1","updated":"2024-06-10T15:30:13Z","published":"2024-06-10T15:30:13Z","title":"Annotation alignment: Comparing LLM and human annotations of\n  conversational safety","summary":"  To what extent to do LLMs align with human perceptions of safety? We study\nthis question via *annotation alignment*, the extent to which LLMs and humans\nagree when annotating the safety of user-chatbot conversations. We leverage the\nrecent DICES dataset (Aroyo et al., 2023), in which 350 conversations are each\nrated for safety by 112 annotators spanning 10 race-gender groups. GPT-4\nachieves a Pearson correlation of $r = 0.59$ with the average annotator rating,\nhigher than the median annotator's correlation with the average ($r=0.51$). We\nshow that larger datasets are needed to resolve whether GPT-4 exhibits\ndisparities in how well it correlates with demographic groups. Also, there is\nsubstantial idiosyncratic variation in correlation *within* groups, suggesting\nthat race & gender do not fully capture differences in alignment. Finally, we\nfind that GPT-4 cannot predict when one demographic group finds a conversation\nmore unsafe than another.\n","authors":["Rajiv Movva","Pang Wei Koh","Emma Pierson"],"pdf_url":"https://arxiv.org/pdf/2406.06369v1.pdf","comment":"Working draft, short paper. 5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2405.06634v2","updated":"2024-06-10T15:28:16Z","published":"2024-05-10T17:51:35Z","title":"Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA\n  Benchmark","summary":"  We evaluate the zero-shot ability of GPT-4 and LLaVa to perform simple Visual\nNetwork Analysis (VNA) tasks on small-scale graphs. We evaluate the Vision\nLanguage Models (VLMs) on 5 tasks related to three foundational network science\nconcepts: identifying nodes of maximal degree on a rendered graph, identifying\nwhether signed triads are balanced or unbalanced, and counting components. The\ntasks are structured to be easy for a human who understands the underlying\ngraph theoretic concepts, and can all be solved by counting the appropriate\nelements in graphs. We find that while GPT-4 consistently outperforms LLaVa,\nboth models struggle with every visual network analysis task we propose. We\npublicly release the first benchmark for the evaluation of VLMs on foundational\nVNA tasks.\n","authors":["Evan M. Williams","Kathleen M. Carley"],"pdf_url":"https://arxiv.org/pdf/2405.06634v2.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.06366v1","updated":"2024-06-10T15:24:15Z","published":"2024-06-10T15:24:15Z","title":"Symmetric Dot-Product Attention for Efficient Training of BERT Language\n  Models","summary":"  Initially introduced as a machine translation model, the Transformer\narchitecture has now become the foundation for modern deep learning\narchitecture, with applications in a wide range of fields, from computer vision\nto natural language processing. Nowadays, to tackle increasingly more complex\ntasks, Transformer-based models are stretched to enormous sizes, requiring\nincreasingly larger training datasets, and unsustainable amount of compute\nresources. The ubiquitous nature of the Transformer and its core component, the\nattention mechanism, are thus prime targets for efficiency research. In this\nwork, we propose an alternative compatibility function for the self-attention\nmechanism introduced by the Transformer architecture. This compatibility\nfunction exploits an overlap in the learned representation of the traditional\nscaled dot-product attention, leading to a symmetric with pairwise coefficient\ndot-product attention. When applied to the pre-training of BERT-like models,\nthis new symmetric attention mechanism reaches a score of 79.36 on the GLUE\nbenchmark against 78.74 for the traditional implementation, leads to a\nreduction of 6% in the number of trainable parameters, and reduces the number\nof training steps required before convergence by half.\n","authors":["Martin Courtois","Malte Ostendorff","Leonhard Hennig","Georg Rehm"],"pdf_url":"https://arxiv.org/pdf/2406.06366v1.pdf","comment":"to be published in Findings of the Association for Computational\n  Linguistics: ACL 2024"},{"id":"http://arxiv.org/abs/2406.06357v1","updated":"2024-06-10T15:19:09Z","published":"2024-06-10T15:19:09Z","title":"MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific\n  Workflows","summary":"  Scientific innovation relies on detailed workflows, which include critical\nsteps such as analyzing literature, generating ideas, validating these ideas,\ninterpreting results, and inspiring follow-up research. However, scientific\npublications that document these workflows are extensive and unstructured. This\nmakes it difficult for both human researchers and AI systems to effectively\nnavigate and explore the space of scientific innovation. To address this issue,\nwe introduce MASSW, a comprehensive text dataset on Multi-Aspect Summarization\nof Scientific Workflows. MASSW includes more than 152,000 peer-reviewed\npublications from 17 leading computer science conferences spanning the past 50\nyears. Using Large Language Models (LLMs), we automatically extract five core\naspects from these publications -- context, key idea, method, outcome, and\nprojected impact -- which correspond to five key steps in the research\nworkflow. These structured summaries facilitate a variety of downstream tasks\nand analyses. The quality of the LLM-extracted summaries is validated by\ncomparing them with human annotations. We demonstrate the utility of MASSW\nthrough multiple novel machine-learning tasks that can be benchmarked using\nthis new dataset, which make various types of predictions and recommendations\nalong the scientific workflow. MASSW holds significant potential for\nresearchers to create and benchmark new AI methods for optimizing scientific\nworkflows and fostering scientific innovation in the field. Our dataset is\nopenly available at \\url{https://github.com/xingjian-zhang/massw}.\n","authors":["Xingjian Zhang","Yutong Xie","Jin Huang","Jinge Ma","Zhaoying Pan","Qijia Liu","Ziyang Xiong","Tolga Ergen","Dongsub Shim","Honglak Lee","Qiaozhu Mei"],"pdf_url":"https://arxiv.org/pdf/2406.06357v1.pdf","comment":"arXiv admin note: text overlap with arXiv:1706.03762 by other authors"},{"id":"http://arxiv.org/abs/2406.06355v1","updated":"2024-06-10T15:17:17Z","published":"2024-06-10T15:17:17Z","title":"Sustained Vowels for Pre- vs Post-Treatment COPD Classification","summary":"  Chronic obstructive pulmonary disease (COPD) is a serious inflammatory lung\ndisease affecting millions of people around the world. Due to an obstructed\nairflow from the lungs, it also becomes manifest in patients' vocal behaviour.\nOf particular importance is the detection of an exacerbation episode, which\nmarks an acute phase and often requires hospitalisation and treatment. Previous\nwork has shown that it is possible to distinguish between a pre- and a\npost-treatment state using automatic analysis of read speech. In this\ncontribution, we examine whether sustained vowels can provide a complementary\nlens for telling apart these two states. Using a cohort of 50 patients, we show\nthat the inclusion of sustained vowels can improve performance to up to 79\\%\nunweighted average recall, from a 71\\% baseline using read speech. We further\nidentify and interpret the most important acoustic features that characterise\nthe manifestation of COPD in sustained vowels.\n","authors":["Andreas Triantafyllopoulos","Anton Batliner","Wolfgang Mayr","Markus Fendler","Florian Pokorny","Maurice Gerczuk","Shahin Amiriparian","Thomas Berghaus","Björn Schuller"],"pdf_url":"https://arxiv.org/pdf/2406.06355v1.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.02528v2","updated":"2024-06-10T14:55:29Z","published":"2024-06-04T17:50:34Z","title":"Scalable MatMul-free Language Modeling","summary":"  Matrix multiplication (MatMul) typically dominates the overall computational\ncost of large language models (LLMs). This cost only grows as LLMs scale to\nlarger embedding dimensions and context lengths. In this work, we show that\nMatMul operations can be completely eliminated from LLMs while maintaining\nstrong performance at billion-parameter scales. Our experiments show that our\nproposed MatMul-free models achieve performance on-par with state-of-the-art\nTransformers that require far more memory during inference at a scale up to at\nleast 2.7B parameters. We investigate the scaling laws and find that the\nperformance gap between our MatMul-free models and full precision Transformers\nnarrows as the model size increases. We also provide a GPU-efficient\nimplementation of this model which reduces memory usage by up to 61% over an\nunoptimized baseline during training. By utilizing an optimized kernel during\ninference, our model's memory consumption can be reduced by more than 10x\ncompared to unoptimized models. To properly quantify the efficiency of our\narchitecture, we build a custom hardware solution on an FPGA which exploits\nlightweight operations beyond what GPUs are capable of. We processed\nbillion-parameter scale models at 13W beyond human readable throughput, moving\nLLMs closer to brain-like efficiency. This work not only shows how far LLMs can\nbe stripped back while still performing effectively, but also points at the\ntypes of operations future accelerators should be optimized for in processing\nthe next generation of lightweight LLMs. Our code implementation is available\nat https://github.com/ridgerchu/matmulfreellm.\n","authors":["Rui-Jie Zhu","Yu Zhang","Ethan Sifferman","Tyler Sheaves","Yiqiao Wang","Dustin Richmond","Peng Zhou","Jason K. Eshraghian"],"pdf_url":"https://arxiv.org/pdf/2406.02528v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06331v1","updated":"2024-06-10T14:47:04Z","published":"2024-06-10T14:47:04Z","title":"MedExQA: Medical Question Answering Benchmark with Multiple Explanations","summary":"  This paper introduces MedExQA, a novel benchmark in medical\nquestion-answering, to evaluate large language models' (LLMs) understanding of\nmedical knowledge through explanations. By constructing datasets across five\ndistinct medical specialties that are underrepresented in current datasets and\nfurther incorporating multiple explanations for each question-answer pair, we\naddress a major gap in current medical QA benchmarks which is the absence of\ncomprehensive assessments of LLMs' ability to generate nuanced medical\nexplanations. Our work highlights the importance of explainability in medical\nLLMs, proposes an effective methodology for evaluating models beyond\nclassification accuracy, and sheds light on one specific domain, speech\nlanguage pathology, where current LLMs including GPT4 lack good understanding.\nOur results show generation evaluation with multiple explanations aligns better\nwith human assessment, highlighting an opportunity for a more robust automated\ncomprehension assessment for LLMs. To diversify open-source medical LLMs\n(currently mostly based on Llama2), this work also proposes a new medical\nmodel, MedPhi-2, based on Phi-2 (2.7B). The model outperformed medical LLMs\nbased on Llama2-70B in generating explanations, showing its effectiveness in\nthe resource-constrained medical domain. We will share our benchmark datasets\nand the trained model.\n","authors":["Yunsoo Kim","Jinge Wu","Yusuf Abdulle","Honghan Wu"],"pdf_url":"https://arxiv.org/pdf/2406.06331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06329v1","updated":"2024-06-10T14:46:07Z","published":"2024-06-10T14:46:07Z","title":"A Parameter-efficient Language Extension Framework for Multilingual ASR","summary":"  Covering all languages with a multilingual speech recognition model (MASR) is\nvery difficult. Performing language extension on top of an existing MASR is a\ndesirable choice. In this study, the MASR continual learning problem is\nprobabilistically decomposed into language identity prediction (LP) and\ncross-lingual adaptation (XLA) sub-problems. Based on this, we propose an\narchitecture-based framework for language extension that can fundamentally\nsolve catastrophic forgetting, debudded as PELE. PELE is designed to be\nparameter-efficient, incrementally incorporating an add-on module to adapt to a\nnew language. Specifically, different parameter-efficient fine-tuning (PEFT)\nmodules and their variants are explored as potential candidates to perform XLA.\nExperiments are carried out on 5 new languages with a wide range of\nlow-resourced data sizes. The best-performing PEFT candidate can achieve\nsatisfactory performance across all languages and demonstrates superiority in\nthree of five languages over the continual joint learning setting. Notably,\nPEFT methods focusing on weight parameters or input features are revealed to be\nlimited in performance, showing significantly inferior extension capabilities\ncompared to inserting a lightweight module in between layers such as an\nAdapter.\n","authors":["Wei Liu","Jingyong Hou","Dong Yang","Muyong Cao","Tan Lee"],"pdf_url":"https://arxiv.org/pdf/2406.06329v1.pdf","comment":"Accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2309.17234v2","updated":"2024-06-10T14:43:34Z","published":"2023-09-29T13:33:06Z","title":"Cooperation, Competition, and Maliciousness: LLM-Stakeholders\n  Interactive Negotiation","summary":"  There is an growing interest in using Large Language Models (LLMs) in\nmulti-agent systems to tackle interactive real-world tasks that require\neffective collaboration and assessing complex situations. Yet, we still have a\nlimited understanding of LLMs' communication and decision-making abilities in\nmulti-agent setups. The fundamental task of negotiation spans many key features\nof communication, such as cooperation, competition, and manipulation\npotentials. Thus, we propose using scorable negotiation to evaluate LLMs. We\ncreate a testbed of complex multi-agent, multi-issue, and semantically rich\nnegotiation games. To reach an agreement, agents must have strong arithmetic,\ninference, exploration, and planning capabilities while integrating them in a\ndynamic and multi-turn setup. We propose multiple metrics to rigorously\nquantify agents' performance and alignment with the assigned role. We provide\nprocedures to create new games and increase games' difficulty to have an\nevolving benchmark. Importantly, we evaluate critical safety aspects such as\nthe interaction dynamics between agents influenced by greedy and adversarial\nplayers. Our benchmark is highly challenging; GPT-3.5 and small models mostly\nfail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform.\n","authors":["Sahar Abdelnabi","Amr Gomaa","Sarath Sivaprasad","Lea Schönherr","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2309.17234v2.pdf","comment":"Updated version with major additions (new experiments, evaluation,\n  and attacks)"},{"id":"http://arxiv.org/abs/2406.06326v1","updated":"2024-06-10T14:42:20Z","published":"2024-06-10T14:42:20Z","title":"Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge\n  through Self-Teaching","summary":"  Large language models (LLMs) often struggle to provide up-to-date information\ndue to their one-time training and the constantly evolving nature of the world.\nTo keep LLMs current, existing approaches typically involve continued\npre-training on new documents. However, they frequently face difficulties in\nextracting stored knowledge. Motivated by the remarkable success of the Feynman\nTechnique in efficient human learning, we introduce Self-Tuning, a learning\nframework aimed at improving an LLM's ability to effectively acquire new\nknowledge from raw documents through self-teaching. Specifically, we develop a\nSelf-Teaching strategy that augments the documents with a set of\nknowledge-intensive tasks created in a self-supervised manner, focusing on\nthree crucial aspects: memorization, comprehension, and self-reflection.\nAdditionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate\nan in-depth analysis of an LLM's knowledge acquisition ability concerning\nmemorization, extraction, and reasoning. Extensive experimental results on\nLlama2 family models reveal that Self-Tuning consistently exhibits superior\nperformance across all knowledge acquisition tasks and excels in preserving\nprevious knowledge.\n","authors":["Xiaoying Zhang","Baolin Peng","Ye Tian","Jingyan Zhou","Yipeng Zhang","Haitao Mi","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2406.06326v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2406.06316v1","updated":"2024-06-10T14:33:02Z","published":"2024-06-10T14:33:02Z","title":"Tx-LLM: A Large Language Model for Therapeutics","summary":"  Developing therapeutics is a lengthy and expensive process that requires the\nsatisfaction of many different criteria, and AI models capable of expediting\nthe process would be invaluable. However, the majority of current AI approaches\naddress only a narrowly defined set of tasks, often circumscribed within a\nparticular domain. To bridge this gap, we introduce Tx-LLM, a generalist large\nlanguage model (LLM) fine-tuned from PaLM-2 which encodes knowledge about\ndiverse therapeutic modalities. Tx-LLM is trained using a collection of 709\ndatasets that target 66 tasks spanning various stages of the drug discovery\npipeline. Using a single set of weights, Tx-LLM simultaneously processes a wide\nvariety of chemical or biological entities(small molecules, proteins, nucleic\nacids, cell lines, diseases) interleaved with free-text, allowing it to predict\na broad range of associated properties, achieving competitive with\nstate-of-the-art (SOTA) performance on 43 out of 66 tasks and exceeding SOTA on\n22. Among these, Tx-LLM is particularly powerful and exceeds best-in-class\nperformance on average for tasks combining molecular SMILES representations\nwith text such as cell line names or disease names, likely due to context\nlearned during pretraining. We observe evidence of positive transfer between\ntasks with diverse drug types (e.g.,tasks involving small molecules and tasks\ninvolving proteins), and we study the impact of model size, domain finetuning,\nand prompting strategies on performance. We believe Tx-LLM represents an\nimportant step towards LLMs encoding biochemical knowledge and could have a\nfuture role as an end-to-end tool across the drug discovery development\npipeline.\n","authors":["Juan Manuel Zambrano Chaves","Eric Wang","Tao Tu","Eeshit Dhaval Vaishnav","Byron Lee","S. Sara Mahdavi","Christopher Semturs","David Fleet","Vivek Natarajan","Shekoofeh Azizi"],"pdf_url":"https://arxiv.org/pdf/2406.06316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06279v1","updated":"2024-06-10T13:58:46Z","published":"2024-06-10T13:58:46Z","title":"Multi-Prompting Decoder Helps Better Language Understanding","summary":"  Recent Pre-trained Language Models (PLMs) usually only provide users with the\ninference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt\nMaaS PLMs to downstream tasks without accessing their parameters and gradients,\nsome existing methods focus on the output-side adaptation of PLMs, viewing the\nPLM as an encoder and then optimizing a task-specific decoder for decoding the\noutput hidden states and class scores of the PLM. Despite the effectiveness of\nthese methods, they only use a single prompt to query PLMs for decoding,\nleading to a heavy reliance on the quality of the adopted prompt. In this\npaper, we propose a simple yet effective Multi-Prompting Decoder (MPD)\nframework for MaaS adaptation. The core idea is to query PLMs with multiple\ndifferent prompts for each sample, thereby obtaining multiple output hidden\nstates and class scores for subsequent decoding. Such multi-prompting decoding\nparadigm can simultaneously mitigate reliance on the quality of a single\nprompt, alleviate the issue of data scarcity under the few-shot setting, and\nprovide richer knowledge extracted from PLMs. Specifically, we propose two\ndecoding strategies: multi-prompting decoding with optimal transport for hidden\nstates and calibrated decoding for class scores. Extensive experiments\ndemonstrate that our method achieves new state-of-the-art results on multiple\nnatural language understanding datasets under the few-shot setting.\n","authors":["Zifeng Cheng","Zhaoling Chen","Zhiwei Jiang","Yafeng Yin","Shiping Ge","Yuliang Liu","Qing Gu"],"pdf_url":"https://arxiv.org/pdf/2406.06279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08189v4","updated":"2024-06-10T13:46:22Z","published":"2024-01-16T08:04:50Z","title":"PRewrite: Prompt Rewriting with Reinforcement Learning","summary":"  Prompt engineering is critical for the development of LLM-based applications.\nHowever, it is usually done manually in a \"trial and error\" fashion that can be\ntime consuming, ineffective, and sub-optimal. Even for the prompts which\nseemingly work well, there is always a lingering question: can the prompts be\nmade better with further modifications?\n  To address these problems, we investigate automated prompt engineering in\nthis paper. Specifically, we propose PRewrite, an automated method to rewrite\nan under-optimized prompt to a more effective prompt. We instantiate the prompt\nrewriter using a LLM. The rewriter LLM is trained using reinforcement learning\nto optimize the performance on a given downstream task. We conduct experiments\non diverse benchmark datasets, which demonstrates the effectiveness of\nPRewrite.\n","authors":["Weize Kong","Spurthi Amba Hombaiah","Mingyang Zhang","Qiaozhu Mei","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2401.08189v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04024v2","updated":"2024-06-10T13:45:36Z","published":"2024-06-06T12:46:21Z","title":"American Sign Language Handshapes Reflect Pressures for Communicative\n  Efficiency","summary":"  Communicative efficiency is a key topic in linguistics and cognitive\npsychology, with many studies demonstrating how the pressure to communicate\nwith minimal effort guides the form of natural language. However, this\nphenomenon is rarely explored in signed languages. This paper shows how\nhandshapes in American Sign Language (ASL) reflect these efficiency pressures\nand provides new evidence of communicative efficiency in the visual-gestural\nmodality.\n  We focus on hand configurations in native ASL signs and signs borrowed from\nEnglish to compare efficiency pressures from both ASL and English usage. First,\nwe develop new methodologies to quantify the articulatory effort needed to\nproduce handshapes and the perceptual effort required to recognize them. Then,\nwe analyze correlations between communicative effort and usage statistics in\nASL or English. Our findings reveal that frequent ASL handshapes are easier to\nproduce and that pressures for communicative efficiency mostly come from ASL\nusage, rather than from English lexical borrowing.\n","authors":["Kayo Yin","Terry Regier","Dan Klein"],"pdf_url":"https://arxiv.org/pdf/2406.04024v2.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.06263v1","updated":"2024-06-10T13:44:29Z","published":"2024-06-10T13:44:29Z","title":"MaskLID: Code-Switching Language Identification through Iterative\n  Masking","summary":"  We present MaskLID, a simple, yet effective, code-switching (CS) language\nidentification (LID) method. MaskLID does not require any training and is\ndesigned to complement current high-performance sentence-level LIDs.\nSentence-level LIDs are classifiers trained on monolingual texts to provide\nsingle labels, typically using a softmax layer to turn scores into\nprobabilities. However, in cases where a sentence is composed in both L1 and L2\nlanguages, the LID classifier often only returns the dominant label L1. To\naddress this limitation, MaskLID employs a strategy to mask text features\nassociated with L1, allowing the LID to classify the text as L2 in the next\nround. This method uses the LID itself to identify the features that require\nmasking and does not rely on any external resource. In this work, we explore\nthe use of MaskLID for two open-source LIDs (GlotLID and OpenLID), that are\nboth based on the FastText architecture. Code and demo are available at\nhttps://github.com/cisnlp/MaskLID.\n","authors":["Amir Hossein Kargaran","François Yvon","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2406.06263v1.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2406.06257v1","updated":"2024-06-10T13:38:15Z","published":"2024-06-10T13:38:15Z","title":"Combining Embeddings and Domain Knowledge for Job Posting Duplicate\n  Detection","summary":"  Job descriptions are posted on many online channels, including company\nwebsites, job boards or social media platforms. These descriptions are usually\npublished with varying text for the same job, due to the requirements of each\nplatform or to target different audiences. However, for the purpose of\nautomated recruitment and assistance of people working with these texts, it is\nhelpful to aggregate job postings across platforms and thus detect duplicate\ndescriptions that refer to the same job. In this work, we propose an approach\nfor detecting duplicates in job descriptions. We show that combining\noverlap-based character similarity with text embedding and keyword matching\nmethods lead to convincing results. In particular, we show that although no\napproach individually achieves satisfying performance, a combination of string\ncomparison, deep textual embeddings, and the use of curated weighted lookup\nlists for specific skills leads to a significant boost in overall performance.\nA tool based on our approach is being used in production and feedback from\nreal-life use confirms our evaluation.\n","authors":["Matthias Engelbach","Dennis Klau","Maximilien Kintz","Alexander Ulrich"],"pdf_url":"https://arxiv.org/pdf/2406.06257v1.pdf","comment":"To be published at 9th International Symposium on Language &\n  Knowledge Engineering LKE 2024"},{"id":"http://arxiv.org/abs/2406.06251v1","updated":"2024-06-10T13:31:18Z","published":"2024-06-10T13:31:18Z","title":"Learning Fine-Grained Controllability on Speech Generation via Efficient\n  Fine-Tuning","summary":"  As the scale of generative models continues to grow, efficient reuse and\nadaptation of pre-trained models have become crucial considerations. In this\nwork, we propose Voicebox Adapter, a novel approach that integrates\nfine-grained conditions into a pre-trained Voicebox speech generation model\nusing a cross-attention module. To ensure a smooth integration of newly added\nmodules with pre-trained ones, we explore various efficient fine-tuning\napproaches. Our experiment shows that the LoRA with bias-tuning configuration\nyields the best performance, enhancing controllability without compromising\nspeech quality. Across three fine-grained conditional generation tasks, we\ndemonstrate the effectiveness and resource efficiency of Voicebox Adapter.\nFollow-up experiments further highlight the robustness of Voicebox Adapter\nacross diverse data setups.\n","authors":["Chung-Ming Chien","Andros Tjandra","Apoorv Vyas","Matt Le","Bowen Shi","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2406.06251v1.pdf","comment":"Accepted by InterSpeech 2024"},{"id":"http://arxiv.org/abs/2403.00071v2","updated":"2024-06-10T13:30:34Z","published":"2024-02-29T19:02:03Z","title":"Resonance RoPE: Improving Context Length Generalization of Large\n  Language Models","summary":"  This paper addresses the challenge of train-short-test-long (TSTL) scenarios\nin Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE),\nwhere models pre-trained on shorter sequences face difficulty with\nout-of-distribution (OOD) token positions in longer sequences. We introduce\nResonance RoPE, a novel approach designed to narrow the generalization gap in\nTSTL scenarios by refining the interpolation of RoPE features for OOD\npositions, significantly improving the model performance without additional\nonline computational costs. Furthermore, we present PosGen, a new synthetic\nbenchmark specifically designed for fine-grained behavior analysis in TSTL\nscenarios, aiming to isolate the constantly increasing difficulty of token\ngeneration on long contexts from the challenges of recognizing new token\npositions. Our experiments on synthetic tasks show that after applying\nResonance RoPE, Transformers recognize OOD position better and more robustly.\nOur extensive LLM experiments also show superior performance after applying\nResonance RoPE to the current state-of-the-art RoPE scaling method, YaRN, on\nboth upstream language modeling tasks and a variety of downstream long-text\napplications.\n","authors":["Suyuchen Wang","Ivan Kobyzev","Peng Lu","Mehdi Rezagholizadeh","Bang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.00071v2.pdf","comment":"13 pages, 4 figures, accepted at ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.04851v2","updated":"2024-06-10T13:20:33Z","published":"2024-06-07T11:33:21Z","title":"Digital assistant in a point of sales","summary":"  This article investigates the deployment of a Voice User Interface\n(VUI)-powered digital assistant in a retail setting and assesses its impact on\ncustomer engagement and service efficiency. The study explores how digital\nassistants can enhance user interactions through advanced conversational\ncapabilities with multilingual support. By integrating a digital assistant into\na high-traffic retail environment, we evaluate its effectiveness in improving\nthe quality of customer service and operational efficiency. Data collected\nduring the experiment demonstrate varied impacts on customer interaction,\nrevealing insights into the future optimizations of digital assistant\ntechnologies in customer-facing roles. This study contributes to the\nunderstanding of digital transformation strategies within the customer\nrelations domain emphasizing the need for service flexibility and user-centric\ndesign in modern retail stores.\n","authors":["Emilia Lesiak","Grzegorz Wolny","Bartosz Przybył","Michał Szczerbak"],"pdf_url":"https://arxiv.org/pdf/2406.04851v2.pdf","comment":"update: cleaned the unnecessary files and updated the metadata"},{"id":"http://arxiv.org/abs/2401.06760v2","updated":"2024-06-10T12:57:48Z","published":"2024-01-12T18:47:40Z","title":"Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies","summary":"  Ten years ago a single metric, BLEU, governed progress in machine translation\nresearch. For better or worse, there is no such consensus today, and\nconsequently it is difficult for researchers to develop and retain the kinds of\nheuristic intuitions about metric deltas that drove earlier research and\ndeployment decisions. This paper investigates the \"dynamic range\" of a number\nof modern metrics in an effort to provide a collective understanding of the\nmeaning of differences in scores both within and among metrics; in other words,\nwe ask what point difference X in metric Y is required between two systems for\nhumans to notice? We conduct our evaluation on a new large dataset, ToShip23,\nusing it to discover deltas at which metrics achieve system-level differences\nthat are meaningful to humans, which we measure by pairwise system accuracy. We\nadditionally show that this method of establishing delta-accuracy is more\nstable than the standard use of statistical p-values in regards to testset\nsize. Where data size permits, we also explore the effect of metric deltas and\naccuracy across finer-grained features such as translation direction, domain,\nand system closeness.\n","authors":["Tom Kocmi","Vilém Zouhar","Christian Federmann","Matt Post"],"pdf_url":"https://arxiv.org/pdf/2401.06760v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02863v6","updated":"2024-06-10T12:51:50Z","published":"2023-07-06T09:03:10Z","title":"ValiText -- a unified validation framework for computational text-based\n  measures of social constructs","summary":"  Guidance on how to validate computational text-based measures of social\nconstructs is fragmented. While researchers generally acknowledge the\nimportance of validating text-based measures, they often lack a shared\nvocabulary and a unified framework to do so. This paper introduces ValiText, a\nnew validation framework designed to assist scholars in validly measuring\nsocial constructs in textual data. The framework is built on a conceptual\nfoundation of validity in the social sciences, strengthened by an empirical\nreview of validation practices in the social sciences and consultations with\nexperts. Ultimately, ValiText prescribes researchers to demonstrate three types\nof validation evidence: substantive evidence (outlining the theoretical\nunderpinning of the measure), structural evidence (examining the properties of\nthe text model and its output) and external evidence (testing for how the\nmeasure relates to independent information). The framework is further\nsupplemented by a checklist of validation steps, offering practical guidance in\nthe form of documentation sheets that guide researchers in the validation\nprocess.\n","authors":["Lukas Birkenmaier","Claudia Wagner","Clemens Lechner"],"pdf_url":"https://arxiv.org/pdf/2307.02863v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16938v2","updated":"2024-06-10T12:39:29Z","published":"2023-09-29T02:41:38Z","title":"\"I'd Like to Have an Argument, Please\": Argumentative Reasoning in Large\n  Language Models","summary":"  We evaluate two large language models (LLMs) ability to perform argumentative\nreasoning. We experiment with argument mining (AM) and argument pair extraction\n(APE), and evaluate the LLMs' ability to recognize arguments under\nprogressively more abstract input and output (I/O) representations (e.g.,\narbitrary label sets, graphs, etc.). Unlike the well-known evaluation of prompt\nphrasings, abstraction evaluation retains the prompt's phrasing but tests\nreasoning capabilities. We find that scoring-wise the LLMs match or surpass the\nSOTA in AM and APE, and under certain I/O abstractions LLMs perform well, even\nbeating chain-of-thought--we call this symbolic prompting. However, statistical\nanalysis on the LLMs outputs when subject to small, yet still human-readable,\nalterations in the I/O representations (e.g., asking for BIO tags as opposed to\nline numbers) showed that the models are not performing reasoning. This\nsuggests that LLM applications to some tasks, such as data labelling and paper\nreviewing, must be done with care.\n","authors":["Adrian de Wynter","Tangming Yuan"],"pdf_url":"https://arxiv.org/pdf/2309.16938v2.pdf","comment":"Accepted to COMMA '24. Final, peer-reviewed version to appear in the\n  proceedings"},{"id":"http://arxiv.org/abs/2406.06220v1","updated":"2024-06-10T12:34:38Z","published":"2024-06-10T12:34:38Z","title":"Label-Looping: Highly Efficient Decoding for Transducers","summary":"  This paper introduces a highly efficient greedy decoding algorithm for\nTransducer inference. We propose a novel data structure using CUDA tensors to\nrepresent partial hypotheses in a batch that supports parallelized hypothesis\nmanipulations. During decoding, our algorithm maximizes GPU parallelism by\nadopting a nested-loop design, where the inner loop consumes all blank\npredictions, while non-blank predictions are handled in the outer loop. Our\nalgorithm is general-purpose and can work with both conventional Transducers\nand Token-and-Duration Transducers. Experiments show that the label-looping\nalgorithm can bring a speedup up to 2.0X compared to conventional batched\ndecoding algorithms when using batch size 32, and can be combined with other\ncompiler or GPU call-related techniques to bring more speedup. We will\nopen-source our implementation to benefit the research community.\n","authors":["Vladimir Bataev","Hainan Xu","Daniel Galvez","Vitaly Lavrukhin","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2406.06220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06196v1","updated":"2024-06-10T11:50:29Z","published":"2024-06-10T11:50:29Z","title":"LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in\n  Low-Resource and Extinct Languages","summary":"  In this paper, we present the LingOly benchmark, a novel benchmark for\nadvanced reasoning abilities in large language models. Using challenging\nLinguistic Olympiad puzzles, we evaluate (i) capabilities for in-context\nidentification and generalisation of linguistic patterns in very low-resource\nor extinct languages, and (ii) abilities to follow complex task instructions.\nThe LingOly benchmark covers more than 90 mostly low-resource languages,\nminimising issues of data contamination, and contains 1,133 problems across 6\nformats and 5 levels of human difficulty. We assess performance with both\ndirect accuracy and comparison to a no-context baseline to penalise\nmemorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to\nbe challenging, and models perform poorly on the higher difficulty problems. On\nharder problems, even the top model only achieved 35.3% accuracy, 21.7%\nimprovement over the no-context baseline. Large closed models typically\noutperform open models, and in general, the higher resource the language, the\nbetter the scores. These results indicate, in absence of memorisation, true\nmulti-step out-of-domain reasoning remains a challenge for current language\nmodels.\n","authors":["Andrew M. Bean","Simi Hellsten","Harry Mayne","Jabez Magomere","Ethan A. Chi","Ryan Chi","Scott A. Hale","Hannah Rose Kirk"],"pdf_url":"https://arxiv.org/pdf/2406.06196v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.16433v3","updated":"2024-06-10T11:43:48Z","published":"2024-05-26T05:18:00Z","title":"CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and\n  Evaluation Framework for Chinese Psychological Counseling","summary":"  Using large language models (LLMs) to assist psychological counseling is a\nsignificant but challenging task at present. Attempts have been made on\nimproving empathetic conversations or acting as effective assistants in the\ntreatment with LLMs. However, the existing datasets lack consulting knowledge,\nresulting in LLMs lacking professional consulting competence. Moreover, how to\nautomatically evaluate multi-turn dialogues within the counseling process\nremains an understudied area. To bridge the gap, we propose CPsyCoun, a\nreport-based multi-turn dialogue reconstruction and evaluation framework for\nChinese psychological counseling. To fully exploit psychological counseling\nreports, a two-phase approach is devised to construct high-quality dialogues\nwhile a comprehensive evaluation benchmark is developed for the effective\nautomatic evaluation of multi-turn psychological consultations. Competitive\nexperimental results demonstrate the effectiveness of our proposed framework in\npsychological counseling. We open-source the datasets and model for future\nresearch at https://github.com/CAS-SIAT-XinHai/CPsyCoun\n","authors":["Chenhao Zhang","Renhao Li","Minghuan Tan","Min Yang","Jingwei Zhu","Di Yang","Jiahao Zhao","Guancheng Ye","Chengming Li","Xiping Hu"],"pdf_url":"https://arxiv.org/pdf/2405.16433v3.pdf","comment":"Appectped to Findings of ACL2024"},{"id":"http://arxiv.org/abs/2405.15362v3","updated":"2024-06-10T11:24:06Z","published":"2024-05-24T08:54:36Z","title":"Pipeline Parallelism with Controllable Memory","summary":"  Pipeline parallelism has been widely explored, but most existing schedules\nlack a systematic methodology. In this paper, we propose a framework to\ndecompose pipeline schedules as repeating a building block and we show that the\nlifespan of the building block decides the peak activation memory of the\npipeline schedule. Guided by the observations, we find that almost all existing\npipeline schedules, to the best of our knowledge, are memory inefficient. To\naddress this, we introduce a family of memory efficient building blocks with\ncontrollable activation memory, which can reduce the peak activation memory to\n1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable\nthroughput. We can also achieve almost zero pipeline bubbles while maintaining\nthe same activation memory as 1F1B. Our evaluations demonstrate that in pure\npipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in\nterms of throughput. When employing a grid search over hybrid parallelism\nhyperparameters in practical scenarios, our proposed methods demonstrate a 16%\nthroughput improvement over the 1F1B baseline for large language models.\n","authors":["Penghui Qi","Xinyi Wan","Nyamdavaa Amar","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2405.15362v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18115v2","updated":"2024-06-10T10:52:54Z","published":"2024-02-28T07:05:27Z","title":"UniVS: Unified and Universal Video Segmentation with Prompts as Queries","summary":"  Despite the recent advances in unified image segmentation (IS), developing a\nunified video segmentation (VS) model remains a challenge. This is mainly\nbecause generic category-specified VS tasks need to detect all objects and\ntrack them across consecutive frames, while prompt-guided VS tasks require\nre-identifying the target with visual/text prompts throughout the entire video,\nmaking it hard to handle the different tasks with the same architecture. We\nmake an attempt to address these issues and present a novel unified VS\narchitecture, namely UniVS, by using prompts as queries. UniVS averages the\nprompt features of the target from previous frames as its initial query to\nexplicitly decode masks, and introduces a target-wise prompt cross-attention\nlayer in the mask decoder to integrate prompt features in the memory pool. By\ntaking the predicted masks of entities from previous frames as their visual\nprompts, UniVS converts different VS tasks into prompt-guided target\nsegmentation, eliminating the heuristic inter-frame matching process. Our\nframework not only unifies the different VS tasks but also naturally achieves\nuniversal training and testing, ensuring robust performance across different\nscenarios. UniVS shows a commendable balance between performance and\nuniversality on 10 challenging VS benchmarks, covering video instance,\nsemantic, panoptic, object, and referring segmentation tasks. Code can be found\nat \\url{https://github.com/MinghanLi/UniVS}.\n","authors":["Minghan Li","Shuai Li","Xindong Zhang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.18115v2.pdf","comment":"21 pages, 11 figures, 10 tabels, CVPR2024"},{"id":"http://arxiv.org/abs/2404.18624v2","updated":"2024-06-10T10:43:20Z","published":"2024-04-29T11:52:20Z","title":"Do Vision & Language Decoders use Images and Text equally? How\n  Self-consistent are their Explanations?","summary":"  Vision and language model (VLM) decoders are currently the best-performing\narchitectures on multimodal tasks. Next to predictions, they can also produce\nexplanations, either in post-hoc or CoT settings. However, it is not clear how\nmuch they use the vision and text modalities when generating predictions or\nexplanations. In this work, we investigate if VLMs rely on modalities\ndifferently when they produce explanations as opposed to providing answers. We\nalso evaluate the self-consistency of VLM decoders in both post-hoc and CoT\nexplanation settings, by extending existing unimodal tests and measures to VLM\ndecoders. We find that VLMs are less self-consistent than LLMs. Text\ncontributions in VL decoders are more important than image contributions in all\nexamined tasks. Moreover, the contributions of images are significantly\nstronger for explanation generation compared to answer generation. This\ndifference is even larger in CoT compared to post-hoc explanations. Lastly, we\nprovide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE\nbenchmark, which before only covered VL encoders. We find that VL decoders\nstill struggle with most phenomena tested by VALSE.\n","authors":["Letitia Parcalabescu","Anette Frank"],"pdf_url":"https://arxiv.org/pdf/2404.18624v2.pdf","comment":"25 pages, 8 figures, 11 tables"},{"id":"http://arxiv.org/abs/2401.13201v3","updated":"2024-06-10T10:21:19Z","published":"2024-01-24T03:07:26Z","title":"MLLMReID: Multimodal Large Language Model-based Person Re-identification","summary":"  Multimodal large language models (MLLM) have achieved satisfactory results in\nmany tasks. However, their performance in the task of ReID (ReID) has not been\nexplored to date. This paper will investigate how to adapt them for the task of\nReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and\nthen use their visual encoder as a backbone for ReID. However, there still\nexist two apparent issues: (1) Designing instructions for ReID, MLLMs may\noverfit specific instructions, and designing a variety of instructions will\nlead to higher costs. (2) When fine-tuning the visual encoder of a MLLM, it is\nnot trained synchronously with the ReID task. As a result, the effectiveness of\nthe visual encoder fine-tuning cannot be directly reflected in the performance\nof the ReID task. To address these problems, this paper proposes MLLMReID:\nMultimodal Large Language Model-based ReID. Firstly, we proposed Common\nInstruction, a simple approach that leverages the essence ability of LLMs to\ncontinue writing, avoiding complex and diverse instruction design. Secondly, we\npropose a multi-task learning-based synchronization module to ensure that the\nvisual encoder of the MLLM is trained synchronously with the ReID task. The\nexperimental results demonstrate the superiority of our method.\n","authors":["Shan Yang","Yongfei Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.13201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02479v2","updated":"2024-06-10T10:18:46Z","published":"2024-02-04T13:16:29Z","title":"BRAIn: Bayesian Reward-conditioned Amortized Inference for natural\n  language generation from feedback","summary":"  Distribution matching methods for language model alignment such as Generation\nwith Distributional Control (GDC) and Distributional Policy Gradient (DPG) have\nnot received the same level of attention in reinforcement learning from human\nfeedback (RLHF) as contrastive methods such as Sequence Likelihood Calibration\n(SLiC), Direct Preference Optimization (DPO) and its variants. We identify high\nvariance of the gradient estimate as the primary reason for the lack of success\nof these methods and propose a self-normalized baseline to reduce the variance.\nWe further generalize the target distribution in DPG, GDC and DPO by using\nBayes' rule to define the reward-conditioned posterior. The resulting approach,\nreferred to as BRAIn - Bayesian Reward-conditioned Amortized Inference acts as\na bridge between distribution matching methods and DPO and significantly\noutperforms prior art in summarization and Antropic HH tasks.\n","authors":["Gaurav Pandey","Yatin Nandwani","Tahira Naseem","Mayank Mishra","Guangxuan Xu","Dinesh Raghu","Sachindra Joshi","Asim Munawar","Ramón Fernandez Astudillo"],"pdf_url":"https://arxiv.org/pdf/2402.02479v2.pdf","comment":"Accepted at ICML 2024 (main conference)"},{"id":"http://arxiv.org/abs/2406.06144v1","updated":"2024-06-10T10:03:16Z","published":"2024-06-10T10:03:16Z","title":"Language Models Resist Alignment","summary":"  Large language models (LLMs) may exhibit undesirable behaviors. Recent\nefforts have focused on aligning these models to prevent harmful generation.\nDespite these efforts, studies have shown that even a well-conducted alignment\nprocess can be easily circumvented, whether intentionally or accidentally. Do\nalignment fine-tuning have robust effects on models, or are merely superficial?\nIn this work, we answer this question through both theoretical and empirical\nmeans. Empirically, we demonstrate the elasticity of post-alignment models,\ni.e., the tendency to revert to the behavior distribution formed during the\npre-training phase upon further fine-tuning. Using compression theory, we\nformally derive that such fine-tuning process \\textit{disproportionately}\nundermines alignment compared to pre-training, potentially by orders of\nmagnitude. We conduct experimental validations to confirm the presence of\nelasticity across models of varying types and sizes. Specifically, we find that\nmodel performance declines rapidly before reverting to the pre-training\ndistribution, after which the rate of decline drops significantly. We further\nreveal that elasticity positively correlates with increased model size and the\nexpansion of pre-training data. Our discovery signifies the importance of\ntaming the inherent elasticity of LLMs, thereby overcoming the resistance of\nLLMs to alignment finetuning.\n","authors":["Jiaming Ji","Kaile Wang","Tianyi Qiu","Boyuan Chen","Jiayi Zhou","Changye Li","Hantao Lou","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.06144v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2402.05602v2","updated":"2024-06-10T09:58:55Z","published":"2024-02-08T12:01:24Z","title":"AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for\n  Transformers","summary":"  Large Language Models are prone to biased predictions and hallucinations,\nunderlining the paramount importance of understanding their model-internal\nreasoning process. However, achieving faithful attributions for the entirety of\na black-box transformer model and maintaining computational efficiency is an\nunsolved challenge. By extending the Layer-wise Relevance Propagation\nattribution method to handle attention layers, we address these challenges\neffectively. While partial solutions exist, our method is the first to\nfaithfully and holistically attribute not only input but also latent\nrepresentations of transformer models with the computational efficiency similar\nto a single backward pass. Through extensive evaluations against existing\nmethods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures,\nwe demonstrate that our proposed approach surpasses alternative methods in\nterms of faithfulness and enables the understanding of latent representations,\nopening up the door for concept-based explanations. We provide an LRP library\nat https://github.com/rachtibat/LRP-eXplains-Transformers.\n","authors":["Reduan Achtibat","Sayed Mohammad Vakilzadeh Hatefi","Maximilian Dreyer","Aakriti Jain","Thomas Wiegand","Sebastian Lapuschkin","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2402.05602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06140v1","updated":"2024-06-10T09:53:54Z","published":"2024-06-10T09:53:54Z","title":"Can I understand what I create? Self-Knowledge Evaluation of Large\n  Language Models","summary":"  Large language models (LLMs) have achieved remarkable progress in linguistic\ntasks, necessitating robust evaluation frameworks to understand their\ncapabilities and limitations. Inspired by Feynman's principle of understanding\nthrough creation, we introduce a self-knowledge evaluation framework that is\neasy to implement, evaluating models on their ability to comprehend and respond\nto self-generated questions. Our findings, based on testing multiple models\nacross diverse tasks, reveal significant gaps in the model's self-knowledge\nability. Further analysis indicates these gaps may be due to misalignment with\nhuman attention mechanisms. Additionally, fine-tuning on self-generated math\ntask may enhance the model's math performance, highlighting the potential of\nthe framework for efficient and insightful model evaluation and may also\ncontribute to the improvement of LLMs.\n","authors":["Zhiquan Tan","Lai Wei","Jindong Wang","Xing Xie","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2406.06140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06139v1","updated":"2024-06-10T09:52:25Z","published":"2024-06-10T09:52:25Z","title":"Thunder : Unified Regression-Diffusion Speech Enhancement with a Single\n  Reverse Step using Brownian Bridge","summary":"  Diffusion-based speech enhancement has shown promising results, but can\nsuffer from a slower inference time. Initializing the diffusion process with\nthe enhanced audio generated by a regression-based model can be used to reduce\nthe computational steps required. However, these approaches often necessitate a\nregression model, further increasing the system's complexity. We propose\nThunder, a unified regression-diffusion model that utilizes the Brownian bridge\nprocess which can allow the model to act in both modes. The regression mode can\nbe accessed by setting the diffusion time step closed to 1. However, the\nstandard score-based diffusion modeling does not perform well in this setup due\nto gradient instability. To mitigate this problem, we modify the diffusion\nmodel to predict the clean speech instead of the score function, achieving\ncompetitive performance with a more compact model size and fewer reverse steps.\n","authors":["Thanapat Trachu","Chawan Piansaddhayanon","Ekapol Chuangsuwanich"],"pdf_url":"https://arxiv.org/pdf/2406.06139v1.pdf","comment":"5 pages, 3 figures, 4 tables, This paper will be submitted in the\n  interspeech conference"},{"id":"http://arxiv.org/abs/2402.17433v3","updated":"2024-06-10T09:51:50Z","published":"2024-02-27T11:45:21Z","title":"Enhancing EEG-to-Text Decoding through Transferable Representations from\n  Pre-trained Contrastive EEG-Text Masked Autoencoder","summary":"  Reconstructing natural language from non-invasive electroencephalography\n(EEG) holds great promise as a language decoding technology for brain-computer\ninterfaces (BCIs). However, EEG-based language decoding is still in its nascent\nstages, facing several technical issues such as: 1) Absence of a hybrid\nstrategy that can effectively integrate cross-modality (between EEG and text)\nself-learning with intra-modality self-reconstruction of EEG features or\ntextual sequences; 2) Under-utilization of large language models (LLMs) to\nenhance EEG-based language decoding. To address above issues, we propose the\nContrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that\norchestrates compound self-supervised learning across and within EEG and text\nthrough a dedicated multi-stream encoder. Furthermore, we develop a framework\ncalled E2T-PTR (EEG-to-Text decoding using Pretrained Transferable\nRepresentations), which leverages pre-trained modules alongside the EEG stream\nfrom CET-MAE and further enables an LLM (specifically BART) to decode text from\nEEG sequences. Comprehensive experiments conducted on the popular text-evoked\nEEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms\nthe state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%,\nrespectively. These results indicate significant advancements in the field and\nunderscores the proposed framework's potential to enable more powerful and\nwidespread BCI applications.\n","authors":["Jiaqi Wang","Zhenxi Song","Zhengyu Ma","Xipeng Qiu","Min Zhang","Zhiguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.17433v3.pdf","comment":"8 pages (excluding references), accepted by ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2401.12997v2","updated":"2024-06-10T09:50:54Z","published":"2024-01-19T07:34:36Z","title":"Progressive Distillation Based on Masked Generation Feature Method for\n  Knowledge Graph Completion","summary":"  In recent years, knowledge graph completion (KGC) models based on pre-trained\nlanguage model (PLM) have shown promising results. However, the large number of\nparameters and high computational cost of PLM models pose challenges for their\napplication in downstream tasks. This paper proposes a progressive distillation\nmethod based on masked generation features for KGC task, aiming to\nsignificantly reduce the complexity of pre-trained models. Specifically, we\nperform pre-distillation on PLM to obtain high-quality teacher models, and\ncompress the PLM network to obtain multi-grade student models. However,\ntraditional feature distillation suffers from the limitation of having a single\nrepresentation of information in teacher models. To solve this problem, we\npropose masked generation of teacher-student features, which contain richer\nrepresentation information. Furthermore, there is a significant gap in\nrepresentation ability between teacher and student. Therefore, we design a\nprogressive distillation method to distill student models at each grade level,\nenabling efficient knowledge transfer from teachers to students. The\nexperimental results demonstrate that the model in the pre-distillation stage\nsurpasses the existing state-of-the-art methods. Furthermore, in the\nprogressive distillation stage, the model significantly reduces the model\nparameters while maintaining a certain level of performance. Specifically, the\nmodel parameters of the lower-grade student model are reduced by 56.7\\%\ncompared to the baseline.\n","authors":["Cunhang Fan","Yujie Chen","Jun Xue","Yonghui Kong","Jianhua Tao","Zhao Lv"],"pdf_url":"https://arxiv.org/pdf/2401.12997v2.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2406.06131v1","updated":"2024-06-10T09:39:19Z","published":"2024-06-10T09:39:19Z","title":"Building Bridges: A Dataset for Evaluating Gender-Fair Machine\n  Translation into German","summary":"  The translation of gender-neutral person-referring terms (e.g., the students)\nis often non-trivial. Translating from English into German poses an interesting\ncase -- in German, person-referring nouns are usually gender-specific, and if\nthe gender of the referent(s) is unknown or diverse, the generic masculine (die\nStudenten (m.)) is commonly used. This solution, however, reduces the\nvisibility of other genders, such as women and non-binary people. To counteract\ngender discrimination, a societal movement towards using gender-fair language\nexists (e.g., by adopting neosystems). However, gender-fair German is currently\nbarely supported in machine translation (MT), requiring post-editing or manual\ntranslations. We address this research gap by studying gender-fair language in\nEnglish-to-German MT. Concretely, we enrich a community-created gender-fair\nlanguage dictionary and sample multi-sentence test instances from encyclopedic\ntext and parliamentary speeches. Using these novel resources, we conduct the\nfirst benchmark study involving two commercial systems and six neural MT models\nfor translating words in isolation and natural contexts across two domains. Our\nfindings show that most systems produce mainly masculine forms and rarely\ngender-neutral variants, highlighting the need for future research. We release\ncode and data at\nhttps://github.com/g8a9/building-bridges-gender-fair-german-mt.\n","authors":["Manuel Lardelli","Giuseppe Attanasio","Anne Lauscher"],"pdf_url":"https://arxiv.org/pdf/2406.06131v1.pdf","comment":"Accepted to Findings of ACL 2024. Code and data at\n  https://github.com/g8a9/building-bridges-gender-fair-german-mt"},{"id":"http://arxiv.org/abs/2406.06127v1","updated":"2024-06-10T09:36:05Z","published":"2024-06-10T09:36:05Z","title":"Comparing Data Augmentation Methods for End-to-End Task-Oriented Dialog\n  Systems","summary":"  Creating effective and reliable task-oriented dialog systems (ToDSs) is\nchallenging, not only because of the complex structure of these systems, but\nalso due to the scarcity of training data, especially when several modules need\nto be trained separately, each one with its own input/output training examples.\nData augmentation (DA), whereby synthetic training examples are added to the\ntraining data, has been successful in other NLP systems, but has not been\nexplored as extensively in ToDSs. We empirically evaluate the effectiveness of\nDA methods in an end-to-end ToDS setting, where a single system is trained to\nhandle all processing stages, from user inputs to system outputs. We experiment\nwith two ToDSs (UBAR, GALAXY) on two datasets (MultiWOZ, KVRET). We consider\nthree types of DA methods (word-level, sentence-level, dialog-level), comparing\neight DA methods that have shown promising results in ToDSs and other NLP\nsystems. We show that all DA methods considered are beneficial, and we\nhighlight the best ones, also providing advice to practitioners. We also\nintroduce a more challenging few-shot cross-domain ToDS setting, reaching\nsimilar conclusions.\n","authors":["Christos Vlachos","Themos Stafylakis","Ion Androutsopoulos"],"pdf_url":"https://arxiv.org/pdf/2406.06127v1.pdf","comment":"There are 25 pages in total, 23 tables, 18 figures. Accepted in ACL\n  2024"},{"id":"http://arxiv.org/abs/2406.06125v1","updated":"2024-06-10T09:32:37Z","published":"2024-06-10T09:32:37Z","title":"Verifiable Generation with Subsentence-Level Fine-Grained Citations","summary":"  Verifiable generation requires large language models (LLMs) to cite source\ndocuments supporting their outputs, thereby improve output transparency and\ntrustworthiness. Yet, previous work mainly targets the generation of\nsentence-level citations, lacking specificity about which parts of a sentence\nare backed by the cited sources. This work studies verifiable generation with\nsubsentence-level fine-grained citations for more precise location of generated\ncontent supported by the cited sources. We first present a dataset, SCiFi,\ncomprising 10K Wikipedia paragraphs with subsentence-level citations. Each\nparagraph is paired with a set of candidate source documents for citation and a\nquery that triggers the generation of the paragraph content. On SCiFi, we\nevaluate the performance of state-of-the-art LLMs and strategies for processing\nlong documents designed for these models. Our experiment results reveals key\nfactors that could enhance the quality of citations, including the expansion of\nthe source documents' context accessible to the models and the implementation\nof specialized model tuning.\n","authors":["Shuyang Cao","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06125v1.pdf","comment":"NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.06124v1","updated":"2024-06-10T09:29:08Z","published":"2024-06-10T09:29:08Z","title":"Enhancing Long-Term Memory using Hierarchical Aggregate Tree for\n  Retrieval Augmented Generation","summary":"  Large language models have limited context capacity, hindering reasoning over\nlong conversations. We propose the Hierarchical Aggregate Tree memory structure\nto recursively aggregate relevant dialogue context through conditional tree\ntraversals. HAT encapsulates information from children nodes, enabling broad\ncoverage with depth control. We formulate finding best context as optimal tree\ntraversal. Experiments show HAT improves dialog coherence and summary quality\nover baseline contexts, demonstrating the techniques effectiveness for multi\nturn reasoning without exponential parameter growth. This memory augmentation\nenables more consistent, grounded longform conversations from LLMs\n","authors":["Aadharsh Aadhithya A","Sachin Kumar S","Soman K. P"],"pdf_url":"https://arxiv.org/pdf/2406.06124v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.06503v3","updated":"2024-06-10T09:06:10Z","published":"2023-11-11T07:56:40Z","title":"Knowledgeable Preference Alignment for LLMs in Domain-specific Question\n  Answering","summary":"  Deploying large language models (LLMs) to real scenarios for domain-specific\nquestion answering (QA) is a key thrust for LLM applications, which poses\nnumerous challenges, especially in ensuring that responses are both\naccommodating to user requirements and appropriately leveraging domain-specific\nknowledge bases. They are the two major difficulties for LLM application as\nvanilla fine-tuning falls short of addressing. Combining these requirements, we\nconceive of them as the requirement for the model's preference to be\nharmoniously aligned with humans'. Thus, we introduce Knowledgeable Preference\nAlignmenT (KnowPAT), which constructs two kinds of preference sets to tackle\nthe two issues. Besides, we design a new alignment objective to align the LLM\npreference with different human preferences uniformly, aiming to optimize LLM\nperformance in real-world, domain-specific QA settings. Adequate experiments\nand comprehensive comparisons with 15 baseline methods illustrate that our\nKnowPAT is a superior pipeline for real-scenario domain-specific QA with LLMs.\n","authors":["Yichi Zhang","Zhuo Chen","Yin Fang","Yanxi Lu","Fangming Li","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2311.06503v3.pdf","comment":"Accepted by ACL 2024 (Findings). Code is available at\n  https://github.com/zjukg/KnowPAT"},{"id":"http://arxiv.org/abs/2405.06459v2","updated":"2024-06-10T09:01:18Z","published":"2024-05-10T13:10:55Z","title":"Are EEG-to-Text Models Working?","summary":"  This work critically analyzes existing models for open-vocabulary EEG-to-Text\ntranslation. We identify a crucial limitation: previous studies often employed\nimplicit teacher-forcing during evaluation, artificially inflating performance\nmetrics. Additionally, they lacked a critical benchmark - comparing model\nperformance on pure noise inputs. We propose a methodology to differentiate\nbetween models that truly learn from EEG signals and those that simply memorize\ntraining data. Our analysis reveals that model performance on noise data can be\ncomparable to that on EEG data. These findings highlight the need for stricter\nevaluation practices in EEG-to-Text research, emphasizing transparent reporting\nand rigorous benchmarking with noise inputs. This approach will lead to more\nreliable assessments of model capabilities and pave the way for robust\nEEG-to-Text communication systems.\n","authors":["Hyejeong Jo","Yiqian Yang","Juhyeok Han","Yiqun Duan","Hui Xiong","Won Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2405.06459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06110v1","updated":"2024-06-10T08:50:59Z","published":"2024-06-10T08:50:59Z","title":"Recurrent Context Compression: Efficiently Expanding the Context Window\n  of LLM","summary":"  To extend the context length of Transformer-based large language models\n(LLMs) and improve comprehension capabilities, we often face limitations due to\ncomputational resources and bounded memory storage capacity. This work\nintroduces a method called Recurrent Context Compression (RCC), designed to\nefficiently expand the context window length of LLMs within constrained storage\nspace. We also investigate the issue of poor model responses when both\ninstructions and context are compressed in downstream tasks, and propose an\ninstruction reconstruction method to mitigate this problem. We validated the\neffectiveness of our approach on multiple tasks, achieving a compression rate\nof up to 32x on text reconstruction tasks with a BLEU4 score close to 0.95, and\nnearly 100\\% accuracy on a passkey retrieval task with a sequence length of 1M.\nFinally, our method demonstrated competitive performance in long-text\nquestion-answering tasks compared to non-compressed methods, while\nsignificantly saving storage resources in long-text inference tasks. Our code,\nmodels, and demo are available at https://github.com/WUHU-G/RCC_Transformer\n","authors":["Chensen Huang","Guibo Zhu","Xuepeng Wang","Yifei Luo","Guojing Ge","Haoran Chen","Dong Yi","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06097v1","updated":"2024-06-10T08:27:58Z","published":"2024-06-10T08:27:58Z","title":"StreamAtt: Direct Streaming Speech-to-Text Translation with\n  Attention-based Audio History Selection","summary":"  Streaming speech-to-text translation (StreamST) is the task of automatically\ntranslating speech while incrementally receiving an audio stream. Unlike\nsimultaneous ST (SimulST), which deals with pre-segmented speech, StreamST\nfaces the challenges of handling continuous and unbounded audio streams. This\nrequires additional decisions about what to retain of the previous history,\nwhich is impractical to keep entirely due to latency and computational\nconstraints. Despite the real-world demand for real-time ST, research on\nstreaming translation remains limited, with existing works solely focusing on\nSimulST. To fill this gap, we introduce StreamAtt, the first StreamST policy,\nand propose StreamLAAL, the first StreamST latency metric designed to be\ncomparable with existing metrics for SimulST. Extensive experiments across all\n8 languages of MuST-C v1.0 show the effectiveness of StreamAtt compared to a\nnaive streaming baseline and the related state-of-the-art SimulST policy,\nproviding a first step in StreamST research.\n","authors":["Sara Papi","Marco Gaido","Matteo Negri","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2406.06097v1.pdf","comment":"Accepted at ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2406.06073v1","updated":"2024-06-10T07:36:55Z","published":"2024-06-10T07:36:55Z","title":"Efficient k-Nearest-Neighbor Machine Translation with Dynamic Retrieval","summary":"  To achieve non-parametric NMT domain adaptation, $k$-Nearest-Neighbor Machine\nTranslation ($k$NN-MT) constructs an external datastore to store\ndomain-specific translation knowledge, which derives a $k$NN distribution to\ninterpolate the prediction distribution of the NMT model via a linear\ninterpolation coefficient $\\lambda$. Despite its success, $k$NN retrieval at\neach timestep leads to substantial time overhead. To address this issue,\ndominant studies resort to $k$NN-MT with adaptive retrieval ($k$NN-MT-AR),\nwhich dynamically estimates $\\lambda$ and skips $k$NN retrieval if $\\lambda$ is\nless than a fixed threshold. Unfortunately, $k$NN-MT-AR does not yield\nsatisfactory results. In this paper, we first conduct a preliminary study to\nreveal two key limitations of $k$NN-MT-AR: 1) the optimization gap leads to\ninaccurate estimation of $\\lambda$ for determining $k$NN retrieval skipping,\nand 2) using a fixed threshold fails to accommodate the dynamic demands for\n$k$NN retrieval at different timesteps. To mitigate these limitations, we then\npropose $k$NN-MT with dynamic retrieval ($k$NN-MT-DR) that significantly\nextends vanilla $k$NN-MT in two aspects. Firstly, we equip $k$NN-MT with a\nMLP-based classifier for determining whether to skip $k$NN retrieval at each\ntimestep. Particularly, we explore several carefully-designed scalar features\nto fully exert the potential of the classifier. Secondly, we propose a\ntimestep-aware threshold adjustment method to dynamically generate the\nthreshold, which further improves the efficiency of our model. Experimental\nresults on the widely-used datasets demonstrate the effectiveness and\ngenerality of our model.\\footnote{Our code is available at\n\\url{https://github.com/DeepLearnXMU/knn-mt-dr}.\n","authors":["Yan Gao","Zhiwei Cao","Zhongjian Miao","Baosong Yang","Shiyu Liu","Min Zhang","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2406.06073v1.pdf","comment":"Accepted to ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.06056v1","updated":"2024-06-10T07:03:36Z","published":"2024-06-10T07:03:36Z","title":"Synth-SBDH: A Synthetic Dataset of Social and Behavioral Determinants of\n  Health for Clinical Text","summary":"  Social and behavioral determinants of health (SBDH) play a crucial role in\nhealth outcomes and are frequently documented in clinical text. Automatically\nextracting SBDH information from clinical text relies on publicly available\ngood-quality datasets. However, existing SBDH datasets exhibit substantial\nlimitations in their availability and coverage. In this study, we introduce\nSynth-SBDH, a novel synthetic dataset with detailed SBDH annotations,\nencompassing status, temporal information, and rationale across 15 SBDH\ncategories. We showcase the utility of Synth-SBDH on three tasks using\nreal-world clinical datasets from two distinct hospital settings, highlighting\nits versatility, generalizability, and distillation capabilities. Models\ntrained on Synth-SBDH consistently outperform counterparts with no Synth-SBDH\ntraining, achieving up to 62.5% macro-F improvements. Additionally, Synth-SBDH\nproves effective for rare SBDH categories and under-resource constraints. Human\nevaluation demonstrates a Human-LLM alignment of 71.06% and uncovers areas for\nfuture refinements.\n","authors":["Avijit Mitra","Emily Druhl","Raelene Goodwin","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2406.06056v1.pdf","comment":"Github: https://github.com/avipartho/Synth-SBDH"},{"id":"http://arxiv.org/abs/2402.13137v2","updated":"2024-06-10T06:53:58Z","published":"2024-02-20T16:53:26Z","title":"The Hidden Space of Transformer Language Adapters","summary":"  We analyze the operation of transformer language adapters, which are small\nmodules trained on top of a frozen language model to adapt its predictions to\nnew target languages. We show that adapted predictions mostly evolve in the\nsource language the model was trained on, while the target language becomes\npronounced only in the very last layers of the model. Moreover, the adaptation\nprocess is gradual and distributed across layers, where it is possible to skip\nsmall groups of adapters without decreasing adaptation performance. Last, we\nshow that adapters operate on top of the model's frozen representation space\nwhile largely preserving its structure, rather than on an 'isolated' subspace.\nOur findings provide a deeper view into the adaptation process of language\nmodels to new languages, showcasing the constraints imposed on it by the\nunderlying model and introduces practical implications to enhance its\nefficiency.\n","authors":["Jesujoba O. Alabi","Marius Mosbach","Matan Eyal","Dietrich Klakow","Mor Geva"],"pdf_url":"https://arxiv.org/pdf/2402.13137v2.pdf","comment":"Accepted to ACL 2024 (main conference)"},{"id":"http://arxiv.org/abs/2406.06052v1","updated":"2024-06-10T06:46:09Z","published":"2024-06-10T06:46:09Z","title":"A Multidimensional Framework for Evaluating Lexical Semantic Change with\n  Social Science Applications","summary":"  Historical linguists have identified multiple forms of lexical semantic\nchange. We present a three-dimensional framework for integrating these forms\nand a unified computational methodology for evaluating them concurrently. The\ndimensions represent increases or decreases in semantic 1) sentiment, 2)\nbreadth, and 3) intensity. These dimensions can be complemented by the\nevaluation of shifts in the frequency of the target words and the thematic\ncontent of its collocates. This framework enables lexical semantic change to be\nmapped economically and systematically and has applications in computational\nsocial science. We present an illustrative analysis of semantic shifts in\nmental health and mental illness in two corpora, demonstrating patterns of\nsemantic change that illuminate contemporary concerns about pathologization,\nstigma, and concept creep.\n","authors":["Naomi Baes","Nick Haslam","Ekaterina Vylomova"],"pdf_url":"https://arxiv.org/pdf/2406.06052v1.pdf","comment":"Accepted to the Proceedings of the Association for Computational\n  Linguistics (ACL), 2024. Copyright c 2020 Association for Computational\n  Linguistics (ACL). All Rights Reserved"},{"id":"http://arxiv.org/abs/2406.06046v1","updated":"2024-06-10T06:27:42Z","published":"2024-06-10T06:27:42Z","title":"MATES: Model-Aware Data Selection for Efficient Pretraining with Data\n  Influence Models","summary":"  Pretraining data selection has the potential to improve language model\npretraining efficiency by utilizing higher-quality data from massive web data\ncorpora. Current data selection methods, which rely on either hand-crafted\nrules or larger reference models, are conducted statically and do not capture\nthe evolving data preferences during pretraining. In this paper, we introduce\nmodel-aware data selection with data influence models (MATES), where a data\ninfluence model continuously adapts to the evolving data preferences of the\npretraining model and then selects the data most effective for the current\npretraining progress. Specifically, we fine-tune a small data influence model\nto approximate oracle data preference signals collected by locally probing the\npretraining model and to select data accordingly for the next pretraining\nstage. Experiments on Pythia and the C4 dataset demonstrate that MATES\nsignificantly outperforms random data selection on extensive downstream tasks\nin both zero- and few-shot settings. It doubles the gains achieved by recent\ndata selection approaches that leverage larger reference models and reduces the\ntotal FLOPs required to reach certain performances by half. Further analysis\nvalidates the ever-changing data preferences of pretraining models and the\neffectiveness of our data influence models to capture them. Our code is\nopen-sourced at https://github.com/cxcscmu/MATES.\n","authors":["Zichun Yu","Spandan Das","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.06046v1.pdf","comment":"The code is open-sourced at https://github.com/cxcscmu/MATES"},{"id":"http://arxiv.org/abs/2402.12690v2","updated":"2024-06-10T05:59:26Z","published":"2024-02-20T03:37:16Z","title":"Simpson's Paradox and the Accuracy-Fluency Tradeoff in Translation","summary":"  A good translation should be faithful to the source and should respect the\nnorms of the target language. We address a theoretical puzzle about the\nrelationship between these objectives. On one hand, intuition and some prior\nwork suggest that accuracy and fluency should trade off against each other, and\nthat capturing every detail of the source can only be achieved at the cost of\nfluency. On the other hand, quality assessment researchers often suggest that\naccuracy and fluency are highly correlated and difficult for human raters to\ndistinguish (Callison-Burch et al., 2007). We show that the tension between\nthese views is an instance of Simpson's paradox, and that accuracy and fluency\nare positively correlated at the level of the corpus but trade off at the level\nof individual source segments. We further suggest that the relationship between\naccuracy and fluency is best evaluated at the segment (or sentence) level, and\nthat the trade off between these dimensions has implications both for assessing\ntranslation quality and developing improved MT systems.\n","authors":["Zheng Wei Lim","Ekaterina Vylomova","Trevor Cohn","Charles Kemp"],"pdf_url":"https://arxiv.org/pdf/2402.12690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06032v1","updated":"2024-06-10T05:50:23Z","published":"2024-06-10T05:50:23Z","title":"The Curse of Popularity: Popular Entities have Catastrophic Side Effects\n  when Deleting Knowledge from Language Models","summary":"  Language models (LMs) encode world knowledge in their internal parameters\nthrough training. However, LMs may learn personal and confidential information\nfrom the training data, leading to privacy concerns such as data leakage.\nTherefore, research on knowledge deletion from LMs is essential. This study\nfocuses on the knowledge stored in LMs and analyzes the relationship between\nthe side effects of knowledge deletion and the entities related to the\nknowledge. Our findings reveal that deleting knowledge related to popular\nentities can have catastrophic side effects. Furthermore, this research is the\nfirst to analyze knowledge deletion in models trained on synthetic knowledge\ngraphs, indicating a new direction for controlled experiments.\n","authors":["Ryosuke Takahashi","Go Kamoda","Benjamin Heinzerling","Keisuke Sakaguchi","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2406.06032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06448v2","updated":"2024-06-10T05:48:30Z","published":"2024-03-11T05:51:03Z","title":"Unsupervised Real-Time Hallucination Detection based on the Internal\n  States of Large Language Models","summary":"  Hallucinations in large language models (LLMs) refer to the phenomenon of\nLLMs producing responses that are coherent yet factually inaccurate. This issue\nundermines the effectiveness of LLMs in practical applications, necessitating\nresearch into detecting and mitigating hallucinations of LLMs. Previous studies\nhave mainly concentrated on post-processing techniques for hallucination\ndetection, which tend to be computationally intensive and limited in\neffectiveness due to their separation from the LLM's inference process. To\novercome these limitations, we introduce MIND, an unsupervised training\nframework that leverages the internal states of LLMs for real-time\nhallucination detection without requiring manual annotations. Additionally, we\npresent HELM, a new benchmark for evaluating hallucination detection across\nmultiple LLMs, featuring diverse LLM outputs and the internal states of LLMs\nduring their inference process. Our experiments demonstrate that MIND\noutperforms existing state-of-the-art methods in hallucination detection.\n","authors":["Weihang Su","Changyue Wang","Qingyao Ai","Yiran HU","Zhijing Wu","Yujia Zhou","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.06448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03897v2","updated":"2024-06-10T05:45:25Z","published":"2024-06-06T09:36:14Z","title":"HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew","summary":"  While large language models (LLMs) excel in various natural language tasks in\nEnglish, their performance in lower-resourced languages like Hebrew, especially\nfor generative tasks such as abstractive summarization, remains unclear. The\nhigh morphological richness in Hebrew adds further challenges due to the\nambiguity in sentence comprehension and the complexities in meaning\nconstruction. In this paper, we address this resource and evaluation gap by\nintroducing HeSum, a novel benchmark specifically designed for abstractive text\nsummarization in Modern Hebrew. HeSum consists of 10,000 article-summary pairs\nsourced from Hebrew news websites written by professionals. Linguistic analysis\nconfirms HeSum's high abstractness and unique morphological challenges. We show\nthat HeSum presents distinct difficulties for contemporary state-of-the-art\nLLMs, establishing it as a valuable testbed for generative language technology\nin Hebrew, and MRLs generative challenges in general.\n","authors":["Tzuf Paz-Argaman","Itai Mondshine","Asaf Achi Mordechai","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2406.03897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06027v1","updated":"2024-06-10T05:22:49Z","published":"2024-06-10T05:22:49Z","title":"HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question\n  Answering using LLMs","summary":"  Given unstructured text, Large Language Models (LLMs) are adept at answering\nsimple (single-hop) questions. However, as the complexity of the questions\nincrease, the performance of LLMs degrade. We believe this is due to the\noverhead associated with understanding the complex question followed by\nfiltering and aggregating unstructured information in the raw text. Recent\nmethods try to reduce this burden by integrating structured knowledge triples\ninto the raw text, aiming to provide a structured overview that simplifies\ninformation processing. However, this simplistic approach is query-agnostic and\nthe extracted facts are ambiguous as they lack context. To address these\ndrawbacks and to enable LLMs to answer complex (multi-hop) questions with ease,\nwe propose to use a knowledge graph (KG) that is context-aware and is distilled\nto contain query-relevant information. The use of our compressed distilled KG\nas input to the LLM results in our method utilizing up to $67\\%$ fewer tokens\nto represent the query relevant information present in the supporting\ndocuments, compared to the state-of-the-art (SoTA) method. Our experiments show\nconsistent improvements over the SoTA across several metrics (EM, F1,\nBERTScore, and Human Eval) on two popular benchmark datasets (HotpotQA and\nMuSiQue).\n","authors":["Pranoy Panda","Ankush Agarwal","Chaitanya Devaguptapu","Manohar Kaul","Prathosh A P"],"pdf_url":"https://arxiv.org/pdf/2406.06027v1.pdf","comment":"Accepted at ACL 2024 in the main track"},{"id":"http://arxiv.org/abs/2406.06025v1","updated":"2024-06-10T05:15:30Z","published":"2024-06-10T05:15:30Z","title":"RepoQA: Evaluating Long Context Code Understanding","summary":"  Recent advances have been improving the context windows of Large Language\nModels (LLMs). To quantify the real long-context capabilities of LLMs,\nevaluators such as the popular Needle in a Haystack have been developed to test\nLLMs over a large chunk of raw texts. While effective, current evaluations\noverlook the insight of how LLMs work with long-context code, i.e.,\nrepositories. To this end, we initiate the RepoQA benchmark to evaluate LLMs on\nlong-context code understanding. Traditional needle testers ask LLMs to\ndirectly retrieve the answer from the context without necessary deep\nunderstanding. In RepoQA, we built our initial task, namely Searching Needle\nFunction (SNF), which exercises LLMs to search functions given their\nnatural-language description, i.e., LLMs cannot find the desired function if\nthey cannot understand the description and code. RepoQA is multilingual and\ncomprehensive: it includes 500 code search tasks gathered from 50 popular\nrepositories across 5 modern programming languages. By evaluating 26 general\nand code-specific LLMs on RepoQA, we show (i) there is still a small gap\nbetween the best open and proprietary models; (ii) different models are good at\ndifferent languages; and (iii) models may understand code better without\ncomments.\n","authors":["Jiawei Liu","Jia Le Tian","Vijay Daita","Yuxiang Wei","Yifeng Ding","Yuhan Katherine Wang","Jun Yang","Lingming Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06021v1","updated":"2024-06-10T04:47:27Z","published":"2024-06-10T04:47:27Z","title":"Shoulders of Giants: A Look at the Degree and Utility of Openness in NLP\n  Research","summary":"  We analysed a sample of NLP research papers archived in ACL Anthology as an\nattempt to quantify the degree of openness and the benefit of such an open\nculture in the NLP community. We observe that papers published in different NLP\nvenues show different patterns related to artefact reuse. We also note that\nmore than 30% of the papers we analysed do not release their artefacts\npublicly, despite promising to do so. Further, we observe a wide language-wise\ndisparity in publicly available NLP-related artefacts.\n","authors":["Surangika Ranathunga","Nisansa de Silva","Dilith Jayakody","Aloka Fernando"],"pdf_url":"https://arxiv.org/pdf/2406.06021v1.pdf","comment":"Will appear in ACL 2024"},{"id":"http://arxiv.org/abs/2404.10198v2","updated":"2024-06-10T04:44:57Z","published":"2024-04-16T00:43:03Z","title":"ClashEval: Quantifying the tug-of-war between an LLM's internal prior\n  and external evidence","summary":"  Retrieval augmented generation (RAG) is frequently used to mitigate\nhallucinations and provide up-to-date knowledge for large language models\n(LLMs). However, given that document retrieval is an imprecise task and\nsometimes results in erroneous or even harmful content being presented in\ncontext, this raises the question of how LLMs handle retrieved information: If\nthe provided content is incorrect, does the model know to ignore it, or does it\nrecapitulate the error? Conversely, when the model's initial response is\nincorrect, does it always know to use the retrieved information to correct\nitself, or does it insist on its wrong prior response? To answer this, we\ncurate a dataset of over 1200 questions across six domains (e.g., drug dosages,\nOlympic records, locations) along with content relevant to answering each\nquestion. We further apply precise perturbations to the answers in the content\nthat range from subtle to blatant errors. We benchmark six top-performing LLMs,\nincluding GPT-4o, on this dataset and find that LLMs are susceptible to\nadopting incorrect retrieved content, overriding their own correct prior\nknowledge over 60% of the time. However, the more unrealistic the retrieved\ncontent is (i.e. more deviated from truth), the less likely the model is to\nadopt it. Also, the less confident a model is in its initial response (via\nmeasuring token probabilities), the more likely it is to adopt the information\nin the retrieved content. We exploit this finding and demonstrate simple\nmethods for improving model accuracy where there is conflicting retrieved\ncontent. Our results highlight a difficult task and benchmark for LLMs --\nnamely, their ability to correctly discern when it is wrong in light of correct\nretrieved content and to reject cases when the provided content is incorrect.\n","authors":["Kevin Wu","Eric Wu","James Zou"],"pdf_url":"https://arxiv.org/pdf/2404.10198v2.pdf","comment":"Revised June 9 2024"},{"id":"http://arxiv.org/abs/2406.06007v1","updated":"2024-06-10T04:07:09Z","published":"2024-06-10T04:07:09Z","title":"CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision\n  Language Models","summary":"  Artificial intelligence has significantly impacted medical applications,\nparticularly with the advent of Medical Large Vision Language Models\n(Med-LVLMs), sparking optimism for the future of automated and personalized\nhealthcare. However, the trustworthiness of Med-LVLMs remains unverified,\nposing significant risks for future model deployment. In this paper, we\nintroduce CARES and aim to comprehensively evaluate the Trustworthiness of\nMed-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs\nacross five dimensions, including trustfulness, fairness, safety, privacy, and\nrobustness. CARES comprises about 41K question-answer pairs in both closed and\nopen-ended formats, covering 16 medical image modalities and 27 anatomical\nregions. Our analysis reveals that the models consistently exhibit concerns\nregarding trustworthiness, often displaying factual inaccuracies and failing to\nmaintain fairness across different demographic groups. Furthermore, they are\nvulnerable to attacks and demonstrate a lack of privacy awareness. We publicly\nrelease our benchmark and code in https://github.com/richard-peng-xia/CARES.\n","authors":["Peng Xia","Ze Chen","Juanxi Tian","Yangrui Gong","Ruibo Hou","Yue Xu","Zhenbang Wu","Zhiyuan Fan","Yiyang Zhou","Kangyu Zhu","Wenhao Zheng","Zhaoyang Wang","Xiao Wang","Xuchao Zhang","Chetan Bansal","Marc Niethammer","Junzhou Huang","Hongtu Zhu","Yun Li","Jimeng Sun","Zongyuan Ge","Gang Li","James Zou","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2406.06007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06004v1","updated":"2024-06-10T03:57:39Z","published":"2024-06-10T03:57:39Z","title":"FLEUR: An Explainable Reference-Free Evaluation Metric for Image\n  Captioning Using a Large Multimodal Model","summary":"  Most existing image captioning evaluation metrics focus on assigning a single\nnumerical score to a caption by comparing it with reference captions. However,\nthese methods do not provide an explanation for the assigned score. Moreover,\nreference captions are expensive to acquire. In this paper, we propose FLEUR,\nan explainable reference-free metric to introduce explainability into image\ncaptioning evaluation metrics. By leveraging a large multimodal model, FLEUR\ncan evaluate the caption against the image without the need for reference\ncaptions, and provide the explanation for the assigned score. We introduce\nscore smoothing to align as closely as possible with human judgment and to be\nrobust to user-defined grading criteria. FLEUR achieves high correlations with\nhuman judgment across various image captioning evaluation benchmarks and\nreaches state-of-the-art results on Flickr8k-CF, COMPOSITE, and Pascal-50S\nwithin the domain of reference-free evaluation metrics. Our source code and\nresults are publicly available at: https://github.com/Yebin46/FLEUR.\n","authors":["Yebin Lee","Imseong Park","Myungjoo Kang"],"pdf_url":"https://arxiv.org/pdf/2406.06004v1.pdf","comment":"Accepted at ACL (Main) 2024"},{"id":"http://arxiv.org/abs/2406.06000v1","updated":"2024-06-10T03:47:24Z","published":"2024-06-10T03:47:24Z","title":"ThaiCoref: Thai Coreference Resolution Dataset","summary":"  While coreference resolution is a well-established research area in Natural\nLanguage Processing (NLP), research focusing on Thai language remains limited\ndue to the lack of large annotated corpora. In this work, we introduce\nThaiCoref, a dataset for Thai coreference resolution. Our dataset comprises\n777,271 tokens, 44,082 mentions and 10,429 entities across four text genres:\nuniversity essays, newspapers, speeches, and Wikipedia. Our annotation scheme\nis built upon the OntoNotes benchmark with adjustments to address Thai-specific\nphenomena. Utilizing ThaiCoref, we train models employing a multilingual\nencoder and cross-lingual transfer techniques, achieving a best F1 score of\n67.88\\% on the test set. Error analysis reveals challenges posed by Thai's\nunique linguistic features. To benefit the NLP community, we make the dataset\nand the model publicly available at http://www.github.com/nlp-chula/thai-coref .\n","authors":["Pontakorn Trakuekul","Wei Qi Leong","Charin Polpanumas","Jitkapat Sawatphol","William Chandra Tjhi","Attapol T. Rutherford"],"pdf_url":"https://arxiv.org/pdf/2406.06000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05995v1","updated":"2024-06-10T03:29:23Z","published":"2024-06-10T03:29:23Z","title":"A Dual-View Approach to Classifying Radiology Reports by Co-Training","summary":"  Radiology report analysis provides valuable information that can aid with\npublic health initiatives, and has been attracting increasing attention from\nthe research community. In this work, we present a novel insight that the\nstructure of a radiology report (namely, the Findings and Impression sections)\noffers different views of a radiology scan. Based on this intuition, we further\npropose a co-training approach, where two machine learning models are built\nupon the Findings and Impression sections, respectively, and use each other's\ninformation to boost performance with massive unlabeled data in a\nsemi-supervised manner. We conducted experiments in a public health\nsurveillance study, and results show that our co-training approach is able to\nimprove performance using the dual views and surpass competing supervised and\nsemi-supervised methods.\n","authors":["Yutong Han","Yan Yuan","Lili Mou"],"pdf_url":"https://arxiv.org/pdf/2406.05995v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2406.05981v1","updated":"2024-06-10T02:47:55Z","published":"2024-06-10T02:47:55Z","title":"ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training\n  Multiplication-Less Reparameterization","summary":"  Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM.\n","authors":["Haoran You","Yipin Guo","Yichao Fu","Wei Zhou","Huihong Shi","Xiaofan Zhang","Souvik Kundu","Amir Yazdanbakhsh","Yingyan Lin"],"pdf_url":"https://arxiv.org/pdf/2406.05981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05968v1","updated":"2024-06-10T02:04:28Z","published":"2024-06-10T02:04:28Z","title":"Prompting Large Language Models with Audio for General-Purpose Speech\n  Summarization","summary":"  In this work, we introduce a framework for speech summarization that\nleverages the processing and reasoning capabilities of large language models\n(LLMs). We propose an end-to-end system that combines an instruction-tuned LLM\nwith an audio encoder that converts speech into token representations that the\nLLM can interpret. Using a dataset with paired speech-text data, the overall\nsystem is trained to generate consistent responses to prompts with the same\nsemantic information regardless of the input modality. The resulting framework\nallows the LLM to process speech inputs in the same way as text, enabling\nspeech summarization by simply prompting the LLM. Unlike prior approaches, our\nmethod is able to summarize spoken content from any arbitrary domain, and it\ncan produce summaries in different styles by varying the LLM prompting\nstrategy. Experiments demonstrate that our approach outperforms a cascade\nbaseline of speech recognition followed by LLM text processing.\n","authors":["Wonjune Kang","Deb Roy"],"pdf_url":"https://arxiv.org/pdf/2406.05968v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.05967v1","updated":"2024-06-10T01:59:00Z","published":"2024-06-10T01:59:00Z","title":"CVQA: Culturally-diverse Multilingual Visual Question Answering\n  Benchmark","summary":"  Visual Question Answering (VQA) is an important task in multimodal AI, and it\nis often used to test the ability of vision-language models to understand and\nreason on knowledge present in both visual and textual data. However, most of\nthe current VQA models use datasets that are primarily focused on English and a\nfew major world languages, with images that are typically Western-centric.\nWhile recent efforts have tried to increase the number of languages covered on\nVQA datasets, they still lack diversity in low-resource languages. More\nimportantly, although these datasets often extend their linguistic range via\ntranslation or some other approaches, they usually keep images the same,\nresulting in narrow cultural representation. To address these limitations, we\nconstruct CVQA, a new Culturally-diverse multilingual Visual Question Answering\nbenchmark, designed to cover a rich set of languages and cultures, where we\nengage native speakers and cultural experts in the data collection process. As\na result, CVQA includes culturally-driven images and questions from across 28\ncountries on four continents, covering 26 languages with 11 scripts, providing\na total of 9k questions. We then benchmark several Multimodal Large Language\nModels (MLLMs) on CVQA, and show that the dataset is challenging for the\ncurrent state-of-the-art models. This benchmark can serve as a probing\nevaluation suite for assessing the cultural capability and bias of multimodal\nmodels and hopefully encourage more research efforts toward increasing cultural\nawareness and linguistic diversity in this field.\n","authors":["David Romero","Chenyang Lyu","Haryo Akbarianto Wibowo","Teresa Lynn","Injy Hamed","Aditya Nanda Kishore","Aishik Mandal","Alina Dragonetti","Artem Abzaliev","Atnafu Lambebo Tonja","Bontu Fufa Balcha","Chenxi Whitehouse","Christian Salamea","Dan John Velasco","David Ifeoluwa Adelani","David Le Meur","Emilio Villa-Cueva","Fajri Koto","Fauzan Farooqui","Frederico Belcavello","Ganzorig Batnasan","Gisela Vallejo","Grainne Caulfield","Guido Ivetta","Haiyue Song","Henok Biadglign Ademtew","Hernán Maina","Holy Lovenia","Israel Abebe Azime","Jan Christian Blaise Cruz","Jay Gala","Jiahui Geng","Jesus-German Ortiz-Barajas","Jinheon Baek","Jocelyn Dunstan","Laura Alonso Alemany","Kumaranage Ravindu Yasas Nagasinghe","Luciana Benotti","Luis Fernando D'Haro","Marcelo Viridiano","Marcos Estecha-Garitagoitia","Maria Camila Buitrago Cabrera","Mario Rodríguez-Cantelar","Mélanie Jouitteau","Mihail Mihaylov","Mohamed Fazli Mohamed Imam","Muhammad Farid Adilazuarda","Munkhjargal Gochoo","Munkh-Erdene Otgonbold","Naome Etori","Olivier Niyomugisha","Paula Mónica Silva","Pranjal Chitale","Raj Dabre","Rendi Chevi","Ruochen Zhang","Ryandito Diandaru","Samuel Cahyawijaya","Santiago Góngora","Soyeong Jeong","Sukannya Purkayastha","Tatsuki Kuribayashi","Thanmay Jayakumar","Tiago Timponi Torrent","Toqeer Ehsan","Vladimir Araujo","Yova Kementchedjhieva","Zara Burzo","Zheng Wei Lim","Zheng Xin Yong","Oana Ignat","Joan Nwatu","Rada Mihalcea","Thamar Solorio","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2406.05967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07177v4","updated":"2024-06-10T01:36:31Z","published":"2023-10-11T04:03:42Z","title":"Online Speculative Decoding","summary":"  Speculative decoding is a pivotal technique to accelerate the inference of\nlarge language models (LLMs) by employing a smaller draft model to predict the\ntarget model's outputs. However, its efficacy can be limited due to the low\npredictive accuracy of the draft model, particularly when faced with diverse\ntext inputs and a significant capability gap between the draft and target\nmodels. We introduce online speculative decoding to address this challenge. The\nmain idea is to continuously update the (multiple) draft model(s) on observed\nuser query data. Adapting to query distribution mitigates the shifts between\nthe training distribution of the draft model and the query distribution,\nenabling the draft model to more accurately predict the target model's outputs.\nWe develop a prototype of online speculative decoding based on knowledge\ndistillation and evaluate it using both synthetic and real query data. The\nresults show a substantial increase in the token acceptance rate by 0.1 to\n0.65, bringing 1.42x to 2.17x latency reduction. Our code is available at\nhttps://github.com/LiuXiaoxuanPKU/OSD.\n","authors":["Xiaoxuan Liu","Lanxiang Hu","Peter Bailis","Alvin Cheung","Zhijie Deng","Ion Stoica","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07177v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02619v6","updated":"2024-06-10T01:32:09Z","published":"2024-02-04T21:33:18Z","title":"Increasing Trust in Language Models through the Reuse of Verified\n  Circuits","summary":"  Language Models (LMs) are increasingly used for a wide range of prediction\ntasks, but their training can often neglect rare edge cases, reducing their\nreliability. Here, we define a stringent standard of trustworthiness whereby\nthe task algorithm and circuit implementation must be verified, accounting for\nedge cases, with no known failure modes. We show that a transformer model can\nbe trained to meet this standard if built using mathematically and logically\nspecified frameworks. In this paper, we fully verify a model for n-digit\ninteger addition. To exhibit the reusability of verified modules, we insert the\ntrained integer addition model into an untrained model and train the combined\nmodel to perform both addition and subtraction. We find extensive reuse of the\naddition circuits for both tasks, easing verification of the more complex\nsubtractor model. We discuss how inserting verified task modules into LMs can\nleverage model reuse to improve verifiability and trustworthiness of language\nmodels built using them. The reuse of verified circuits reduces the effort to\nverify more complex composite models which we believe to be a significant step\ntowards safety of language models.\n","authors":["Philip Quirke","Clement Neo","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2402.02619v6.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.05955v1","updated":"2024-06-10T01:21:59Z","published":"2024-06-10T01:21:59Z","title":"Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated\n  Parameters","summary":"  Exploiting activation sparsity is a promising approach to significantly\naccelerating the inference process of large language models (LLMs) without\ncompromising performance. However, activation sparsity is determined by\nactivation functions, and commonly used ones like SwiGLU and GeGLU exhibit\nlimited sparsity. Simply replacing these functions with ReLU fails to achieve\nsufficient sparsity. Moreover, inadequate training data can further increase\nthe risk of performance degradation. To address these challenges, we propose a\nnovel dReLU function, which is designed to improve LLM activation sparsity,\nalong with a high-quality training data mixture ratio to facilitate effective\nsparsification. Additionally, we leverage sparse activation patterns within the\nFeed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to\nfurther boost efficiency. By applying our neuron sparsification method to the\nMistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are\nactivated per inference iteration, respectively, while achieving even more\npowerful model performance. Evaluation results demonstrate that this sparsity\nachieves a 2-5x decoding speedup. Remarkably, on mobile phones, our\nTurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second.\nOur models are available at \\url{https://huggingface.co/PowerInfer}\n","authors":["Yixin Song","Haotong Xie","Zhengyan Zhang","Bo Wen","Li Ma","Zeyu Mi","Haibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.05955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.12307v2","updated":"2024-06-10T01:12:48Z","published":"2023-05-21T00:32:37Z","title":"OntoType: Ontology-Guided and Pre-Trained Language Model Assisted\n  Fine-Grained Entity Typing","summary":"  Fine-grained entity typing (FET), which assigns entities in text with\ncontext-sensitive, fine-grained semantic types, is a basic but important task\nfor knowledge extraction from unstructured text. FET has been studied\nextensively in natural language processing and typically relies on\nhuman-annotated corpora for training, which is costly and difficult to scale.\nRecent studies explore the utilization of pre-trained language models (PLMs) as\na knowledge base to generate rich and context-aware weak supervision for FET.\nHowever, a PLM still requires direction and guidance to serve as a knowledge\nbase as they often generate a mixture of rough and fine-grained types, or\ntokens unsuitable for typing. In this study, we vision that an ontology\nprovides a semantics-rich, hierarchical structure, which will help select the\nbest results generated by multiple PLM models and head words. Specifically, we\npropose a novel annotation-free, ontology-guided FET method, OntoType, which\nfollows a type ontological structure, from coarse to fine, ensembles multiple\nPLM prompting results to generate a set of type candidates, and refines its\ntype resolution, under the local context with a natural language inference\nmodel. Our experiments on the Ontonotes, FIGER, and NYT datasets using their\nassociated ontological structures demonstrate that our method outperforms the\nstate-of-the-art zero-shot fine-grained entity typing methods as well as a\ntypical LLM method, ChatGPT. Our error analysis shows that refinement of the\nexisting ontology structures will further improve fine-grained entity typing.\n","authors":["Tanay Komarlu","Minhao Jiang","Xuan Wang","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2305.12307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00715v4","updated":"2024-06-10T01:09:03Z","published":"2024-04-25T15:34:53Z","title":"Adapting Open-Source Large Language Models for Cost-Effective,\n  Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning","summary":"  Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have\ndemonstrated promising capabilities in clinical text summarization tasks.\nHowever, due to patient data privacy concerns and computational costs, many\nhealthcare providers prefer using small, locally-hosted models over external\ngeneric LLMs. This study presents a comprehensive domain- and task-specific\nadaptation process for the open-source LLaMA-2 13 billion parameter model,\nenabling it to generate high-quality clinical notes from outpatient\npatient-doctor dialogues. Our process incorporates continued pre-training,\nsupervised fine-tuning, and reinforcement learning from both AI and human\nfeedback. We introduced a new approach, DistillDirect, for performing on-policy\nreinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting\nmodel, LLaMA-Clinic, can generate clinical notes comparable in quality to those\nauthored by physicians. In a blinded physician reader study, the majority\n(90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as\n\"acceptable\" or higher across all three criteria: real-world readiness,\ncompleteness, and accuracy. In the more challenging \"Assessment and Plan\"\nsection, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than\nphysician-authored notes (4.1/5). Our cost analysis for inference shows that\nour LLaMA-Clinic model achieves a 3.75-fold cost reduction compared to an\nexternal generic LLM service. Additionally, we highlight key considerations for\nfuture clinical note-generation tasks, emphasizing the importance of\npre-defining a best-practice note format, rather than relying on LLMs to\ndetermine this for clinical practice. We have made our newly created synthetic\nclinic dialogue-note dataset and the physician feedback dataset publicly\navailable to foster future research.\n","authors":["Hanyin Wang","Chufan Gao","Bolun Liu","Qiping Xu","Guleid Hussein","Mohamad El Labban","Kingsley Iheasirim","Hariprasad Korsapati","Chuck Outcalt","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2405.00715v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11879v4","updated":"2024-06-10T00:44:32Z","published":"2023-06-20T20:37:54Z","title":"Open-Domain Text Evaluation via Contrastive Distribution Methods","summary":"  Recent advancements in open-domain text generation, driven by the power of\nlarge pre-trained language models (LLMs), have demonstrated remarkable\nperformance. However, assessing these models' generation quality remains a\nchallenge. In this paper, we introduce a novel method for evaluating\nopen-domain text generation called Contrastive Distribution Methods (CDM).\nLeveraging the connection between increasing model parameters and enhanced LLM\nperformance, CDM creates a mapping from the _contrast_ of two probabilistic\ndistributions -- one known to be superior to the other -- to quality measures.\nWe investigate CDM for open-domain text generation evaluation under two\nparadigms: 1) _Generative_ CDM, which harnesses the contrast of two language\nmodels' distributions to generate synthetic examples for training\ndiscriminator-based metrics; 2) _Discriminative_ CDM, which directly uses\ndistribution disparities between two language models for evaluation. Our\nexperiments on coherence evaluation for multi-turn dialogue and commonsense\nevaluation for controllable generation demonstrate CDM's superior correlate\nwith human judgment than existing automatic evaluation metrics, highlighting\nthe strong performance and generalizability of our approach.\n","authors":["Sidi Lu","Hongyi Liu","Asli Celikyilmaz","Tianlu Wang","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2306.11879v4.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2406.06852v1","updated":"2024-06-10T23:54:21Z","published":"2024-06-10T23:54:21Z","title":"A Survey of Backdoor Attacks and Defenses on Large Language Models:\n  Implications for Security Measures","summary":"  The large language models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LMMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and attacks without fine-tuning.\nBased on insights from a substantial review, we also discuss crucial issues for\nfuture research on backdoor attacks, such as further exploring attack\nalgorithms that do not require fine-tuning, or developing more covert attack\nalgorithms.\n","authors":["Shuai Zhao","Meihuizi Jia","Zhongliang Guo","Leilei Gan","Jie Fu","Yichao Feng","Fengjun Pan","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2406.06852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17647v2","updated":"2024-06-10T23:39:24Z","published":"2023-11-29T14:08:53Z","title":"Text as Images: Can Multimodal Large Language Models Follow Printed\n  Instructions in Pixels?","summary":"  Recent multimodal large language models (MLLMs) have shown promising\ninstruction following capabilities on vision-language tasks. In this work, we\nintroduce VISUAL MODALITY INSTRUCTION (VIM), and investigate how well\nmultimodal models can understand textual instructions provided in pixels,\ndespite not being explicitly trained on such data during pretraining or\nfine-tuning. We adapt VIM to eight benchmarks, including OKVQA, MM-Vet,\nMathVista, MMMU, and probe diverse MLLMs in both the text-modality instruction\n(TEM) setting and VIM setting. Notably, we observe a significant performance\ndisparity between the original TEM and VIM settings for open-source MLLMs,\nindicating that open-source MLLMs face greater challenges when text instruction\nis presented solely in image form. To address this issue, we train v-MLLM, a\ngeneralizable model that is capable to conduct robust instruction following in\nboth text-modality and visual-modality instructions.\n","authors":["Xiujun Li","Yujie Lu","Zhe Gan","Jianfeng Gao","William Yang Wang","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2311.17647v2.pdf","comment":"Github: https://github.com/VIM-Bench/VIM_TOOL, Model and Data:\n  https://huggingface.co/VIM-Bench"},{"id":"http://arxiv.org/abs/2406.04240v3","updated":"2024-06-10T23:33:10Z","published":"2024-06-06T16:39:00Z","title":"Hypernetworks for Personalizing ASR to Atypical Speech","summary":"  Parameter-efficient fine-tuning (PEFT) for personalizing automatic speech\nrecognition (ASR) has recently shown promise for adapting general population\nmodels to atypical speech. However, these approaches assume a priori knowledge\nof the atypical speech disorder being adapted for -- the diagnosis of which\nrequires expert knowledge that is not always available. Even given this\nknowledge, data scarcity and high inter/intra-speaker variability further limit\nthe effectiveness of traditional fine-tuning. To circumvent these challenges,\nwe first identify the minimal set of model parameters required for ASR\nadaptation. Our analysis of each individual parameter's effect on adaptation\nperformance allows us to reduce Word Error Rate (WER) by half while adapting\n0.03% of all weights. Alleviating the need for cohort-specific models, we next\npropose the novel use of a meta-learned hypernetwork to generate highly\nindividualized, utterance-level adaptations on-the-fly for a diverse set of\natypical speech characteristics. Evaluating adaptation at the global, cohort\nand individual-level, we show that hypernetworks generalize better to\nout-of-distribution speakers, while maintaining an overall relative WER\nreduction of 75.2% using 0.1% of the full parameter budget.\n","authors":["Max Müller-Eberstein","Dianna Yee","Karren Yang","Gautam Varma Mantena","Colin Lea"],"pdf_url":"https://arxiv.org/pdf/2406.04240v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06840v1","updated":"2024-06-10T23:09:19Z","published":"2024-06-10T23:09:19Z","title":"Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded\n  Dog Whistles","summary":"  A dog whistle is a form of coded communication that carries a secondary\nmeaning to specific audiences and is often weaponized for racial and\nsocioeconomic discrimination. Dog whistling historically originated from United\nStates politics, but in recent years has taken root in social media as a means\nof evading hate speech detection systems and maintaining plausible deniability.\nIn this paper, we present an approach for word-sense disambiguation of dog\nwhistles from standard speech using Large Language Models (LLMs), and leverage\nthis technique to create a dataset of 16,550 high-confidence coded examples of\ndog whistles used in formal and informal communication. Silent Signals is the\nlargest dataset of disambiguated dog whistle usage, created for applications in\nhate speech detection, neology, and political science.\n  The dataset can be found at\nhttps://huggingface.co/datasets/SALT-NLP/silent_signals.\n","authors":["Julia Kruk","Michela Marchini","Rijul Ragu","Caleb Ziems","David Muchlinski","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2406.06840v1.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2406.06839v1","updated":"2024-06-10T23:06:38Z","published":"2024-06-10T23:06:38Z","title":"EAVE: Efficient Product Attribute Value Extraction via Lightweight\n  Sparse-layer Interaction","summary":"  Product attribute value extraction involves identifying the specific values\nassociated with various attributes from a product profile. While existing\nmethods often prioritize the development of effective models to improve\nextraction performance, there has been limited emphasis on extraction\nefficiency. However, in real-world scenarios, products are typically associated\nwith multiple attributes, necessitating multiple extractions to obtain all\ncorresponding values. In this work, we propose an Efficient product Attribute\nValue Extraction (EAVE) approach via lightweight sparse-layer interaction.\nSpecifically, we employ a heavy encoder to separately encode the product\ncontext and attribute. The resulting non-interacting heavy representations of\nthe context can be cached and reused for all attributes. Additionally, we\nintroduce a light encoder to jointly encode the context and the attribute,\nfacilitating lightweight interactions between them. To enrich the interaction\nwithin the lightweight encoder, we design a sparse-layer interaction module to\nfuse the non-interacting heavy representation into the lightweight encoder.\nComprehensive evaluation on two benchmarks demonstrate that our method achieves\nsignificant efficiency gains with neutral or marginal loss in performance when\nthe context is long and number of attributes is large. Our code is available\n\\href{https://anonymous.4open.science/r/EAVE-EA18}{here}.\n","authors":["Li Yang","Qifan Wang","Jianfeng Chi","Jiahao Liu","Jingang Wang","Fuli Feng","Zenglin Xu","Yi Fang","Lifu Huang","Dongfang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.06839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15729v2","updated":"2024-06-10T21:58:24Z","published":"2024-05-24T17:19:03Z","title":"Optimizing Large Language Models for OpenAPI Code Completion","summary":"  Recent advancements in Large Language Models (LLMs) and their utilization in\ncode generation tasks have significantly reshaped the field of software\ndevelopment. Despite the remarkable efficacy of code completion solutions in\nmainstream programming languages, their performance lags when applied to less\nubiquitous formats such as OpenAPI definitions. This study evaluates the\nOpenAPI completion performance of GitHub Copilot, a prevalent commercial code\ncompletion tool, and proposes a set of task-specific optimizations leveraging\nMeta's open-source model Code Llama. A semantics-aware OpenAPI completion\nbenchmark proposed in this research is used to perform a series of experiments\nthrough which the impact of various prompt-engineering and fine-tuning\ntechniques on the Code Llama model's performance is analyzed. The fine-tuned\nCode Llama model reaches a peak correctness improvement of 55.2% over GitHub\nCopilot despite utilizing 25 times fewer parameters than the commercial\nsolution's underlying Codex model. Additionally, this research proposes an\nenhancement to a widely used code infilling training technique, addressing the\nissue of underperformance when the model is prompted with context sizes smaller\nthan those used during training. The dataset, the benchmark, and the model\nfine-tuning code are made publicly available.\n","authors":["Bohdan Petryshyn","Mantas Lukoševičius"],"pdf_url":"https://arxiv.org/pdf/2405.15729v2.pdf","comment":"Update: a better quality and readability of figures, better\n  explanation of code infilling and document splitting in training, some text\n  polishing, making it more compact"},{"id":"http://arxiv.org/abs/2406.04289v3","updated":"2024-06-10T21:53:32Z","published":"2024-06-06T17:34:24Z","title":"What Languages are Easy to Language-Model? A Perspective from Learning\n  Probabilistic Regular Languages","summary":"  What can large language models learn? By definition, language models (LM) are\ndistributions over strings. Therefore, an intuitive way of addressing the above\nquestion is to formalize it as a matter of learnability of classes of\ndistributions over strings. While prior work in this direction focused on\nassessing the theoretical limits, in contrast, we seek to understand the\nempirical learnability. Unlike prior empirical work, we evaluate neural LMs on\ntheir home turf-learning probabilistic languages-rather than as classifiers of\nformal languages. In particular, we investigate the learnability of regular LMs\n(RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs\nas a function of various complexity parameters of the RLM and the hidden state\nsize of the neural LM. We find that the RLM rank, which corresponds to the size\nof linear space spanned by the logits of its conditional distributions, and the\nexpected length of sampled strings are strong and significant predictors of\nlearnability for both RNNs and Transformers. Several other predictors also\nreach significance, but with differing patterns between RNNs and Transformers.\n","authors":["Nadav Borenstein","Anej Svete","Robin Chan","Josef Valvoda","Franz Nowak","Isabelle Augenstein","Eleanor Chodroff","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.04289v3.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.06809v1","updated":"2024-06-10T21:27:13Z","published":"2024-06-10T21:27:13Z","title":"AGB-DE: A Corpus for the Automated Legal Assessment of Clauses in German\n  Consumer Contracts","summary":"  Legal tasks and datasets are often used as benchmarks for the capabilities of\nlanguage models. However, openly available annotated datasets are rare. In this\npaper, we introduce AGB-DE, a corpus of 3,764 clauses from German consumer\ncontracts that have been annotated and legally assessed by legal experts.\nTogether with the data, we present a first baseline for the task of detecting\npotentially void clauses, comparing the performance of an SVM baseline with\nthree fine-tuned open language models and the performance of GPT-3.5. Our\nresults show the challenging nature of the task, with no approach exceeding an\nF1-score of 0.54. While the fine-tuned models often performed better with\nregard to precision, GPT-3.5 outperformed the other approaches with regard to\nrecall. An analysis of the errors indicates that one of the main challenges\ncould be the correct interpretation of complex clauses, rather than the\ndecision boundaries of what is permissible and what is not.\n","authors":["Daniel Braun","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2406.06809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08718v2","updated":"2024-06-10T21:17:24Z","published":"2023-11-15T05:58:35Z","title":"Decomposing Uncertainty for Large Language Models through Input\n  Clarification Ensembling","summary":"  Uncertainty decomposition refers to the task of decomposing the total\nuncertainty of a predictive model into aleatoric (data) uncertainty, resulting\nfrom inherent randomness in the data-generating process, and epistemic (model)\nuncertainty, resulting from missing information in the model's training data.\nIn large language models (LLMs) specifically, identifying sources of\nuncertainty is an important step toward improving reliability, trustworthiness,\nand interpretability, but remains an important open research question. In this\npaper, we introduce an uncertainty decomposition framework for LLMs, called\ninput clarification ensembling, which can be applied to any pre-trained LLM.\nOur approach generates a set of clarifications for the input, feeds them into\nan LLM, and ensembles the corresponding predictions. We show that, when\naleatoric uncertainty arises from ambiguity or under-specification in LLM\ninputs, this approach makes it possible to factor an (unclarified) LLM's\npredictions into separate aleatoric and epistemic terms, using a decomposition\nsimilar to the one employed by Bayesian neural networks. Empirical evaluations\ndemonstrate that input clarification ensembling provides accurate and reliable\nuncertainty quantification on several language processing tasks. Code and data\nare available at https://github.com/UCSB-NLP-Chang/llm_uncertainty.\n","authors":["Bairu Hou","Yujian Liu","Kaizhi Qian","Jacob Andreas","Shiyu Chang","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08718v2.pdf","comment":"ICML 2024, 19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.06799v1","updated":"2024-06-10T21:08:39Z","published":"2024-06-10T21:08:39Z","title":"LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching","summary":"  As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.\n","authors":["Simranjit Singh","Michael Fore","Andreas Karatzas","Chaehong Lee","Yanan Jian","Longfei Shangguan","Fuxun Yu","Iraklis Anagnostopoulos","Dimitrios Stamoulis"],"pdf_url":"https://arxiv.org/pdf/2406.06799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01331v2","updated":"2024-06-10T20:59:48Z","published":"2024-03-29T21:32:50Z","title":"LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact\n  Language Model","summary":"  We train a suite of multimodal foundation models (MMFM) using the popular\nLLaVA framework with the recently released Gemma family of large language\nmodels (LLMs). Of particular interest is the 2B parameter Gemma model, which\nprovides opportunities to construct capable small-scale MMFMs. In line with\nfindings from other papers in this space, we test the effect of ablating three\ndesign features: pretraining the connector, utilizing a more powerful image\nbackbone, and increasing the size of the language backbone. The resulting\nmodels, which we call LLaVA-Gemma, exhibit moderate performance on an array of\nevaluations, but fail to improve past the current comparably sized SOTA models.\nCloser analysis of performance shows mixed effects; skipping pretraining tends\nto reduce performance, larger vision models sometimes improve performance, and\nincreasing language model size has inconsistent effects. We publicly release\ntraining recipes, code and weights for our models for the LLaVA-Gemma models.\n","authors":["Musashi Hinck","Matthew L. Olson","David Cobbley","Shao-Yen Tseng","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2404.01331v2.pdf","comment":"CVPR 2024, MMFM workshop. Authors 1 and 2 contributed equally. Models\n  available at https://huggingface.co/intel/llava-gemma-2b/ and\n  https://huggingface.co/intel/llava-gemma-7b/ Training code at\n  https://github.com/IntelLabs/multimodal_cognitive_ai/tree/main/LLaVA-Gemma"},{"id":"http://arxiv.org/abs/2406.01866v2","updated":"2024-06-10T20:33:51Z","published":"2024-06-04T00:37:29Z","title":"#EpiTwitter: Public Health Messaging During the COVID-19 Pandemic","summary":"  Effective communication during health crises is critical, with social media\nserving as a key platform for public health experts (PHEs) to engage with the\npublic. However, it also amplifies pseudo-experts promoting contrarian views.\nDespite its importance, the role of emotional and moral language in PHEs'\ncommunication during COVID-19 remains under explored. This study examines how\nPHEs and pseudo-experts communicated on Twitter during the pandemic, focusing\non emotional and moral language and their engagement with political elites.\nAnalyzing tweets from 489 PHEs and 356 pseudo-experts from January 2020 to\nJanuary 2021, alongside public responses, we identified key priorities and\ndifferences in messaging strategy. PHEs prioritize masking, healthcare,\neducation, and vaccines, using positive emotional language like optimism. In\ncontrast, pseudo-experts discuss therapeutics and lockdowns more frequently,\nemploying negative emotions like pessimism and disgust. Negative emotional and\nmoral language tends to drive engagement, but positive language from PHEs\nfosters positivity in public responses. PHEs exhibit liberal partisanship,\nexpressing more positivity towards liberals and negativity towards conservative\nelites, while pseudo-experts show conservative partisanship. These findings\nshed light on the polarization of COVID-19 discourse and underscore the\nimportance of strategic use of emotional and moral language by experts to\nmitigate polarization and enhance public trust.\n","authors":["Ashwin Rao","Nazanin Sabri","Siyi Guo","Louiqa Raschid","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2406.01866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17882v2","updated":"2024-06-10T20:27:28Z","published":"2024-02-27T20:48:24Z","title":"BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in\n  Relational Algebra","summary":"  Many existing end-to-end systems for hybrid question answering tasks can\noften be boiled down to a \"prompt-and-pray\" paradigm, where the user has\nlimited control and insight into the intermediate reasoning steps used to\nachieve the final result. Additionally, due to the context size limitation of\nmany transformer-based LLMs, it is often not reasonable to expect that the full\nstructured and unstructured context will fit into a given prompt in a zero-shot\nsetting, let alone a few-shot setting. We introduce BlendSQL, a superset of\nSQLite to act as a unified dialect for orchestrating reasoning across both\nunstructured and structured data. For hybrid question answering tasks involving\nmulti-hop reasoning, we encode the full decomposed reasoning roadmap into a\nsingle interpretable BlendSQL query. Notably, we show that BlendSQL can scale\nto massive datasets and improve the performance of end-to-end systems while\nusing 35% fewer tokens. Our code is available and installable as a package at\nhttps://github.com/parkervg/blendsql.\n","authors":["Parker Glenn","Parag Pravin Dakle","Liang Wang","Preethi Raghavan"],"pdf_url":"https://arxiv.org/pdf/2402.17882v2.pdf","comment":"For associated codebase, see https://github.com/parkervg/blendsql"},{"id":"http://arxiv.org/abs/2406.06773v1","updated":"2024-06-10T20:19:55Z","published":"2024-06-10T20:19:55Z","title":"Evaluating Zero-Shot Long-Context LLM Compression","summary":"  This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K.\n","authors":["Chenyu Wang","Yihan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04550v3","updated":"2024-06-10T20:11:42Z","published":"2023-09-08T18:44:47Z","title":"Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges","summary":"  Unstructured data in Electronic Health Records (EHRs) often contains critical\ninformation -- complementary to imaging -- that could inform radiologists'\ndiagnoses. But the large volume of notes often associated with patients\ntogether with time constraints renders manually identifying relevant evidence\npractically infeasible. In this work we propose and evaluate a zero-shot\nstrategy for using LLMs as a mechanism to efficiently retrieve and summarize\nunstructured evidence in patient EHR relevant to a given query. Our method\nentails tasking an LLM to infer whether a patient has, or is at risk of, a\nparticular condition on the basis of associated notes; if so, we ask the model\nto summarize the supporting evidence. Under expert evaluation, we find that\nthis LLM-based approach provides outputs consistently preferred to a pre-LLM\ninformation retrieval baseline. Manual evaluation is expensive, so we also\npropose and validate a method using an LLM to evaluate (other) LLM outputs for\nthis task, allowing us to scale up evaluation. Our findings indicate the\npromise of LLMs as interfaces to EHR, but also highlight the outstanding\nchallenge posed by \"hallucinations\". In this setting, however, we show that\nmodel confidence in outputs strongly correlates with faithful summaries,\noffering a practical means to limit confabulations.\n","authors":["Hiba Ahsan","Denis Jered McInerney","Jisoo Kim","Christopher Potter","Geoffrey Young","Silvio Amir","Byron C. Wallace"],"pdf_url":"https://arxiv.org/pdf/2309.04550v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06769v1","updated":"2024-06-10T20:08:44Z","published":"2024-06-10T20:08:44Z","title":"DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating\n  Automated Scientific Discovery Agents","summary":"  Automated scientific discovery promises to accelerate progress across\nscientific domains. However, developing and evaluating an AI agent's capacity\nfor end-to-end scientific reasoning is challenging as running real-world\nexperiments is often prohibitively expensive or infeasible. In this work we\nintroduce DISCOVERYWORLD, the first virtual environment for developing and\nbenchmarking an agent's ability to perform complete cycles of novel scientific\ndiscovery. DISCOVERYWORLD contains a variety of different challenges, covering\ntopics as diverse as radioisotope dating, rocket science, and proteomics, to\nencourage development of general discovery skills rather than task-specific\nsolutions. DISCOVERYWORLD itself is an inexpensive, simulated, text-based\nenvironment (with optional 2D visual overlay). It includes 120 different\nchallenge tasks, spanning eight topics each with three levels of difficulty and\nseveral parametric variations. Each task requires an agent to form hypotheses,\ndesign and run experiments, analyze results, and act on conclusions.\nDISCOVERYWORLD further provides three automatic metrics for evaluating\nperformance, based on (a) task completion, (b) task-relevant actions taken, and\n(c) the discovered explanatory knowledge. We find that strong baseline agents,\nthat perform well in prior published environments, struggle on most\nDISCOVERYWORLD tasks, suggesting that DISCOVERYWORLD captures some of the novel\nchallenges of discovery, and thus that DISCOVERYWORLD may help accelerate\nnear-term development and assessment of scientific discovery competency in\nagents. Code available at: www.github.com/allenai/discoveryworld\n","authors":["Peter Jansen","Marc-Alexandre Côté","Tushar Khot","Erin Bransom","Bhavana Dalvi Mishra","Bodhisattwa Prasad Majumder","Oyvind Tafjord","Peter Clark"],"pdf_url":"https://arxiv.org/pdf/2406.06769v1.pdf","comment":"9 pages, 4 figures. Preprint, under review"},{"id":"http://arxiv.org/abs/2406.06764v1","updated":"2024-06-10T19:50:16Z","published":"2024-06-10T19:50:16Z","title":"$Classi|Q\\rangle$ Towards a Translation Framework To Bridge The\n  Classical-Quantum Programming Gap","summary":"  Quantum computing, albeit readily available as hardware or emulated on the\ncloud, is still far from being available in general regarding complex\nprogramming paradigms and learning curves. This vision paper introduces\n$Classi|Q\\rangle$, a translation framework idea to bridge Classical and Quantum\nComputing by translating high-level programming languages, e.g., Python or C++,\ninto a low-level language, e.g., Quantum Assembly. Our idea paper serves as a\nblueprint for ongoing efforts in quantum software engineering, offering a\nroadmap for further $Classi|Q\\rangle$ development to meet the diverse needs of\nresearchers and practitioners. $Classi|Q\\rangle$ is designed to empower\nresearchers and practitioners with no prior quantum experience to harness the\npotential of hybrid quantum computation. We also discuss future enhancements to\n$Classi|Q\\rangle$, including support for additional quantum languages, improved\noptimization strategies, and integration with emerging quantum computing\nplatforms.\n","authors":["Matteo Esposito","Maryam Tavassoli Sabzevari","Boshuai Ye","Davide Falessi","Arif Ali Khan","Davide Taibi"],"pdf_url":"https://arxiv.org/pdf/2406.06764v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.06527v1","updated":"2024-06-10T17:59:59Z","published":"2024-06-10T17:59:59Z","title":"IllumiNeRF: 3D Relighting without Inverse Rendering","summary":"  Existing methods for relightable view synthesis -- using a set of images of\nan object under unknown lighting to recover a 3D representation that can be\nrendered from novel viewpoints under a target illumination -- are based on\ninverse rendering, and attempt to disentangle the object geometry, materials,\nand lighting that explain the input images. Furthermore, this typically\ninvolves optimization through differentiable Monte Carlo rendering, which is\nbrittle and computationally-expensive. In this work, we propose a simpler\napproach: we first relight each input image using an image diffusion model\nconditioned on lighting and then reconstruct a Neural Radiance Field (NeRF)\nwith these relit images, from which we render novel views under the target\nlighting. We demonstrate that this strategy is surprisingly competitive and\nachieves state-of-the-art results on multiple relighting benchmarks. Please see\nour project page at https://illuminerf.github.io/.\n","authors":["Xiaoming Zhao","Pratul P. Srinivasan","Dor Verbin","Keunhong Park","Ricardo Martin Brualla","Philipp Henzler"],"pdf_url":"https://arxiv.org/pdf/2406.06527v1.pdf","comment":"Project page: https://illuminerf.github.io/"},{"id":"http://arxiv.org/abs/2402.13254v3","updated":"2024-06-10T17:59:55Z","published":"2024-02-20T18:59:55Z","title":"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples","summary":"  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V. To facilitate future research, we release our code, dataset, benchmark,\nand checkpoints at https://countercurate.github.io.\n","authors":["Jianrui Zhang","Mu Cai","Tengyang Xie","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2402.13254v3.pdf","comment":"15 pages, 6 figures, 12 tables, Project Page:\n  https://countercurate.github.io/"},{"id":"http://arxiv.org/abs/2406.06526v1","updated":"2024-06-10T17:59:55Z","published":"2024-06-10T17:59:55Z","title":"GaussianCity: Generative Gaussian Splatting for Unbounded 3D City\n  Generation","summary":"  3D city generation with NeRF-based methods shows promising generation results\nbut is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has\nemerged as a highly efficient alternative for object-level 3D generation.\nHowever, adapting 3D-GS from finite-scale 3D objects and humans to\ninfinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails\nsignificant storage overhead (out-of-memory issues), arising from the need to\nexpand points to billions, often demanding hundreds of Gigabytes of VRAM for a\ncity scene spanning 10km^2. In this paper, we propose GaussianCity, a\ngenerative Gaussian Splatting framework dedicated to efficiently synthesizing\nunbounded 3D cities with a single feed-forward pass. Our key insights are\ntwo-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a\nhighly compact intermediate representation, ensuring that the growth in VRAM\nusage for unbounded scenes remains constant, thus enabling unbounded city\ngeneration. 2) Spatial-aware Gaussian Attribute Decoder: We present\nspatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which\nleverages Point Serializer to integrate the structural and contextual\ncharacteristics of BEV points. Extensive experiments demonstrate that\nGaussianCity achieves state-of-the-art results in both drone-view and\nstreet-view 3D city generation. Notably, compared to CityDreamer, GaussianCity\nexhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18\nFPS).\n","authors":["Haozhe Xie","Zhaoxi Chen","Fangzhou Hong","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2406.06526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06525v1","updated":"2024-06-10T17:59:52Z","published":"2024-06-10T17:59:52Z","title":"Autoregressive Model Beats Diffusion: Llama for Scalable Image\n  Generation","summary":"  We introduce LlamaGen, a new family of image generation models that apply\noriginal ``next-token prediction'' paradigm of large language models to visual\ngeneration domain. It is an affirmative answer to whether vanilla\nautoregressive models, e.g., Llama, without inductive biases on visual signals\ncan achieve state-of-the-art image generation performance if scaling properly.\nWe reexamine design spaces of image tokenizers, scalability properties of image\ngeneration models, and their training data quality. The outcome of this\nexploration consists of: (1) An image tokenizer with downsample ratio of 16,\nreconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet\nbenchmark. (2) A series of class-conditional image generation models ranging\nfrom 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256\nbenchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A\ntext-conditional image generation model with 775M parameters, from two-stage\ntraining on LAION-COCO and high aesthetics quality images, demonstrating\ncompetitive performance of visual quality and text alignment. (4) We verify the\neffectiveness of LLM serving frameworks in optimizing the inference speed of\nimage generation models and achieve 326% - 414% speedup. We release all models\nand codes to facilitate open-source community of visual generation and\nmultimodal foundation models.\n","authors":["Peize Sun","Yi Jiang","Shoufa Chen","Shilong Zhang","Bingyue Peng","Ping Luo","Zehuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.06525v1.pdf","comment":"Codes and models: \\url{https://github.com/FoundationVision/LlamaGen}"},{"id":"http://arxiv.org/abs/2406.06523v1","updated":"2024-06-10T17:59:46Z","published":"2024-06-10T17:59:46Z","title":"NaRCan: Natural Refined Canonical Image with Integration of Diffusion\n  Prior for Video Editing","summary":"  We propose a video editing framework, NaRCan, which integrates a hybrid\ndeformation field and diffusion prior to generate high-quality natural\ncanonical images to represent the input video. Our approach utilizes homography\nto model global motion and employs multi-layer perceptrons (MLPs) to capture\nlocal residual deformations, enhancing the model's ability to handle complex\nvideo dynamics. By introducing a diffusion prior from the early stages of\ntraining, our model ensures that the generated images retain a high-quality\nnatural appearance, making the produced canonical images suitable for various\ndownstream tasks in video editing, a capability not achieved by current\ncanonical-based methods. Furthermore, we incorporate low-rank adaptation (LoRA)\nfine-tuning and introduce a noise and diffusion prior update scheduling\ntechnique that accelerates the training process by 14 times. Extensive\nexperimental results show that our method outperforms existing approaches in\nvarious video editing tasks and produces coherent and high-quality edited video\nsequences. See our project page for video results at\nhttps://koi953215.github.io/NaRCan_page/.\n","authors":["Ting-Hsuan Chen","Jiewen Chan","Hau-Shiang Shiu","Shih-Han Yen","Chang-Han Yeh","Yu-Lun Liu"],"pdf_url":"https://arxiv.org/pdf/2406.06523v1.pdf","comment":"Project page: https://koi953215.github.io/NaRCan_page/"},{"id":"http://arxiv.org/abs/2404.02905v2","updated":"2024-06-10T17:59:07Z","published":"2024-04-03T17:59:53Z","title":"Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale\n  Prediction","summary":"  We present Visual AutoRegressive modeling (VAR), a new generation paradigm\nthat redefines the autoregressive learning on images as coarse-to-fine\n\"next-scale prediction\" or \"next-resolution prediction\", diverging from the\nstandard raster-scan \"next-token prediction\". This simple, intuitive\nmethodology allows autoregressive (AR) transformers to learn visual\ndistributions fast and generalize well: VAR, for the first time, makes GPT-like\nAR models surpass diffusion transformers in image generation. On ImageNet\n256x256 benchmark, VAR significantly improve AR baseline by improving Frechet\ninception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to\n350.2, with around 20x faster inference speed. It is also empirically verified\nthat VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions\nincluding image quality, inference speed, data efficiency, and scalability.\nScaling up VAR models exhibits clear power-law scaling laws similar to those\nobserved in LLMs, with linear correlation coefficients near -0.998 as solid\nevidence. VAR further showcases zero-shot generalization ability in downstream\ntasks including image in-painting, out-painting, and editing. These results\nsuggest VAR has initially emulated the two important properties of LLMs:\nScaling Laws and zero-shot task generalization. We have released all models and\ncodes to promote the exploration of AR/VAR models for visual generation and\nunified learning.\n","authors":["Keyu Tian","Yi Jiang","Zehuan Yuan","Bingyue Peng","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.02905v2.pdf","comment":"Demo website: https://var.vision/"},{"id":"http://arxiv.org/abs/2406.06521v1","updated":"2024-06-10T17:59:01Z","published":"2024-06-10T17:59:01Z","title":"PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity\n  Surface Reconstruction","summary":"  Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due\nto its high-quality rendering, and ultra-fast training and rendering speed.\nHowever, due to the unstructured and irregular nature of Gaussian point clouds,\nit is difficult to guarantee geometric reconstruction accuracy and multi-view\nconsistency simply by relying on image reconstruction loss. Although many\nstudies on surface reconstruction based on 3DGS have emerged recently, the\nquality of their meshes is generally unsatisfactory. To address this problem,\nwe propose a fast planar-based Gaussian splatting reconstruction representation\n(PGSR) to achieve high-fidelity surface reconstruction while ensuring\nhigh-quality rendering. Specifically, we first introduce an unbiased depth\nrendering method, which directly renders the distance from the camera origin to\nthe Gaussian plane and the corresponding normal map based on the Gaussian\ndistribution of the point cloud, and divides the two to obtain the unbiased\ndepth. We then introduce single-view geometric, multi-view photometric, and\ngeometric regularization to preserve global geometric accuracy. We also propose\na camera exposure compensation model to cope with scenes with large\nillumination variations. Experiments on indoor and outdoor scenes show that our\nmethod achieves fast training and rendering while maintaining high-fidelity\nrendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based\nmethods.\n","authors":["Danpeng Chen","Hai Li","Weicai Ye","Yifan Wang","Weijian Xie","Shangjin Zhai","Nan Wang","Haomin Liu","Hujun Bao","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06521v1.pdf","comment":"project page: https://zju3dv.github.io/pgsr/"},{"id":"http://arxiv.org/abs/2402.05917v2","updated":"2024-06-10T17:58:56Z","published":"2024-02-08T18:52:23Z","title":"Point-VOS: Pointing Up Video Object Segmentation","summary":"  Current state-of-the-art Video Object Segmentation (VOS) methods rely on\ndense per-object mask annotations both during training and testing. This\nrequires time-consuming and costly video annotation mechanisms. We propose a\nnovel Point-VOS task with a spatio-temporally sparse point-wise annotation\nscheme that substantially reduces the annotation effort. We apply our\nannotation scheme to two large-scale video datasets with text descriptions and\nannotate over 19M points across 133K objects in 32K videos. Based on our\nannotations, we propose a new Point-VOS benchmark, and a corresponding\npoint-based training mechanism, which we use to establish strong baseline\nresults. We show that existing VOS methods can easily be adapted to leverage\nour point annotations during training, and can achieve results close to the\nfully-supervised performance when trained on pseudo-masks generated from these\npoints. In addition, we show that our data can be used to improve models that\nconnect vision and language, by evaluating it on the Video Narrative Grounding\n(VNG) task. We will make our code and annotations available at\nhttps://pointvos.github.io.\n","authors":["Idil Esen Zulfikar","Sabarinath Mahadevan","Paul Voigtlaender","Bastian Leibe"],"pdf_url":"https://arxiv.org/pdf/2402.05917v2.pdf","comment":"Accepted to CVPR2024!"},{"id":"http://arxiv.org/abs/2406.06520v1","updated":"2024-06-10T17:58:48Z","published":"2024-06-10T17:58:48Z","title":"Decentralized Personalized Federated Learning","summary":"  This work tackles the challenges of data heterogeneity and communication\nlimitations in decentralized federated learning. We focus on creating a\ncollaboration graph that guides each client in selecting suitable collaborators\nfor training personalized models that leverage their local data effectively.\nOur approach addresses these issues through a novel, communication-efficient\nstrategy that enhances resource efficiency. Unlike traditional methods, our\nformulation identifies collaborators at a granular level by considering\ncombinatorial relations of clients, enhancing personalization while minimizing\ncommunication overhead. We achieve this through a bi-level optimization\nframework that employs a constrained greedy algorithm, resulting in a\nresource-efficient collaboration graph for personalized learning. Extensive\nevaluation against various baselines across diverse datasets demonstrates the\nsuperiority of our method, named DPFL. DPFL consistently outperforms other\napproaches, showcasing its effectiveness in handling real-world data\nheterogeneity, minimizing communication overhead, enhancing resource\nefficiency, and building personalized models in decentralized federated\nlearning scenarios.\n","authors":["Salma Kharrat","Marco Canini","Samuel Horvath"],"pdf_url":"https://arxiv.org/pdf/2406.06520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06517v1","updated":"2024-06-10T17:56:21Z","published":"2024-06-10T17:56:21Z","title":"Genomics-guided Representation Learning for Pathologic Pan-cancer Tumor\n  Microenvironment Subtype Prediction","summary":"  The characterization of Tumor MicroEnvironment (TME) is challenging due to\nits complexity and heterogeneity. Relatively consistent TME characteristics\nembedded within highly specific tissue features, render them difficult to\npredict. The capability to accurately classify TME subtypes is of critical\nsignificance for clinical tumor diagnosis and precision medicine. Based on the\nobservation that tumors with different origins share similar microenvironment\npatterns, we propose PathoTME, a genomics-guided Siamese representation\nlearning framework employing Whole Slide Image (WSI) for pan-cancer TME\nsubtypes prediction. Specifically, we utilize Siamese network to leverage\ngenomic information as a regularization factor to assist WSI embeddings\nlearning during the training phase. Additionally, we employ Domain Adversarial\nNeural Network (DANN) to mitigate the impact of tissue type variations. To\neliminate domain bias, a dynamic WSI prompt is designed to further unleash the\nmodel's capabilities. Our model achieves better performance than other\nstate-of-the-art methods across 23 cancer types on TCGA dataset. Our code is\navailable at https://github.com/Mengflz/PathoTME.\n","authors":["Fangliangzi Meng","Hongrun Zhang","Ruodan Yan","Guohui Chuai","Chao Li","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2406.06517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06512v1","updated":"2024-06-10T17:53:01Z","published":"2024-06-10T17:53:01Z","title":"Merlin: A Vision Language Foundation Model for 3D Computed Tomography","summary":"  Over 85 million computed tomography (CT) scans are performed annually in the\nUS, of which approximately one quarter focus on the abdomen. Given the current\nradiologist shortage, there is a large impetus to use artificial intelligence\nto alleviate the burden of interpreting these complex imaging studies. Prior\nstate-of-the-art approaches for automated medical image interpretation leverage\nvision language models (VLMs). However, current medical VLMs are generally\nlimited to 2D images and short reports, and do not leverage electronic health\nrecord (EHR) data for supervision. We introduce Merlin - a 3D VLM that we train\nusing paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes\n(1.8+ million codes), and radiology reports (6+ million tokens). We evaluate\nMerlin on 6 task types and 752 individual tasks. The non-adapted\n(off-the-shelf) tasks include zero-shot findings classification (31 findings),\nphenotype classification (692 phenotypes), and zero-shot cross-modal retrieval\n(image to findings and image to impressions), while model adapted tasks include\n5-year disease prediction (6 diseases), radiology report generation, and 3D\nsemantic segmentation (20 organs). We perform internal validation on a test set\nof 5,137 CTs, and external validation on 7,000 clinical CTs and on two public\nCT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant\nevaluations, we assess the efficacy of various network architectures and\ntraining strategies to depict that Merlin has favorable performance to existing\ntask-specific baselines. We derive data scaling laws to empirically assess\ntraining data needs for requisite downstream task performance. Furthermore,\nunlike conventional VLMs that require hundreds of GPUs for training, we perform\nall training on a single GPU.\n","authors":["Louis Blankemeier","Joseph Paul Cohen","Ashwin Kumar","Dave Van Veen","Syed Jamal Safdar Gardezi","Magdalini Paschali","Zhihong Chen","Jean-Benoit Delbrouck","Eduardo Reis","Cesar Truyts","Christian Bluethgen","Malte Engmann Kjeldskov Jensen","Sophie Ostmeier","Maya Varma","Jeya Maria Jose Valanarasu","Zhongnan Fang","Zepeng Huo","Zaid Nabulsi","Diego Ardila","Wei-Hung Weng","Edson Amaro Junior","Neera Ahuja","Jason Fries","Nigam H. Shah","Andrew Johnston","Robert D. Boutin","Andrew Wentland","Curtis P. Langlotz","Jason Hom","Sergios Gatidis","Akshay S. Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2406.06512v1.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.06508v1","updated":"2024-06-10T17:47:14Z","published":"2024-06-10T17:47:14Z","title":"Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion for\n  Zero-shot Motion Transfer","summary":"  Given the remarkable results of motion synthesis with diffusion models, a\nnatural question arises: how can we effectively leverage these models for\nmotion editing? Existing diffusion-based motion editing methods overlook the\nprofound potential of the prior embedded within the weights of pre-trained\nmodels, which enables manipulating the latent feature space; hence, they\nprimarily center on handling the motion space. In this work, we explore the\nattention mechanism of pre-trained motion diffusion models. We uncover the\nroles and interactions of attention elements in capturing and representing\nintricate human motion patterns, and carefully integrate these elements to\ntransfer a leader motion to a follower one while maintaining the nuanced\ncharacteristics of the follower, resulting in zero-shot motion transfer.\nEditing features associated with selected motions allows us to confront a\nchallenge observed in prior motion diffusion approaches, which use general\ndirectives (e.g., text, music) for editing, ultimately failing to convey subtle\nnuances effectively. Our work is inspired by how a monkey closely imitates what\nit sees while maintaining its unique motion patterns; hence we call it Monkey\nSee, Monkey Do, and dub it MoMo. Employing our technique enables accomplishing\ntasks such as synthesizing out-of-distribution motions, style transfer, and\nspatial editing. Furthermore, diffusion inversion is seldom employed for\nmotions; as a result, editing efforts focus on generated motions, limiting the\neditability of real ones. MoMo harnesses motion inversion, extending its\napplication to both real and generated motions. Experimental results show the\nadvantage of our approach over the current art. In particular, unlike methods\ntailored for specific applications through training, our approach is applied at\ninference time, requiring no training. Our webpage is at\nhttps://monkeyseedocg.github.io.\n","authors":["Sigal Raab","Inbar Gat","Nathan Sala","Guy Tevet","Rotem Shalev-Arkushin","Ohad Fried","Amit H. Bermano","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2406.06508v1.pdf","comment":"Video: https://www.youtube.com/watch?v=s5oo3sKV0YU, Project page:\n  https://monkeyseedocg.github.io, Code:\n  https://github.com/MonkeySeeDoCG/MoMo-code"},{"id":"http://arxiv.org/abs/2406.04313v2","updated":"2024-06-10T17:40:19Z","published":"2024-06-06T17:57:04Z","title":"Improving Alignment and Robustness with Circuit Breakers","summary":"  AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that interrupts the models as they respond with harmful outputs\nwith \"circuit breakers.\" Existing techniques aimed at improving alignment, such\nas refusal training, are often bypassed. Techniques such as adversarial\ntraining try to plug these holes by countering specific attacks. As an\nalternative to refusal training and adversarial training, circuit-breaking\ndirectly controls the representations that are responsible for harmful outputs\nin the first place. Our technique can be applied to both text-only and\nmultimodal language models to prevent the generation of harmful outputs without\nsacrificing utility -- even in the presence of powerful unseen attacks.\nNotably, while adversarial robustness in standalone image recognition remains\nan open challenge, circuit breakers allow the larger multimodal system to\nreliably withstand image \"hijacks\" that aim to produce harmful content.\nFinally, we extend our approach to AI agents, demonstrating considerable\nreductions in the rate of harmful actions when they are under attack. Our\napproach represents a significant step forward in the development of reliable\nsafeguards to harmful behavior and adversarial attacks.\n","authors":["Andy Zou","Long Phan","Justin Wang","Derek Duenas","Maxwell Lin","Maksym Andriushchenko","Rowan Wang","Zico Kolter","Matt Fredrikson","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2406.04313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12716v3","updated":"2024-06-10T17:39:04Z","published":"2023-12-20T02:22:49Z","title":"BloomVQA: Assessing Hierarchical Multi-modal Comprehension","summary":"  We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive\nevaluation of large vision-language models on comprehension tasks. Unlike\ncurrent benchmarks that often focus on fact-based memorization and simple\nreasoning tasks without theoretical grounding, we collect multiple-choice\nsamples based on picture stories that reflect different levels of\ncomprehension, as laid out in Bloom's Taxonomy, a classic framework for\nlearning assessment widely adopted in education research. Our data maps to a\nnovel hierarchical graph representation which enables automatic data\naugmentation and novel measures characterizing model consistency. We perform\ngraded evaluation and reliability analysis on recent multi-modal models. In\ncomparison to low-level tasks, we observe decreased performance on tasks\nrequiring advanced comprehension and cognitive skills with up to 38.0\\% drop in\nVQA accuracy. In comparison to earlier models, GPT-4V demonstrates improved\naccuracy over all comprehension levels and shows a tendency of bypassing visual\ninputs especially for higher-level tasks. Current models also show consistency\npatterns misaligned with human comprehension in various scenarios,\ndemonstrating the need for improvement based on theoretically-grounded\ncriteria.\n","authors":["Yunye Gong","Robik Shrestha","Jared Claypoole","Michael Cogswell","Arijit Ray","Christopher Kanan","Ajay Divakaran"],"pdf_url":"https://arxiv.org/pdf/2312.12716v3.pdf","comment":"Accepted by ACL Findings (2024). Dataset available at\n  https://huggingface.co/datasets/ygong/BloomVQA"},{"id":"http://arxiv.org/abs/2312.04063v2","updated":"2024-06-10T17:35:23Z","published":"2023-12-07T06:03:07Z","title":"An unsupervised approach towards promptable defect segmentation in\n  laser-based additive manufacturing by Segment Anything","summary":"  Foundation models are currently driving a paradigm shift in computer vision\ntasks for various fields including biology, astronomy, and robotics among\nothers, leveraging user-generated prompts to enhance their performance. In the\nmanufacturing domain, accurate image-based defect segmentation is imperative to\nensure product quality and facilitate real-time process control. However, such\ntasks are often characterized by multiple challenges including the absence of\nlabels and the requirement for low latency inference among others. To address\nthese issues, we construct a framework for image segmentation using a\nstate-of-the-art Vision Transformer (ViT) based Foundation model (Segment\nAnything Model) with a novel multi-point prompt generation scheme using\nunsupervised clustering. Utilizing our framework we perform porosity\nsegmentation in a case study of laser-based powder bed fusion (L-PBF) and\nobtain high accuracy without using any labeled data to guide the prompt tuning\nprocess. By capitalizing on lightweight foundation model inference combined\nwith unsupervised prompt generation, we envision constructing a real-time\nanomaly detection pipeline that could revolutionize current laser additive\nmanufacturing processes, thereby facilitating the shift towards Industry 4.0\nand promoting defect-free production along with operational efficiency.\n","authors":["Israt Zarin Era","Imtiaz Ahmed","Zhichao Liu","Srinjoy Das"],"pdf_url":"https://arxiv.org/pdf/2312.04063v2.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.06499v1","updated":"2024-06-10T17:34:24Z","published":"2024-06-10T17:34:24Z","title":"NarrativeBridge: Enhancing Video Captioning with Causal-Temporal\n  Narrative","summary":"  Existing video captioning benchmarks and models lack coherent representations\nof causal-temporal narrative, which is sequences of events linked through cause\nand effect, unfolding over time and driven by characters or agents. This lack\nof narrative restricts models' ability to generate text descriptions that\ncapture the causal and temporal dynamics inherent in video content. To address\nthis gap, we propose NarrativeBridge, an approach comprising of: (1) a novel\nCausal-Temporal Narrative (CTN) captions benchmark generated using a large\nlanguage model and few-shot prompting, explicitly encoding cause-effect\ntemporal relationships in video descriptions, evaluated automatically to ensure\ncaption quality and relevance; and (2) a dedicated Cause-Effect Network (CEN)\narchitecture with separate encoders for capturing cause and effect dynamics\nindependently, enabling effective learning and generation of captions with\ncausal-temporal narrative. Extensive experiments demonstrate that CEN is more\naccurate in articulating the causal and temporal aspects of video content than\nthe second best model (GIT): 17.88 and 17.44 CIDEr on the MSVD and MSR-VTT\ndatasets, respectively. The proposed framework understands and generates\nnuanced text descriptions with intricate causal-temporal narrative structures\npresent in videos, addressing a critical limitation in video captioning. For\nproject details, visit https://narrativebridge.github.io/.\n","authors":["Asmar Nadeem","Faegheh Sardari","Robert Dawes","Syed Sameed Husain","Adrian Hilton","Armin Mustafa"],"pdf_url":"https://arxiv.org/pdf/2406.06499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06496v1","updated":"2024-06-10T17:31:36Z","published":"2024-06-10T17:31:36Z","title":"Direct Preference Optimization for Suppressing Hallucinated Prior Exams\n  in Radiology Report Generation","summary":"  Recent advances in generative vision-language models (VLMs) have exciting\npotential implications for AI in radiology, yet VLMs are also known to produce\nhallucinations, nonsensical text, and other unwanted behaviors that can waste\nclinicians' time and cause patient harm. Drawing on recent work on direct\npreference optimization (DPO), we propose a simple method for modifying the\nbehavior of pretrained VLMs performing radiology report generation by\nsuppressing unwanted types of generations. We apply our method to the\nprevention of hallucinations of prior exams, addressing a long-established\nproblem behavior in models performing chest X-ray report generation. Across our\nexperiments, we find that DPO fine-tuning achieves a 3.2-4.8x reduction in\nlines hallucinating prior exams while maintaining model performance on clinical\naccuracy metrics. Our work is, to the best of our knowledge, the first work to\napply DPO to medical VLMs, providing a data- and compute- efficient way to\nsuppress problem behaviors while maintaining overall clinical accuracy.\n","authors":["Oishi Banerjee","Hong-Yu Zhou","Subathra Adithan","Stephen Kwak","Kay Wu","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2406.06496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05059v2","updated":"2024-06-10T17:23:32Z","published":"2024-06-07T16:31:41Z","title":"GenHeld: Generating and Editing Handheld Objects","summary":"  Grasping is an important human activity that has long been studied in\nrobotics, computer vision, and cognitive science. Most existing works study\ngrasping from the perspective of synthesizing hand poses conditioned on 3D or\n2D object representations. We propose GenHeld to address the inverse problem of\nsynthesizing held objects conditioned on 3D hand model or 2D image. Given a 3D\nmodel of hand, GenHeld 3D can select a plausible held object from a large\ndataset using compact object representations called object codes.The selected\nobject is then positioned and oriented to form a plausible grasp without\nchanging hand pose. If only a 2D hand image is available, GenHeld 2D can edit\nthis image to add or replace a held object. GenHeld 2D operates by combining\nthe abilities of GenHeld 3D with diffusion-based image editing. Results and\nexperiments show that we outperform baselines and can generate plausible held\nobjects in both 2D and 3D. Our experiments demonstrate that our method achieves\nhigh quality and plausibility of held object synthesis in both 3D and 2D.\n","authors":["Chaerin Min","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2406.05059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02568v2","updated":"2024-06-10T17:05:28Z","published":"2024-05-04T05:01:58Z","title":"Active Neural 3D Reconstruction with Colorized Surface Voxel-based View\n  Selection","summary":"  Active view selection in 3D scene reconstruction has been widely studied\nsince training on informative views is critical for reconstruction. Recently,\nNeural Radiance Fields (NeRF) variants have shown promising results in active\n3D reconstruction using uncertainty-guided view selection. They utilize\nuncertainties estimated with neural networks that encode scene geometry and\nappearance. However, the choice of uncertainty integration methods, either\nvoxel-based or neural rendering, has conventionally depended on the types of\nscene uncertainty being estimated, whether geometric or appearance-related. In\nthis paper, we introduce Colorized Surface Voxel (CSV)-based view selection, a\nnew next-best view (NBV) selection method exploiting surface voxel-based\nmeasurement of uncertainty in scene appearance. CSV encapsulates the\nuncertainty of estimated scene appearance (e.g., color uncertainty) and\nestimated geometric information (e.g., surface). Using the geometry\ninformation, we interpret the uncertainty of scene appearance 3D-wise during\nthe aggregation of the per-voxel uncertainty. Consequently, the uncertainty\nfrom occluded and complex regions is recognized under challenging scenarios\nwith limited input data. Our method outperforms previous works on popular\ndatasets, DTU and Blender, and our new dataset with imbalanced viewpoints,\nshowing that the CSV-based view selection significantly improves performance by\nup to 30%.\n","authors":["Hyunseo Kim","Hyeonseo Yang","Taekyung Kim","YoonSung Kim","Jin-Hwa Kim","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.02568v2.pdf","comment":"New experiments with newly published dataset are added. The main\n  claims are the same as in the previous version, but the naming and\n  explanations have been changed"},{"id":"http://arxiv.org/abs/2406.06465v1","updated":"2024-06-10T17:02:08Z","published":"2024-06-10T17:02:08Z","title":"AID: Adapting Image2Video Diffusion Models for Instruction-guided Video\n  Prediction","summary":"  Text-guided video prediction (TVP) involves predicting the motion of future\nframes from the initial frame according to an instruction, which has wide\napplications in virtual reality, robotics, and content creation. Previous TVP\nmethods make significant breakthroughs by adapting Stable Diffusion for this\ntask. However, they struggle with frame consistency and temporal stability\nprimarily due to the limited scale of video datasets. We observe that\npretrained Image2Video diffusion models possess good priors for video dynamics\nbut they lack textual control. Hence, transferring Image2Video models to\nleverage their video dynamic priors while injecting instruction control to\ngenerate controllable videos is both a meaningful and challenging task. To\nachieve this, we introduce the Multi-Modal Large Language Model (MLLM) to\npredict future video states based on initial frames and text instructions. More\nspecifically, we design a dual query transformer (DQFormer) architecture, which\nintegrates the instructions and frames into the conditional embeddings for\nfuture frame prediction. Additionally, we develop Long-Short Term Temporal\nAdapters and Spatial Adapters that can quickly transfer general video diffusion\nmodels to specific scenarios with minimal training costs. Experimental results\nshow that our method significantly outperforms state-of-the-art techniques on\nfour datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and\nUCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and\nSSv2 respectively, demonstrating its effectiveness in various domains. More\nexamples can be found at our website https://chenhsing.github.io/AID.\n","authors":["Zhen Xing","Qi Dai","Zejia Weng","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.06465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06462v1","updated":"2024-06-10T16:58:48Z","published":"2024-06-10T16:58:48Z","title":"VCR: Visual Caption Restoration","summary":"  We introduce Visual Caption Restoration (VCR), a novel vision-language task\nthat challenges models to accurately restore partially obscured texts using\npixel-level hints within images. This task stems from the observation that text\nembedded in images is intrinsically different from common visual elements and\nnatural language due to the need to align the modalities of vision, text, and\ntext embedded in images. While numerous works have integrated text embedded in\nimages into visual question-answering tasks, approaches to these tasks\ngenerally rely on optical character recognition or masked language modeling,\nthus reducing the task to mainly text-based processing. However, text-based\nprocessing becomes ineffective in VCR as accurate text restoration depends on\nthe combined information from provided images, context, and subtle cues from\nthe tiny exposed areas of masked texts. We develop a pipeline to generate\nsynthetic images for the VCR task using image-caption pairs, with adjustable\ncaption visibility to control the task difficulty. With this pipeline, we\nconstruct a dataset for VCR called VCR-Wiki using images with captions from\nWikipedia, comprising 2.11M English and 346K Chinese entities in both easy and\nhard split variants. Our results reveal that current vision language models\nsignificantly lag behind human performance in the VCR task, and merely\nfine-tuning the models on our dataset does not lead to notable improvements. We\nrelease VCR-Wiki and the data construction code to facilitate future research.\n","authors":["Tianyu Zhang","Suyuchen Wang","Lu Li","Ge Zhang","Perouz Taslakian","Sai Rajeswar","Jie Fu","Bang Liu","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2406.06462v1.pdf","comment":"18 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.06438v1","updated":"2024-06-10T16:31:34Z","published":"2024-06-10T16:31:34Z","title":"Multimodal Contextualized Semantic Parsing from Speech","summary":"  We introduce Semantic Parsing in Contextual Environments (SPICE), a task\ndesigned to enhance artificial agents' contextual awareness by integrating\nmultimodal inputs with prior contexts. SPICE goes beyond traditional semantic\nparsing by offering a structured, interpretable framework for dynamically\nupdating an agent's knowledge with new information, mirroring the complexity of\nhuman communication. We develop the VG-SPICE dataset, crafted to challenge\nagents with visual scene graph construction from spoken conversational\nexchanges, highlighting speech and visual data integration. We also present the\nAudio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE.\nThese innovations aim to improve multimodal information processing and\nintegration. Both the VG-SPICE dataset and the AViD-SP model are publicly\navailable.\n","authors":["Jordan Voas","Raymond Mooney","David Harwath"],"pdf_url":"https://arxiv.org/pdf/2406.06438v1.pdf","comment":"10 Pages, 3 figures, ACL 2024 Main"},{"id":"http://arxiv.org/abs/2406.06434v1","updated":"2024-06-10T16:24:46Z","published":"2024-06-10T16:24:46Z","title":"Spatiotemporal Graph Neural Network Modelling Perfusion MRI","summary":"  Perfusion MRI (pMRI) offers valuable insights into tumor vascularity and\npromises to predict tumor genotypes, thus benefiting prognosis for glioma\npatients, yet effective models tailored to 4D pMRI are still lacking. This\nstudy presents the first attempt to model 4D pMRI using a GNN-based\nspatiotemporal model PerfGAT, integrating spatial information and temporal\nkinetics to predict Isocitrate DeHydrogenase (IDH) mutation status in glioma\npatients. Specifically, we propose a graph structure learning approach based on\nedge attention and negative graphs to optimize temporal correlations modeling.\nMoreover, we design a dual-attention feature fusion module to integrate\nspatiotemporal features while addressing tumor-related brain regions. Further,\nwe develop a class-balanced augmentation methods tailored to spatiotemporal\ndata, which could mitigate the common label imbalance issue in clinical\ndatasets. Our experimental results demonstrate that the proposed method\noutperforms other state-of-the-art approaches, promising to model pMRI\neffectively for patient characterization.\n","authors":["Ruodan Yan","Carola-Bibiane Schönlieb","Chao Li"],"pdf_url":"https://arxiv.org/pdf/2406.06434v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.06432v1","updated":"2024-06-10T16:24:07Z","published":"2024-06-10T16:24:07Z","title":"SYM3D: Learning Symmetric Triplanes for Better 3D-Awareness of GANs","summary":"  Despite the growing success of 3D-aware GANs, which can be trained on 2D\nimages to generate high-quality 3D assets, they still rely on multi-view images\nwith camera annotations to synthesize sufficient details from all viewing\ndirections. However, the scarce availability of calibrated multi-view image\ndatasets, especially in comparison to single-view images, has limited the\npotential of 3D GANs. Moreover, while bypassing camera pose annotations with a\ncamera distribution constraint reduces dependence on exact camera parameters,\nit still struggles to generate a consistent orientation of 3D assets. To this\nend, we propose SYM3D, a novel 3D-aware GAN designed to leverage the prevalent\nreflectional symmetry structure found in natural and man-made objects,\nalongside a proposed view-aware spatial attention mechanism in learning the 3D\nrepresentation. We evaluate SYM3D on both synthetic (ShapeNet Chairs, Cars, and\nAirplanes) and real-world datasets (ABO-Chair), demonstrating its superior\nperformance in capturing detailed geometry and texture, even when trained on\nonly single-view images. Finally, we demonstrate the effectiveness of\nincorporating symmetry regularization in helping reduce artifacts in the\nmodeling of 3D assets in the text-to-3D task.\n","authors":["Jing Yang","Kyle Fogarty","Fangcheng Zhong","Cengiz Oztireli"],"pdf_url":"https://arxiv.org/pdf/2406.06432v1.pdf","comment":"11"},{"id":"http://arxiv.org/abs/2406.06424v1","updated":"2024-06-10T16:14:45Z","published":"2024-06-10T16:14:45Z","title":"Margin-aware Preference Optimization for Aligning Diffusion Models\n  without Reference","summary":"  Modern alignment techniques based on human preferences, such as RLHF and DPO,\ntypically employ divergence regularization relative to the reference model to\nensure training stability. However, this often limits the flexibility of models\nduring alignment, especially when there is a clear distributional discrepancy\nbetween the preference data and the reference model. In this paper, we focus on\nthe alignment of recent text-to-image diffusion models, such as Stable\nDiffusion XL (SDXL), and find that this \"reference mismatch\" is indeed a\nsignificant problem in aligning these models due to the unstructured nature of\nvisual modalities: e.g., a preference for a particular stylistic aspect can\neasily induce such a discrepancy. Motivated by this observation, we propose a\nnovel and memory-friendly preference alignment method for diffusion models that\ndoes not depend on any reference model, coined margin-aware preference\noptimization (MaPO). MaPO jointly maximizes the likelihood margin between the\npreferred and dispreferred image sets and the likelihood of the preferred sets,\nsimultaneously learning general stylistic features and preferences. For\nevaluation, we introduce two new pairwise preference datasets, which comprise\nself-generated image pairs from SDXL, Pick-Style and Pick-Safety, simulating\ndiverse scenarios of reference mismatch. Our experiments validate that MaPO can\nsignificantly improve alignment on Pick-Style and Pick-Safety and general\npreference alignment when used with Pick-a-Pic v2, surpassing the base SDXL and\nother existing methods. Our code, models, and datasets are publicly available\nvia https://mapo-t2i.github.io\n","authors":["Jiwoo Hong","Sayak Paul","Noah Lee","Kashif Rasul","James Thorne","Jongheon Jeong"],"pdf_url":"https://arxiv.org/pdf/2406.06424v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.06423v1","updated":"2024-06-10T16:14:33Z","published":"2024-06-10T16:14:33Z","title":"Hybrid Video Anomaly Detection for Anomalous Scenarios in Autonomous\n  Driving","summary":"  In autonomous driving, the most challenging scenarios are the ones that can\nonly be detected within their temporal context. Most video anomaly detection\napproaches focus either on surveillance or traffic accidents, which are only a\nsubfield of autonomous driving. In this work, we present HF$^2$-VAD$_{AD}$, a\nvariation of the HF$^2$-VAD surveillance video anomaly detection method for\nautonomous driving. We learn a representation of normality from a vehicle's ego\nperspective and evaluate pixel-wise anomaly detections in rare and critical\nscenarios.\n","authors":["Daniel Bogdoll","Jan Imhof","Tim Joseph","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2406.06423v1.pdf","comment":"Daniel Bogdoll and Jan Imhof contributed equally"},{"id":"http://arxiv.org/abs/2402.06353v2","updated":"2024-06-10T15:58:53Z","published":"2024-02-09T12:01:22Z","title":"Copycats: the many lives of a publicly available medical imaging dataset","summary":"  Medical Imaging (MI) datasets are fundamental to artificial intelligence in\nhealthcare. The accuracy, robustness, and fairness of diagnostic algorithms\ndepend on the data (and its quality) used to train and evaluate the models. MI\ndatasets used to be proprietary, but have become increasingly available to the\npublic, including on community-contributed platforms (CCPs) like Kaggle or\nHuggingFace. While open data is important to enhance the redistribution of\ndata's public value, we find that the current CCP governance model fails to\nuphold the quality needed and recommended practices for sharing, documenting,\nand evaluating datasets. In this paper, we conduct an analysis of publicly\navailable machine learning datasets on CCPs, discussing datasets' context, and\nidentifying limitations and gaps in the current CCP landscape. We highlight\ndifferences between MI and computer vision datasets, particularly in the\npotentially harmful downstream effects from poor adoption of recommended\ndataset management practices. We compare the analyzed datasets across several\ndimensions, including data sharing, data documentation, and maintenance. We\nfind vague licenses, lack of persistent identifiers and storage, duplicates,\nand missing metadata, with differences between the platforms. Our research\ncontributes to efforts in responsible data curation and AI algorithms for\nhealthcare.\n","authors":["Amelia Jiménez-Sánchez","Natalia-Rozalia Avlona","Dovile Juodelyte","Théo Sourget","Caroline Vang-Larsen","Anna Rogers","Hubert Dariusz Zając","Veronika Cheplygina"],"pdf_url":"https://arxiv.org/pdf/2402.06353v2.pdf","comment":"Manuscript under review"},{"id":"http://arxiv.org/abs/2406.06393v1","updated":"2024-06-10T15:48:07Z","published":"2024-06-10T15:48:07Z","title":"STimage-1K4M: A histopathology image-gene expression dataset for spatial\n  transcriptomics","summary":"  Recent advances in multi-modal algorithms have driven and been driven by the\nincreasing availability of large image-text datasets, leading to significant\nstrides in various fields, including computational pathology. However, in most\nexisting medical image-text datasets, the text typically provides high-level\nsummaries that may not sufficiently describe sub-tile regions within a large\npathology image. For example, an image might cover an extensive tissue area\ncontaining cancerous and healthy regions, but the accompanying text might only\nspecify that this image is a cancer slide, lacking the nuanced details needed\nfor in-depth analysis. In this study, we introduce STimage-1K4M, a novel\ndataset designed to bridge this gap by providing genomic features for sub-tile\nimages. STimage-1K4M contains 1,149 images derived from spatial transcriptomics\ndata, which captures gene expression information at the level of individual\nspatial spots within a pathology image. Specifically, each image in the dataset\nis broken down into smaller sub-image tiles, with each tile paired with\n15,000-30,000 dimensional gene expressions. With 4,293,195 pairs of sub-tile\nimages and gene expressions, STimage-1K4M offers unprecedented granularity,\npaving the way for a wide range of advanced research in multi-modal data\nanalysis an innovative applications in computational pathology, and beyond.\n","authors":["Jiawen Chen","Muqing Zhou","Wenrong Wu","Jinwei Zhang","Yun Li","Didong Li"],"pdf_url":"https://arxiv.org/pdf/2406.06393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06386v1","updated":"2024-06-10T15:44:41Z","published":"2024-06-10T15:44:41Z","title":"FPN-IAIA-BL: A Multi-Scale Interpretable Deep Learning Model for\n  Classification of Mass Margins in Digital Mammography","summary":"  Digital mammography is essential to breast cancer detection, and deep\nlearning offers promising tools for faster and more accurate mammogram\nanalysis. In radiology and other high-stakes environments, uninterpretable\n(\"black box\") deep learning models are unsuitable and there is a call in these\nfields to make interpretable models. Recent work in interpretable computer\nvision provides transparency to these formerly black boxes by utilizing\nprototypes for case-based explanations, achieving high accuracy in applications\nincluding mammography. However, these models struggle with precise feature\nlocalization, reasoning on large portions of an image when only a small part is\nrelevant. This paper addresses this gap by proposing a novel multi-scale\ninterpretable deep learning model for mammographic mass margin classification.\nOur contribution not only offers an interpretable model with reasoning aligned\nwith radiologist practices, but also provides a general architecture for\ncomputer vision with user-configurable prototypes from coarse- to fine-grained\nprototypes.\n","authors":["Julia Yang","Alina Jade Barnett","Jon Donnelly","Satvik Kishore","Jerry Fang","Fides Regina Schwartz","Chaofan Chen","Joseph Y. Lo","Cynthia Rudin"],"pdf_url":"https://arxiv.org/pdf/2406.06386v1.pdf","comment":"8 pages, 6 figures, Accepted for oral presentation at the 2024 CVPR\n  Workshop on Domain adaptation, Explainability, Fairness in AI for Medical\n  Image Analysis (DEF-AI-MIA)"},{"id":"http://arxiv.org/abs/2406.06384v1","updated":"2024-06-10T15:43:56Z","published":"2024-06-10T15:43:56Z","title":"Generalizing to Unseen Domains in Diabetic Retinopathy with Disentangled\n  Representations","summary":"  Diabetic Retinopathy (DR), induced by diabetes, poses a significant risk of\nvisual impairment. Accurate and effective grading of DR aids in the treatment\nof this condition. Yet existing models experience notable performance\ndegradation on unseen domains due to domain shifts. Previous methods address\nthis issue by simulating domain style through simple visual transformation and\nmitigating domain noise via learning robust representations. However, domain\nshifts encompass more than image styles. They overlook biases caused by\nimplicit factors such as ethnicity, age, and diagnostic criteria. In our work,\nwe propose a novel framework where representations of paired data from\ndifferent domains are decoupled into semantic features and domain noise. The\nresulting augmented representation comprises original retinal semantics and\ndomain noise from other domains, aiming to generate enhanced representations\naligned with real-world clinical needs, incorporating rich information from\ndiverse domains. Subsequently, to improve the robustness of the decoupled\nrepresentations, class and domain prototypes are employed to interpolate the\ndisentangled representations while data-aware weights are designed to focus on\nrare classes and domains. Finally, we devise a robust pixel-level semantic\nalignment loss to align retinal semantics decoupled from features, maintaining\na balance between intra-class diversity and dense class features. Experimental\nresults on multiple benchmarks demonstrate the effectiveness of our method on\nunseen domains. The code implementations are accessible on\nhttps://github.com/richard-peng-xia/DECO.\n","authors":["Peng Xia","Ming Hu","Feilong Tang","Wenxue Li","Wenhao Zheng","Lie Ju","Peibo Duan","Huaxiu Yao","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2406.06384v1.pdf","comment":"Early Accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.06382v1","updated":"2024-06-10T15:42:03Z","published":"2024-06-10T15:42:03Z","title":"Diffusion-RPO: Aligning Diffusion Models through Relative Preference\n  Optimization","summary":"  Aligning large language models with human preferences has emerged as a\ncritical focus in language modeling research. Yet, integrating preference\nlearning into Text-to-Image (T2I) generative models is still relatively\nuncharted territory. The Diffusion-DPO technique made initial strides by\nemploying pairwise preference learning in diffusion models tailored for\nspecific text prompts. We introduce Diffusion-RPO, a new method designed to\nalign diffusion-based T2I models with human preferences more effectively. This\napproach leverages both prompt-image pairs with identical prompts and those\nwith semantically related content across various modalities. Furthermore, we\nhave developed a new evaluation metric, style alignment, aimed at overcoming\nthe challenges of high costs, low reproducibility, and limited interpretability\nprevalent in current evaluations of human preference alignment. Our findings\ndemonstrate that Diffusion-RPO outperforms established methods such as\nSupervised Fine-Tuning and Diffusion-DPO in tuning Stable Diffusion versions\n1.5 and XL-1.0, achieving superior results in both automated evaluations of\nhuman preferences and style alignment. Our code is available at\nhttps://github.com/yigu1008/Diffusion-RPO\n","authors":["Yi Gu","Zhendong Wang","Yueqin Yin","Yujia Xie","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.06382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06374v1","updated":"2024-06-10T15:36:23Z","published":"2024-06-10T15:36:23Z","title":"Multicam-SLAM: Non-overlapping Multi-camera SLAM for Indirect Visual\n  Localization and Navigation","summary":"  This paper presents a novel approach to visual simultaneous localization and\nmapping (SLAM) using multiple RGB-D cameras. The proposed method,\nMulticam-SLAM, significantly enhances the robustness and accuracy of SLAM\nsystems by capturing more comprehensive spatial information from various\nperspectives. This method enables the accurate determination of pose\nrelationships among multiple cameras without the need for overlapping fields of\nview. The proposed Muticam-SLAM includes a unique multi-camera model, a\nmulti-keyframes structure, and several parallel SLAM threads. The multi-camera\nmodel allows for the integration of data from multiple cameras, while the\nmulti-keyframes and parallel SLAM threads ensure efficient and accurate pose\nestimation and mapping. Extensive experiments in various environments\ndemonstrate the superior accuracy and robustness of the proposed method\ncompared to conventional single-camera SLAM systems. The results highlight the\npotential of the proposed Multicam-SLAM for more complex and challenging\napplications. Code is available at\n\\url{https://github.com/AlterPang/Multi_ORB_SLAM}.\n","authors":["Shenghao Li","Luchao Pang","Xianglong Hu"],"pdf_url":"https://arxiv.org/pdf/2406.06374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06372v1","updated":"2024-06-10T15:34:23Z","published":"2024-06-10T15:34:23Z","title":"Improving Deep Learning-based Automatic Cranial Defect Reconstruction by\n  Heavy Data Augmentation: From Image Registration to Latent Diffusion Models","summary":"  Modeling and manufacturing of personalized cranial implants are important\nresearch areas that may decrease the waiting time for patients suffering from\ncranial damage. The modeling of personalized implants may be partially\nautomated by the use of deep learning-based methods. However, this task suffers\nfrom difficulties with generalizability into data from previously unseen\ndistributions that make it difficult to use the research outcomes in real\nclinical settings. Due to difficulties with acquiring ground-truth annotations,\ndifferent techniques to improve the heterogeneity of datasets used for training\nthe deep networks have to be considered and introduced. In this work, we\npresent a large-scale study of several augmentation techniques, varying from\nclassical geometric transformations, image registration, variational\nautoencoders, and generative adversarial networks, to the most recent advances\nin latent diffusion models. We show that the use of heavy data augmentation\nsignificantly increases both the quantitative and qualitative outcomes,\nresulting in an average Dice Score above 0.94 for the SkullBreak and above 0.96\nfor the SkullFix datasets. Moreover, we show that the synthetically augmented\nnetwork successfully reconstructs real clinical defects. The work is a\nconsiderable contribution to the field of artificial intelligence in the\nautomatic modeling of personalized cranial implants.\n","authors":["Marek Wodzinski","Kamil Kwarciak","Mateusz Daniol","Daria Hemmerling"],"pdf_url":"https://arxiv.org/pdf/2406.06372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06370v1","updated":"2024-06-10T15:32:16Z","published":"2024-06-10T15:32:16Z","title":"UMAD: Unsupervised Mask-Level Anomaly Detection for Autonomous Driving","summary":"  Dealing with atypical traffic scenarios remains a challenging task in\nautonomous driving. However, most anomaly detection approaches cannot be\ntrained on raw sensor data but require exposure to outlier data and powerful\nsemantic segmentation models trained in a supervised fashion. This limits the\nrepresentation of normality to labeled data, which does not scale well. In this\nwork, we revisit unsupervised anomaly detection and present UMAD, leveraging\ngenerative world models and unsupervised image segmentation. Our method\noutperforms state-of-the-art unsupervised anomaly detection.\n","authors":["Daniel Bogdoll","Noël Ollick","Tim Joseph","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2406.06370v1.pdf","comment":"Daniel Bogdoll and No\\\"el Ollick contributed equally"},{"id":"http://arxiv.org/abs/2402.09944v2","updated":"2024-06-10T15:28:53Z","published":"2024-02-14T18:18:32Z","title":"Loopy-SLAM: Dense Neural SLAM with Loop Closures","summary":"  Neural RGBD SLAM techniques have shown promise in dense Simultaneous\nLocalization And Mapping (SLAM), yet face challenges such as error accumulation\nduring camera tracking resulting in distorted maps. In response, we introduce\nLoopy-SLAM that globally optimizes poses and the dense 3D model. We use\nframe-to-model tracking using a data-driven point-based submap generation\nmethod and trigger loop closures online by performing global place recognition.\nRobust pose graph optimization is used to rigidly align the local submaps. As\nour representation is point based, map corrections can be performed efficiently\nwithout the need to store the entire history of input frames used for mapping\nas typically required by methods employing a grid based mapping structure.\nEvaluation on the synthetic Replica and real-world TUM-RGBD and ScanNet\ndatasets demonstrate competitive or superior performance in tracking, mapping,\nand rendering accuracy when compared to existing dense neural RGBD SLAM\nmethods. Project page: notchla.github.io/Loopy-SLAM.\n","authors":["Lorenzo Liso","Erik Sandström","Vladimir Yugay","Luc Van Gool","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2402.09944v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06634v2","updated":"2024-06-10T15:28:16Z","published":"2024-05-10T17:51:35Z","title":"Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA\n  Benchmark","summary":"  We evaluate the zero-shot ability of GPT-4 and LLaVa to perform simple Visual\nNetwork Analysis (VNA) tasks on small-scale graphs. We evaluate the Vision\nLanguage Models (VLMs) on 5 tasks related to three foundational network science\nconcepts: identifying nodes of maximal degree on a rendered graph, identifying\nwhether signed triads are balanced or unbalanced, and counting components. The\ntasks are structured to be easy for a human who understands the underlying\ngraph theoretic concepts, and can all be solved by counting the appropriate\nelements in graphs. We find that while GPT-4 consistently outperforms LLaVa,\nboth models struggle with every visual network analysis task we propose. We\npublicly release the first benchmark for the evaluation of VLMs on foundational\nVNA tasks.\n","authors":["Evan M. Williams","Kathleen M. Carley"],"pdf_url":"https://arxiv.org/pdf/2405.06634v2.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.06367v1","updated":"2024-06-10T15:26:48Z","published":"2024-06-10T15:26:48Z","title":"MVGamba: Unify 3D Content Generation as State Space Sequence Modeling","summary":"  Recent 3D large reconstruction models (LRMs) can generate high-quality 3D\ncontent in sub-seconds by integrating multi-view diffusion models with scalable\nmulti-view reconstructors. Current works further leverage 3D Gaussian Splatting\nas 3D representation for improved visual quality and rendering efficiency.\nHowever, we observe that existing Gaussian reconstruction models often suffer\nfrom multi-view inconsistency and blurred textures. We attribute this to the\ncompromise of multi-view information propagation in favor of adopting powerful\nyet computationally intensive architectures (\\eg, Transformers). To address\nthis issue, we introduce MVGamba, a general and lightweight Gaussian\nreconstruction model featuring a multi-view Gaussian reconstructor based on the\nRNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal\ncontext containing multi-view information for cross-view self-refinement while\ngenerating a long sequence of Gaussians for fine-detail modeling with linear\ncomplexity. With off-the-shelf multi-view diffusion models integrated, MVGamba\nunifies 3D generation tasks from a single image, sparse images, or text\nprompts. Extensive experiments demonstrate that MVGamba outperforms\nstate-of-the-art baselines in all 3D content generation scenarios with\napproximately only $0.1\\times$ of the model size.\n","authors":["Xuanyu Yi","Zike Wu","Qiuhong Shen","Qingshan Xu","Pan Zhou","Joo-Hwee Lim","Shuicheng Yan","Xinchao Wang","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00425v2","updated":"2024-06-10T15:21:41Z","published":"2024-03-01T10:21:52Z","title":"HALC: Object Hallucination Reduction via Adaptive Focal-Contrast\n  Decoding","summary":"  While large vision-language models (LVLMs) have demonstrated impressive\ncapabilities in interpreting multi-modal contexts, they invariably suffer from\nobject hallucinations (OH). We introduce HALC, a novel decoding algorithm\ndesigned to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal\nvisual information in vision-language tasks and operates on both local and\nglobal contexts simultaneously. Specifically, HALC integrates a robust\nauto-focal grounding mechanism (locally) to correct hallucinated tokens on the\nfly, and a specialized beam search algorithm (globally) to significantly reduce\nOH while preserving text generation quality. Additionally, HALC can be\nintegrated into any LVLMs as a plug-and-play module without extra training.\nExtensive experimental studies demonstrate the effectiveness of HALC in\nreducing OH, outperforming state-of-the-arts across four benchmarks.\n","authors":["Zhaorun Chen","Zhuokai Zhao","Hongyin Luo","Huaxiu Yao","Bo Li","Jiawei Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.00425v2.pdf","comment":"ICML camera-ready version. Code is released at\n  https://github.com/BillChan226/HALC"},{"id":"http://arxiv.org/abs/2406.06352v1","updated":"2024-06-10T15:13:51Z","published":"2024-06-10T15:13:51Z","title":"Latent Directions: A Simple Pathway to Bias Mitigation in Generative AI","summary":"  Mitigating biases in generative AI and, particularly in text-to-image models,\nis of high importance given their growing implications in society. The biased\ndatasets used for training pose challenges in ensuring the responsible\ndevelopment of these models, and mitigation through hard prompting or embedding\nalteration, are the most common present solutions. Our work introduces a novel\napproach to achieve diverse and inclusive synthetic images by learning a\ndirection in the latent space and solely modifying the initial Gaussian noise\nprovided for the diffusion process. Maintaining a neutral prompt and untouched\nembeddings, this approach successfully adapts to diverse debiasing scenarios,\nsuch as geographical biases. Moreover, our work proves it is possible to\nlinearly combine these learned latent directions to introduce new mitigations,\nand if desired, integrate it with text embedding adjustments. Furthermore,\ntext-to-image models lack transparency for assessing bias in outputs, unless\nvisually inspected. Thus, we provide a tool to empower developers to select\ntheir desired concepts to mitigate. The project page with code is available\nonline.\n","authors":["Carolina Lopez Olmos","Alexandros Neophytou","Sunando Sengupta","Dim P. Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2406.06352v1.pdf","comment":"Accepted at CVPR workshop 2024, proceedings of ReGenAI: First\n  Workshop on Responsible Generative AI"},{"id":"http://arxiv.org/abs/2406.06351v1","updated":"2024-06-10T15:13:07Z","published":"2024-06-10T15:13:07Z","title":"Cascading Unknown Detection with Known Classification for Open Set\n  Recognition","summary":"  Deep learners tend to perform well when trained under the closed set\nassumption but struggle when deployed under open set conditions. This motivates\nthe field of Open Set Recognition in which we seek to give deep learners the\nability to recognize whether a data sample belongs to the known classes trained\non or comes from the surrounding infinite world. Existing open set recognition\nmethods typically rely upon a single function for the dual task of\ndistinguishing between knowns and unknowns as well as making known class\ndistinction. This dual process leaves performance on the table as the function\nis not specialized for either task. In this work, we introduce Cascading\nUnknown Detection with Known Classification (Cas-DC), where we instead learn\nspecialized functions in a cascading fashion for both known/unknown detection\nand fine class classification amongst the world of knowns. Our experiments and\nanalysis demonstrate that Cas-DC handily outperforms modern methods in open set\nrecognition when compared using AUROC scores and correct classification rate at\nvarious true positive rates.\n","authors":["Daniel Brignac","Abhijit Mahalanobis"],"pdf_url":"https://arxiv.org/pdf/2406.06351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08252v4","updated":"2024-06-10T15:11:40Z","published":"2023-05-14T21:18:18Z","title":"Parameter-Efficient Fine-Tuning for Medical Image Analysis: The Missed\n  Opportunity","summary":"  Foundation models have significantly advanced medical image analysis through\nthe pre-train fine-tune paradigm. Among various fine-tuning algorithms,\nParameter-Efficient Fine-Tuning (PEFT) is increasingly utilized for knowledge\ntransfer across diverse tasks, including vision-language and text-to-image\ngeneration. However, its application in medical image analysis is relatively\nunexplored due to the lack of a structured benchmark for evaluating PEFT\nmethods. This study fills this gap by evaluating 17 distinct PEFT algorithms\nacross convolutional and transformer-based networks on image classification and\ntext-to-image generation tasks using six medical datasets of varying size,\nmodality, and complexity. Through a battery of over 700 controlled experiments,\nour findings demonstrate PEFT's effectiveness, particularly in low data regimes\ncommon in medical imaging, with performance gains of up to 22% in\ndiscriminative and generative tasks. These recommendations can assist the\ncommunity in incorporating PEFT into their workflows and facilitate fair\ncomparisons of future PEFT methods, ensuring alignment with advancements in\nother areas of machine learning and AI.\n","authors":["Raman Dutt","Linus Ericsson","Pedro Sanchez","Sotirios A. Tsaftaris","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2305.08252v4.pdf","comment":"Accepted as Oral Presentation at MIDL 2024"},{"id":"http://arxiv.org/abs/2401.12946v7","updated":"2024-06-10T15:10:49Z","published":"2024-01-23T18:07:07Z","title":"Coverage Axis++: Efficient Inner Point Selection for 3D Shape\n  Skeletonization","summary":"  We introduce Coverage Axis++, a novel and efficient approach to 3D shape\nskeletonization. The current state-of-the-art approaches for this task often\nrely on the watertightness of the input or suffer from substantial\ncomputational costs, thereby limiting their practicality. To address this\nchallenge, Coverage Axis++ proposes a heuristic algorithm to select skeletal\npoints, offering a high-accuracy approximation of the Medial Axis Transform\n(MAT) while significantly mitigating computational intensity for various shape\nrepresentations. We introduce a simple yet effective strategy that considers\nshape coverage, uniformity, and centrality to derive skeletal points. The\nselection procedure enforces consistency with the shape structure while\nfavoring the dominant medial balls, which thus introduces a compact underlying\nshape representation in terms of MAT. As a result, Coverage Axis++ allows for\nskeletonization for various shape representations (e.g., water-tight meshes,\ntriangle soups, point clouds), specification of the number of skeletal points,\nfew hyperparameters, and highly efficient computation with improved\nreconstruction accuracy. Extensive experiments across a wide range of 3D shapes\nvalidate the efficiency and effectiveness of Coverage Axis++. Our codes are\navailable at https://github.com/Frank-ZY-Dou/Coverage_Axis.\n","authors":["Zimeng Wang","Zhiyang Dou","Rui Xu","Cheng Lin","Yuan Liu","Xiaoxiao Long","Shiqing Xin","Taku Komura","Xiaoming Yuan","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2401.12946v7.pdf","comment":"SGP2024. Project Page:\n  https://frank-zy-dou.github.io/projects/CoverageAxis++/index.html"},{"id":"http://arxiv.org/abs/2312.09558v3","updated":"2024-06-10T15:10:41Z","published":"2023-12-15T06:33:14Z","title":"Towards Transferable Targeted 3D Adversarial Attack in the Physical\n  World","summary":"  Compared with transferable untargeted attacks, transferable targeted\nadversarial attacks could specify the misclassification categories of\nadversarial samples, posing a greater threat to security-critical tasks. In the\nmeanwhile, 3D adversarial samples, due to their potential of multi-view\nrobustness, can more comprehensively identify weaknesses in existing deep\nlearning systems, possessing great application value. However, the field of\ntransferable targeted 3D adversarial attacks remains vacant. The goal of this\nwork is to develop a more effective technique that could generate transferable\ntargeted 3D adversarial examples, filling the gap in this field. To achieve\nthis goal, we design a novel framework named TT3D that could rapidly\nreconstruct from few multi-view images into Transferable Targeted 3D textured\nmeshes. While existing mesh-based texture optimization methods compute\ngradients in the high-dimensional mesh space and easily fall into local optima,\nleading to unsatisfactory transferability and distinct distortions, TT3D\ninnovatively performs dual optimization towards both feature grid and\nMulti-layer Perceptron (MLP) parameters in the grid-based NeRF space, which\nsignificantly enhances black-box transferability while enjoying naturalness.\nExperimental results show that TT3D not only exhibits superior cross-model\ntransferability but also maintains considerable adaptability across different\nrenders and vision tasks. More importantly, we produce 3D adversarial examples\nwith 3D printing techniques in the real world and verify their robust\nperformance under various scenarios.\n","authors":["Yao Huang","Yinpeng Dong","Shouwei Ruan","Xiao Yang","Hang Su","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2312.09558v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2406.06342v1","updated":"2024-06-10T15:02:30Z","published":"2024-06-10T15:02:30Z","title":"A Guide to Stochastic Optimisation for Large-Scale Inverse Problems","summary":"  Stochastic optimisation algorithms are the de facto standard for machine\nlearning with large amounts of data. Handling only a subset of available data\nin each optimisation step dramatically reduces the per-iteration computational\ncosts, while still ensuring significant progress towards the solution. Driven\nby the need to solve large-scale optimisation problems as efficiently as\npossible, the last decade has witnessed an explosion of research in this area.\nLeveraging the parallels between machine learning and inverse problems has\nallowed harnessing the power of this research wave for solving inverse\nproblems. In this survey, we provide a comprehensive account of the\nstate-of-the-art in stochastic optimisation from the viewpoint of inverse\nproblems. We present algorithms with diverse modalities of problem\nrandomisation and discuss the roles of variance reduction, acceleration,\nhigher-order methods, and other algorithmic modifications, and compare\ntheoretical results with practical behaviour. We focus on the potential and the\nchallenges for stochastic optimisation that are unique to inverse imaging\nproblems and are not commonly encountered in machine learning. We conclude the\nsurvey with illustrative examples from imaging problems to examine the\nadvantages and disadvantages that this new generation of algorithms bring to\nthe field of inverse problems.\n","authors":["Matthias J. Ehrhardt","Zeljko Kereta","Jingwei Liang","Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2406.06342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20287v2","updated":"2024-06-10T14:47:46Z","published":"2024-03-29T16:58:13Z","title":"Benchmarking Counterfactual Image Generation","summary":"  Generative AI has revolutionised visual content editing, empowering users to\neffortlessly modify images and videos. However, not all edits are equal. To\nperform realistic edits in domains such as natural image or medical imaging,\nmodifications must respect causal relationships inherent to the data generation\nprocess. Such image editing falls into the counterfactual image generation\nregime. Evaluating counterfactual image generation is substantially complex:\nnot only it lacks observable ground truths, but also requires adherence to\ncausal constraints. Although several counterfactual image generation methods\nand evaluation metrics exist, a comprehensive comparison within a unified\nsetting is lacking. We present a comparison framework to thoroughly benchmark\ncounterfactual image generation methods. We integrate all models that have been\nused for the task at hand and expand them to novel datasets and causal graphs,\ndemonstrating the superiority of Hierarchical VAEs across most datasets and\nmetrics. Our framework is implemented in a user-friendly Python package that\ncan be extended to incorporate additional SCMs, causal methods, generative\nmodels, and datasets for the community to build on.\n","authors":["Thomas Melistas","Nikos Spyrou","Nefeli Gkouti","Pedro Sanchez","Athanasios Vlontzos","Yannis Panagakis","Giorgos Papanastasiou","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2403.20287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05264v4","updated":"2024-06-10T14:37:45Z","published":"2023-10-08T19:02:46Z","title":"The Emergence of Reproducibility and Generalizability in Diffusion\n  Models","summary":"  In this work, we investigate an intriguing and prevalent phenomenon of\ndiffusion models which we term as \"consistent model reproducibility\": given the\nsame starting noise input and a deterministic sampler, different diffusion\nmodels often yield remarkably similar outputs. We confirm this phenomenon\nthrough comprehensive experiments, implying that different diffusion models\nconsistently reach the same data distribution and scoring function regardless\nof diffusion model frameworks, model architectures, or training procedures.\nMore strikingly, our further investigation implies that diffusion models are\nlearning distinct distributions affected by the training data size. This is\nsupported by the fact that the model reproducibility manifests in two distinct\ntraining regimes: (i) \"memorization regime\", where the diffusion model overfits\nto the training data distribution, and (ii) \"generalization regime\", where the\nmodel learns the underlying data distribution. Our study also finds that this\nvaluable property generalizes to many variants of diffusion models, including\nthose for conditional use, solving inverse problems, and model fine-tuning.\nFinally, our work raises numerous intriguing theoretical questions for future\ninvestigation and highlights practical implications regarding training\nefficiency, model privacy, and the controlled generation of diffusion models.\n","authors":["Huijie Zhang","Jinfan Zhou","Yifu Lu","Minzhe Guo","Peng Wang","Liyue Shen","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2310.05264v4.pdf","comment":"NeurIPS Diffusion Model Workshop 2023 (best paper award), the\n  Forty-first International Conference on Machine Learning (ICML 2024)"},{"id":"http://arxiv.org/abs/2312.09181v2","updated":"2024-06-10T14:37:41Z","published":"2023-12-14T17:48:09Z","title":"Improving Efficiency of Diffusion Models via Multi-Stage Framework and\n  Tailored Multi-Decoder Architectures","summary":"  Diffusion models, emerging as powerful deep generative tools, excel in\nvarious applications. They operate through a two-steps process: introducing\nnoise into training samples and then employing a model to convert random noise\ninto new samples (e.g., images). However, their remarkable generative\nperformance is hindered by slow training and sampling. This is due to the\nnecessity of tracking extensive forward and reverse diffusion trajectories, and\nemploying a large model with numerous parameters across multiple timesteps\n(i.e., noise levels). To tackle these challenges, we present a multi-stage\nframework inspired by our empirical findings. These observations indicate the\nadvantages of employing distinct parameters tailored to each timestep while\nretaining universal parameters shared across all time steps. Our approach\ninvolves segmenting the time interval into multiple stages where we employ\ncustom multi-decoder U-net architecture that blends time-dependent models with\na universally shared encoder. Our framework enables the efficient distribution\nof computational resources and mitigates inter-stage interference, which\nsubstantially improves training efficiency. Extensive numerical experiments\naffirm the effectiveness of our framework, showcasing significant training and\nsampling efficiency enhancements on three state-of-the-art diffusion models,\nincluding large-scale latent diffusion models. Furthermore, our ablation\nstudies illustrate the impact of two important components in our framework: (i)\na novel timestep clustering algorithm for stage division, and (ii) an\ninnovative multi-decoder U-net architecture, seamlessly integrating universal\nand customized hyperparameters.\n","authors":["Huijie Zhang","Yifu Lu","Ismail Alkhouri","Saiprasad Ravishankar","Dogyoon Song","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2312.09181v2.pdf","comment":"The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2024"},{"id":"http://arxiv.org/abs/2406.06320v1","updated":"2024-06-10T14:35:59Z","published":"2024-06-10T14:35:59Z","title":"Vehicle Vectors and Traffic Patterns from Planet Imagery","summary":"  We explore methods to detect automobiles in Planet imagery and build a large\nscale vector field for moving objects. Planet operates two distinct\nconstellations: high-resolution SkySat satellites as well as medium-resolution\nSuperDove satellites. We show that both static and moving cars can be\nidentified reliably in high-resolution SkySat imagery. We are able to estimate\nthe speed and heading of moving vehicles by leveraging the inter-band\ndisplacement (or \"rainbow\" effect) of moving objects. Identifying cars and\ntrucks in medium-resolution SuperDove imagery is far more difficult, though a\nsimilar rainbow effect is observed in these satellites and enables moving\nvehicles to be detected and vectorized. The frequent revisit of Planet\nsatellites enables the categorization of automobile and truck activity patterns\nover broad areas of interest and lengthy timeframes.\n","authors":["Adam Van Etten"],"pdf_url":"https://arxiv.org/pdf/2406.06320v1.pdf","comment":"8 pages, 15 figures"},{"id":"http://arxiv.org/abs/2211.10858v3","updated":"2024-06-10T14:28:18Z","published":"2022-11-20T03:33:33Z","title":"An interpretable imbalanced semi-supervised deep learning framework for\n  improving differential diagnosis of skin diseases","summary":"  Dermatological diseases are among the most common disorders worldwide. This\npaper presents the first study of the interpretability and imbalanced\nsemi-supervised learning of the multiclass intelligent skin diagnosis framework\n(ISDL) using 58,457 skin images with 10,857 unlabeled samples. Pseudo-labelled\nsamples from minority classes have a higher probability at each iteration of\nclass-rebalancing self-training, thereby promoting the utilization of unlabeled\nsamples to solve the class imbalance problem. Our ISDL achieved a promising\nperformance with an accuracy of 0.979, sensitivity of 0.975, specificity of\n0.973, macro-F1 score of 0.974 and area under the receiver operating\ncharacteristic curve (AUC) of 0.999 for multi-label skin disease\nclassification. The Shapley Additive explanation (SHAP) method is combined with\nour ISDL to explain how the deep learning model makes predictions. This finding\nis consistent with the clinical diagnosis. We also proposed a sampling\ndistribution optimisation strategy to select pseudo-labelled samples in a more\neffective manner using ISDLplus. Furthermore, it has the potential to relieve\nthe pressure placed on professional doctors, as well as help with practical\nissues associated with a shortage of such doctors in rural areas.\n","authors":["Futian Weng","Yuanting Ma","Jinghan Sun","Shijun Shan","Qiyuan Li","Jianping Zhu","Yang Wang","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2211.10858v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07087v3","updated":"2024-06-10T14:22:45Z","published":"2024-02-11T02:34:42Z","title":"Self-Correcting Self-Consuming Loops for Generative Model Training","summary":"  As synthetic data becomes higher quality and proliferates on the internet,\nmachine learning models are increasingly trained on a mix of human- and\nmachine-generated data. Despite the successful stories of using synthetic data\nfor representation learning, using synthetic data for generative model training\ncreates \"self-consuming loops\" which may lead to training instability or even\ncollapse, unless certain conditions are met. Our paper aims to stabilize\nself-consuming generative model training. Our theoretical results demonstrate\nthat by introducing an idealized correction function, which maps a data point\nto be more likely under the true data distribution, self-consuming loops can be\nmade exponentially more stable. We then propose self-correction functions,\nwhich rely on expert knowledge (e.g. the laws of physics programmed in a\nsimulator), and aim to approximate the idealized corrector automatically and at\nscale. We empirically validate the effectiveness of self-correcting\nself-consuming loops on the challenging human motion synthesis task, and\nobserve that it successfully avoids model collapse, even when the ratio of\nsynthetic data to real data is as high as 100%.\n","authors":["Nate Gillman","Michael Freeman","Daksh Aggarwal","Chia-Hong Hsu","Calvin Luo","Yonglong Tian","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2402.07087v3.pdf","comment":"Camera ready version (ICML 2024). Code at\n  https://nategillman.com/sc-sc.html"},{"id":"http://arxiv.org/abs/2406.06305v1","updated":"2024-06-10T14:20:48Z","published":"2024-06-10T14:20:48Z","title":"NeuroMoCo: A Neuromorphic Momentum Contrast Learning Method for Spiking\n  Neural Networks","summary":"  Recently, brain-inspired spiking neural networks (SNNs) have attracted great\nresearch attention owing to their inherent bio-interpretability,\nevent-triggered properties and powerful perception of spatiotemporal\ninformation, which is beneficial to handling event-based neuromorphic datasets.\nIn contrast to conventional static image datasets, event-based neuromorphic\ndatasets present heightened complexity in feature extraction due to their\ndistinctive time series and sparsity characteristics, which influences their\nclassification accuracy. To overcome this challenge, a novel approach termed\nNeuromorphic Momentum Contrast Learning (NeuroMoCo) for SNNs is introduced in\nthis paper by extending the benefits of self-supervised pre-training to SNNs to\neffectively stimulate their potential. This is the first time that\nself-supervised learning (SSL) based on momentum contrastive learning is\nrealized in SNNs. In addition, we devise a novel loss function named MixInfoNCE\ntailored to their temporal characteristics to further increase the\nclassification accuracy of neuromorphic datasets, which is verified through\nrigorous ablation experiments. Finally, experiments on DVS-CIFAR10,\nDVS128Gesture and N-Caltech101 have shown that NeuroMoCo of this paper\nestablishes new state-of-the-art (SOTA) benchmarks: 83.6% (Spikformer-2-256),\n98.62% (Spikformer-2-256), and 84.4% (SEW-ResNet-18), respectively.\n","authors":["Yuqi Ma","Huamin Wang","Hangchi Shen","Xuemei Chen","Shukai Duan","Shiping Wen"],"pdf_url":"https://arxiv.org/pdf/2406.06305v1.pdf","comment":"32 pages,4 figures,4 tables"},{"id":"http://arxiv.org/abs/2406.06302v1","updated":"2024-06-10T14:18:56Z","published":"2024-06-10T14:18:56Z","title":"Unveiling the Safety of GPT-4o: An Empirical Study using Jailbreak\n  Attacks","summary":"  The recent release of GPT-4o has garnered widespread attention due to its\npowerful general capabilities. While its impressive performance is widely\nacknowledged, its safety aspects have not been sufficiently explored. Given the\npotential societal impact of risky content generated by advanced generative AI\nsuch as GPT-4o, it is crucial to rigorously evaluate its safety. In response to\nthis question, this paper for the first time conducts a rigorous evaluation of\nGPT-4o against jailbreak attacks. Specifically, this paper adopts a series of\nmulti-modal and uni-modal jailbreak attacks on 4 commonly used benchmarks\nencompassing three modalities (\\ie, text, speech, and image), which involves\nthe optimization of over 4,000 initial text queries and the analysis and\nstatistical evaluation of nearly 8,000+ response on GPT-4o. Our extensive\nexperiments reveal several novel observations: (1) In contrast to the previous\nversion (such as GPT-4V), GPT-4o has enhanced safety in the context of text\nmodality jailbreak; (2) The newly introduced audio modality opens up new attack\nvectors for jailbreak attacks on GPT-4o; (3) Existing black-box multimodal\njailbreak attack methods are largely ineffective against GPT-4o and GPT-4V.\nThese findings provide critical insights into the safety implications of GPT-4o\nand underscore the need for robust alignment guardrails in large models. Our\ncode is available at \\url{https://github.com/NY1024/Jailbreak_GPT4o}.\n","authors":["Zonghao Ying","Aishan Liu","Xianglong Liu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2406.06302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12477v2","updated":"2024-06-10T14:15:08Z","published":"2024-05-21T03:40:56Z","title":"Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery","summary":"  Although 3D Gaussian Splatting (3DGS) has recently made progress in 3D human\nreconstruction, it primarily relies on 2D pixel-level supervision, overlooking\nthe geometric complexity and topological relationships of different body parts.\nTo address this gap, we introduce the Hierarchical Graph Human Gaussian Control\n(HUGS) framework for achieving high-fidelity 3D human reconstruction. Our\napproach involves leveraging explicitly semantic priors of body parts to ensure\nthe consistency of geometric topology, thereby enabling the capture of the\ncomplex geometrical and topological associations among body parts.\nAdditionally, we disentangle high-frequency features from global human features\nto refine surface details in body parts. Extensive experiments demonstrate that\nour method exhibits superior performance in human body reconstruction,\nparticularly in enhancing surface details and accurately reconstructing body\npart junctions. Codes are available at https://wanghongsheng01.github.io/HUGS/.\n","authors":["Hongsheng Wang","Weiyue Zhang","Sihao Liu","Xinrui Zhou","Jing Li","Zhanyun Tang","Shengyu Zhang","Fei Wu","Feng Lin"],"pdf_url":"https://arxiv.org/pdf/2405.12477v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09717v2","updated":"2024-06-10T14:13:41Z","published":"2024-05-15T22:18:39Z","title":"From NeRFs to Gaussian Splats, and Back","summary":"  For robotics applications where there is a limited number of (typically\nego-centric) views, parametric representations such as neural radiance fields\n(NeRFs) generalize better than non-parametric ones such as Gaussian splatting\n(GS) to views that are very different from those in the training data; GS\nhowever can render much faster than NeRFs. We develop a procedure to convert\nback and forth between the two. Our approach achieves the best of both NeRFs\n(superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact\nrepresentation) and GS (real-time rendering and ability for easily modifying\nthe representation); the computational cost of these conversions is minor\ncompared to training the two from scratch.\n","authors":["Siming He","Zach Osman","Pratik Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2405.09717v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06131v3","updated":"2024-06-10T14:13:18Z","published":"2023-05-10T13:26:08Z","title":"Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era","summary":"  Generative AI (AIGC, a.k.a. AI generated content) has made significant\nprogress in recent years, with text-guided content generation being the most\npractical as it facilitates interaction between human instructions and AIGC.\nDue to advancements in text-to-image and 3D modeling technologies (like NeRF),\ntext-to-3D has emerged as a nascent yet highly active research field. Our work\nconducts the first comprehensive survey and follows up on subsequent research\nprogress in the overall field, aiming to help readers interested in this\ndirection quickly catch up with its rapid development. First, we introduce 3D\ndata representations, including both Euclidean and non-Euclidean data. Building\non this foundation, we introduce various foundational technologies and\nsummarize how recent work combines these foundational technologies to achieve\nsatisfactory text-to-3D results. Additionally, we present mainstream baselines\nand research directions in recent text-to-3D technology, including fidelity,\nefficiency, consistency, controllability, diversity, and applicability.\nFurthermore, we summarize the usage of text-to-3D technology in various\napplications, including avatar generation, texture generation, shape editing,\nand scene generation.\n","authors":["Chenghao Li","Chaoning Zhang","Atish Waghwase","Lik-Hang Lee","Francois Rameau","Yang Yang","Sung-Ho Bae","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2305.06131v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15374v2","updated":"2024-06-10T14:10:38Z","published":"2024-02-23T15:19:37Z","title":"Outlier detection by ensembling uncertainty with negative objectness","summary":"  Outlier detection is an essential capability in safety-critical applications\nof supervised visual recognition. Most of the existing methods deliver best\nresults by encouraging standard closed-set models to produce low-confidence\npredictions in negative training data. However, that approach conflates\nprediction uncertainty with recognition of the negative class. We therefore\nreconsider direct prediction of K+1 logits that correspond to K groundtruth\nclasses and one outlier class. This setup allows us to formulate a novel\nanomaly score as an ensemble of in-distribution uncertainty and the posterior\nof the outlier class which we term negative objectness. Now outliers can be\nindependently detected due to i) high prediction uncertainty or ii) similarity\nwith negative data. We embed our method into a dense prediction architecture\nwith mask-level recognition over K+2 classes. The training procedure encourages\nthe novel K+2-th class to learn negative objectness at pasted negative\ninstances. Our models outperform the current state-of-the art on standard\nbenchmarks for image-wide and pixel-level outlier detection with and without\ntraining on real negative data.\n","authors":["Anja Delić","Matej Grcić","Siniša Šegvić"],"pdf_url":"https://arxiv.org/pdf/2402.15374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17142v3","updated":"2024-06-10T14:07:53Z","published":"2023-12-28T17:16:44Z","title":"DreamGaussian4D: Generative 4D Gaussian Splatting","summary":"  4D content generation has achieved remarkable progress recently. However,\nexisting methods suffer from long optimization times, a lack of motion\ncontrollability, and a low quality of details. In this paper, we introduce\nDreamGaussian4D (DG4D), an efficient 4D generation framework that builds on\nGaussian Splatting (GS). Our key insight is that combining explicit modeling of\nspatial transformations with static GS makes an efficient and powerful\nrepresentation for 4D generation. Moreover, video generation methods have the\npotential to offer valuable spatial-temporal priors, enhancing the high-quality\n4D generation. Specifically, we propose an integral framework with two major\nmodules: 1) Image-to-4D GS - we initially generate static GS with\nDreamGaussianHD, followed by HexPlane-based dynamic generation with Gaussian\ndeformation; and 2) Video-to-Video Texture Refinement - we refine the generated\nUV-space texture maps and meanwhile enhance their temporal consistency by\nutilizing a pre-trained image-to-video diffusion model. Notably, DG4D reduces\nthe optimization time from several hours to just a few minutes, allows the\ngenerated 3D motion to be visually controlled, and produces animated meshes\nthat can be realistically rendered in 3D engines.\n","authors":["Jiawei Ren","Liang Pan","Jiaxiang Tang","Chi Zhang","Ang Cao","Gang Zeng","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2312.17142v3.pdf","comment":"Technical report. Project page is at\n  https://jiawei-ren.github.io/projects/dreamgaussian4d Code is at\n  https://github.com/jiawei-ren/dreamgaussian4d"},{"id":"http://arxiv.org/abs/2403.03849v4","updated":"2024-06-10T14:06:05Z","published":"2024-03-06T16:49:33Z","title":"MedMamba: Vision Mamba for Medical Image Classification","summary":"  Since the era of deep learning, convolutional neural networks (CNNs) and\nvision transformers (ViTs) have been extensively studied and widely used in\nmedical image classification tasks. Unfortunately, CNN's limitations in\nmodeling long-range dependencies result in poor classification performances. In\ncontrast, ViTs are hampered by the quadratic computational complexity of their\nself-attention mechanism, making them difficult to deploy in real-world\nsettings with limited computational resources. Recent studies have shown that\nstate space models (SSMs) represented by Mamba can effectively model long-range\ndependencies while maintaining linear computational complexity. Inspired by it,\nwe proposed MedMamba, the first vision Mamba for generalized medical image\nclassification. Concretely, we introduced a novel hybrid basic block named\nSS-Conv-SSM, which integrates the convolutional layers for extracting local\nfeatures with the abilities of SSM to capture long-range dependencies, aiming\nto model medical images from different image modalities efficiently. By\nemploying the grouped convolution strategy and channel-shuffle operation,\nMedMamba successfully provides fewer model parameters and a lower computational\nburden for efficient applications. To demonstrate the potential of MedMamba, we\nconducted extensive experiments using 16 datasets containing ten imaging\nmodalities and 411,007 images. Experimental results show that the proposed\nMedMamba demonstrates competitive performance in classifying various medical\nimages compared with the state-of-the-art methods. Our work is aims to\nestablish a new baseline for medical image classification and provide valuable\ninsights for developing more powerful SSM-based artificial intelligence\nalgorithms and application systems in the medical field. The source codes and\nall pre-trained weights of MedMamba are available at\nhttps://github.com/YubiaoYue/MedMamba.\n","authors":["Yubiao Yue","Zhenzhang Li"],"pdf_url":"https://arxiv.org/pdf/2403.03849v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03492v2","updated":"2024-06-10T14:04:43Z","published":"2024-02-05T20:08:53Z","title":"Beyond Strong labels: Weakly-supervised Learning Based on Gaussian\n  Pseudo Labels for The Segmentation of Ellipse-like Vascular Structures in\n  Non-contrast CTs","summary":"  Deep-learning-based automated segmentation of vascular structures in\npreoperative CT scans contributes to computer-assisted diagnosis and\nintervention procedure in vascular diseases. While CT angiography (CTA) is the\ncommon standard, non-contrast CT imaging is significant as a contrast-risk-free\nalternative, avoiding complications associated with contrast agents. However,\nthe challenges of labor-intensive labeling and high labeling variability due to\nthe ambiguity of vascular boundaries hinder conventional strong-label-based,\nfully-supervised learning in non-contrast CTs. This paper introduces a\nweakly-supervised framework using ellipses' topology in slices, including 1) an\nefficient annotation process based on predefined standards, 2) ellipse-fitting\nprocessing, 3) the generation of 2D Gaussian heatmaps serving as pseudo labels,\n4) a training process through a combination of voxel reconstruction loss and\ndistribution loss with the pseudo labels. We assess the effectiveness of the\nproposed method on one local and two public datasets comprising non-contrast CT\nscans, particularly focusing on the abdominal aorta. On the local dataset, our\nweakly-supervised learning approach based on pseudo labels outperforms\nstrong-label-based fully-supervised learning (1.54\\% of Dice score on average),\nreducing labeling time by around 82.0\\%. The efficiency in generating pseudo\nlabels allows the inclusion of label-agnostic external data in the training\nset, leading to an additional improvement in performance (2.74\\% of Dice score\non average) with a reduction of 66.3\\% labeling time, where the labeling time\nremains considerably less than that of strong labels. On the public dataset,\nthe pseudo labels achieve an overall improvement of 1.95\\% in Dice score for 2D\nmodels while a reduction of 11.65 voxel spacing in Hausdorff distance for 3D\nmodel.\n","authors":["Qixiang Ma","Antoine Łucas","Huazhong Shu","Adrien Kaladji","Pascal Haigron"],"pdf_url":"https://arxiv.org/pdf/2402.03492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07798v2","updated":"2024-06-10T13:55:21Z","published":"2024-05-13T14:42:13Z","title":"FreeVA: Offline MLLM as Training-Free Video Assistant","summary":"  This paper undertakes an empirical study to revisit the latest advancements\nin Multimodal Large Language Models (MLLMs): Video Assistant. This study,\nnamely FreeVA, aims to extend existing image-based MLLM to the video domain in\na training-free manner. The study provides an essential, yet must-know\nbaseline, and reveals several surprising findings: 1) FreeVA, leveraging only\noffline image-based MLLM without additional training, excels in zero-shot video\nquestion-answering (e.g., MSVD-QA, ActivityNet-QA, and MSRVTT-QA), even\nsurpassing state-of-the-art methods that involve video instruction tuning. 2)\nWhile mainstream video-based MLLMs typically initialize with an image-based\nMLLM (e.g., LLaVA) and then fine-tune using video instruction tuning, the study\nindicates that utilizing the widely adopted VideoInstruct-100K for video\ninstruction tuning doesn't actually lead to better performance compared to not\ntraining at all. 3) The commonly used evaluation metrics in existing works are\nsignificantly influenced by changes in the GPT API version over time. If\nignored, this could affect the fairness and uniformity of comparisons between\ndifferent methods and impact the analysis and judgment of researchers in the\nfield. The advancement of MLLMs is currently thriving, drawing numerous\nresearchers into the field. We aim for this work to serve as a plug-and-play,\nsimple yet effective baseline, encouraging the direct evaluation of existing\nMLLMs in video domain while also standardizing the field of video\nconversational models to a certain extent. Also, we encourage researchers to\nreconsider: Have current video MLLM methods truly acquired knowledge beyond\nimage MLLM? Code is available at https://github.com/whwu95/FreeVA\n","authors":["Wenhao Wu"],"pdf_url":"https://arxiv.org/pdf/2405.07798v2.pdf","comment":"Preprint. Work in progress"},{"id":"http://arxiv.org/abs/2406.06264v1","updated":"2024-06-10T13:46:07Z","published":"2024-06-10T13:46:07Z","title":"DualAD: Disentangling the Dynamic and Static World for End-to-End\n  Driving","summary":"  State-of-the-art approaches for autonomous driving integrate multiple\nsub-tasks of the overall driving task into a single pipeline that can be\ntrained in an end-to-end fashion by passing latent representations between the\ndifferent modules. In contrast to previous approaches that rely on a unified\ngrid to represent the belief state of the scene, we propose dedicated\nrepresentations to disentangle dynamic agents and static scene elements. This\nallows us to explicitly compensate for the effect of both ego and object motion\nbetween consecutive time steps and to flexibly propagate the belief state\nthrough time. Furthermore, dynamic objects can not only attend to the input\ncamera images, but also directly benefit from the inferred static scene\nstructure via a novel dynamic-static cross-attention. Extensive experiments on\nthe challenging nuScenes benchmark demonstrate the benefits of the proposed\ndual-stream design, especially for modelling highly dynamic agents in the\nscene, and highlight the improved temporal consistency of our approach. Our\nmethod titled DualAD not only outperforms independently trained single-task\nnetworks, but also improves over previous state-of-the-art end-to-end models by\na large margin on all tasks along the functional chain of driving.\n","authors":["Simon Doll","Niklas Hanselmann","Lukas Schneider","Richard Schulz","Marius Cordts","Markus Enzweiler","Hendrik P. A. Lensch"],"pdf_url":"https://arxiv.org/pdf/2406.06264v1.pdf","comment":"Accepted at CVPR 2024; Copyright 2024 IEEE; Project Website:\n  https://simondoll.github.io/publications/dualad"},{"id":"http://arxiv.org/abs/2406.06258v1","updated":"2024-06-10T13:41:10Z","published":"2024-06-10T13:41:10Z","title":"Tuning-Free Visual Customization via View Iterative Self-Attention\n  Control","summary":"  Fine-Tuning Diffusion Models enable a wide range of personalized generation\nand editing applications on diverse visual modalities. While Low-Rank\nAdaptation (LoRA) accelerates the fine-tuning process, it still requires\nmultiple reference images and time-consuming training, which constrains its\nscalability for large-scale and real-time applications. In this paper, we\npropose \\textit{View Iterative Self-Attention Control (VisCtrl)} to tackle this\nchallenge. Specifically, VisCtrl is a training-free method that injects the\nappearance and structure of a user-specified subject into another subject in\nthe target image, unlike previous approaches that require fine-tuning the\nmodel. Initially, we obtain the initial noise for both the reference and target\nimages through DDIM inversion. Then, during the denoising phase, features from\nthe reference image are injected into the target image via the self-attention\nmechanism. Notably, by iteratively performing this feature injection process,\nwe ensure that the reference image features are gradually integrated into the\ntarget image. This approach results in consistent and harmonious editing with\nonly one reference image in a few denoising steps. Moreover, benefiting from\nour plug-and-play architecture design and the proposed Feature Gradual Sampling\nstrategy for multi-view editing, our method can be easily extended to edit in\ncomplex visual domains. Extensive experiments show the efficacy of VisCtrl\nacross a spectrum of tasks, including personalized editing of images, videos,\nand 3D scenes.\n","authors":["Xiaojie Li","Chenghao Gu","Shuzhao Xie","Yunpeng Bai","Weixiang Zhang","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06258v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.15769v3","updated":"2024-06-10T13:09:53Z","published":"2024-03-23T08:54:03Z","title":"FusionINN: Decomposable Image Fusion for Brain Tumor Monitoring","summary":"  Image fusion typically employs non-invertible neural networks to merge\nmultiple source images into a single fused image. However, for clinical\nexperts, solely relying on fused images may be insufficient for making\ndiagnostic decisions, as the fusion mechanism blends features from source\nimages, thereby making it difficult to interpret the underlying tumor\npathology. We introduce FusionINN, a novel decomposable image fusion framework,\ncapable of efficiently generating fused images and also decomposing them back\nto the source images. FusionINN is designed to be bijective by including a\nlatent image alongside the fused image, while ensuring minimal transfer of\ninformation from the source images to the latent representation. To the best of\nour knowledge, we are the first to investigate the decomposability of fused\nimages, which is particularly crucial for life-sensitive applications such as\nmedical image fusion compared to other tasks like multi-focus or multi-exposure\nimage fusion. Our extensive experimentation validates FusionINN over existing\ndiscriminative and generative fusion methods, both subjectively and\nobjectively. Moreover, compared to a recent denoising diffusion-based fusion\nmodel, our approach offers faster and qualitatively better fusion results.\n","authors":["Nishant Kumar","Ziyan Tao","Jaikirat Singh","Yang Li","Peiwen Sun","Binghui Zhao","Stefan Gumhold"],"pdf_url":"https://arxiv.org/pdf/2403.15769v3.pdf","comment":"Accepted at IJCAI Workshop 2024. Source code available at\n  https://github.com/nish03/FusionINN"},{"id":"http://arxiv.org/abs/2406.06239v1","updated":"2024-06-10T13:08:31Z","published":"2024-06-10T13:08:31Z","title":"I-MPN: Inductive Message Passing Network for Effective and Efficient\n  Human-in-the-Loop Annotation of Mobile Eye Tracking Data","summary":"  Understanding human visual processing in dynamic environments is essential\nfor psychology and human-centered interaction design. Mobile eye-tracking\nsystems, combining egocentric video and gaze signals, offer valuable insights.\nHowever, manual analysis of these recordings is time-intensive. In this work,\nwe present a novel human-centered learning algorithm designed for automated\nobject recognition within mobile eye-tracking settings. Our approach seamlessly\nintegrates an object detector with an inductive message-passing network\ntechnique (I-MPN), harnessing node features such as node profile information\nand positions. This integration enables our algorithm to learn embedding\nfunctions capable of generalizing to new object angle views, thereby\nfacilitating rapid adaptation and efficient reasoning in dynamic contexts as\nusers navigate through their environment. Through experiments conducted on\nthree distinct video sequences, our \\textit{interactive-based method} showcases\nsignificant performance improvements over fixed training/testing algorithms,\neven when trained on considerably smaller annotated samples collected through\nuser feedback. Furthermore, we showcase exceptional efficiency in data\nannotation processes, surpassing approaches that use complete object detectors,\ncombine detectors with convolutional networks, or employ interactive video\nsegmentation.\n","authors":["Hoang H. Le","Duy M. H. Nguyen","Omair Shahzad Bhatti","Laszlo Kopacsi","Thinh P. Ngo","Binh T. Nguyen","Michael Barz","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2406.06239v1.pdf","comment":"First version"},{"id":"http://arxiv.org/abs/2406.06236v1","updated":"2024-06-10T13:06:28Z","published":"2024-06-10T13:06:28Z","title":"UnSupDLA: Towards Unsupervised Document Layout Analysis","summary":"  Document layout analysis is a key area in document research, involving\ntechniques like text mining and visual analysis. Despite various methods\ndeveloped to tackle layout analysis, a critical but frequently overlooked\nproblem is the scarcity of labeled data needed for analyses. With the rise of\ninternet use, an overwhelming number of documents are now available online,\nmaking the process of accurately labeling them for research purposes\nincreasingly challenging and labor-intensive. Moreover, the diversity of\ndocuments online presents a unique set of challenges in maintaining the quality\nand consistency of these labels, further complicating document layout analysis\nin the digital era. To address this, we employ a vision-based approach for\nanalyzing document layouts designed to train a network without labels. Instead,\nwe focus on pre-training, initially generating simple object masks from the\nunlabeled document images. These masks are then used to train a detector,\nenhancing object detection and segmentation performance. The model's\neffectiveness is further amplified through several unsupervised training\niterations, continuously refining its performance. This approach significantly\nadvances document layout analysis, particularly precision and efficiency,\nwithout labels.\n","authors":["Talha Uddin Sheikh","Tahira Shehzadi","Khurram Azeem Hashmi","Didier Stricker","Muhammad Zeshan Afzal"],"pdf_url":"https://arxiv.org/pdf/2406.06236v1.pdf","comment":"ICDAR 2024 - Workshop"},{"id":"http://arxiv.org/abs/2406.06230v1","updated":"2024-06-10T13:00:22Z","published":"2024-06-10T13:00:22Z","title":"UEMM-Air: A Synthetic Multi-modal Dataset for Unmanned Aerial Vehicle\n  Object Detection","summary":"  The development of multi-modal object detection for Unmanned Aerial Vehicles\n(UAVs) typically relies on a large amount of pixel-aligned multi-modal image\ndata. However, existing datasets face challenges such as limited modalities,\nhigh construction costs, and imprecise annotations. To this end, we propose a\nsynthetic multi-modal UAV-based object detection dataset, UEMM-Air. Specially,\nwe simulate various UAV flight scenarios and object types using the Unreal\nEngine (UE). Then we design the UAV's flight logic to automatically collect\ndata from different scenarios, perspectives, and altitudes. Finally, we propose\na novel heuristic automatic annotation algorithm to generate accurate object\ndetection labels. In total, our UEMM-Air consists of 20k pairs of images with 5\nmodalities and precise annotations. Moreover, we conduct numerous experiments\nand establish new benchmark results on our dataset. We found that models\npre-trained on UEMM-Air exhibit better performance on downstream tasks compared\nto other similar datasets. The dataset is publicly available\n(https://github.com/1e12Leon/UEMM-Air) to support the research of multi-modal\nUAV object detection models.\n","authors":["Fan Liu","Liang Yao","Shengxiang Xu","Chuanyi Zhang","Xinlei Zhang","Ting Wu"],"pdf_url":"https://arxiv.org/pdf/2406.06230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06218v1","updated":"2024-06-10T12:33:47Z","published":"2024-06-10T12:33:47Z","title":"Data Augmentation in Earth Observation: A Diffusion Model Approach","summary":"  The scarcity of high-quality Earth Observation (EO) imagery poses a\nsignificant challenge, despite its critical role in enabling precise analysis\nand informed decision-making across various sectors. This scarcity is primarily\ndue to atmospheric conditions, seasonal variations, and limited geographical\ncoverage, which complicates the application of Artificial Intelligence (AI) in\nEO. Data augmentation, a widely used technique in AI that involves generating\nadditional data mainly through parameterized image transformations, has been\nemployed to increase the volume and diversity of data. However, this method\noften falls short in generating sufficient diversity across key semantic axes,\nadversely affecting the accuracy of EO applications. To address this issue, we\npropose a novel four-stage approach aimed at improving the diversity of\naugmented data by integrating diffusion models. Our approach employs\nmeta-prompts for instruction generation, harnesses general-purpose\nvision-language models for generating rich captions, fine-tunes an Earth\nObservation diffusion model, and iteratively augments data. We conducted\nextensive experiments using four different data augmentation techniques, and\nour approach consistently demonstrated improvements, outperforming the\nestablished augmentation methods, revealing its effectiveness in generating\nsemantically rich and diverse EO images.\n","authors":["Tiago Sousa","Benoît Ries","Nicolas Guelfi"],"pdf_url":"https://arxiv.org/pdf/2406.06218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06216v1","updated":"2024-06-10T12:33:08Z","published":"2024-06-10T12:33:08Z","title":"Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering\n  for HDR View Synthesis","summary":"  Volumetric rendering based methods, like NeRF, excel in HDR view synthesis\nfrom RAWimages, especially for nighttime scenes. While, they suffer from long\ntraining times and cannot perform real-time rendering due to dense sampling\nrequirements. The advent of 3D Gaussian Splatting (3DGS) enables real-time\nrendering and faster training. However, implementing RAW image-based view\nsynthesis directly using 3DGS is challenging due to its inherent drawbacks: 1)\nin nighttime scenes, extremely low SNR leads to poor structure-from-motion\n(SfM) estimation in distant views; 2) the limited representation capacity of\nspherical harmonics (SH) function is unsuitable for RAW linear color space; and\n3) inaccurate scene structure hampers downstream tasks such as refocusing. To\naddress these issues, we propose LE3D (Lighting Every darkness with 3DGS). Our\nmethod proposes Cone Scatter Initialization to enrich the estimation of SfM,\nand replaces SH with a Color MLP to represent the RAW linear color space.\nAdditionally, we introduce depth distortion and near-far regularizations to\nimprove the accuracy of scene structure for downstream tasks. These designs\nenable LE3D to perform real-time novel view synthesis, HDR rendering,\nrefocusing, and tone-mapping changes. Compared to previous volumetric rendering\nbased methods, LE3D reduces training time to 1% and improves rendering speed by\nup to 4,000 times for 2K resolution images in terms of FPS. Code and viewer can\nbe found in https://github.com/Srameo/LE3D .\n","authors":["Xin Jin","Pengyi Jiao","Zheng-Peng Duan","Xingchao Yang","Chun-Le Guo","Bo Ren","Chongyi Li"],"pdf_url":"https://arxiv.org/pdf/2406.06216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06211v1","updated":"2024-06-10T12:22:06Z","published":"2024-06-10T12:22:06Z","title":"iMotion-LLM: Motion Prediction Instruction Tuning","summary":"  We introduce iMotion-LLM: a Multimodal Large Language Models (LLMs) with\ntrajectory prediction, tailored to guide interactive multi-agent scenarios.\nDifferent from conventional motion prediction approaches, iMotion-LLM\ncapitalizes on textual instructions as key inputs for generating contextually\nrelevant trajectories.By enriching the real-world driving scenarios in the\nWaymo Open Dataset with textual motion instructions, we created InstructWaymo.\nLeveraging this dataset, iMotion-LLM integrates a pretrained LLM, fine-tuned\nwith LoRA, to translate scene features into the LLM input space. iMotion-LLM\noffers significant advantages over conventional motion prediction models.\nFirst, it can generate trajectories that align with the provided instructions\nif it is a feasible direction. Second, when given an infeasible direction, it\ncan reject the instruction, thereby enhancing safety. These findings act as\nmilestones in empowering autonomous navigation systems to interpret and predict\nthe dynamics of multi-agent environments, laying the groundwork for future\nadvancements in this field.\n","authors":["Abdulwahab Felemban","Eslam Mohamed Bakr","Xiaoqian Shen","Jian Ding","Abduallah Mohamed","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2406.06211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19876v2","updated":"2024-06-10T12:12:38Z","published":"2024-05-30T09:30:28Z","title":"IReNe: Instant Recoloring of Neural Radiance Fields","summary":"  Advances in NERFs have allowed for 3D scene reconstructions and novel view\nsynthesis. Yet, efficiently editing these representations while retaining\nphotorealism is an emerging challenge. Recent methods face three primary\nlimitations: they're slow for interactive use, lack precision at object\nboundaries, and struggle to ensure multi-view consistency. We introduce IReNe\nto address these limitations, enabling swift, near real-time color editing in\nNeRF. Leveraging a pre-trained NeRF model and a single training image with\nuser-applied color edits, IReNe swiftly adjusts network parameters in seconds.\nThis adjustment allows the model to generate new scene views, accurately\nrepresenting the color changes from the training image while also controlling\nobject boundaries and view-specific effects. Object boundary control is\nachieved by integrating a trainable segmentation module into the model. The\nprocess gains efficiency by retraining only the weights of the last network\nlayer. We observed that neurons in this layer can be classified into those\nresponsible for view-dependent appearance and those contributing to diffuse\nappearance. We introduce an automated classification approach to identify these\nneuron types and exclusively fine-tune the weights of the diffuse neurons. This\nfurther accelerates training and ensures consistent color edits across\ndifferent views. A thorough validation on a new dataset, with edited object\ncolors, shows significant quantitative and qualitative advancements over\ncompetitors, accelerating speeds by 5x to 500x.\n","authors":["Alessio Mazzucchelli","Adrian Garcia-Garcia","Elena Garces","Fernando Rivas-Manzaneque","Francesc Moreno-Noguer","Adrian Penate-Sanchez"],"pdf_url":"https://arxiv.org/pdf/2405.19876v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03586v2","updated":"2024-06-10T12:09:37Z","published":"2024-06-05T19:05:08Z","title":"CountCLIP -- [Re] Teaching CLIP to Count to Ten","summary":"  Large vision-language models (VLMs) are shown to learn rich joint image-text\nrepresentations enabling high performances in relevant downstream tasks.\nHowever, they fail to showcase their quantitative understanding of objects, and\nthey lack good counting-aware representation. This paper conducts a\nreproducibility study of 'Teaching CLIP to Count to Ten' (Paiss et al., 2023),\nwhich presents a method to finetune a CLIP model (Radford et al., 2021) to\nimprove zero-shot counting accuracy in an image while maintaining the\nperformance for zero-shot classification by introducing a counting-contrastive\nloss term. We improve the model's performance on a smaller subset of their\ntraining data with lower computational resources. We verify these claims by\nreproducing their study with our own code. The implementation can be found at\nhttps://github.com/SforAiDl/CountCLIP.\n","authors":["Harshvardhan Mestha","Tejas Agrawal","Karan Bania","Shreyas V","Yash Bhisikar"],"pdf_url":"https://arxiv.org/pdf/2406.03586v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06201v1","updated":"2024-06-10T11:53:29Z","published":"2024-06-10T11:53:29Z","title":"2DP-2MRC: 2-Dimensional Pointer-based Machine Reading Comprehension\n  Method for Multimodal Moment Retrieval","summary":"  Moment retrieval aims to locate the most relevant moment in an untrimmed\nvideo based on a given natural language query. Existing solutions can be\nroughly categorized into moment-based and clip-based methods. The former often\ninvolves heavy computations, while the latter, due to overlooking\ncoarse-grained information, typically underperforms compared to moment-based\nmodels. Hence, this paper proposes a novel 2-Dimensional Pointer-based Machine\nReading Comprehension for Moment Retrieval Choice (2DP-2MRC) model to address\nthe issue of imprecise localization in clip-based methods while maintaining\nlower computational complexity than moment-based methods. Specifically, we\nintroduce an AV-Encoder to capture coarse-grained information at moment and\nvideo levels. Additionally, a 2D pointer encoder module is introduced to\nfurther enhance boundary detection for target moment. Extensive experiments on\nthe HiREST dataset demonstrate that 2DP-2MRC significantly outperforms existing\nbaseline models.\n","authors":["Jiajun He","Tomoki Toda"],"pdf_url":"https://arxiv.org/pdf/2406.06201v1.pdf","comment":"Accepted by INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2309.15519v3","updated":"2024-06-10T11:47:57Z","published":"2023-09-27T09:37:29Z","title":"Defending Against Physical Adversarial Patch Attacks on Infrared Human\n  Detection","summary":"  Infrared detection is an emerging technique for safety-critical tasks owing\nto its remarkable anti-interference capability. However, recent studies have\nrevealed that it is vulnerable to physically-realizable adversarial patches,\nposing risks in its real-world applications. To address this problem, we are\nthe first to investigate defense strategies against adversarial patch attacks\non infrared detection, especially human detection. We propose a straightforward\ndefense strategy, patch-based occlusion-aware detection (POD), which\nefficiently augments training samples with random patches and subsequently\ndetects them. POD not only robustly detects people but also identifies\nadversarial patch locations. Surprisingly, while being extremely\ncomputationally efficient, POD easily generalizes to state-of-the-art\nadversarial patch attacks that are unseen during training. Furthermore, POD\nimproves detection precision even in a clean (i.e., no-attack) situation due to\nthe data augmentation effect. Our evaluation demonstrates that POD is robust to\nadversarial patches of various shapes and sizes. The effectiveness of our\nbaseline approach is shown to be a viable defense mechanism for real-world\ninfrared human detection systems, paving the way for exploring future research\ndirections.\n","authors":["Lukas Strack","Futa Waseda","Huy H. Nguyen","Yinqiang Zheng","Isao Echizen"],"pdf_url":"https://arxiv.org/pdf/2309.15519v3.pdf","comment":"Accepted at ICIP2024. Lukas Strack and Futa Waseda contributed\n  equally"},{"id":"http://arxiv.org/abs/2406.06187v1","updated":"2024-06-10T11:33:34Z","published":"2024-06-10T11:33:34Z","title":"An Effective-Efficient Approach for Dense Multi-Label Action Detection","summary":"  Unlike the sparse label action detection task, where a single action occurs\nin each timestamp of a video, in a dense multi-label scenario, actions can\noverlap. To address this challenging task, it is necessary to simultaneously\nlearn (i) temporal dependencies and (ii) co-occurrence action relationships.\nRecent approaches model temporal information by extracting multi-scale features\nthrough hierarchical transformer-based networks. However, the self-attention\nmechanism in transformers inherently loses temporal positional information. We\nargue that combining this with multiple sub-sampling processes in hierarchical\ndesigns can lead to further loss of positional information. Preserving this\ninformation is essential for accurate action detection. In this paper, we\naddress this issue by proposing a novel transformer-based network that (a)\nemploys a non-hierarchical structure when modelling different ranges of\ntemporal dependencies and (b) embeds relative positional encoding in its\ntransformer layers. Furthermore, to model co-occurrence action relationships,\ncurrent methods explicitly embed class relations into the transformer network.\nHowever, these approaches are not computationally efficient, as the network\nneeds to compute all possible pair action class relations. We also overcome\nthis challenge by introducing a novel learning paradigm that allows the network\nto benefit from explicitly modelling temporal co-occurrence action dependencies\nwithout imposing their additional computational costs during inference. We\nevaluate the performance of our proposed approach on two challenging dense\nmulti-label benchmark datasets and show that our method improves the current\nstate-of-the-art results.\n","authors":["Faegheh Sardari","Armin Mustafa","Philip J. B. Jackson","Adrian Hilton"],"pdf_url":"https://arxiv.org/pdf/2406.06187v1.pdf","comment":"14 pages. arXiv admin note: substantial text overlap with\n  arXiv:2308.05051"},{"id":"http://arxiv.org/abs/2406.06183v1","updated":"2024-06-10T11:27:46Z","published":"2024-06-10T11:27:46Z","title":"Black carbon plumes from gas flaring in North Africa identified from\n  multi-spectral imagery with deep learning","summary":"  Black carbon (BC) is an important pollutant aerosol emitted by numerous human\nactivities, including gas flaring. Improper combustion in flaring activities\ncan release large amounts of BC, which is harmful to human health and has a\nstrong climate warming effect. To our knowledge, no study has ever directly\nmonitored BC emissions from satellite imagery. Previous works quantified BC\nemissions indirectly, by applying emission coefficients to flaring volumes\nestimated from satellite imagery. Here, we develop a deep learning framework\nand apply it to Sentinel-2 imagery over North Africa during 2022 to detect and\nquantify BC emissions from gas flaring. We find that BC emissions in this\nregion amount to about 1 million tCO$_{2,\\mathrm{eq}}$, or 1 million passenger\ncars, more than a quarter of which are due to 10 sites alone. This work\ndemonstrates the operational monitoring of BC emissions from flaring, a key\nstep in implementing effective mitigation policies to reduce the climate impact\nof oil and gas operations.\n","authors":["Tuel Alexandre","Kerdreux Thomas","Thiry Louis"],"pdf_url":"https://arxiv.org/pdf/2406.06183v1.pdf","comment":"Published at the workshop Tackling Climate Change with Machine\n  Learning at ICLR 2024"},{"id":"http://arxiv.org/abs/2405.12728v2","updated":"2024-06-10T11:15:08Z","published":"2024-05-21T12:34:03Z","title":"Leveraging Neural Radiance Fields for Pose Estimation of an Unknown\n  Space Object during Proximity Operations","summary":"  We address the estimation of the 6D pose of an unknown target spacecraft\nrelative to a monocular camera, a key step towards the autonomous rendezvous\nand proximity operations required by future Active Debris Removal missions. We\npresent a novel method that enables an \"off-the-shelf\" spacecraft pose\nestimator, which is supposed to known the target CAD model, to be applied on an\nunknown target. Our method relies on an in-the wild NeRF, i.e., a Neural\nRadiance Field that employs learnable appearance embeddings to represent\nvarying illumination conditions found in natural scenes. We train the NeRF\nmodel using a sparse collection of images that depict the target, and in turn\ngenerate a large dataset that is diverse both in terms of viewpoint and\nillumination. This dataset is then used to train the pose estimation network.\nWe validate our method on the Hardware-In-the-Loop images of SPEED+ that\nemulate lighting conditions close to those encountered on orbit. We demonstrate\nthat our method successfully enables the training of an off-the-shelf\nspacecraft pose estimation network from a sparse set of images. Furthermore, we\nshow that a network trained using our method performs similarly to a model\ntrained on synthetic images generated using the CAD model of the target.\n","authors":["Antoine Legrand","Renaud Detry","Christophe De Vleeschouwer"],"pdf_url":"https://arxiv.org/pdf/2405.12728v2.pdf","comment":"Accepted at IEEE International Conference on Space Robotics 2024\n  (ISpaRo 2024)"},{"id":"http://arxiv.org/abs/2406.06165v1","updated":"2024-06-10T11:00:26Z","published":"2024-06-10T11:00:26Z","title":"Generalized Nested Latent Variable Models for Lossy Coding applied to\n  Wind Turbine Scenarios","summary":"  Rate-distortion optimization through neural networks has accomplished\ncompetitive results in compression efficiency and image quality. This\nlearning-based approach seeks to minimize the compromise between compression\nrate and reconstructed image quality by automatically extracting and retaining\ncrucial information, while discarding less critical details. A successful\ntechnique consists in introducing a deep hyperprior that operates within a\n2-level nested latent variable model, enhancing compression by capturing\ncomplex data dependencies. This paper extends this concept by designing a\ngeneralized L-level nested generative model with a Markov chain structure. We\ndemonstrate as L increases that a trainable prior is detrimental and explore a\ncommon dimensionality along the distinct latent variables to boost compression\nperformance. As this structured framework can represent autoregressive coders,\nwe outperform the hyperprior model and achieve state-of-the-art performance\nwhile reducing substantially the computational cost. Our experimental\nevaluation is performed on wind turbine scenarios to study its application on\nvisual inspections\n","authors":["Raül Pérez-Gonzalo","Andreas Espersen","Antonio Agudo"],"pdf_url":"https://arxiv.org/pdf/2406.06165v1.pdf","comment":"Accepted to ICIP 2024"},{"id":"http://arxiv.org/abs/2406.06163v1","updated":"2024-06-10T10:53:23Z","published":"2024-06-10T10:53:23Z","title":"Extending Segment Anything Model into Auditory and Temporal Dimensions\n  for Audio-Visual Segmentation","summary":"  Audio-visual segmentation (AVS) aims to segment sound sources in the video\nsequence, requiring a pixel-level understanding of audio-visual correspondence.\nAs the Segment Anything Model (SAM) has strongly impacted extensive fields of\ndense prediction problems, prior works have investigated the introduction of\nSAM into AVS with audio as a new modality of the prompt. Nevertheless,\nconstrained by SAM's single-frame segmentation scheme, the temporal context\nacross multiple frames of audio-visual data remains insufficiently utilized. To\nthis end, we study the extension of SAM's capabilities to the sequence of\naudio-visual scenes by analyzing contextual cross-modal relationships across\nthe frames. To achieve this, we propose a Spatio-Temporal, Bidirectional\nAudio-Visual Attention (ST-BAVA) module integrated into the middle of SAM's\nimage encoder and mask decoder. It adaptively updates the audio-visual features\nto convey the spatio-temporal correspondence between the video frames and audio\nstreams. Extensive experiments demonstrate that our proposed model outperforms\nthe state-of-the-art methods on AVS benchmarks, especially with an 8.3% mIoU\ngain on a challenging multi-sources subset.\n","authors":["Juhyeong Seon","Woobin Im","Sebin Lee","Jumin Lee","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2406.06163v1.pdf","comment":"Accepted to ICIP 2024"},{"id":"http://arxiv.org/abs/2402.18115v2","updated":"2024-06-10T10:52:54Z","published":"2024-02-28T07:05:27Z","title":"UniVS: Unified and Universal Video Segmentation with Prompts as Queries","summary":"  Despite the recent advances in unified image segmentation (IS), developing a\nunified video segmentation (VS) model remains a challenge. This is mainly\nbecause generic category-specified VS tasks need to detect all objects and\ntrack them across consecutive frames, while prompt-guided VS tasks require\nre-identifying the target with visual/text prompts throughout the entire video,\nmaking it hard to handle the different tasks with the same architecture. We\nmake an attempt to address these issues and present a novel unified VS\narchitecture, namely UniVS, by using prompts as queries. UniVS averages the\nprompt features of the target from previous frames as its initial query to\nexplicitly decode masks, and introduces a target-wise prompt cross-attention\nlayer in the mask decoder to integrate prompt features in the memory pool. By\ntaking the predicted masks of entities from previous frames as their visual\nprompts, UniVS converts different VS tasks into prompt-guided target\nsegmentation, eliminating the heuristic inter-frame matching process. Our\nframework not only unifies the different VS tasks but also naturally achieves\nuniversal training and testing, ensuring robust performance across different\nscenarios. UniVS shows a commendable balance between performance and\nuniversality on 10 challenging VS benchmarks, covering video instance,\nsemantic, panoptic, object, and referring segmentation tasks. Code can be found\nat \\url{https://github.com/MinghanLi/UniVS}.\n","authors":["Minghan Li","Shuai Li","Xindong Zhang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.18115v2.pdf","comment":"21 pages, 11 figures, 10 tabels, CVPR2024"},{"id":"http://arxiv.org/abs/2404.18624v2","updated":"2024-06-10T10:43:20Z","published":"2024-04-29T11:52:20Z","title":"Do Vision & Language Decoders use Images and Text equally? How\n  Self-consistent are their Explanations?","summary":"  Vision and language model (VLM) decoders are currently the best-performing\narchitectures on multimodal tasks. Next to predictions, they can also produce\nexplanations, either in post-hoc or CoT settings. However, it is not clear how\nmuch they use the vision and text modalities when generating predictions or\nexplanations. In this work, we investigate if VLMs rely on modalities\ndifferently when they produce explanations as opposed to providing answers. We\nalso evaluate the self-consistency of VLM decoders in both post-hoc and CoT\nexplanation settings, by extending existing unimodal tests and measures to VLM\ndecoders. We find that VLMs are less self-consistent than LLMs. Text\ncontributions in VL decoders are more important than image contributions in all\nexamined tasks. Moreover, the contributions of images are significantly\nstronger for explanation generation compared to answer generation. This\ndifference is even larger in CoT compared to post-hoc explanations. Lastly, we\nprovide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE\nbenchmark, which before only covered VL encoders. We find that VL decoders\nstill struggle with most phenomena tested by VALSE.\n","authors":["Letitia Parcalabescu","Anette Frank"],"pdf_url":"https://arxiv.org/pdf/2404.18624v2.pdf","comment":"25 pages, 8 figures, 11 tables"},{"id":"http://arxiv.org/abs/2401.13201v3","updated":"2024-06-10T10:21:19Z","published":"2024-01-24T03:07:26Z","title":"MLLMReID: Multimodal Large Language Model-based Person Re-identification","summary":"  Multimodal large language models (MLLM) have achieved satisfactory results in\nmany tasks. However, their performance in the task of ReID (ReID) has not been\nexplored to date. This paper will investigate how to adapt them for the task of\nReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and\nthen use their visual encoder as a backbone for ReID. However, there still\nexist two apparent issues: (1) Designing instructions for ReID, MLLMs may\noverfit specific instructions, and designing a variety of instructions will\nlead to higher costs. (2) When fine-tuning the visual encoder of a MLLM, it is\nnot trained synchronously with the ReID task. As a result, the effectiveness of\nthe visual encoder fine-tuning cannot be directly reflected in the performance\nof the ReID task. To address these problems, this paper proposes MLLMReID:\nMultimodal Large Language Model-based ReID. Firstly, we proposed Common\nInstruction, a simple approach that leverages the essence ability of LLMs to\ncontinue writing, avoiding complex and diverse instruction design. Secondly, we\npropose a multi-task learning-based synchronization module to ensure that the\nvisual encoder of the MLLM is trained synchronously with the ReID task. The\nexperimental results demonstrate the superiority of our method.\n","authors":["Shan Yang","Yongfei Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.13201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17993v2","updated":"2024-06-10T10:15:30Z","published":"2024-04-27T19:54:42Z","title":"MinBackProp -- Backpropagating through Minimal Solvers","summary":"  We present an approach to backpropagating through minimal problem solvers in\nend-to-end neural network training. Traditional methods relying on manually\nconstructed formulas, finite differences, and autograd are laborious,\napproximate, and unstable for complex minimal problem solvers. We show that\nusing the Implicit function theorem (IFT) to calculate derivatives to\nbackpropagate through the solution of a minimal problem solver is simple, fast,\nand stable. We compare our approach to (i) using the standard autograd on\nminimal problem solvers and relate it to existing backpropagation formulas\nthrough SVD-based and Eig-based solvers and (ii) implementing the backprop with\nan existing PyTorch Deep Declarative Networks (DDN) framework. We demonstrate\nour technique on a toy example of training outlier-rejection weights for 3D\npoint registration and on a real application of training an outlier-rejection\nand RANSAC sampling network in image matching. Our method provides $100\\%$\nstability and is 10 times faster compared to autograd, which is unstable and\nslow, and compared to DDN, which is stable but also slow.\n","authors":["Diana Sungatullina","Tomas Pajdla"],"pdf_url":"https://arxiv.org/pdf/2404.17993v2.pdf","comment":"WSCG 2024"},{"id":"http://arxiv.org/abs/2403.17787v2","updated":"2024-06-10T10:07:24Z","published":"2024-03-26T15:20:49Z","title":"Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models\n  Versus Fine-Tuned Vision Transformers in Image-Based Security Applications","summary":"  The success of Large Language Models (LLMs) has led to a parallel rise in the\ndevelopment of Large Multimodal Models (LMMs), which have begun to transform a\nvariety of applications. These sophisticated multimodal models are designed to\ninterpret and analyze complex data by integrating multiple modalities such as\ntext and images, thereby opening new avenues for a range of applications. This\npaper investigates the applicability and effectiveness of prompt-engineered\nLMMs that process both images and text, including models such as LLaVA,\nBakLLaVA, Moondream, Gemini-pro-vision, and GPT-4o, compared to fine-tuned\nVision Transformer (ViT) models in addressing critical security challenges. We\nfocus on two distinct security tasks: 1) a visually evident task of detecting\nsimple triggers, such as small pixel variations in images that could be\nexploited to access potential backdoors in the models, and 2) a visually\nnon-evident task of malware classification through visual representations. In\nthe visually evident task, some LMMs, such as Gemini-pro-vision and GPT-4o,\nhave demonstrated the potential to achieve good performance with careful prompt\nengineering, with GPT-4o achieving the highest accuracy and F1-score of 91.9\\%\nand 91\\%, respectively. However, the fine-tuned ViT models exhibit perfect\nperformance in this task due to its simplicity. For the visually non-evident\ntask, the results highlight a significant divergence in performance, with ViT\nmodels achieving F1-scores of 97.11\\% in predicting 25 malware classes and\n97.61\\% in predicting 5 malware families, whereas LMMs showed suboptimal\nperformance despite iterative prompt improvements. This study not only\nshowcases the strengths and limitations of prompt-engineered LMMs in\ncybersecurity applications but also emphasizes the unmatched efficacy of\nfine-tuned ViT models for precise and dependable tasks.\n","authors":["Fouad Trad","Ali Chehab"],"pdf_url":"https://arxiv.org/pdf/2403.17787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12172v2","updated":"2024-06-10T10:05:01Z","published":"2024-04-18T13:27:29Z","title":"How to Benchmark Vision Foundation Models for Semantic Segmentation?","summary":"  Recent vision foundation models (VFMs) have demonstrated proficiency in\nvarious tasks but require supervised fine-tuning to perform the task of\nsemantic segmentation effectively. Benchmarking their performance is essential\nfor selecting current models and guiding future model developments for this\ntask. The lack of a standardized benchmark complicates comparisons. Therefore,\nthe primary objective of this paper is to study how VFMs should be benchmarked\nfor semantic segmentation. To do so, various VFMs are fine-tuned under various\nsettings, and the impact of individual settings on the performance ranking and\ntraining time is assessed. Based on the results, the recommendation is to\nfine-tune the ViT-B variants of VFMs with a 16x16 patch size and a linear\ndecoder, as these settings are representative of using a larger model, more\nadvanced decoder and smaller patch size, while reducing training time by more\nthan 13 times. Using multiple datasets for training and evaluation is also\nrecommended, as the performance ranking across datasets and domain shifts\nvaries. Linear probing, a common practice for some VFMs, is not recommended, as\nit is not representative of end-to-end fine-tuning. The benchmarking setup\nrecommended in this paper enables a performance analysis of VFMs for semantic\nsegmentation. The findings of such an analysis reveal that pretraining with\npromptable segmentation is not beneficial, whereas masked image modeling (MIM)\nwith abstract representations is crucial, even more important than the type of\nsupervision used. The code for efficiently fine-tuning VFMs for semantic\nsegmentation can be accessed through the project page at:\nhttps://tue-mps.github.io/benchmark-vfm-ss/.\n","authors":["Tommie Kerssies","Daan de Geus","Gijs Dubbelman"],"pdf_url":"https://arxiv.org/pdf/2404.12172v2.pdf","comment":"CVPR 2024 Workshop Proceedings for the Second Workshop on Foundation\n  Models. v2 updates image normalization preprocessing for linear probing with\n  EVA-02, EVA-02-CLIP, SigLIP, DFN (the impact on end-to-end fine-tuning is\n  negligible; no changes made)"},{"id":"http://arxiv.org/abs/2402.05602v2","updated":"2024-06-10T09:58:55Z","published":"2024-02-08T12:01:24Z","title":"AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for\n  Transformers","summary":"  Large Language Models are prone to biased predictions and hallucinations,\nunderlining the paramount importance of understanding their model-internal\nreasoning process. However, achieving faithful attributions for the entirety of\na black-box transformer model and maintaining computational efficiency is an\nunsolved challenge. By extending the Layer-wise Relevance Propagation\nattribution method to handle attention layers, we address these challenges\neffectively. While partial solutions exist, our method is the first to\nfaithfully and holistically attribute not only input but also latent\nrepresentations of transformer models with the computational efficiency similar\nto a single backward pass. Through extensive evaluations against existing\nmethods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures,\nwe demonstrate that our proposed approach surpasses alternative methods in\nterms of faithfulness and enables the understanding of latent representations,\nopening up the door for concept-based explanations. We provide an LRP library\nat https://github.com/rachtibat/LRP-eXplains-Transformers.\n","authors":["Reduan Achtibat","Sayed Mohammad Vakilzadeh Hatefi","Maximilian Dreyer","Aakriti Jain","Thomas Wiegand","Sebastian Lapuschkin","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2402.05602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06136v1","updated":"2024-06-10T09:48:13Z","published":"2024-06-10T09:48:13Z","title":"A Comparative Survey of Vision Transformers for Feature Extraction in\n  Texture Analysis","summary":"  Texture, a significant visual attribute in images, has been extensively\ninvestigated across various image recognition applications. Convolutional\nNeural Networks (CNNs), which have been successful in many computer vision\ntasks, are currently among the best texture analysis approaches. On the other\nhand, Vision Transformers (ViTs) have been surpassing the performance of CNNs\non tasks such as object recognition, causing a paradigm shift in the field.\nHowever, ViTs have so far not been scrutinized for texture recognition,\nhindering a proper appreciation of their potential in this specific setting.\nFor this reason, this work explores various pre-trained ViT architectures when\ntransferred to tasks that rely on textures. We review 21 different ViT variants\nand perform an extensive evaluation and comparison with CNNs and\nhand-engineered models on several tasks, such as assessing robustness to\nchanges in texture rotation, scale, and illumination, and distinguishing color\ntextures, material textures, and texture attributes. The goal is to understand\nthe potential and differences among these models when directly applied to\ntexture recognition, using pre-trained ViTs primarily for feature extraction\nand employing linear classifiers for evaluation. We also evaluate their\nefficiency, which is one of the main drawbacks in contrast to other methods.\nOur results show that ViTs generally outperform both CNNs and hand-engineered\nmodels, especially when using stronger pre-training and tasks involving\nin-the-wild textures (images from the internet). We highlight the following\npromising models: ViT-B with DINO pre-training, BeiTv2, and the Swin\narchitecture, as well as the EfficientFormer as a low-cost alternative. In\nterms of efficiency, although having a higher number of GFLOPs and parameters,\nViT-B and BeiT(v2) can achieve a lower feature extraction time on GPUs compared\nto ResNet50.\n","authors":["Leonardo Scabini","Andre Sacilotti","Kallil M. Zielinski","Lucas C. Ribas","Bernard De Baets","Odemir M. Bruno"],"pdf_url":"https://arxiv.org/pdf/2406.06136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06134v1","updated":"2024-06-10T09:45:38Z","published":"2024-06-10T09:45:38Z","title":"DiffInject: Revisiting Debias via Synthetic Data Generation using\n  Diffusion-based Style Injection","summary":"  Dataset bias is a significant challenge in machine learning, where specific\nattributes, such as texture or color of the images are unintentionally learned\nresulting in detrimental performance. To address this, previous efforts have\nfocused on debiasing models either by developing novel debiasing algorithms or\nby generating synthetic data to mitigate the prevalent dataset biases. However,\ngenerative approaches to date have largely relied on using bias-specific\nsamples from the dataset, which are typically too scarce. In this work, we\npropose, DiffInject, a straightforward yet powerful method to augment synthetic\nbias-conflict samples using a pretrained diffusion model. This approach\nsignificantly advances the use of diffusion models for debiasing purposes by\nmanipulating the latent space. Our framework does not require any explicit\nknowledge of the bias types or labelling, making it a fully unsupervised\nsetting for debiasing. Our methodology demonstrates substantial result in\neffectively reducing dataset bias.\n","authors":["Donggeun Ko","Sangwoo Jo","Dongjun Lee","Namjun Park","Jaekwang Kim"],"pdf_url":"https://arxiv.org/pdf/2406.06134v1.pdf","comment":"10 pages (including supplementary), 3 figures, SynData4CV@CVPR 24\n  (Workshop)"},{"id":"http://arxiv.org/abs/2312.15289v2","updated":"2024-06-10T09:45:32Z","published":"2023-12-23T16:10:53Z","title":"Fréchet Wavelet Distance: A Domain-Agnostic Metric for Image\n  Generation","summary":"  Modern metrics for generative learning like Fr\\'echet Inception Distance\n(FID) demonstrate impressive performance. However, they suffer from various\nshortcomings, like a bias towards specific generators and datasets. To address\nthis problem, we propose the Fr\\'echet Wavelet Distance (FWD) as a\ndomain-agnostic metric based on Wavelet Packet Transform ($W_p$). FWD provides\na sight across a broad spectrum of frequencies in images with a high\nresolution, along with preserving both spatial and textural aspects.\nSpecifically, we use Wp to project generated and dataset images to packet\ncoefficient space. Further, we compute Fr\\'echet distance with the resultant\ncoefficients to evaluate the quality of a generator. This metric is\ngeneral-purpose and dataset-domain agnostic, as it does not rely on any\npre-trained network while being more interpretable because of frequency band\ntransparency. We conclude with an extensive evaluation of a wide variety of\ngenerators across various datasets that the proposed FWD is able to generalize\nand improve robustness to domain shift and various corruptions compared to\nother metrics.\n","authors":["Lokesh Veeramacheneni","Moritz Wolter","Hildegard Kuehne","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2312.15289v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06133v1","updated":"2024-06-10T09:44:06Z","published":"2024-06-10T09:44:06Z","title":"ExtraNeRF: Visibility-Aware View Extrapolation of Neural Radiance Fields\n  with Diffusion Models","summary":"  We propose ExtraNeRF, a novel method for extrapolating the range of views\nhandled by a Neural Radiance Field (NeRF). Our main idea is to leverage NeRFs\nto model scene-specific, fine-grained details, while capitalizing on diffusion\nmodels to extrapolate beyond our observed data. A key ingredient is to track\nvisibility to determine what portions of the scene have not been observed, and\nfocus on reconstructing those regions consistently with diffusion models. Our\nprimary contributions include a visibility-aware diffusion-based inpainting\nmodule that is fine-tuned on the input imagery, yielding an initial NeRF with\nmoderate quality (often blurry) inpainted regions, followed by a second\ndiffusion model trained on the input imagery to consistently enhance, notably\nsharpen, the inpainted imagery from the first pass. We demonstrate high-quality\nresults, extrapolating beyond a small number of (typically six or fewer) input\nviews, effectively outpainting the NeRF as well as inpainting newly disoccluded\nregions inside the original viewing volume. We compare with related work both\nquantitatively and qualitatively and show significant gains over prior art.\n","authors":["Meng-Li Shih","Wei-Chiu Ma","Aleksander Holynski","Forrester Cole","Brian L. Curless","Janne Kontkanen"],"pdf_url":"https://arxiv.org/pdf/2406.06133v1.pdf","comment":"8 pages, 8 figures, CVPR2024"},{"id":"http://arxiv.org/abs/2406.06122v1","updated":"2024-06-10T09:16:27Z","published":"2024-06-10T09:16:27Z","title":"W-Net: One-Shot Arbitrary-Style Chinese Character Generation with Deep\n  Neural Networks","summary":"  Due to the huge category number, the sophisticated combinations of various\nstrokes and radicals, and the free writing or printing styles, generating\nChinese characters with diverse styles is always considered as a difficult\ntask. In this paper, an efficient and generalized deep framework, namely, the\nW-Net, is introduced for the one-shot arbitrary-style Chinese character\ngeneration task. Specifically, given a single character (one-shot) with a\nspecific style (e.g., a printed font or hand-writing style), the proposed W-Net\nmodel is capable of learning and generating any arbitrary characters sharing\nthe style similar to the given single character. Such appealing property was\nrarely seen in the literature. We have compared the proposed W-Net framework to\nmany other competitive methods. Experimental results showed the proposed method\nis significantly superior in the one-shot setting.\n","authors":["Haochuan Jiang","Guanyu Yang","Kaizhu Huang","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16456v2","updated":"2024-06-10T09:10:37Z","published":"2024-04-25T09:35:09Z","title":"Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment\n  Analysis with Incomplete Modalities","summary":"  Multimodal sentiment analysis (MSA) aims to understand human sentiment\nthrough multimodal data. Most MSA efforts are based on the assumption of\nmodality completeness. However, in real-world applications, some practical\nfactors cause uncertain modality missingness, which drastically degrades the\nmodel's performance. To this end, we propose a Correlation-decoupled Knowledge\nDistillation (CorrKD) framework for the MSA task under uncertain missing\nmodalities. Specifically, we present a sample-level contrastive distillation\nmechanism that transfers comprehensive knowledge containing cross-sample\ncorrelations to reconstruct missing semantics. Moreover, a category-guided\nprototype distillation mechanism is introduced to capture cross-category\ncorrelations using category prototypes to align feature distributions and\ngenerate favorable joint representations. Eventually, we design a\nresponse-disentangled consistency distillation strategy to optimize the\nsentiment decision boundaries of the student network through response\ndisentanglement and mutual information maximization. Comprehensive experiments\non three datasets indicate that our framework can achieve favorable\nimprovements compared with several baselines.\n","authors":["Mingcheng Li","Dingkang Yang","Xiao Zhao","Shuaibing Wang","Yan Wang","Kun Yang","Mingyang Sun","Dongliang Kou","Ziyun Qian","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.16456v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2310.06836v3","updated":"2024-06-10T09:10:07Z","published":"2023-10-10T17:59:28Z","title":"A General Protocol to Probe Large Vision Models for 3D Physical\n  Understanding","summary":"  Our objective in this paper is to probe large vision models to determine to\nwhat extent they 'understand' different physical properties of the 3D scene\ndepicted in an image. To this end, we make the following contributions: (i) We\nintroduce a general and lightweight protocol to evaluate whether features of an\noff-the-shelf large vision model encode a number of physical 'properties' of\nthe 3D scene, by training discriminative classifiers on the features for these\nproperties. The probes are applied on datasets of real images with annotations\nfor the property. (ii) We apply this protocol to properties covering scene\ngeometry, scene material, support relations, lighting, and view-dependent\nmeasures, and large vision models including CLIP, DINOv1, DINOv2, VQGAN, Stable\nDiffusion. (iii) We find that features from Stable Diffusion and DINOv2 are\ngood for discriminative learning of a number of properties, including scene\ngeometry, support relations, shadows and depth, but less performant for\nocclusion and material, while outperforming DINOv1, CLIP and VQGAN for all\nproperties. (iv) It is observed that different time steps of Stable Diffusion\nfeatures, as well as different transformer layers of DINO/CLIP/VQGAN, are good\nat different properties, unlocking potential applications of 3D physical\nunderstanding.\n","authors":["Guanqi Zhan","Chuanxia Zheng","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2310.06836v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09736v2","updated":"2024-06-10T09:06:49Z","published":"2024-04-15T12:37:26Z","title":"FSRT: Facial Scene Representation Transformer for Face Reenactment from\n  Factorized Appearance, Head-pose, and Facial Expression Features","summary":"  The task of face reenactment is to transfer the head motion and facial\nexpressions from a driving video to the appearance of a source image, which may\nbe of a different person (cross-reenactment). Most existing methods are\nCNN-based and estimate optical flow from the source image to the current\ndriving frame, which is then inpainted and refined to produce the output\nanimation. We propose a transformer-based encoder for computing a set-latent\nrepresentation of the source image(s). We then predict the output color of a\nquery pixel using a transformer-based decoder, which is conditioned with\nkeypoints and a facial expression vector extracted from the driving frame.\nLatent representations of the source person are learned in a self-supervised\nmanner that factorize their appearance, head pose, and facial expressions.\nThus, they are perfectly suited for cross-reenactment. In contrast to most\nrelated work, our method naturally extends to multiple source images and can\nthus adapt to person-specific facial dynamics. We also propose data\naugmentation and regularization schemes that are necessary to prevent\noverfitting and support generalizability of the learned representations. We\nevaluated our approach in a randomized user study. The results indicate\nsuperior performance compared to the state-of-the-art in terms of motion\ntransfer quality and temporal consistency.\n","authors":["Andre Rochow","Max Schwarz","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2404.09736v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2405.04211v2","updated":"2024-06-10T08:52:58Z","published":"2024-05-07T11:24:37Z","title":"Breast Histopathology Image Retrieval by Attention-based Adversarially\n  Regularized Variational Graph Autoencoder with Contrastive Learning-Based\n  Feature Extraction","summary":"  Breast cancer is the most common cancer type in women worldwide. Early\ndetection and appropriate treatment can significantly reduce its impact. While\nhistopathology examinations play a vital role in rapid and accurate diagnosis,\nthey often require a substantial workforce and experienced medical experts for\nproper recognition and cancer grading. Automated image retrieval systems have\nthe potential to assist pathologists in identifying cancerous tissues, thereby\naccelerating the diagnostic process. Nevertheless, due to considerable\nvariability among the tissue and cell patterns in histological images,\nproposing an accurate image retrieval model is very challenging.\n  This work introduces a novel attention-based adversarially regularized\nvariational graph autoencoder model for breast histological image retrieval.\nAdditionally, we incorporated cluster-guided contrastive learning as the graph\nfeature extractor to boost the retrieval performance. We evaluated the\nperformance of the proposed model on two publicly available datasets of breast\ncancer histological images and achieved superior or very competitive retrieval\nperformance, with average mAP scores of 96.5% for the BreakHis dataset and\n94.7% for the BACH dataset, and mVP scores of 91.9% and 91.3%, respectively.\n  Our proposed retrieval model has the potential to be used in clinical\nsettings to enhance diagnostic performance and ultimately benefit patients.\n","authors":["Nematollah Saeidi","Hossein Karshenas","Bijan Shoushtarian","Sepideh Hatamikia","Ramona Woitek","Amirreza Mahbod"],"pdf_url":"https://arxiv.org/pdf/2405.04211v2.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2311.10320v2","updated":"2024-06-10T08:31:35Z","published":"2023-11-17T04:06:20Z","title":"Learning transformer-based heterogeneously salient graph representation\n  for multimodal remote sensing image classification","summary":"  Data collected by different modalities can provide a wealth of complementary\ninformation, such as hyperspectral image (HSI) to offer rich spectral-spatial\nproperties, synthetic aperture radar (SAR) to provide structural information\nabout the Earth's surface, and light detection and ranging (LiDAR) to cover\naltitude information about ground elevation. Therefore, a natural idea is to\ncombine multimodal images for refined and accurate land-cover interpretation.\nAlthough many efforts have been attempted to achieve multi-source remote\nsensing image classification, there are still three issues as follows: 1)\nindiscriminate feature representation without sufficiently considering modal\nheterogeneity, 2) abundant features and complex computations associated with\nmodeling long-range dependencies, and 3) overfitting phenomenon caused by\nsparsely labeled samples. To overcome the above barriers, a transformer-based\nheterogeneously salient graph representation (THSGR) approach is proposed in\nthis paper. First, a multimodal heterogeneous graph encoder is presented to\nencode distinctively non-Euclidean structural features from heterogeneous data.\nThen, a self-attention-free multi-convolutional modulator is designed for\neffective and efficient long-term dependency modeling. Finally, a mean forward\nis put forward in order to avoid overfitting. Based on the above structures,\nthe proposed model is able to break through modal gaps to obtain differentiated\ngraph representation with competitive time cost, even for a small fraction of\ntraining samples. Experiments and analyses on three benchmark datasets with\nvarious state-of-the-art (SOTA) methods show the performance of the proposed\napproach.\n","authors":["Jiaqi Yang","Bo Du","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.10320v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14857v2","updated":"2024-06-10T08:23:03Z","published":"2024-05-23T17:58:03Z","title":"Semantica: An Adaptable Image-Conditioned Diffusion Model","summary":"  We investigate the task of adapting image generative models to different\ndatasets without finetuneing. To this end, we introduce Semantica, an\nimage-conditioned diffusion model capable of generating images based on the\nsemantics of a conditioning image. Semantica is trained exclusively on\nweb-scale image pairs, that is it receives a random image from a webpage as\nconditional input and models another random image from the same webpage. Our\nexperiments highlight the expressivity of pretrained image encoders and\nnecessity of semantic-based data filtering in achieving high-quality image\ngeneration. Once trained, it can adaptively generate new images from a dataset\nby simply using images from that dataset as input. We study the transfer\nproperties of Semantica on ImageNet, LSUN Churches, LSUN Bedroom and SUN397.\n","authors":["Manoj Kumar","Neil Houlsby","Emiel Hoogeboom"],"pdf_url":"https://arxiv.org/pdf/2405.14857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06089v1","updated":"2024-06-10T08:18:55Z","published":"2024-06-10T08:18:55Z","title":"Texture Re-scalable Universal Adversarial Perturbation","summary":"  Universal adversarial perturbation (UAP), also known as image-agnostic\nperturbation, is a fixed perturbation map that can fool the classifier with\nhigh probabilities on arbitrary images, making it more practical for attacking\ndeep models in the real world. Previous UAP methods generate a scale-fixed and\ntexture-fixed perturbation map for all images, which ignores the multi-scale\nobjects in images and usually results in a low fooling ratio. Since the widely\nused convolution neural networks tend to classify objects according to semantic\ninformation stored in local textures, it seems a reasonable and intuitive way\nto improve the UAP from the perspective of utilizing local contents\neffectively. In this work, we find that the fooling ratios significantly\nincrease when we add a constraint to encourage a small-scale UAP map and repeat\nit vertically and horizontally to fill the whole image domain. To this end, we\npropose texture scale-constrained UAP (TSC-UAP), a simple yet effective UAP\nenhancement method that automatically generates UAPs with category-specific\nlocal textures that can fool deep models more easily. Through a low-cost\noperation that restricts the texture scale, TSC-UAP achieves a considerable\nimprovement in the fooling ratio and attack transferability for both\ndata-dependent and data-free UAP methods. Experiments conducted on two\nstate-of-the-art UAP methods, eight popular CNN models and four classical\ndatasets show the remarkable performance of TSC-UAP.\n","authors":["Yihao Huang","Qing Guo","Felix Juefei-Xu","Ming Hu","Xiaojun Jia","Xiaochun Cao","Geguang Pu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.06089v1.pdf","comment":"14 pages (accepted by TIFS2024)"},{"id":"http://arxiv.org/abs/2406.06087v1","updated":"2024-06-10T08:18:07Z","published":"2024-06-10T08:18:07Z","title":"GAIA: Rethinking Action Quality Assessment for AI-Generated Videos","summary":"  Assessing action quality is both imperative and challenging due to its\nsignificant impact on the quality of AI-generated videos, further complicated\nby the inherently ambiguous nature of actions within AI-generated video (AIGV).\nCurrent action quality assessment (AQA) algorithms predominantly focus on\nactions from real specific scenarios and are pre-trained with normative action\nfeatures, thus rendering them inapplicable in AIGVs. To address these problems,\nwe construct GAIA, a Generic AI-generated Action dataset, by conducting a\nlarge-scale subjective evaluation from a novel causal reasoning-based\nperspective, resulting in 971,244 ratings among 9,180 video-action pairs. Based\non GAIA, we evaluate a suite of popular text-to-video (T2V) models on their\nability to generate visually rational actions, revealing their pros and cons on\ndifferent categories of actions. We also extend GAIA as a testbed to benchmark\nthe AQA capacity of existing automatic evaluation methods. Results show that\ntraditional AQA methods, action-related metrics in recent T2V benchmarks, and\nmainstream video quality methods correlate poorly with human opinions,\nindicating a sizable gap between current models and human action perception\npatterns in AIGVs. Our findings underscore the significance of action quality\nas a unique perspective for studying AIGVs and can catalyze progress towards\nmethods with enhanced capacities for AQA in AIGVs.\n","authors":["Zijian Chen","Wei Sun","Yuan Tian","Jun Jia","Zicheng Zhang","Jiarui Wang","Ru Huang","Xiongkuo Min","Guangtao Zhai","Wenjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06087v1.pdf","comment":"28 pages, 13 figures"},{"id":"http://arxiv.org/abs/2406.06081v1","updated":"2024-06-10T07:54:56Z","published":"2024-06-10T07:54:56Z","title":"An Open and Large-Scale Dataset for Multi-Modal Climate Change-aware\n  Crop Yield Predictions","summary":"  Precise crop yield predictions are of national importance for ensuring food\nsecurity and sustainable agricultural practices. While AI-for-science\napproaches have exhibited promising achievements in solving many scientific\nproblems such as drug discovery, precipitation nowcasting, etc., the\ndevelopment of deep learning models for predicting crop yields is constantly\nhindered by the lack of an open and large-scale deep learning-ready dataset\nwith multiple modalities to accommodate sufficient information. To remedy this,\nwe introduce the CropNet dataset, the first terabyte-sized, publicly available,\nand multi-modal dataset specifically targeting climate change-aware crop yield\npredictions for the contiguous United States (U.S.) continent at the county\nlevel. Our CropNet dataset is composed of three modalities of data, i.e.,\nSentinel-2 Imagery, WRF-HRRR Computed Dataset, and USDA Crop Dataset, for over\n2200 U.S. counties spanning 6 years (2017-2022), expected to facilitate\nresearchers in developing versatile deep learning models for timely and\nprecisely predicting crop yields at the county-level, by accounting for the\neffects of both short-term growing season weather variations and long-term\nclimate change on crop yields. Besides, we develop the CropNet package,\noffering three types of APIs, for facilitating researchers in downloading the\nCropNet data on the fly over the time and region of interest, and flexibly\nbuilding their deep learning models for accurate crop yield predictions.\nExtensive experiments have been conducted on our CropNet dataset via employing\nvarious types of deep learning solutions, with the results validating the\ngeneral applicability and the efficacy of the CropNet dataset in climate\nchange-aware crop yield predictions.\n","authors":["Fudong Lin","Kaleb Guillot","Summer Crawford","Yihe Zhang","Xu Yuan","Nian-Feng Tzeng"],"pdf_url":"https://arxiv.org/pdf/2406.06081v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2406.06079v1","updated":"2024-06-10T07:52:29Z","published":"2024-06-10T07:52:29Z","title":"Latent Representation Matters: Human-like Sketches in One-shot Drawing\n  Tasks","summary":"  Humans can effortlessly draw new categories from a single exemplar, a feat\nthat has long posed a challenge for generative models. However, this gap has\nstarted to close with recent advances in diffusion models. This one-shot\ndrawing task requires powerful inductive biases that have not been\nsystematically investigated. Here, we study how different inductive biases\nshape the latent space of Latent Diffusion Models (LDMs). Along with standard\nLDM regularizers (KL and vector quantization), we explore supervised\nregularizations (including classification and prototype-based representation)\nand contrastive inductive biases (using SimCLR and redundancy reduction\nobjectives). We demonstrate that LDMs with redundancy reduction and\nprototype-based regularizations produce near-human-like drawings (regarding\nboth samples' recognizability and originality) -- better mimicking human\nperception (as evaluated psychophysically). Overall, our results suggest that\nthe gap between humans and machines in one-shot drawings is almost closed.\n","authors":["Victor Boutin","Rishav Mukherji","Aditya Agrawal","Sabine Muzellec","Thomas Fel","Thomas Serre","Rufin VanRullen"],"pdf_url":"https://arxiv.org/pdf/2406.06079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02077v3","updated":"2024-06-10T07:49:05Z","published":"2024-06-04T07:57:34Z","title":"Multi-target stain normalization for histology slides","summary":"  Traditional staining normalization approaches, e.g. Macenko, typically rely\non the choice of a single representative reference image, which may not\nadequately account for the diverse staining patterns of datasets collected in\npractical scenarios. In this study, we introduce a novel approach that\nleverages multiple reference images to enhance robustness against stain\nvariation. Our method is parameter-free and can be adopted in existing\ncomputational pathology pipelines with no significant changes. We evaluate the\neffectiveness of our method through experiments using a deep-learning pipeline\nfor automatic nuclei segmentation on colorectal images. Our results show that\nby leveraging multiple reference images, better results can be achieved when\ngeneralizing to external data, where the staining can widely differ from the\ntraining set.\n","authors":["Desislav Ivanov","Carlo Alberto Barbano","Marco Grangetto"],"pdf_url":"https://arxiv.org/pdf/2406.02077v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17883v3","updated":"2024-06-10T07:43:30Z","published":"2024-04-27T12:42:26Z","title":"Underwater Variable Zoom: Depth-Guided Perception Network for Underwater\n  Image Enhancement","summary":"  Underwater scenes intrinsically involve degradation problems owing to\nheterogeneous ocean elements. Prevailing underwater image enhancement (UIE)\nmethods stick to straightforward feature modeling to learn the mapping\nfunction, which leads to limited vision gain as it lacks more explicit physical\ncues (e.g., depth). In this work, we investigate injecting the depth prior into\nthe deep UIE model for more precise scene enhancement capability. To this end,\nwe present a novel depth-guided perception UIE framework, dubbed underwater\nvariable zoom (UVZ). Specifically, UVZ resorts to a two-stage pipeline. First,\na depth estimation network is designed to generate critical depth maps,\ncombined with an auxiliary supervision network introduced to suppress\nestimation differences during training. Second, UVZ parses near-far scenarios\nby harnessing the predicted depth maps, enabling local and non-local perceiving\nin different regions. Extensive experiments on five benchmark datasets\ndemonstrate that UVZ achieves superior visual gain and delivers promising\nquantitative metrics. Besides, UVZ is confirmed to exhibit good generalization\nin some visual tasks, especially in unusual lighting conditions. The code,\nmodels and results are available at: https://github.com/WindySprint/UVZ.\n","authors":["Zhixiong Huang","Xinying Wang","Chengpei Xu","Jinjiang Li","Lin Feng"],"pdf_url":"https://arxiv.org/pdf/2404.17883v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06072v1","updated":"2024-06-10T07:36:24Z","published":"2024-06-10T07:36:24Z","title":"Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor\n  Control","summary":"  Vision Transformers (ViT), when paired with large-scale pretraining, have\nshown remarkable performance across various computer vision tasks, primarily\ndue to their weak inductive bias. However, while such weak inductive bias aids\nin pretraining scalability, this may hinder the effective adaptation of ViTs\nfor visuo-motor control tasks as a result of the absence of control-centric\ninductive biases. Such absent inductive biases include spatial locality and\ntranslation equivariance bias which convolutions naturally offer. To this end,\nwe introduce Convolution Injector (CoIn), an add-on module that injects\nconvolutions which are rich in locality and equivariance biases into a\npretrained ViT for effective adaptation in visuo-motor control. We evaluate\nCoIn with three distinct types of pretrained ViTs (CLIP, MVP, VC-1) across 12\nvaried control tasks within three separate domains (Adroit, MetaWorld, DMC),\nand demonstrate that CoIn consistently enhances control task performance across\nall experimented environments and models, validating the effectiveness of\nproviding pretrained ViTs with control-centric biases.\n","authors":["Dongyoon Hwang","Byungkun Lee","Hojoon Lee","Hyunseung Kim","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.06072v1.pdf","comment":"accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2406.06069v1","updated":"2024-06-10T07:24:22Z","published":"2024-06-10T07:24:22Z","title":"PointABM:Integrating Bidirectional State Space Model with Multi-Head\n  Self-Attention for Point Cloud Analysis","summary":"  Mamba, based on state space model (SSM) with its linear complexity and great\nsuccess in classification provide its superiority in 3D point cloud analysis.\nPrior to that, Transformer has emerged as one of the most prominent and\nsuccessful architectures for point cloud analysis. We present PointABM, a\nhybrid model that integrates the Mamba and Transformer architectures for\nenhancing local feature to improve performance of 3D point cloud analysis. In\norder to enhance the extraction of global features, we introduce a\nbidirectional SSM (bi-SSM) framework, which comprises both a traditional token\nforward SSM and an innovative backward SSM. To enhance the bi-SSM's capability\nof capturing more comprehensive features without disrupting the sequence\nrelationships required by the bidirectional Mamba, we introduce Transformer,\nutilizing its self-attention mechanism to process point clouds. Extensive\nexperimental results demonstrate that integrating Mamba with Transformer\nsignificantly enhance the model's capability to analysis 3D point cloud.\n","authors":["Jia-wei Chen","Yu-jie Xiong","Yong-bin Gao"],"pdf_url":"https://arxiv.org/pdf/2406.06069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06062v1","updated":"2024-06-10T07:18:41Z","published":"2024-06-10T07:18:41Z","title":"ProcessPainter: Learn Painting Process from Sequence Data","summary":"  The painting process of artists is inherently stepwise and varies\nsignificantly among different painters and styles. Generating detailed,\nstep-by-step painting processes is essential for art education and research,\nyet remains largely underexplored. Traditional stroke-based rendering methods\nbreak down images into sequences of brushstrokes, yet they fall short of\nreplicating the authentic processes of artists, with limitations confined to\nbasic brushstroke modifications. Text-to-image models utilizing diffusion\nprocesses generate images through iterative denoising, also diverge\nsubstantially from artists' painting process. To address these challenges, we\nintroduce ProcessPainter, a text-to-video model that is initially pre-trained\non synthetic data and subsequently fine-tuned with a select set of artists'\npainting sequences using the LoRA model. This approach successfully generates\npainting processes from text prompts for the first time. Furthermore, we\nintroduce an Artwork Replication Network capable of accepting arbitrary-frame\ninput, which facilitates the controlled generation of painting processes,\ndecomposing images into painting sequences, and completing semi-finished\nartworks. This paper offers new perspectives and tools for advancing art\neducation and image generation technology.\n","authors":["Yiren Song","Shijie Huang","Chen Yao","Xiaojun Ye","Hai Ci","Jiaming Liu","Yuxuan Zhang","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2406.06062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12042v3","updated":"2024-06-10T06:38:16Z","published":"2023-12-19T10:55:46Z","title":"Pose2Gaze: Eye-body Coordination during Daily Activities for Gaze\n  Prediction from Full-body Poses","summary":"  Human eye gaze plays a significant role in many virtual and augmented reality\n(VR/AR) applications, such as gaze-contingent rendering, gaze-based\ninteraction, or eye-based activity recognition. However, prior works on gaze\nanalysis and prediction have only explored eye-head coordination and were\nlimited to human-object interactions. We first report a comprehensive analysis\nof eye-body coordination in various human-object and human-human interaction\nactivities based on four public datasets collected in real-world (MoGaze), VR\n(ADT), as well as AR (GIMO and EgoBody) environments. We show that in\nhuman-object interactions, e.g. pick and place, eye gaze exhibits strong\ncorrelations with full-body motion while in human-human interactions, e.g. chat\nand teach, a person's gaze direction is correlated with the body orientation\ntowards the interaction partner. Informed by these analyses we then present\nPose2Gaze, a novel eye-body coordination model that uses a convolutional neural\nnetwork and a spatio-temporal graph convolutional neural network to extract\nfeatures from head direction and full-body poses, respectively, and then uses a\nconvolutional neural network to predict eye gaze. We compare our method with\nstate-of-the-art methods that predict eye gaze only from head movements and\nshow that Pose2Gaze outperforms these baselines with an average improvement of\n24.0% on MoGaze, 10.1% on ADT, 21.3% on GIMO, and 28.6% on EgoBody in mean\nangular error, respectively. We also show that our method significantly\noutperforms prior methods in the sample downstream task of eye-based activity\nrecognition. These results underline the significant information content\navailable in eye-body coordination during daily activities and open up a new\ndirection for gaze prediction.\n","authors":["Zhiming Hu","Jiahui Xu","Syn Schmitt","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2312.12042v3.pdf","comment":"Accepted at TVCG 2024, code available at\n  https://zhiminghu.net/hu24_pose2gaze.html"},{"id":"http://arxiv.org/abs/2406.06050v1","updated":"2024-06-10T06:38:11Z","published":"2024-06-10T06:38:11Z","title":"Generalizable Human Gaussians from Single-View Image","summary":"  In this work, we tackle the task of learning generalizable 3D human Gaussians\nfrom a single image. The main challenge for this task is to recover detailed\ngeometry and appearance, especially for the unobserved regions. To this end, we\npropose single-view generalizable Human Gaussian model (HGM), a\ndiffusion-guided framework for 3D human modeling from a single image. We design\na diffusion-based coarse-to-fine pipeline, where the diffusion model is adapted\nto refine novel-view images rendered from a coarse human Gaussian model. The\nrefined images are then used together with the input image to learn a refined\nhuman Gaussian model. Although effective in hallucinating the unobserved views,\nthe approach may generate unrealistic human pose and shapes due to the lack of\nsupervision. We circumvent this problem by further encoding the geometric\npriors from SMPL model. Specifically, we propagate geometric features from SMPL\nvolume to the predicted Gaussians via sparse convolution and attention\nmechanism. We validate our approach on publicly available datasets and\ndemonstrate that it significantly surpasses state-of-the-art methods in terms\nof PSNR and SSIM. Additionally, our method exhibits strong generalization for\nin-the-wild images.\n","authors":["Jinnan Chen","Chen Li","Jianfeng Zhang","Hanlin Chen","Buzhen Huang","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2406.06050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06048v1","updated":"2024-06-10T06:29:00Z","published":"2024-06-10T06:29:00Z","title":"Robust Latent Representation Tuning for Image-text Classification","summary":"  Large models have demonstrated exceptional generalization capabilities in\ncomputer vision and natural language processing. Recent efforts have focused on\nenhancing these models with multimodal processing abilities. However,\naddressing the challenges posed by scenarios where one modality is absent\nremains a significant hurdle. In response to this issue, we propose a robust\nlatent representation tuning method for large models. Specifically, our\napproach introduces a modality latent translation module to maximize the\ncorrelation between modalities. Following this, a newly designed fusion module\nis employed to facilitate information interaction between the modalities. In\nthis framework, not only are common semantics refined during training, but the\nmethod also yields robust representations in the absence of one modality.\nImportantly, our method maintains the frozen state of the image and text\nfoundation models to preserve their abilities acquired through large-scale\npretraining. We conduct experiments on several public datasets, and the results\nunderscore the effectiveness of our proposed method.\n","authors":["Hao Sun","Yu Song"],"pdf_url":"https://arxiv.org/pdf/2406.06048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06045v1","updated":"2024-06-10T06:26:03Z","published":"2024-06-10T06:26:03Z","title":"Synthesizing Efficient Data with Diffusion Models for Person\n  Re-Identification Pre-Training","summary":"  Existing person re-identification (Re-ID) methods principally deploy the\nImageNet-1K dataset for model initialization, which inevitably results in\nsub-optimal situations due to the large domain gap. One of the key challenges\nis that building large-scale person Re-ID datasets is time-consuming. Some\nprevious efforts address this problem by collecting person images from the\ninternet e.g., LUPerson, but it struggles to learn from unlabeled,\nuncontrollable, and noisy data. In this paper, we present a novel paradigm\nDiffusion-ReID to efficiently augment and generate diverse images based on\nknown identities without requiring any cost of data collection and annotation.\nTechnically, this paradigm unfolds in two stages: generation and filtering.\nDuring the generation stage, we propose Language Prompts Enhancement (LPE) to\nensure the ID consistency between the input image sequence and the generated\nimages. In the diffusion process, we propose a Diversity Injection (DI) module\nto increase attribute diversity. In order to make the generated data have\nhigher quality, we apply a Re-ID confidence threshold filter to further remove\nthe low-quality images. Benefiting from our proposed paradigm, we first create\na new large-scale person Re-ID dataset Diff-Person, which consists of over 777K\nimages from 5,183 identities. Next, we build a stronger person Re-ID backbone\npre-trained on our Diff-Person. Extensive experiments are conducted on four\nperson Re-ID benchmarks in six widely used settings. Compared with other\npre-training and self-supervised competitors, our approach shows significant\nsuperiority.\n","authors":["Ke Niu","Haiyang Yu","Xuelin Qian","Teng Fu","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2406.06045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06044v1","updated":"2024-06-10T06:24:19Z","published":"2024-06-10T06:24:19Z","title":"FRAG: Frequency Adapting Group for Diffusion Video Editing","summary":"  In video editing, the hallmark of a quality edit lies in its consistent and\nunobtrusive adjustment. Modification, when integrated, must be smooth and\nsubtle, preserving the natural flow and aligning seamlessly with the original\nvision. Therefore, our primary focus is on overcoming the current challenges in\nhigh quality edit to ensure that each edit enhances the final product without\ndisrupting its intended essence. However, quality deterioration such as\nblurring and flickering is routinely observed in recent diffusion video editing\nsystems. We confirm that this deterioration often stems from high-frequency\nleak: the diffusion model fails to accurately synthesize high-frequency\ncomponents during denoising process. To this end, we devise Frequency Adapting\nGroup (FRAG) which enhances the video quality in terms of consistency and\nfidelity by introducing a novel receptive field branch to preserve\nhigh-frequency components during the denoising process. FRAG is performed in a\nmodel-agnostic manner without additional training and validates the\neffectiveness on video editing benchmarks (i.e., TGVE, DAVIS).\n","authors":["Sunjae Yoon","Gwanhyeong Koo","Geonwoo Kim","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2406.06044v1.pdf","comment":"16 pages, 16 figures, ICML 2024"},{"id":"http://arxiv.org/abs/2406.06040v1","updated":"2024-06-10T06:17:55Z","published":"2024-06-10T06:17:55Z","title":"Vript: A Video Is Worth Thousands of Words","summary":"  Advancements in multimodal learning, particularly in video understanding and\ngeneration, require high-quality video-text datasets for improved model\nperformance. Vript addresses this issue with a meticulously annotated corpus of\n12K high-resolution videos, offering detailed, dense, and script-like captions\nfor over 420K clips. Each clip has a caption of ~145 words, which is over 10x\nlonger than most video-text datasets. Unlike captions only documenting static\ncontent in previous datasets, we enhance video captioning to video scripting by\ndocumenting not just the content, but also the camera operations, which include\nthe shot types (medium shot, close-up, etc) and camera movements (panning,\ntilting, etc). By utilizing the Vript, we explore three training paradigms of\naligning more text with the video modality rather than clip-caption pairs. This\nresults in Vriptor, a top-performing video captioning model among open-source\nmodels, comparable to GPT-4V in performance. Vriptor is also a powerful model\ncapable of end-to-end generation of dense and detailed captions for long\nvideos. Moreover, we introduce Vript-Hard, a benchmark consisting of three\nvideo understanding tasks that are more challenging than existing benchmarks:\nVript-HAL is the first benchmark evaluating action and object hallucinations in\nvideo LLMs, Vript-RR combines reasoning with retrieval resolving question\nambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the\ntemporal understanding of events in long videos rather than actions in short\nvideos in previous works. All code, models, and datasets are available in\nhttps://github.com/mutonix/Vript.\n","authors":["Dongjie Yang","Suyuan Huang","Chengqiang Lu","Xiaodong Han","Haoxin Zhang","Yan Gao","Yao Hu","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.06040v1.pdf","comment":"submitted to NeurIPS Dataset & Benchmark track"},{"id":"http://arxiv.org/abs/2406.06039v1","updated":"2024-06-10T06:17:33Z","published":"2024-06-10T06:17:33Z","title":"Diving into Underwater: Segment Anything Model Guided Underwater Salient\n  Instance Segmentation and A Large-scale Dataset","summary":"  With the breakthrough of large models, Segment Anything Model (SAM) and its\nextensions have been attempted to apply in diverse tasks of computer vision.\nUnderwater salient instance segmentation is a foundational and vital step for\nvarious underwater vision tasks, which often suffer from low segmentation\naccuracy due to the complex underwater circumstances and the adaptive ability\nof models. Moreover, the lack of large-scale datasets with pixel-level salient\ninstance annotations has impeded the development of machine learning techniques\nin this field. To address these issues, we construct the first large-scale\nunderwater salient instance segmentation dataset (USIS10K), which contains\n10,632 underwater images with pixel-level annotations in 7 categories from\nvarious underwater scenes. Then, we propose an Underwater Salient Instance\nSegmentation architecture based on Segment Anything Model (USIS-SAM)\nspecifically for the underwater domain. We devise an Underwater Adaptive Visual\nTransformer (UA-ViT) encoder to incorporate underwater domain visual prompts\ninto the segmentation network. We further design an out-of-the-box underwater\nSalient Feature Prompter Generator (SFPG) to automatically generate salient\nprompters instead of explicitly providing foreground points or boxes as prompts\nin SAM. Comprehensive experimental results show that our USIS-SAM method can\nachieve superior performance on USIS10K datasets compared to the\nstate-of-the-art methods. Datasets and codes are released on\nhttps://github.com/LiamLian0727/USIS10K.\n","authors":["Shijie Lian","Ziyi Zhang","Hua Li","Wenjie Li","Laurence Tianruo Yang","Sam Kwong","Runmin Cong"],"pdf_url":"https://arxiv.org/pdf/2406.06039v1.pdf","comment":"Accepted to ICML 2024, Code released at:\n  https://github.com/LiamLian0727/USIS10K"},{"id":"http://arxiv.org/abs/2406.06037v1","updated":"2024-06-10T06:06:38Z","published":"2024-06-10T06:06:38Z","title":"Investigating Pre-Training Objectives for Generalization in Vision-Based\n  Reinforcement Learning","summary":"  Recently, various pre-training methods have been introduced in vision-based\nReinforcement Learning (RL). However, their generalization ability remains\nunclear due to evaluations being limited to in-distribution environments and\nnon-unified experimental setups. To address this, we introduce the Atari\nPre-training Benchmark (Atari-PB), which pre-trains a ResNet-50 model on 10\nmillion transitions from 50 Atari games and evaluates it across diverse\nenvironment distributions. Our experiments show that pre-training objectives\nfocused on learning task-agnostic features (e.g., identifying objects and\nunderstanding temporal dynamics) enhance generalization across different\nenvironments. In contrast, objectives focused on learning task-specific\nknowledge (e.g., identifying agents and fitting reward functions) improve\nperformance in environments similar to the pre-training dataset but not in\nvaried ones. We publicize our codes, datasets, and model checkpoints at\nhttps://github.com/dojeon-ai/Atari-PB.\n","authors":["Donghu Kim","Hojoon Lee","Kyungmin Lee","Dongyoon Hwang","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.06037v1.pdf","comment":"accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2406.06028v1","updated":"2024-06-10T05:40:03Z","published":"2024-06-10T05:40:03Z","title":"ReCon1M:A Large-scale Benchmark Dataset for Relation Comprehension in\n  Remote Sensing Imagery","summary":"  Scene Graph Generation (SGG) is a high-level visual understanding and\nreasoning task aimed at extracting entities (such as objects) and their\ninterrelationships from images. Significant progress has been made in the study\nof SGG in natural images in recent years, but its exploration in the domain of\nremote sensing images remains very limited. The complex characteristics of\nremote sensing images necessitate higher time and manual interpretation costs\nfor annotation compared to natural images. The lack of a large-scale public SGG\nbenchmark is a major impediment to the advancement of SGG-related research in\naerial imagery. In this paper, we introduce the first publicly available\nlarge-scale, million-level relation dataset in the field of remote sensing\nimages which is named as ReCon1M. Specifically, our dataset is built upon\nFair1M and comprises 21,392 images. It includes annotations for 859,751 object\nbounding boxes across 60 different categories, and 1,149,342 relation triplets\nacross 64 categories based on these bounding boxes. We provide a detailed\ndescription of the dataset's characteristics and statistical information. We\nconducted two object detection tasks and three sub-tasks within SGG on this\ndataset, assessing the performance of mainstream methods on these tasks.\n","authors":["Xian Sun","Qiwei Yan","Chubo Deng","Chenglong Liu","Yi Jiang","Zhongyan Hou","Wanxuan Lu","Fanglong Yao","Xiaoyu Liu","Lingxiang Hao","Hongfeng Yu"],"pdf_url":"https://arxiv.org/pdf/2406.06028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.09571v8","updated":"2024-06-10T05:30:38Z","published":"2023-04-19T11:19:10Z","title":"LLIC: Large Receptive Field Transform Coding with Adaptive Weights for\n  Learned Image Compression","summary":"  The effective receptive field (ERF) plays an important role in transform\ncoding, which determines how much redundancy can be removed during transform\nand how many spatial priors can be utilized to synthesize textures during\ninverse transform. Existing methods rely on stacks of small kernels, whose ERFs\nremain insufficiently large, or heavy non-local attention mechanisms, which\nlimit the potential of high-resolution image coding. To tackle this issue, we\npropose Large Receptive Field Transform Coding with Adaptive Weights for\nLearned Image Compression (LLIC). Specifically, for the first time in the\nlearned image compression community, we introduce a few large kernelbased\ndepth-wise convolutions to reduce more redundancy while maintaining modest\ncomplexity. Due to the wide range of image diversity, we further propose a\nmechanism to augment convolution adaptability through the self-conditioned\ngeneration of weights. The large kernels cooperate with non-linear embedding\nand gate mechanisms for better expressiveness and lighter pointwise\ninteractions. Our investigation extends to refined training methods that unlock\nthe full potential of these large kernels. Moreover, to promote more dynamic\ninter-channel interactions, we introduce an adaptive channel-wise bit\nallocation strategy that autonomously generates channel importance factors in a\nself-conditioned manner. To demonstrate the effectiveness of the proposed\ntransform coding, we align the entropy model to compare with existing transform\nmethods and obtain models LLIC-STF, LLIC-ELIC, and LLIC-TCM. Extensive\nexperiments demonstrate that our proposed LLIC models have significant\nimprovements over the corresponding baselines and reduce the BD-Rate by 9.49%,\n9.47%, 10.94% on Kodak over VTM-17.0 Intra, respectively. Our LLIC models\nachieve state-of-the-art performances and better trade-offs between performance\nand complexity.\n","authors":["Wei Jiang","Peirong Ning","Jiayu Yang","Yongqi Zhai","Feng Gao","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2304.09571v8.pdf","comment":"Accepted to IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2309.13242v2","updated":"2024-06-10T05:17:16Z","published":"2023-09-23T03:22:48Z","title":"UniHead: Unifying Multi-Perception for Detection Heads","summary":"  The detection head constitutes a pivotal component within object detectors,\ntasked with executing both classification and localization functions.\nRegrettably, the commonly used parallel head often lacks omni perceptual\ncapabilities, such as deformation perception, global perception and cross-task\nperception. Despite numerous methods attempting to enhance these abilities from\na single aspect, achieving a comprehensive and unified solution remains a\nsignificant challenge. In response to this challenge, we develop an innovative\ndetection head, termed UniHead, to unify three perceptual abilities\nsimultaneously. More precisely, our approach (1) introduces deformation\nperception, enabling the model to adaptively sample object features; (2)\nproposes a Dual-axial Aggregation Transformer (DAT) to adeptly model long-range\ndependencies, thereby achieving global perception; and (3) devises a Cross-task\nInteraction Transformer (CIT) that facilitates interaction between the\nclassification and localization branches, thus aligning the two tasks. As a\nplug-and-play method, the proposed UniHead can be conveniently integrated with\nexisting detectors. Extensive experiments on the COCO dataset demonstrate that\nour UniHead can bring significant improvements to many detectors. For instance,\nthe UniHead can obtain +2.7 AP gains in RetinaNet, +2.9 AP gains in FreeAnchor,\nand +2.1 AP gains in GFL. The code is available at\nhttps://github.com/zht8506/UniHead.\n","authors":["Hantao Zhou","Rui Yang","Yachao Zhang","Haoran Duan","Yawen Huang","Runze Hu","Xiu Li","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2309.13242v2.pdf","comment":"Accepted by TNNLS"},{"id":"http://arxiv.org/abs/2406.06007v1","updated":"2024-06-10T04:07:09Z","published":"2024-06-10T04:07:09Z","title":"CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision\n  Language Models","summary":"  Artificial intelligence has significantly impacted medical applications,\nparticularly with the advent of Medical Large Vision Language Models\n(Med-LVLMs), sparking optimism for the future of automated and personalized\nhealthcare. However, the trustworthiness of Med-LVLMs remains unverified,\nposing significant risks for future model deployment. In this paper, we\nintroduce CARES and aim to comprehensively evaluate the Trustworthiness of\nMed-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs\nacross five dimensions, including trustfulness, fairness, safety, privacy, and\nrobustness. CARES comprises about 41K question-answer pairs in both closed and\nopen-ended formats, covering 16 medical image modalities and 27 anatomical\nregions. Our analysis reveals that the models consistently exhibit concerns\nregarding trustworthiness, often displaying factual inaccuracies and failing to\nmaintain fairness across different demographic groups. Furthermore, they are\nvulnerable to attacks and demonstrate a lack of privacy awareness. We publicly\nrelease our benchmark and code in https://github.com/richard-peng-xia/CARES.\n","authors":["Peng Xia","Ze Chen","Juanxi Tian","Yangrui Gong","Ruibo Hou","Yue Xu","Zhenbang Wu","Zhiyuan Fan","Yiyang Zhou","Kangyu Zhu","Wenhao Zheng","Zhaoyang Wang","Xiao Wang","Xuchao Zhang","Chetan Bansal","Marc Niethammer","Junzhou Huang","Hongtu Zhu","Yun Li","Jimeng Sun","Zongyuan Ge","Gang Li","James Zou","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2406.06007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06004v1","updated":"2024-06-10T03:57:39Z","published":"2024-06-10T03:57:39Z","title":"FLEUR: An Explainable Reference-Free Evaluation Metric for Image\n  Captioning Using a Large Multimodal Model","summary":"  Most existing image captioning evaluation metrics focus on assigning a single\nnumerical score to a caption by comparing it with reference captions. However,\nthese methods do not provide an explanation for the assigned score. Moreover,\nreference captions are expensive to acquire. In this paper, we propose FLEUR,\nan explainable reference-free metric to introduce explainability into image\ncaptioning evaluation metrics. By leveraging a large multimodal model, FLEUR\ncan evaluate the caption against the image without the need for reference\ncaptions, and provide the explanation for the assigned score. We introduce\nscore smoothing to align as closely as possible with human judgment and to be\nrobust to user-defined grading criteria. FLEUR achieves high correlations with\nhuman judgment across various image captioning evaluation benchmarks and\nreaches state-of-the-art results on Flickr8k-CF, COMPOSITE, and Pascal-50S\nwithin the domain of reference-free evaluation metrics. Our source code and\nresults are publicly available at: https://github.com/Yebin46/FLEUR.\n","authors":["Yebin Lee","Imseong Park","Myungjoo Kang"],"pdf_url":"https://arxiv.org/pdf/2406.06004v1.pdf","comment":"Accepted at ACL (Main) 2024"},{"id":"http://arxiv.org/abs/2406.05992v1","updated":"2024-06-10T03:24:43Z","published":"2024-06-10T03:24:43Z","title":"MHS-VM: Multi-Head Scanning in Parallel Subspaces for Vision Mamba","summary":"  Recently, State Space Models (SSMs), with Mamba as a prime example, have\nshown great promise for long-range dependency modeling with linear complexity.\nThen, Vision Mamba and the subsequent architectures are presented successively,\nand they perform well on visual tasks. The crucial step of applying Mamba to\nvisual tasks is to construct 2D visual features in sequential manners. To\neffectively organize and construct visual features within the 2D image space\nthrough 1D selective scan, we propose a novel Multi-Head Scan (MHS) module. The\nembeddings extracted from the preceding layer are projected into multiple\nlower-dimensional subspaces. Subsequently, within each subspace, the selective\nscan is performed along distinct scan routes. The resulting sub-embeddings,\nobtained from the multi-head scan process, are then integrated and ultimately\nprojected back into the high-dimensional space. Moreover, we incorporate a Scan\nRoute Attention (SRA) mechanism to enhance the module's capability to discern\ncomplex structures. To validate the efficacy of our module, we exclusively\nsubstitute the 2D-Selective-Scan (SS2D) block in VM-UNet with our proposed\nmodule, and we train our models from scratch without using any pre-trained\nweights. The results indicate a significant improvement in performance while\nreducing the parameters of the original VM-UNet. The code for this study is\npublicly available at https://github.com/PixDeep/MHS-VM.\n","authors":["Zhongping Ji"],"pdf_url":"https://arxiv.org/pdf/2406.05992v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.00949v3","updated":"2024-06-10T03:16:09Z","published":"2023-11-02T02:33:09Z","title":"POS: A Prompts Optimization Suite for Augmenting Text-to-Video\n  Generation","summary":"  This paper targets to enhance the diffusion-based text-to-video generation by\nimproving the two input prompts, including the noise and the text. Accommodated\nwith this goal, we propose POS, a training-free Prompt Optimization Suite to\nboost text-to-video models. POS is motivated by two observations: (1) Video\ngeneration shows instability in terms of noise. Given the same text, different\nnoises lead to videos that differ significantly in terms of both frame quality\nand temporal consistency. This observation implies that there exists an optimal\nnoise matched to each textual input; To capture the potential noise, we propose\nan optimal noise approximator to approach the potential optimal noise.\nParticularly, the optimal noise approximator initially searches a video that\nclosely relates to the text prompt and then inverts it into the noise space to\nserve as an improved noise prompt for the textual input. (2) Improving the text\nprompt via LLMs often causes semantic deviation. Many existing text-to-vision\nworks have utilized LLMs to improve the text prompts for generation\nenhancement. However, existing methods often neglect the semantic alignment\nbetween the original text and the rewritten one. In response to this issue, we\ndesign a semantic-preserving rewriter to impose contraints in both rewritng and\ndenoising phrases to preserve the semantic consistency. Extensive experiments\non popular benchmarks show that our POS can improve the text-to-video models\nwith a clear margin. The code will be open-sourced.\n","authors":["Shijie Ma","Huayi Xu","Mengjian Li","Weidong Geng","Yaxiong Wang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2311.00949v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05980v1","updated":"2024-06-10T02:42:25Z","published":"2024-06-10T02:42:25Z","title":"Causality-inspired Latent Feature Augmentation for Single Domain\n  Generalization","summary":"  Single domain generalization (Single-DG) intends to develop a generalizable\nmodel with only one single training domain to perform well on other unknown\ntarget domains. Under the domain-hungry configuration, how to expand the\ncoverage of source domain and find intrinsic causal features across different\ndistributions is the key to enhancing the models' generalization ability.\nExisting methods mainly depend on the meticulous design of finite image-level\ntransformation techniques and learning invariant features across domains based\non statistical correlation between samples and labels in source domain. This\nmakes it difficult to capture stable semantics between source and target\ndomains, which hinders the improvement of the model's generalization\nperformance. In this paper, we propose a novel causality-inspired latent\nfeature augmentation method for Single-DG by learning the meta-knowledge of\nfeature-level transformation based on causal learning and interventions.\nInstead of strongly relying on the finite image-level transformation, with the\nlearned meta-knowledge, we can generate diverse implicit feature-level\ntransformations in latent space based on the consistency of causal features and\ndiversity of non-causal features, which can better compensate for the\ndomain-hungry defect and reduce the strong reliance on initial finite\nimage-level transformations and capture more stable domain-invariant causal\nfeatures for generalization. Extensive experiments on several open-access\nbenchmarks demonstrate the outstanding performance of our model over other\nstate-of-the-art single domain generalization and also multi-source domain\ngeneralization methods.\n","authors":["Jian Xu","Chaojie Ji","Yankai Cao","Ye Li","Ruxin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.05980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05974v1","updated":"2024-06-10T02:20:26Z","published":"2024-06-10T02:20:26Z","title":"Inter-slice Super-resolution of Magnetic Resonance Images by\n  Pre-training and Self-supervised Fine-tuning","summary":"  In clinical practice, 2D magnetic resonance (MR) sequences are widely\nadopted. While individual 2D slices can be stacked to form a 3D volume, the\nrelatively large slice spacing can pose challenges for both image visualization\nand subsequent analysis tasks, which often require isotropic voxel spacing. To\nreduce slice spacing, deep-learning-based super-resolution techniques are\nwidely investigated. However, most current solutions require a substantial\nnumber of paired high-resolution and low-resolution images for supervised\ntraining, which are typically unavailable in real-world scenarios. In this\nwork, we propose a self-supervised super-resolution framework for inter-slice\nsuper-resolution of MR images. Our framework is first featured by pre-training\non video dataset, as temporal correlation of videos is found beneficial for\nmodeling the spatial relation among MR slices. Then, we use public high-quality\nMR dataset to fine-tune our pre-trained model, for enhancing awareness of our\nmodel to medical data. Finally, given a target dataset at hand, we utilize\nself-supervised fine-tuning to further ensure our model works well with\nuser-specific super-resolution tasks. The proposed method demonstrates superior\nperformance compared to other self-supervised methods and also holds the\npotential to benefit various downstream applications.\n","authors":["Xin Wang","Zhiyun Song","Yitao Zhu","Sheng Wang","Lichi Zhang","Dinggang Shen","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2406.05974v1.pdf","comment":"ISBI 2024"},{"id":"http://arxiv.org/abs/2406.05967v1","updated":"2024-06-10T01:59:00Z","published":"2024-06-10T01:59:00Z","title":"CVQA: Culturally-diverse Multilingual Visual Question Answering\n  Benchmark","summary":"  Visual Question Answering (VQA) is an important task in multimodal AI, and it\nis often used to test the ability of vision-language models to understand and\nreason on knowledge present in both visual and textual data. However, most of\nthe current VQA models use datasets that are primarily focused on English and a\nfew major world languages, with images that are typically Western-centric.\nWhile recent efforts have tried to increase the number of languages covered on\nVQA datasets, they still lack diversity in low-resource languages. More\nimportantly, although these datasets often extend their linguistic range via\ntranslation or some other approaches, they usually keep images the same,\nresulting in narrow cultural representation. To address these limitations, we\nconstruct CVQA, a new Culturally-diverse multilingual Visual Question Answering\nbenchmark, designed to cover a rich set of languages and cultures, where we\nengage native speakers and cultural experts in the data collection process. As\na result, CVQA includes culturally-driven images and questions from across 28\ncountries on four continents, covering 26 languages with 11 scripts, providing\na total of 9k questions. We then benchmark several Multimodal Large Language\nModels (MLLMs) on CVQA, and show that the dataset is challenging for the\ncurrent state-of-the-art models. This benchmark can serve as a probing\nevaluation suite for assessing the cultural capability and bias of multimodal\nmodels and hopefully encourage more research efforts toward increasing cultural\nawareness and linguistic diversity in this field.\n","authors":["David Romero","Chenyang Lyu","Haryo Akbarianto Wibowo","Teresa Lynn","Injy Hamed","Aditya Nanda Kishore","Aishik Mandal","Alina Dragonetti","Artem Abzaliev","Atnafu Lambebo Tonja","Bontu Fufa Balcha","Chenxi Whitehouse","Christian Salamea","Dan John Velasco","David Ifeoluwa Adelani","David Le Meur","Emilio Villa-Cueva","Fajri Koto","Fauzan Farooqui","Frederico Belcavello","Ganzorig Batnasan","Gisela Vallejo","Grainne Caulfield","Guido Ivetta","Haiyue Song","Henok Biadglign Ademtew","Hernán Maina","Holy Lovenia","Israel Abebe Azime","Jan Christian Blaise Cruz","Jay Gala","Jiahui Geng","Jesus-German Ortiz-Barajas","Jinheon Baek","Jocelyn Dunstan","Laura Alonso Alemany","Kumaranage Ravindu Yasas Nagasinghe","Luciana Benotti","Luis Fernando D'Haro","Marcelo Viridiano","Marcos Estecha-Garitagoitia","Maria Camila Buitrago Cabrera","Mario Rodríguez-Cantelar","Mélanie Jouitteau","Mihail Mihaylov","Mohamed Fazli Mohamed Imam","Muhammad Farid Adilazuarda","Munkhjargal Gochoo","Munkh-Erdene Otgonbold","Naome Etori","Olivier Niyomugisha","Paula Mónica Silva","Pranjal Chitale","Raj Dabre","Rendi Chevi","Ruochen Zhang","Ryandito Diandaru","Samuel Cahyawijaya","Santiago Góngora","Soyeong Jeong","Sukannya Purkayastha","Tatsuki Kuribayashi","Thanmay Jayakumar","Tiago Timponi Torrent","Toqeer Ehsan","Vladimir Araujo","Yova Kementchedjhieva","Zara Burzo","Zheng Wei Lim","Zheng Xin Yong","Oana Ignat","Joan Nwatu","Rada Mihalcea","Thamar Solorio","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2406.05967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05963v1","updated":"2024-06-10T01:45:55Z","published":"2024-06-10T01:45:55Z","title":"Solution for SMART-101 Challenge of CVPR Multi-modal Algorithmic\n  Reasoning Task 2024","summary":"  In this paper, the solution of HYU MLLAB KT Team to the Multimodal\nAlgorithmic Reasoning Task: SMART-101 CVPR 2024 Challenge is presented. Beyond\nconventional visual question-answering problems, the SMART-101 challenge aims\nto achieve human-level multimodal understanding by tackling complex\nvisio-linguistic puzzles designed for children in the 6-8 age group. To solve\nthis problem, we suggest two main ideas. First, to utilize the reasoning\nability of a large-scale language model (LLM), the given visual cues (images)\nare grounded in the text modality. For this purpose, we generate highly\ndetailed text captions that describe the context of the image and use these\ncaptions as input for the LLM. Second, due to the nature of puzzle images,\nwhich often contain various geometric visual patterns, we utilize an object\ndetection algorithm to ensure these patterns are not overlooked in the\ncaptioning process. We employed the SAM algorithm, which can detect\nvarious-size objects, to capture the visual features of these geometric\npatterns and used this information as input for the LLM. Under the puzzle split\nconfiguration, we achieved an option selection accuracy Oacc of 29.5 on the\ntest set and a weighted option selection accuracy (WOSA) of 27.1 on the\nchallenge set.\n","authors":["Jinwoo Ahn","Junhyeok Park","Min-Jun Kim","Kang-Hyeon Kim","So-Yeong Sohn","Yun-Ji Lee","Du-Seong Chang","Yu-Jung Heo","Eun-Sol Kim"],"pdf_url":"https://arxiv.org/pdf/2406.05963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05424v2","updated":"2024-06-10T01:36:53Z","published":"2023-06-08T17:59:56Z","title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and\n  Language Models","summary":"  Conversation agents fueled by Large Language Models (LLMs) are providing a\nnew way to interact with visual data. While there have been initial attempts\nfor image-based conversation models, this work addresses the under-explored\nfield of \\emph{video-based conversation} by introducing Video-ChatGPT. It is a\nmultimodal model that merges a video-adapted visual encoder with an LLM. The\nresulting model is capable of understanding and generating detailed\nconversations about videos. We introduce a new dataset of 100,000\nvideo-instruction pairs used to train Video-ChatGPT acquired via manual and\nsemi-automated pipeline that is easily scalable and robust to label noise. We\nalso develop a quantitative evaluation framework for video-based dialogue\nmodels to objectively analyze the strengths and weaknesses of video-based\ndialogue models. Code: https://github.com/mbzuai-oryx/Video-ChatGPT.\n","authors":["Muhammad Maaz","Hanoona Rasheed","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2306.05424v2.pdf","comment":"ACL 2024 (Main)"},{"id":"http://arxiv.org/abs/2403.03309v5","updated":"2024-06-10T01:13:22Z","published":"2024-03-05T20:21:49Z","title":"Learning Zero-Shot Material States Segmentation, by Implanting Natural\n  Image Patterns in Synthetic Data","summary":"  Visual recognition of materials and their states is essential for\nunderstanding the physical world, from identifying wet regions on surfaces or\nstains on fabrics to detecting infected areas on plants or minerals in rocks.\nCollecting data that captures this vast variability is complex due to the\nscattered and gradual nature of material states. Manually annotating real-world\nimages is constrained by cost and precision, while synthetic data, although\naccurate and inexpensive, lacks real-world diversity. This work aims to bridge\nthis gap by infusing patterns automatically extracted from real-world images\ninto synthetic data. Hence, patterns collected from natural images are used to\ngenerate and map materials into synthetic scenes. This unsupervised approach\ncaptures the complexity of the real world while maintaining the precision and\nscalability of synthetic data. We also present the first comprehensive\nbenchmark for zero-shot material state segmentation, utilizing real-world\nimages across a diverse range of domains, including food, soils, construction,\nplants, liquids, and more, each appears in various states such as wet, dry,\ninfected, cooked, burned, and many others. The annotation includes partial\nsimilarity between regions with similar but not identical materials and hard\nsegmentation of only identical material states. This benchmark eluded top\nfoundation models, exposing the limitations of existing data collection\nmethods. Meanwhile, nets trained on the infused data performed significantly\nbetter on this and related tasks. The dataset, code, and trained model are\navailable. We also share 300,000 extracted textures and SVBRDF/PBR materials to\nfacilitate future datasets generation.\n","authors":["Sagi Eppel","Jolina Li","Manuel Drehwald","Alan Aspuru-Guzik"],"pdf_url":"https://arxiv.org/pdf/2403.03309v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14773v3","updated":"2024-06-10T01:06:49Z","published":"2023-11-24T13:56:06Z","title":"Set Features for Anomaly Detection","summary":"  This paper proposes to use set features for detecting anomalies in samples\nthat consist of unusual combinations of normal elements. Many leading methods\ndiscover anomalies by detecting an unusual part of a sample. For example,\nstate-of-the-art segmentation-based approaches, first classify each element of\nthe sample (e.g., image patch) as normal or anomalous and then classify the\nentire sample as anomalous if it contains anomalous elements. However, such\napproaches do not extend well to scenarios where the anomalies are expressed by\nan unusual combination of normal elements. In this paper, we overcome this\nlimitation by proposing set features that model each sample by the distribution\nof its elements. We compute the anomaly score of each sample using a simple\ndensity estimation method, using fixed features. Our approach outperforms the\nprevious state-of-the-art in image-level logical anomaly detection and\nsequence-level time series anomaly detection.\n","authors":["Niv Cohen","Issar Tzachor","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2311.14773v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2302.12245"},{"id":"http://arxiv.org/abs/2312.08012v2","updated":"2024-06-10T00:22:46Z","published":"2023-12-13T09:34:01Z","title":"uSF: Learning Neural Semantic Field with Uncertainty","summary":"  Recently, there has been an increased interest in NeRF methods which\nreconstruct differentiable representation of three-dimensional scenes. One of\nthe main limitations of such methods is their inability to assess the\nconfidence of the model in its predictions. In this paper, we propose a new\nneural network model for the formation of extended vector representations,\ncalled uSF, which allows the model to predict not only color and semantic label\nof each point, but also estimate the corresponding values of uncertainty. We\nshow that with a small number of images available for training, a model\nquantifying uncertainty performs better than a model without such\nfunctionality. Code of the uSF approach is publicly available at\nhttps://github.com/sevashasla/usf/.\n","authors":["Vsevolod Skorokhodov","Darya Drozdova","Dmitry Yudin"],"pdf_url":"https://arxiv.org/pdf/2312.08012v2.pdf","comment":"12 pages, 4 figures. This a preprint of the Work accepted for\n  publication in Optical Memory and Neural Networks (Information Optics),\n  \\copyright, copyright 2024, Optical Memory and Neural Networks;\n  https://www.pleiades.online/en/journal/optmem/"},{"id":"http://arxiv.org/abs/2405.03103v2","updated":"2024-06-10T23:41:18Z","published":"2024-05-06T01:39:59Z","title":"Learning from Students: Applying t-Distributions to Explore Accurate and\n  Efficient Formats for LLMs","summary":"  The increasing size of large language models (LLMs) traditionally requires\nlow-precision integer formats to meet strict latency and power demands. Yet\nrecently, alternative formats such as Normal Float (NF4) have increased model\naccuracy at the cost of increased chip area. In this work, we first conduct a\nlarge-scale analysis of LLM weights and activations across 30 networks and\nconclude that most distributions follow a Student's t-distribution. We then\nderive a new theoretically optimal format, Student Float (SF4), that improves\nover NF4 across modern LLMs, for example increasing the average accuracy on\nLLaMA2-7B by 0.76% across tasks. Using this format as a high-accuracy\nreference, we then propose augmenting E2M1 with two variants of supernormal\nsupport for higher model accuracy. Finally, we explore the quality and\nefficiency frontier across 11 datatypes by evaluating their model accuracy and\nhardware complexity. We discover a Pareto curve composed of INT4, E2M1, and\nE2M1 with supernormal support, which offers a continuous tradeoff between model\naccuracy and chip area. For example, E2M1 with supernormal support increases\nthe accuracy of Phi-2 by up to 2.19% with 1.22% area overhead, enabling more\nLLM-based applications to be run at four bits. The supporting code is hosted at\nhttps://github.com/cornell-zhang/llm-datatypes.\n","authors":["Jordan Dotzel","Yuzong Chen","Bahaa Kotb","Sushma Prasad","Gang Wu","Sheng Li","Mohamed S. Abdelfattah","Zhiru Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.03103v2.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2311.17647v2","updated":"2024-06-10T23:39:24Z","published":"2023-11-29T14:08:53Z","title":"Text as Images: Can Multimodal Large Language Models Follow Printed\n  Instructions in Pixels?","summary":"  Recent multimodal large language models (MLLMs) have shown promising\ninstruction following capabilities on vision-language tasks. In this work, we\nintroduce VISUAL MODALITY INSTRUCTION (VIM), and investigate how well\nmultimodal models can understand textual instructions provided in pixels,\ndespite not being explicitly trained on such data during pretraining or\nfine-tuning. We adapt VIM to eight benchmarks, including OKVQA, MM-Vet,\nMathVista, MMMU, and probe diverse MLLMs in both the text-modality instruction\n(TEM) setting and VIM setting. Notably, we observe a significant performance\ndisparity between the original TEM and VIM settings for open-source MLLMs,\nindicating that open-source MLLMs face greater challenges when text instruction\nis presented solely in image form. To address this issue, we train v-MLLM, a\ngeneralizable model that is capable to conduct robust instruction following in\nboth text-modality and visual-modality instructions.\n","authors":["Xiujun Li","Yujie Lu","Zhe Gan","Jianfeng Gao","William Yang Wang","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2311.17647v2.pdf","comment":"Github: https://github.com/VIM-Bench/VIM_TOOL, Model and Data:\n  https://huggingface.co/VIM-Bench"},{"id":"http://arxiv.org/abs/2406.06848v1","updated":"2024-06-10T23:36:58Z","published":"2024-06-10T23:36:58Z","title":"Taxes Are All You Need: Integration of Taxonomical Hierarchy\n  Relationships into the Contrastive Loss","summary":"  In this work, we propose a novel supervised contrastive loss that enables the\nintegration of taxonomic hierarchy information during the representation\nlearning process. A supervised contrastive loss operates by enforcing that\nimages with the same class label (positive samples) project closer to each\nother than images with differing class labels (negative samples). The advantage\nof this approach is that it directly penalizes the structure of the\nrepresentation space itself. This enables greater flexibility with respect to\nencoding semantic concepts. However, the standard supervised contrastive loss\nonly enforces semantic structure based on the downstream task (i.e. the class\nlabel). In reality, the class label is only one level of a \\emph{hierarchy of\ndifferent semantic relationships known as a taxonomy}. For example, the class\nlabel is oftentimes the species of an animal, but between different classes\nthere are higher order relationships such as all animals with wings being\n``birds\". We show that by explicitly accounting for these relationships with a\nweighting penalty in the contrastive loss we can out-perform the supervised\ncontrastive loss. Additionally, we demonstrate the adaptability of the notion\nof a taxonomy by integrating our loss into medical and noise-based settings\nthat show performance improvements by as much as 7%.\n","authors":["Kiran Kokilepersaud","Yavuz Yarici","Mohit Prabhushankar","Ghassan AlRegib"],"pdf_url":"https://arxiv.org/pdf/2406.06848v1.pdf","comment":"Accepted at IEEE International Conference on Image Processing"},{"id":"http://arxiv.org/abs/2406.06847v1","updated":"2024-06-10T23:31:36Z","published":"2024-06-10T23:31:36Z","title":"Generalized W-Net: Arbitrary-style Chinese Character Synthesization","summary":"  Synthesizing Chinese characters with consistent style using few stylized\nexamples is challenging. Existing models struggle to generate arbitrary style\ncharacters with limited examples. In this paper, we propose the Generalized\nW-Net, a novel class of W-shaped architectures that addresses this. By\nincorporating Adaptive Instance Normalization and introducing multi-content,\nour approach can synthesize Chinese characters in any desired style, even with\nlimited examples. It handles seen and unseen styles during training and can\ngenerate new character contents. Experimental results demonstrate the\neffectiveness of our approach.\n","authors":["Haochuan Jiang","Guanyu Yang","Fei Cheng","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2406.06847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06843v1","updated":"2024-06-10T23:25:19Z","published":"2024-06-10T23:25:19Z","title":"HO-Cap: A Capture System and Dataset for 3D Reconstruction and Pose\n  Tracking of Hand-Object Interaction","summary":"  We introduce a data capture system and a new dataset named HO-Cap that can be\nused to study 3D reconstruction and pose tracking of hands and objects in\nvideos. The capture system uses multiple RGB-D cameras and a HoloLens headset\nfor data collection, avoiding the use of expensive 3D scanners or mocap\nsystems. We propose a semi-automatic method to obtain annotations of shape and\npose of hands and objects in the collected videos, which significantly reduces\nthe required annotation time compared to manual labeling. With this system, we\ncaptured a video dataset of humans using objects to perform different tasks, as\nwell as simple pick-and-place and handover of an object from one hand to the\nother, which can be used as human demonstrations for embodied AI and robot\nmanipulation research. Our data capture setup and annotation framework can be\nused by the community to reconstruct 3D shapes of objects and human hands and\ntrack their poses in videos.\n","authors":["Jikai Wang","Qifan Zhang","Yu-Wei Chao","Bowen Wen","Xiaohu Guo","Yu Xiang"],"pdf_url":"https://arxiv.org/pdf/2406.06843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03659v2","updated":"2024-06-10T22:51:42Z","published":"2024-05-06T17:36:44Z","title":"A Construct-Optimize Approach to Sparse View Synthesis without Camera\n  Pose","summary":"  Novel view synthesis from a sparse set of input images is a challenging\nproblem of great practical interest, especially when camera poses are absent or\ninaccurate. Direct optimization of camera poses and usage of estimated depths\nin neural radiance field algorithms usually do not produce good results because\nof the coupling between poses and depths, and inaccuracies in monocular depth\nestimation. In this paper, we leverage the recent 3D Gaussian splatting method\nto develop a novel construct-and-optimize method for sparse view synthesis\nwithout camera poses. Specifically, we construct a solution progressively by\nusing monocular depth and projecting pixels back into the 3D world. During\nconstruction, we optimize the solution by detecting 2D correspondences between\ntraining views and the corresponding rendered images. We develop a unified\ndifferentiable pipeline for camera registration and adjustment of both camera\nposes and depths, followed by back-projection. We also introduce a novel notion\nof an expected surface in Gaussian splatting, which is critical to our\noptimization. These steps enable a coarse solution, which can then be low-pass\nfiltered and refined using standard optimization methods. We demonstrate\nresults on the Tanks and Temples and Static Hikes datasets with as few as three\nwidely-spaced views, showing significantly better quality than competing\nmethods, including those with approximate camera pose information. Moreover,\nour results improve with more views and outperform previous InstantNGP and\nGaussian Splatting algorithms even when using half the dataset. Project page:\nhttps://raymondjiangkw.github.io/cogs.github.io/\n","authors":["Kaiwen Jiang","Yang Fu","Mukund Varma T","Yash Belhe","Xiaolong Wang","Hao Su","Ravi Ramamoorthi"],"pdf_url":"https://arxiv.org/pdf/2405.03659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06820v1","updated":"2024-06-10T22:07:57Z","published":"2024-06-10T22:07:57Z","title":"Adapters Strike Back","summary":"  Adapters provide an efficient and lightweight mechanism for adapting trained\ntransformer models to a variety of different tasks. However, they have often\nbeen found to be outperformed by other adaptation mechanisms, including\nlow-rank adaptation. In this paper, we provide an in-depth study of adapters,\ntheir internal structure, as well as various implementation choices. We uncover\npitfalls for using adapters and suggest a concrete, improved adapter\narchitecture, called Adapter+, that not only outperforms previous adapter\nimplementations but surpasses a number of other, more complex adaptation\nmechanisms in several challenging settings. Despite this, our suggested adapter\nis highly robust and, unlike previous work, requires little to no manual\nintervention when addressing a novel scenario. Adapter+ reaches\nstate-of-the-art average accuracy on the VTAB benchmark, even without a\nper-task hyperparameter optimization.\n","authors":["Jan-Martin O. Steitz","Stefan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.06820v1.pdf","comment":"To appear at CVPR 2024. Code: https://github.com/visinf/adapter_plus"},{"id":"http://arxiv.org/abs/2406.06813v1","updated":"2024-06-10T21:44:52Z","published":"2024-06-10T21:44:52Z","title":"Stable Neighbor Denoising for Source-free Domain Adaptive Segmentation","summary":"  We study source-free unsupervised domain adaptation (SFUDA) for semantic\nsegmentation, which aims to adapt a source-trained model to the target domain\nwithout accessing the source data. Many works have been proposed to address\nthis challenging problem, among which uncertainty-based self-training is a\npredominant approach. However, without comprehensive denoising mechanisms, they\nstill largely fall into biased estimates when dealing with different domains\nand confirmation bias. In this paper, we observe that pseudo-label noise is\nmainly contained in unstable samples in which the predictions of most pixels\nundergo significant variations during self-training. Inspired by this, we\npropose a novel mechanism to denoise unstable samples with stable ones.\nSpecifically, we introduce the Stable Neighbor Denoising (SND) approach, which\neffectively discovers highly correlated stable and unstable samples by nearest\nneighbor retrieval and guides the reliable optimization of unstable samples by\nbi-level learning. Moreover, we compensate for the stable set by object-level\nobject paste, which can further eliminate the bias caused by less learned\nclasses. Our SND enjoys two advantages. First, SND does not require a specific\nsegmentor structure, endowing its universality. Second, SND simultaneously\naddresses the issues of class, domain, and confirmation biases during\nadaptation, ensuring its effectiveness. Extensive experiments show that SND\nconsistently outperforms state-of-the-art methods in various SFUDA semantic\nsegmentation settings. In addition, SND can be easily integrated with other\napproaches, obtaining further improvements.\n","authors":["Dong Zhao","Shuang Wang","Qi Zang","Licheng Jiao","Nicu Sebe","Zhun Zhong"],"pdf_url":"https://arxiv.org/pdf/2406.06813v1.pdf","comment":"2024 Conference on Computer Vision and Pattern Recognition"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.06519v1","updated":"2024-06-10T17:58:29Z","published":"2024-06-10T17:58:29Z","title":"UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance\n  Assessor","summary":"  Copious amounts of relevance judgments are necessary for the effective\ntraining and accurate evaluation of retrieval systems. Conventionally, these\njudgments are made by human assessors, rendering this process expensive and\nlaborious. A recent study by Thomas et al. from Microsoft Bing suggested that\nlarge language models (LLMs) can accurately perform the relevance assessment\ntask and provide human-quality judgments, but unfortunately their study did not\nyield any reusable software artifacts. Our work presents UMBRELA (a recursive\nacronym that stands for UMbrela is the Bing RELevance Assessor), an open-source\ntoolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o\nmodel and adds more nuance to the original paper. Across Deep Learning Tracks\nfrom TREC 2019 to 2023, we find that LLM-derived relevance judgments correlate\nhighly with rankings generated by effective multi-stage retrieval systems. Our\ntoolkit is designed to be easily extensible and can be integrated into existing\nmulti-stage retrieval and evaluation pipelines, offering researchers a valuable\nresource for studying retrieval evaluation methodologies. UMBRELA will be used\nin the TREC 2024 RAG Track to aid in relevance assessments, and we envision our\ntoolkit becoming a foundation for further innovation in the field. UMBRELA is\navailable at https://github.com/castorini/umbrela.\n","authors":["Shivani Upadhyay","Ronak Pradeep","Nandan Thakur","Nick Craswell","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2406.06519v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2401.07453v4","updated":"2024-06-10T17:50:14Z","published":"2024-01-15T03:57:15Z","title":"Model Editing at Scale leads to Gradual and Catastrophic Forgetting","summary":"  Editing knowledge in large language models is an attractive capability to\nhave which allows us to correct incorrectly learnt facts during pre-training,\nas well as update the model with an ever-growing list of new facts. While\nexisting model editing techniques have shown promise, they are usually\nevaluated using metrics for reliability, specificity and generalization over\none or few edits. We argue that for model editing to have practical utility, we\nmust be able to make multiple edits to the same model. With this in mind, we\nevaluate the current model editing methods at scale, focusing on two state of\nthe art methods: ROME and MEMIT. We find that as the model is edited\nsequentially with multiple facts, it continually forgets previously edited\nfacts and the ability to perform downstream tasks. This forgetting happens in\ntwo phases -- an initial gradual but progressive forgetting phase followed by\nabrupt or catastrophic forgetting phase. Both gradual and catastrophic\nforgetting limit the usefulness of model editing methods at scale -- the former\nmaking model editing less effective as multiple edits are made to the model\nwhile the latter caps the scalability of such model editing methods. Our\nanalysis also highlights other key limitations of ROME and MEMIT at scale. With\nour work, we push for the development and evaluation of model editing methods\nkeeping scalability in mind.\n","authors":["Akshat Gupta","Anurag Rao","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2401.07453v4.pdf","comment":"ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.06475v1","updated":"2024-06-10T17:16:59Z","published":"2024-06-10T17:16:59Z","title":"Survey for Landing Generative AI in Social and E-commerce Recsys -- the\n  Industry Perspectives","summary":"  Recently, generative AI (GAI), with their emerging capabilities, have\npresented unique opportunities for augmenting and revolutionizing industrial\nrecommender systems (Recsys). Despite growing research efforts at the\nintersection of these fields, the integration of GAI into industrial Recsys\nremains in its infancy, largely due to the intricate nature of modern\nindustrial Recsys infrastructure, operations, and product sophistication.\nDrawing upon our experiences in successfully integrating GAI into several major\nsocial and e-commerce platforms, this survey aims to comprehensively examine\nthe underlying system and AI foundations, solution frameworks, connections to\nkey research advancements, as well as summarize the practical insights and\nchallenges encountered in the endeavor to integrate GAI into industrial Recsys.\nAs pioneering work in this domain, we hope outline the representative\ndevelopments of relevant fields, shed lights on practical GAI adoptions in the\nindustry, and motivate future research.\n","authors":["Da Xu","Danqing Zhang","Guangyu Yang","Bo Yang","Shuyuan Xu","Lingling Zheng","Cindy Liang"],"pdf_url":"https://arxiv.org/pdf/2406.06475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06458v1","updated":"2024-06-10T16:46:22Z","published":"2024-06-10T16:46:22Z","title":"Evaluating the Retrieval Component in LLM-Based Question Answering\n  Systems","summary":"  Question answering systems (QA) utilizing Large Language Models (LLMs)\nheavily depend on the retrieval component to provide them with domain-specific\ninformation and reduce the risk of generating inaccurate responses or\nhallucinations. Although the evaluation of retrievers dates back to the early\nresearch in Information Retrieval, assessing their performance within LLM-based\nchatbots remains a challenge.\n  This study proposes a straightforward baseline for evaluating retrievers in\nRetrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstrate\nthat this evaluation framework provides a better image of how the retriever\nperforms and is more aligned with the overall performance of the QA system.\nAlthough conventional metrics such as precision, recall, and F1 score may not\nfully capture LLMs' capabilities - as they can yield accurate responses despite\nimperfect retrievers - our method considers LLMs' strengths to ignore\nirrelevant contexts, as well as potential errors and hallucinations in their\nresponses.\n","authors":["Ashkan Alinejad","Krtin Kumar","Ali Vahdat"],"pdf_url":"https://arxiv.org/pdf/2406.06458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02630v4","updated":"2024-06-10T14:57:11Z","published":"2024-03-05T03:40:39Z","title":"FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal\n  Decoupling","summary":"  In recent years, Cross-Domain Recommendation (CDR) has drawn significant\nattention, which utilizes user data from multiple domains to enhance the\nrecommendation performance. However, current CDR methods require sharing user\ndata across domains, thereby violating the General Data Protection Regulation\n(GDPR). Consequently, numerous approaches have been proposed for Federated\nCross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity\nacross different domains inevitably influences the overall performance of\nfederated learning. In this study, we propose FedHCDR, a novel Federated\nCross-Domain Recommendation framework with Hypergraph signal decoupling.\nSpecifically, to address the data heterogeneity across domains, we introduce an\napproach called hypergraph signal decoupling (HSD) to decouple the user\nfeatures into domain-exclusive and domain-shared features. The approach employs\nhigh-pass and low-pass hypergraph filters to decouple domain-exclusive and\ndomain-shared user representations, which are trained by the local-global\nbi-directional transfer algorithm. In addition, a hypergraph contrastive\nlearning (HCL) module is devised to enhance the learning of domain-shared user\nrelationship information by perturbing the user hypergraph. Extensive\nexperiments conducted on three real-world scenarios demonstrate that FedHCDR\noutperforms existing baselines significantly.\n","authors":["Hongyu Zhang","Dongyi Zheng","Lin Zhong","Xu Yang","Jiyuan Feng","Yunqing Feng","Qing Liao"],"pdf_url":"https://arxiv.org/pdf/2403.02630v4.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.06257v1","updated":"2024-06-10T13:38:15Z","published":"2024-06-10T13:38:15Z","title":"Combining Embeddings and Domain Knowledge for Job Posting Duplicate\n  Detection","summary":"  Job descriptions are posted on many online channels, including company\nwebsites, job boards or social media platforms. These descriptions are usually\npublished with varying text for the same job, due to the requirements of each\nplatform or to target different audiences. However, for the purpose of\nautomated recruitment and assistance of people working with these texts, it is\nhelpful to aggregate job postings across platforms and thus detect duplicate\ndescriptions that refer to the same job. In this work, we propose an approach\nfor detecting duplicates in job descriptions. We show that combining\noverlap-based character similarity with text embedding and keyword matching\nmethods lead to convincing results. In particular, we show that although no\napproach individually achieves satisfying performance, a combination of string\ncomparison, deep textual embeddings, and the use of curated weighted lookup\nlists for specific skills leads to a significant boost in overall performance.\nA tool based on our approach is being used in production and feedback from\nreal-life use confirms our evaluation.\n","authors":["Matthias Engelbach","Dennis Klau","Maximilien Kintz","Alexander Ulrich"],"pdf_url":"https://arxiv.org/pdf/2406.06257v1.pdf","comment":"To be published at 9th International Symposium on Language &\n  Knowledge Engineering LKE 2024"},{"id":"http://arxiv.org/abs/2406.06183v1","updated":"2024-06-10T11:27:46Z","published":"2024-06-10T11:27:46Z","title":"Black carbon plumes from gas flaring in North Africa identified from\n  multi-spectral imagery with deep learning","summary":"  Black carbon (BC) is an important pollutant aerosol emitted by numerous human\nactivities, including gas flaring. Improper combustion in flaring activities\ncan release large amounts of BC, which is harmful to human health and has a\nstrong climate warming effect. To our knowledge, no study has ever directly\nmonitored BC emissions from satellite imagery. Previous works quantified BC\nemissions indirectly, by applying emission coefficients to flaring volumes\nestimated from satellite imagery. Here, we develop a deep learning framework\nand apply it to Sentinel-2 imagery over North Africa during 2022 to detect and\nquantify BC emissions from gas flaring. We find that BC emissions in this\nregion amount to about 1 million tCO$_{2,\\mathrm{eq}}$, or 1 million passenger\ncars, more than a quarter of which are due to 10 sites alone. This work\ndemonstrates the operational monitoring of BC emissions from flaring, a key\nstep in implementing effective mitigation policies to reduce the climate impact\nof oil and gas operations.\n","authors":["Tuel Alexandre","Kerdreux Thomas","Thiry Louis"],"pdf_url":"https://arxiv.org/pdf/2406.06183v1.pdf","comment":"Published at the workshop Tackling Climate Change with Machine\n  Learning at ICLR 2024"},{"id":"http://arxiv.org/abs/2405.19046v2","updated":"2024-06-10T09:25:27Z","published":"2024-05-29T12:43:39Z","title":"Continual Collaborative Distillation for Recommender System","summary":"  Knowledge distillation (KD) has emerged as a promising technique for\naddressing the computational challenges associated with deploying large-scale\nrecommender systems. KD transfers the knowledge of a massive teacher system to\na compact student model, to reduce the huge computational burdens for inference\nwhile retaining high accuracy. The existing KD studies primarily focus on\none-time distillation in static environments, leaving a substantial gap in\ntheir applicability to real-world scenarios dealing with continuously incoming\nusers, items, and their interactions. In this work, we delve into a systematic\napproach to operating the teacher-student KD in a non-stationary data stream.\nOur goal is to enable efficient deployment through a compact student, which\npreserves the high performance of the massive teacher, while effectively\nadapting to continuously incoming data. We propose Continual Collaborative\nDistillation (CCD) framework, where both the teacher and the student\ncontinually and collaboratively evolve along the data stream. CCD facilitates\nthe student in effectively adapting to new data, while also enabling the\nteacher to fully leverage accumulated knowledge. We validate the effectiveness\nof CCD through extensive quantitative, ablative, and exploratory experiments on\ntwo real-world datasets. We expect this research direction to contribute to\nnarrowing the gap between existing KD studies and practical applications,\nthereby enhancing the applicability of KD in real-world systems.\n","authors":["Gyuseok Lee","SeongKu Kang","Wonbin Kweon","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2405.19046v2.pdf","comment":"Accepted by KDD 2024 research track. 9 main pages + 1 appendix page,\n  5 figures"},{"id":"http://arxiv.org/abs/2406.06061v1","updated":"2024-06-10T07:18:24Z","published":"2024-06-10T07:18:24Z","title":"Greedy SLIM: A SLIM-Based Approach For Preference Elicitation","summary":"  Preference elicitation is an active learning approach to tackle the\ncold-start problem of recommender systems. Roughly speaking, new users are\nasked to rate some carefully selected items in order to compute appropriate\nrecommendations for them. To the best of our knowledge, we are the first to\npropose a method for preference elicitation that is based on SLIM , a\nstate-of-the-art technique for top-N recommendation. Our approach mainly\nconsists of a new training technique for SLIM, which we call Greedy SLIM. This\ntechnique iteratively selects items for the training in order to minimize the\nSLIM loss greedily. We conduct offline experiments as well as a user study to\nassess the performance of this new method. The results are remarkable,\nespecially with respect to the user study. We conclude that Greedy SLIM seems\nto be more suitable for preference elicitation than widely used methods based\non latent factor models.\n","authors":["Claudius Proissl","Amel Vatic","Helmut Waldschmidt"],"pdf_url":"https://arxiv.org/pdf/2406.06061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03637v2","updated":"2024-06-10T06:28:21Z","published":"2024-04-04T17:55:38Z","title":"Sequential Recommendation for Optimizing Both Immediate Feedback and\n  Long-term Retention","summary":"  In the landscape of Recommender System (RS) applications, reinforcement\nlearning (RL) has recently emerged as a powerful tool, primarily due to its\nproficiency in optimizing long-term rewards. Nevertheless, it suffers from\ninstability in the learning process, stemming from the intricate interactions\namong bootstrapping, off-policy training, and function approximation. Moreover,\nin multi-reward recommendation scenarios, designing a proper reward setting\nthat reconciles the inner dynamics of various tasks is quite intricate. In\nresponse to these challenges, we introduce DT4IER, an advanced decision\ntransformer-based recommendation model that is engineered to not only elevate\nthe effectiveness of recommendations but also to achieve a harmonious balance\nbetween immediate user engagement and long-term retention. The DT4IER applies\nan innovative multi-reward design that adeptly balances short and long-term\nrewards with user-specific attributes, which serve to enhance the contextual\nrichness of the reward sequence ensuring a more informed and personalized\nrecommendation process. To enhance its predictive capabilities, DT4IER\nincorporates a high-dimensional encoder, skillfully designed to identify and\nleverage the intricate interrelations across diverse tasks. Furthermore, we\nintegrate a contrastive learning approach within the action embedding\npredictions, a strategy that significantly boosts the model's overall\nperformance. Experiments on three real-world datasets demonstrate the\neffectiveness of DT4IER against state-of-the-art Sequential Recommender Systems\n(SRSs) and Multi-Task Learning (MTL) models in terms of both prediction\naccuracy and effectiveness in specific tasks. The source code is accessible\nonline to facilitate replication\n","authors":["Ziru Liu","Shuchang Liu","Zijian Zhang","Qingpeng Cai","Xiangyu Zhao","Kesen Zhao","Lantao Hu","Peng Jiang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2404.03637v2.pdf","comment":"SIGIR 2024"},{"id":"http://arxiv.org/abs/2406.06043v1","updated":"2024-06-10T06:22:18Z","published":"2024-06-10T06:22:18Z","title":"Modeling User Retention through Generative Flow Networks","summary":"  Recommender systems aim to fulfill the user's daily demands. While most\nexisting research focuses on maximizing the user's engagement with the system,\nit has recently been pointed out that how frequently the users come back for\nthe service also reflects the quality and stability of recommendations.\nHowever, optimizing this user retention behavior is non-trivial and poses\nseveral challenges including the intractable leave-and-return user activities,\nthe sparse and delayed signal, and the uncertain relations between users'\nretention and their immediate feedback towards each item in the recommendation\nlist. In this work, we regard the retention signal as an overall estimation of\nthe user's end-of-session satisfaction and propose to estimate this signal\nthrough a probabilistic flow. This flow-based modeling technique can\nback-propagate the retention reward towards each recommended item in the user\nsession, and we show that the flow combined with traditional learning-to-rank\nobjectives eventually optimizes a non-discounted cumulative reward for both\nimmediate user feedback and user retention. We verify the effectiveness of our\nmethod through both offline empirical studies on two public datasets and online\nA/B tests in an industrial platform.\n","authors":["Ziru Liu","Shuchang Liu","Bin Yang","Zhenghai Xue","Qingpeng Cai","Xiangyu Zhao","Zijian Zhang","Lantao Hu","Han Li","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.06043v1.pdf","comment":"KDD-ADS 2024"},{"id":"http://arxiv.org/abs/2406.06031v1","updated":"2024-06-10T05:43:55Z","published":"2024-06-10T05:43:55Z","title":"A WT-ResNet based fault diagnosis model for the urban rail train\n  transmission system","summary":"  This study presents a novel fault diagnosis model for urban rail transit\nsystems based on Wavelet Transform Residual Neural Network (WT-ResNet). The\nmodel integrates the advantages of wavelet transform for feature extraction and\nResNet for pattern recognition, offering enhanced diagnostic accuracy and\nrobustness. Experimental results demonstrate the effectiveness of the proposed\nmodel in identifying faults in urban rail trains, paving the way for improved\nmaintenance strategies and reduced downtime.\n","authors":["Zuyu Cheng","Zhengcai Zhao","Yixiao Wang","Wentao Guo","Yufei Wang","Xiang Gao"],"pdf_url":"https://arxiv.org/pdf/2406.06031v1.pdf","comment":"12 pages,10 figures"},{"id":"http://arxiv.org/abs/2406.06006v1","updated":"2024-06-10T04:06:55Z","published":"2024-06-10T04:06:55Z","title":"Thanking the World: Exploring Gender-Based Differences in Acknowledgment\n  Patterns and Support Systems in Theses","summary":"  Research on acknowledgment sections of scientific papers has gained\nsignificant attention, but there remains a dearth of studies examining\nacknowledgments in the context of Electronic Theses and Dissertations. This\npaper addresses this gap by investigating the sources of support for male and\nfemale researchers in completing their master's or doctoral theses, focusing on\nthe discipline of Library and Information Science. We utilize a novel method of\nextracting the various types of support systems that are acknowledged in 1252\nETDs using RoBERTa-based models. The most prominent forms of support\nacknowledged by researchers are academic, moral, financial, and religious\nsupport. While there are no significant gender-based differences in religious\nand financial support, the ratio of academic to moral support acknowledged by\nresearchers shows strong gender-based variation. Additionally, advisors display\na preference for supervising same-gender researchers. By comprehending the\nnuances of support systems and the unique challenges faced by researchers of\ndifferent genders, we can foster a more inclusive and supportive academic\nenvironment. The insights gained from this research have implications for\nimproving mentoring practices and promoting gender equality in academia.\n","authors":["Manika Lamba","Hendrik Erz"],"pdf_url":"https://arxiv.org/pdf/2406.06006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05984v1","updated":"2024-06-10T02:51:16Z","published":"2024-06-10T02:51:16Z","title":"Explainable AI for Mental Disorder Detection via Social Media: A survey\n  and outlook","summary":"  Mental health constitutes a complex and pervasive global challenge, affecting\nmillions of lives and often leading to severe consequences. In this paper, we\nconduct a thorough survey to explore the intersection of data science,\nartificial intelligence, and mental healthcare, focusing on the recent\ndevelopments of mental disorder detection through online social media (OSM). A\nsignificant portion of the population actively engages in OSM platforms,\ncreating a vast repository of personal data that holds immense potential for\nmental health analytics. The paper navigates through traditional diagnostic\nmethods, state-of-the-art data- and AI-driven research studies, and the\nemergence of explainable AI (XAI) models for mental healthcare. We review\nstate-of-the-art machine learning methods, particularly those based on modern\ndeep learning, while emphasising the need for explainability in healthcare AI\nmodels. The experimental design section provides insights into prevalent\npractices, including available datasets and evaluation approaches. We also\nidentify key issues and challenges in the field and propose promising future\nresearch directions. As mental health decisions demand transparency,\ninterpretability, and ethical considerations, this paper contributes to the\nongoing discourse on advancing XAI in mental healthcare through social media.\nThe comprehensive overview presented here aims to guide researchers,\npractitioners, and policymakers in developing the area of mental disorder\ndetection.\n","authors":["Yusif Ibrahimov","Tarique Anwar","Tommy Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.05984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05977v1","updated":"2024-06-10T02:29:35Z","published":"2024-06-10T02:29:35Z","title":"Weighted KL-Divergence for Document Ranking Model Refinement","summary":"  Transformer-based retrieval and reranking models for text document search are\noften refined through knowledge distillation together with contrastive\nlearning. A tight distribution matching between the teacher and student models\ncan be hard as over-calibration may degrade training effectiveness when a\nteacher does not perform well. This paper contrastively reweights KL divergence\nterms to prioritize the alignment between a student and a teacher model for\nproper separation of positive and negative documents. This paper analyzes and\nevaluates the proposed loss function on the MS MARCO and BEIR datasets to\ndemonstrate its effectiveness in improving the relevance of tested student\nmodels.\n","authors":["Yingrui Yang","Yifan Qiao","Shanxiu He","Tao Yang"],"pdf_url":"https://arxiv.org/pdf/2406.05977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06739v1","updated":"2024-06-10T19:01:15Z","published":"2024-06-10T19:01:15Z","title":"Scaling the Vocabulary of Non-autoregressive Models for Efficient\n  Generative Retrieval","summary":"  Generative Retrieval introduces a new approach to Information Retrieval by\nreframing it as a constrained generation task, leveraging recent advancements\nin Autoregressive (AR) language models. However, AR-based Generative Retrieval\nmethods suffer from high inference latency and cost compared to traditional\ndense retrieval techniques, limiting their practical applicability. This paper\ninvestigates fully Non-autoregressive (NAR) language models as a more efficient\nalternative for generative retrieval. While standard NAR models alleviate\nlatency and cost concerns, they exhibit a significant drop in retrieval\nperformance (compared to AR models) due to their inability to capture\ndependencies between target tokens. To address this, we question the\nconventional choice of limiting the target token space to solely words or\nsub-words. We propose PIXAR, a novel approach that expands the target\nvocabulary of NAR models to include multi-word entities and common phrases (up\nto 5 million tokens), thereby reducing token dependencies. PIXAR employs\ninference optimization strategies to maintain low inference latency despite the\nsignificantly larger vocabulary. Our results demonstrate that PIXAR achieves a\nrelative improvement of 31.0% in MRR@10 on MS MARCO and 23.2% in Hits@5 on\nNatural Questions compared to standard NAR models with similar latency and\ncost. Furthermore, online A/B experiments on a large commercial search engine\nshow that PIXAR increases ad clicks by 5.08% and revenue by 4.02%.\n","authors":["Ravisri Valluri","Akash Kumar Mohankumar","Kushal Dave","Amit Singh","Jian Jiao","Manik Varma","Gaurav Sinha"],"pdf_url":"https://arxiv.org/pdf/2406.06739v1.pdf","comment":"14 pages, 6 tables, 2 figures"},{"id":"http://arxiv.org/abs/2406.06729v1","updated":"2024-06-10T18:50:57Z","published":"2024-06-10T18:50:57Z","title":"Synthetic Query Generation using Large Language Models for Virtual\n  Assistants","summary":"  Virtual Assistants (VAs) are important Information Retrieval platforms that\nhelp users accomplish various tasks through spoken commands. The speech\nrecognition system (speech-to-text) uses query priors, trained solely on text,\nto distinguish between phonetically confusing alternatives. Hence, the\ngeneration of synthetic queries that are similar to existing VA usage can\ngreatly improve upon the VA's abilities -- especially for use-cases that do not\n(yet) occur in paired audio/text data.\n  In this paper, we provide a preliminary exploration of the use of Large\nLanguage Models (LLMs) to generate synthetic queries that are complementary to\ntemplate-based methods. We investigate whether the methods (a) generate queries\nthat are similar to randomly sampled, representative, and anonymized user\nqueries from a popular VA, and (b) whether the generated queries are specific.\n  We find that LLMs generate more verbose queries, compared to template-based\nmethods, and reference aspects specific to the entity. The generated queries\nare similar to VA user queries, and are specific enough to retrieve the\nrelevant entity. We conclude that queries generated by LLMs and templates are\ncomplementary.\n","authors":["Sonal Sannigrahi","Thiago Fraga-Silva","Youssef Oualil","Christophe Van Gysel"],"pdf_url":"https://arxiv.org/pdf/2406.06729v1.pdf","comment":"SIGIR '24. The 47th International ACM SIGIR Conference on Research &\n  Development in Information Retrieval"},{"id":"http://arxiv.org/abs/2406.06723v1","updated":"2024-06-10T18:34:48Z","published":"2024-06-10T18:34:48Z","title":"Leveraging Large Language Models for Knowledge-free Weak Supervision in\n  Clinical Natural Language Processing","summary":"  The performance of deep learning-based natural language processing systems is\nbased on large amounts of labeled training data which, in the clinical domain,\nare not easily available or affordable. Weak supervision and in-context\nlearning offer partial solutions to this issue, particularly using large\nlanguage models (LLMs), but their performance still trails traditional\nsupervised methods with moderate amounts of gold-standard data. In particular,\ninferencing with LLMs is computationally heavy. We propose an approach\nleveraging fine-tuning LLMs and weak supervision with virtually no domain\nknowledge that still achieves consistently dominant performance. Using a\nprompt-based approach, the LLM is used to generate weakly-labeled data for\ntraining a downstream BERT model. The weakly supervised model is then further\nfine-tuned on small amounts of gold standard data. We evaluate this approach\nusing Llama2 on three different n2c2 datasets. With no more than 10 gold\nstandard notes, our final BERT models weakly supervised by fine-tuned\nLlama2-13B consistently outperformed out-of-the-box PubMedBERT by 4.7% to 47.9%\nin F1 scores. With only 50 gold standard notes, our models achieved close\nperformance to fully fine-tuned systems.\n","authors":["Enshuo Hsu","Kirk Roberts"],"pdf_url":"https://arxiv.org/pdf/2406.06723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06658v1","updated":"2024-06-10T11:23:30Z","published":"2024-06-10T11:23:30Z","title":"Link Prediction in Bipartite Networks","summary":"  Bipartite networks serve as highly suitable models to represent systems\ninvolving interactions between two distinct types of entities, such as online\ndating platforms, job search services, or ecommerce websites. These models can\nbe leveraged to tackle a number of tasks, including link prediction among the\nmost useful ones, especially to design recommendation systems. However, if this\ntask has garnered much interest when conducted on unipartite (i.e. standard)\nnetworks, it is far from being the case for bipartite ones. In this study, we\naddress this gap by performing an experimental comparison of 19 link prediction\nmethods able to handle bipartite graphs. Some come directly from the\nliterature, and some are adapted by us from techniques originally designed for\nunipartite networks. We also propose to repurpose recommendation systems based\non graph convolutional networks (GCN) as a novel link prediction solution for\nbipartite networks. To conduct our experiments, we constitute a benchmark of 3\nreal-world bipartite network datasets with various topologies. Our results\nindicate that GCN-based personalized recommendation systems, which have\nreceived significant attention in recent years, can produce successful results\nfor link prediction in bipartite networks. Furthermore, purely heuristic\nmetrics that do not rely on any learning process, like the Structural\nPerturbation Method (SPM), can also achieve success.\n","authors":["Şükrü Demir İnan Özer","Günce Keziban Orman","Vincent Labatut"],"pdf_url":"https://arxiv.org/pdf/2406.06658v1.pdf","comment":"28th International Conference on Knowledge-Based and Intelligent\n  Information & Engineering Systems (KES), Sep 2024, Sevilla, Spain"},{"id":"http://arxiv.org/abs/2406.06657v1","updated":"2024-06-10T11:19:28Z","published":"2024-06-10T11:19:28Z","title":"Harnessing AI for efficient analysis of complex policy documents: a case\n  study of Executive Order 14110","summary":"  Policy documents, such as legislation, regulations, and executive orders, are\ncrucial in shaping society. However, their length and complexity make\ninterpretation and application challenging and time-consuming. Artificial\nintelligence (AI), particularly large language models (LLMs), has the potential\nto automate the process of analyzing these documents, improving accuracy and\nefficiency. This study aims to evaluate the potential of AI in streamlining\npolicy analysis and to identify the strengths and limitations of current AI\napproaches. The research focuses on question answering and tasks involving\ncontent extraction from policy documents. A case study was conducted using\nExecutive Order 14110 on \"Safe, Secure, and Trustworthy Development and Use of\nArtificial Intelligence\" as a test case. Four commercial AI systems were used\nto analyze the document and answer a set of representative policy questions.\nThe performance of the AI systems was compared to manual analysis conducted by\nhuman experts. The study found that two AI systems, Gemini 1.5 Pro and Claude 3\nOpus, demonstrated significant potential for supporting policy analysis,\nproviding accurate and reliable information extraction from complex documents.\nThey performed comparably to human analysts but with significantly higher\nefficiency. However, achieving reproducibility remains a challenge,\nnecessitating further research and development.\n","authors":["Mark A. Kramer","Allen Leavens","Alexander Scarlat"],"pdf_url":"https://arxiv.org/pdf/2406.06657v1.pdf","comment":"28 pages, 1 figure"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2402.13254v3","updated":"2024-06-10T17:59:55Z","published":"2024-02-20T18:59:55Z","title":"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples","summary":"  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V. To facilitate future research, we release our code, dataset, benchmark,\nand checkpoints at https://countercurate.github.io.\n","authors":["Jianrui Zhang","Mu Cai","Tengyang Xie","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2402.13254v3.pdf","comment":"15 pages, 6 figures, 12 tables, Project Page:\n  https://countercurate.github.io/"},{"id":"http://arxiv.org/abs/2406.06520v1","updated":"2024-06-10T17:58:48Z","published":"2024-06-10T17:58:48Z","title":"Decentralized Personalized Federated Learning","summary":"  This work tackles the challenges of data heterogeneity and communication\nlimitations in decentralized federated learning. We focus on creating a\ncollaboration graph that guides each client in selecting suitable collaborators\nfor training personalized models that leverage their local data effectively.\nOur approach addresses these issues through a novel, communication-efficient\nstrategy that enhances resource efficiency. Unlike traditional methods, our\nformulation identifies collaborators at a granular level by considering\ncombinatorial relations of clients, enhancing personalization while minimizing\ncommunication overhead. We achieve this through a bi-level optimization\nframework that employs a constrained greedy algorithm, resulting in a\nresource-efficient collaboration graph for personalized learning. Extensive\nevaluation against various baselines across diverse datasets demonstrates the\nsuperiority of our method, named DPFL. DPFL consistently outperforms other\napproaches, showcasing its effectiveness in handling real-world data\nheterogeneity, minimizing communication overhead, enhancing resource\nefficiency, and building personalized models in decentralized federated\nlearning scenarios.\n","authors":["Salma Kharrat","Marco Canini","Samuel Horvath"],"pdf_url":"https://arxiv.org/pdf/2406.06520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06518v1","updated":"2024-06-10T17:58:02Z","published":"2024-06-10T17:58:02Z","title":"Data Augmentation for Multivariate Time Series Classification: An\n  Experimental Study","summary":"  Our study investigates the impact of data augmentation on the performance of\nmultivariate time series models, focusing on datasets from the UCR archive.\nDespite the limited size of these datasets, we achieved classification accuracy\nimprovements in 10 out of 13 datasets using the Rocket and InceptionTime\nmodels. This highlights the essential role of sufficient data in training\neffective models, paralleling the advancements seen in computer vision. Our\nwork delves into adapting and applying existing methods in innovative ways to\nthe domain of multivariate time series classification. Our comprehensive\nexploration of these techniques sets a new standard for addressing data\nscarcity in time series analysis, emphasizing that diverse augmentation\nstrategies are crucial for unlocking the potential of both traditional and deep\nlearning models. Moreover, by meticulously analyzing and applying a variety of\naugmentation techniques, we demonstrate that strategic data enrichment can\nenhance model accuracy. This not only establishes a benchmark for future\nresearch in time series analysis but also underscores the importance of\nadopting varied augmentation approaches to improve model performance in the\nface of limited data availability.\n","authors":["Romain Ilbert","Thai V. Hoang","Zonghua Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06518v1.pdf","comment":"Workshop on Multivariate Time Series Analytics (MulTiSA), ICDE\n  Workshop"},{"id":"http://arxiv.org/abs/2406.06516v1","updated":"2024-06-10T17:55:43Z","published":"2024-06-10T17:55:43Z","title":"Distribution-Free Predictive Inference under Unknown Temporal Drift","summary":"  Distribution-free prediction sets play a pivotal role in uncertainty\nquantification for complex statistical models. Their validity hinges on\nreliable calibration data, which may not be readily available as real-world\nenvironments often undergo unknown changes over time. In this paper, we propose\na strategy for choosing an adaptive window and use the data therein to\nconstruct prediction sets. The window is selected by optimizing an estimated\nbias-variance tradeoff. We provide sharp coverage guarantees for our method,\nshowing its adaptivity to the underlying temporal drift. We also illustrate its\nefficacy through numerical experiments on synthetic and real data.\n","authors":["Elise Han","Chengpiao Huang","Kaizheng Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06516v1.pdf","comment":"25 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.06514v1","updated":"2024-06-10T17:54:57Z","published":"2024-06-10T17:54:57Z","title":"Random Features Approximation for Control-Affine Systems","summary":"  Modern data-driven control applications call for flexible nonlinear models\nthat are amenable to principled controller synthesis and realtime feedback.\nMany nonlinear dynamical systems of interest are control affine. We propose two\nnovel classes of nonlinear feature representations which capture control affine\nstructure while allowing for arbitrary complexity in the state dependence. Our\nmethods make use of random features (RF) approximations, inheriting the\nexpressiveness of kernel methods at a lower computational cost. We formalize\nthe representational capabilities of our methods by showing their relationship\nto the Affine Dot Product (ADP) kernel proposed by Casta\\~neda et al. (2021)\nand a novel Affine Dense (AD) kernel that we introduce. We further illustrate\nthe utility by presenting a case study of data-driven optimization-based\ncontrol using control certificate functions (CCF). Simulation experiments on a\ndouble pendulum empirically demonstrate the advantages of our methods.\n","authors":["Kimia Kazemian","Yahya Sattar","Sarah Dean"],"pdf_url":"https://arxiv.org/pdf/2406.06514v1.pdf","comment":"25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.06509v1","updated":"2024-06-10T17:48:36Z","published":"2024-06-10T17:48:36Z","title":"Robust Distribution Learning with Local and Global Adversarial\n  Corruptions","summary":"  We consider learning in an adversarial environment, where an\n$\\varepsilon$-fraction of samples from a distribution $P$ are arbitrarily\nmodified (*global* corruptions) and the remaining perturbations have average\nmagnitude bounded by $\\rho$ (*local* corruptions). Given access to $n$ such\ncorrupted samples, we seek a computationally efficient estimator $\\hat{P}_n$\nthat minimizes the Wasserstein distance $\\mathsf{W}_1(\\hat{P}_n,P)$. In fact,\nwe attack the fine-grained task of minimizing $\\mathsf{W}_1(\\Pi_\\# \\hat{P}_n,\n\\Pi_\\# P)$ for all orthogonal projections $\\Pi \\in \\mathbb{R}^{d \\times d}$,\nwith performance scaling with $\\mathrm{rank}(\\Pi) = k$. This allows us to\naccount simultaneously for mean estimation ($k=1$), distribution estimation\n($k=d$), as well as the settings interpolating between these two extremes. We\ncharacterize the optimal population-limit risk for this task and then develop\nan efficient finite-sample algorithm with error bounded by $\\sqrt{\\varepsilon\nk} + \\rho + d^{O(1)}\\tilde{O}(n^{-1/k})$ when $P$ has bounded moments of order\n$2+\\delta$, for constant $\\delta > 0$. For data distributions with bounded\ncovariance, our finite-sample bounds match the minimax population-level optimum\nfor large sample sizes. Our efficient procedure relies on a novel trace norm\napproximation of an ideal yet intractable 2-Wasserstein projection estimator.\nWe apply this algorithm to robust stochastic optimization, and, in the process,\nuncover a new method for overcoming the curse of dimensionality in Wasserstein\ndistributionally robust optimization.\n","authors":["Sloan Nietert","Ziv Goldfeld","Soroosh Shafiee"],"pdf_url":"https://arxiv.org/pdf/2406.06509v1.pdf","comment":"Accepted for presentation at the Conference on Learning Theory (COLT)\n  2024"},{"id":"http://arxiv.org/abs/2406.06507v1","updated":"2024-06-10T17:44:59Z","published":"2024-06-10T17:44:59Z","title":"Verification-Guided Shielding for Deep Reinforcement Learning","summary":"  In recent years, Deep Reinforcement Learning (DRL) has emerged as an\neffective approach to solving real-world tasks. However, despite their\nsuccesses, DRL-based policies suffer from poor reliability, which limits their\ndeployment in safety-critical domains. As a result, various methods have been\nput forth to address this issue by providing formal safety guarantees. Two main\napproaches include shielding and verification. While shielding ensures the safe\nbehavior of the policy by employing an external online component (i.e., a\n``shield'') that overruns potentially dangerous actions, this approach has a\nsignificant computational cost as the shield must be invoked at runtime to\nvalidate every decision. On the other hand, verification is an offline process\nthat can identify policies that are unsafe, prior to their deployment, yet,\nwithout providing alternative actions when such a policy is deemed unsafe. In\nthis work, we present verification-guided shielding -- a novel approach that\nbridges the DRL reliability gap by integrating these two methods. Our approach\ncombines both formal and probabilistic verification tools to partition the\ninput domain into safe and unsafe regions. In addition, we employ clustering\nand symbolic representation procedures that compress the unsafe regions into a\ncompact representation. This, in turn, allows to temporarily activate the\nshield solely in (potentially) unsafe regions, in an efficient manner. Our\nnovel approach allows to significantly reduce runtime overhead while still\npreserving formal safety guarantees. We extensively evaluate our approach on\ntwo benchmarks from the robotic navigation domain, as well as provide an\nin-depth analysis of its scalability and completeness.\n","authors":["Davide Corsi","Guy Amir","Andoni Rodriguez","Cesar Sanchez","Guy Katz","Roy Fox"],"pdf_url":"https://arxiv.org/pdf/2406.06507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06506v1","updated":"2024-06-10T17:44:11Z","published":"2024-06-10T17:44:11Z","title":"Online Newton Method for Bandit Convex Optimisation","summary":"  We introduce a computationally efficient algorithm for zeroth-order bandit\nconvex optimisation and prove that in the adversarial setting its regret is at\nmost $d^{3.5} \\sqrt{n} \\mathrm{polylog}(n, d)$ with high probability where $d$\nis the dimension and $n$ is the time horizon. In the stochastic setting the\nbound improves to $M d^{2} \\sqrt{n} \\mathrm{polylog}(n, d)$ where $M \\in\n[d^{-1/2}, d^{-1 / 4}]$ is a constant that depends on the geometry of the\nconstraint set and the desired computational properties.\n","authors":["Hidde Fokkema","Dirk van der Hoeven","Tor Lattimore","Jack J. Mayo"],"pdf_url":"https://arxiv.org/pdf/2406.06506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06504v1","updated":"2024-06-10T17:43:13Z","published":"2024-06-10T17:43:13Z","title":"Equivariant Neural Tangent Kernels","summary":"  Equivariant neural networks have in recent years become an important\ntechnique for guiding architecture selection for neural networks with many\napplications in domains ranging from medical image analysis to quantum\nchemistry. In particular, as the most general linear equivariant layers with\nrespect to the regular representation, group convolutions have been highly\nimpactful in numerous applications. Although equivariant architectures have\nbeen studied extensively, much less is known about the training dynamics of\nequivariant neural networks. Concurrently, neural tangent kernels (NTKs) have\nemerged as a powerful tool to analytically understand the training dynamics of\nwide neural networks. In this work, we combine these two fields for the first\ntime by giving explicit expressions for NTKs of group convolutional neural\nnetworks. In numerical experiments, we demonstrate superior performance for\nequivariant NTKs over non-equivariant NTKs on a classification task for medical\nimages.\n","authors":["Philipp Misof","Pan Kessel","Jan E. Gerken"],"pdf_url":"https://arxiv.org/pdf/2406.06504v1.pdf","comment":"13 pages + 5 pages appendices"},{"id":"http://arxiv.org/abs/2406.04313v2","updated":"2024-06-10T17:40:19Z","published":"2024-06-06T17:57:04Z","title":"Improving Alignment and Robustness with Circuit Breakers","summary":"  AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that interrupts the models as they respond with harmful outputs\nwith \"circuit breakers.\" Existing techniques aimed at improving alignment, such\nas refusal training, are often bypassed. Techniques such as adversarial\ntraining try to plug these holes by countering specific attacks. As an\nalternative to refusal training and adversarial training, circuit-breaking\ndirectly controls the representations that are responsible for harmful outputs\nin the first place. Our technique can be applied to both text-only and\nmultimodal language models to prevent the generation of harmful outputs without\nsacrificing utility -- even in the presence of powerful unseen attacks.\nNotably, while adversarial robustness in standalone image recognition remains\nan open challenge, circuit breakers allow the larger multimodal system to\nreliably withstand image \"hijacks\" that aim to produce harmful content.\nFinally, we extend our approach to AI agents, demonstrating considerable\nreductions in the rate of harmful actions when they are under attack. Our\napproach represents a significant step forward in the development of reliable\nsafeguards to harmful behavior and adversarial attacks.\n","authors":["Andy Zou","Long Phan","Justin Wang","Derek Duenas","Maxwell Lin","Maksym Andriushchenko","Rowan Wang","Zico Kolter","Matt Fredrikson","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2406.04313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12716v3","updated":"2024-06-10T17:39:04Z","published":"2023-12-20T02:22:49Z","title":"BloomVQA: Assessing Hierarchical Multi-modal Comprehension","summary":"  We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive\nevaluation of large vision-language models on comprehension tasks. Unlike\ncurrent benchmarks that often focus on fact-based memorization and simple\nreasoning tasks without theoretical grounding, we collect multiple-choice\nsamples based on picture stories that reflect different levels of\ncomprehension, as laid out in Bloom's Taxonomy, a classic framework for\nlearning assessment widely adopted in education research. Our data maps to a\nnovel hierarchical graph representation which enables automatic data\naugmentation and novel measures characterizing model consistency. We perform\ngraded evaluation and reliability analysis on recent multi-modal models. In\ncomparison to low-level tasks, we observe decreased performance on tasks\nrequiring advanced comprehension and cognitive skills with up to 38.0\\% drop in\nVQA accuracy. In comparison to earlier models, GPT-4V demonstrates improved\naccuracy over all comprehension levels and shows a tendency of bypassing visual\ninputs especially for higher-level tasks. Current models also show consistency\npatterns misaligned with human comprehension in various scenarios,\ndemonstrating the need for improvement based on theoretically-grounded\ncriteria.\n","authors":["Yunye Gong","Robik Shrestha","Jared Claypoole","Michael Cogswell","Arijit Ray","Christopher Kanan","Ajay Divakaran"],"pdf_url":"https://arxiv.org/pdf/2312.12716v3.pdf","comment":"Accepted by ACL Findings (2024). Dataset available at\n  https://huggingface.co/datasets/ygong/BloomVQA"},{"id":"http://arxiv.org/abs/2406.06500v1","updated":"2024-06-10T17:34:44Z","published":"2024-06-10T17:34:44Z","title":"Adaptive Opponent Policy Detection in Multi-Agent MDPs: Real-Time\n  Strategy Switch Identification Using Running Error Estimation","summary":"  In Multi-agent Reinforcement Learning (MARL), accurately perceiving\nopponents' strategies is essential for both cooperative and adversarial\ncontexts, particularly within dynamic environments. While Proximal Policy\nOptimization (PPO) and related algorithms such as Actor-Critic with Experience\nReplay (ACER), Trust Region Policy Optimization (TRPO), and Deep Deterministic\nPolicy Gradient (DDPG) perform well in single-agent, stationary environments,\nthey suffer from high variance in MARL due to non-stationary and hidden\npolicies of opponents, leading to diminished reward performance. Additionally,\nexisting methods in MARL face significant challenges, including the need for\ninter-agent communication, reliance on explicit reward information, high\ncomputational demands, and sampling inefficiencies. These issues render them\nless effective in continuous environments where opponents may abruptly change\ntheir policies without prior notice. Against this background, we present\nOPS-DeMo (Online Policy Switch-Detection Model), an online algorithm that\nemploys dynamic error decay to detect changes in opponents' policies. OPS-DeMo\ncontinuously updates its beliefs using an Assumed Opponent Policy (AOP) Bank\nand selects corresponding responses from a pre-trained Response Policy Bank.\nEach response policy is trained against consistently strategizing opponents,\nreducing training uncertainty and enabling the effective use of algorithms like\nPPO in multi-agent environments. Comparative assessments show that our approach\noutperforms PPO-trained models in dynamic scenarios like the Predator-Prey\nsetting, providing greater robustness to sudden policy shifts and enabling more\ninformed decision-making through precise opponent policy insights.\n","authors":["Mohidul Haque Mridul","Mohammad Foysal Khan","Redwan Ahmed Rizvee","Md Mosaddek Khan"],"pdf_url":"https://arxiv.org/pdf/2406.06500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06496v1","updated":"2024-06-10T17:31:36Z","published":"2024-06-10T17:31:36Z","title":"Direct Preference Optimization for Suppressing Hallucinated Prior Exams\n  in Radiology Report Generation","summary":"  Recent advances in generative vision-language models (VLMs) have exciting\npotential implications for AI in radiology, yet VLMs are also known to produce\nhallucinations, nonsensical text, and other unwanted behaviors that can waste\nclinicians' time and cause patient harm. Drawing on recent work on direct\npreference optimization (DPO), we propose a simple method for modifying the\nbehavior of pretrained VLMs performing radiology report generation by\nsuppressing unwanted types of generations. We apply our method to the\nprevention of hallucinations of prior exams, addressing a long-established\nproblem behavior in models performing chest X-ray report generation. Across our\nexperiments, we find that DPO fine-tuning achieves a 3.2-4.8x reduction in\nlines hallucinating prior exams while maintaining model performance on clinical\naccuracy metrics. Our work is, to the best of our knowledge, the first work to\napply DPO to medical VLMs, providing a data- and compute- efficient way to\nsuppress problem behaviors while maintaining overall clinical accuracy.\n","authors":["Oishi Banerjee","Hong-Yu Zhou","Subathra Adithan","Stephen Kwak","Kay Wu","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2406.06496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06495v1","updated":"2024-06-10T17:31:07Z","published":"2024-06-10T17:31:07Z","title":"Boosting Robustness in Preference-Based Reinforcement Learning with\n  Dynamic Sparsity","summary":"  For autonomous agents to successfully integrate into human-centered\nenvironments, agents should be able to learn from and adapt to humans in their\nnative settings. Preference-based reinforcement learning (PbRL) is a promising\napproach that learns reward functions from human preferences. This enables RL\nagents to adapt their behavior based on human desires. However, humans live in\na world full of diverse information, most of which is not relevant to\ncompleting a particular task. It becomes essential that agents learn to focus\non the subset of task-relevant environment features. Unfortunately, prior work\nhas largely ignored this aspect; primarily focusing on improving PbRL\nalgorithms in standard RL environments that are carefully constructed to\ncontain only task-relevant features. This can result in algorithms that may not\neffectively transfer to a more noisy real-world setting. To that end, this work\nproposes R2N (Robust-to-Noise), the first PbRL algorithm that leverages\nprinciples of dynamic sparse training to learn robust reward models that can\nfocus on task-relevant features. We study the effectiveness of R2N in the\nExtremely Noisy Environment setting, an RL problem setting where up to 95% of\nthe state features are irrelevant distractions. In experiments with a simulated\nteacher, we demonstrate that R2N can adapt the sparse connectivity of its\nneural networks to focus on task-relevant features, enabling R2N to\nsignificantly outperform several state-of-the-art PbRL algorithms in multiple\nlocomotion and control environments.\n","authors":["Calarina Muslimani","Bram Grooten","Deepak Ranganatha Sastry Mamillapalli","Mykola Pechenizkiy","Decebal Constantin Mocanu","Matthew E. Taylor"],"pdf_url":"https://arxiv.org/pdf/2406.06495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14573v2","updated":"2024-06-10T17:30:49Z","published":"2024-05-23T13:48:54Z","title":"AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents","summary":"  Autonomous agents that execute human tasks by controlling computers can\nenhance human productivity and application accessibility. However, progress in\nthis field will be driven by realistic and reproducible benchmarks. We present\nAndroidWorld, a fully functional Android environment that provides reward\nsignals for 116 programmatic tasks across 20 real-world Android apps. Unlike\nexisting interactive environments, which provide a static test set,\nAndroidWorld dynamically constructs tasks that are parameterized and expressed\nin natural language in unlimited ways, thus enabling testing on a much larger\nand more realistic suite of tasks. Reward signals are derived from the\ncomputer's system state, making them durable across task variations and\nextensible across different apps. To demonstrate AndroidWorld's benefits and\nmode of operation, we introduce a new computer control agent, M3A. M3A can\ncomplete 30.6% of the AndroidWorld's tasks, leaving ample room for future work.\nFurthermore, we adapt a popular desktop web agent to work on Android, which we\nfind to be less effective on mobile, suggesting future research is needed to\nachieve universal, cross-domain agents. Finally, we conduct a robustness\nanalysis by testing M3A against a range of task variations on a representative\nsubset of tasks, demonstrating that variations in task parameters can\nsignificantly alter a task's complexity and, consequently, an agent's\nperformance, highlighting the importance of testing agents under diverse\nconditions. AndroidWorld and the experiments in this paper are available at\nhttps://github.com/google-research/android_world.\n","authors":["Christopher Rawles","Sarah Clinckemaillie","Yifan Chang","Jonathan Waltz","Gabrielle Lau","Marybeth Fair","Alice Li","William Bishop","Wei Li","Folawiyo Campbell-Ajala","Daniel Toyama","Robert Berry","Divya Tyamagundlu","Timothy Lillicrap","Oriana Riva"],"pdf_url":"https://arxiv.org/pdf/2405.14573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06494v1","updated":"2024-06-10T17:30:17Z","published":"2024-06-10T17:30:17Z","title":"Scaling Continuous Latent Variable Models as Probabilistic Integral\n  Circuits","summary":"  Probabilistic integral circuits (PICs) have been recently introduced as\nprobabilistic models enjoying the key ingredient behind expressive generative\nmodels: continuous latent variables (LVs). PICs are symbolic computational\ngraphs defining continuous LV models as hierarchies of functions that are\nsummed and multiplied together, or integrated over some LVs. They are tractable\nif LVs can be analytically integrated out, otherwise they can be approximated\nby tractable probabilistic circuits (PC) encoding a hierarchical numerical\nquadrature process, called QPCs.\n  So far, only tree-shaped PICs have been explored, and training them via\nnumerical quadrature requires memory-intensive processing at scale. In this\npaper, we address these issues, and present: (i) a pipeline for building\nDAG-shaped PICs out of arbitrary variable decompositions, (ii) a procedure for\ntraining PICs using tensorized circuit architectures, and (iii) neural\nfunctional sharing techniques to allow scalable training. In extensive\nexperiments, we showcase the effectiveness of functional sharing and the\nsuperiority of QPCs over traditional PCs.\n","authors":["Gennaro Gala","Cassio de Campos","Antonio Vergari","Erik Quaeghebeur"],"pdf_url":"https://arxiv.org/pdf/2406.06494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06487v1","updated":"2024-06-10T17:26:39Z","published":"2024-06-10T17:26:39Z","title":"When is Multicalibration Post-Processing Necessary?","summary":"  Calibration is a well-studied property of predictors which guarantees\nmeaningful uncertainty estimates. Multicalibration is a related notion --\noriginating in algorithmic fairness -- which requires predictors to be\nsimultaneously calibrated over a potentially complex and overlapping collection\nof protected subpopulations (such as groups defined by ethnicity, race, or\nincome). We conduct the first comprehensive study evaluating the usefulness of\nmulticalibration post-processing across a broad set of tabular, image, and\nlanguage datasets for models spanning from simple decision trees to 90 million\nparameter fine-tuned LLMs. Our findings can be summarized as follows: (1)\nmodels which are calibrated out of the box tend to be relatively\nmulticalibrated without any additional post-processing; (2) multicalibration\npost-processing can help inherently uncalibrated models; and (3) traditional\ncalibration measures may sometimes provide multicalibration implicitly. More\ngenerally, we also distill many independent observations which may be useful\nfor practical and effective applications of multicalibration post-processing in\nreal-world contexts.\n","authors":["Dutch Hansen","Siddartha Devic","Preetum Nakkiran","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2406.06487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06486v1","updated":"2024-06-10T17:25:46Z","published":"2024-06-10T17:25:46Z","title":"Continuum Attention for Neural Operators","summary":"  Transformers, and the attention mechanism in particular, have become\nubiquitous in machine learning. Their success in modeling nonlocal, long-range\ncorrelations has led to their widespread adoption in natural language\nprocessing, computer vision, and time-series problems. Neural operators, which\nmap spaces of functions into spaces of functions, are necessarily both\nnonlinear and nonlocal if they are universal; it is thus natural to ask whether\nthe attention mechanism can be used in the design of neural operators.\nMotivated by this, we study transformers in the function space setting. We\nformulate attention as a map between infinite dimensional function spaces and\nprove that the attention mechanism as implemented in practice is a Monte Carlo\nor finite difference approximation of this operator. The function space\nformulation allows for the design of transformer neural operators, a class of\narchitectures designed to learn mappings between function spaces, for which we\nprove a universal approximation result. The prohibitive cost of applying the\nattention operator to functions defined on multi-dimensional domains leads to\nthe need for more efficient attention-based architectures. For this reason we\nalso introduce a function space generalization of the patching strategy from\ncomputer vision, and introduce a class of associated neural operators.\nNumerical results, on an array of operator learning problems, demonstrate the\npromise of our approaches to function space formulations of attention and their\nuse in neural operators.\n","authors":["Edoardo Calvello","Nikola B. Kovachki","Matthew E. Levine","Andrew M. Stuart"],"pdf_url":"https://arxiv.org/pdf/2406.06486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06484v1","updated":"2024-06-10T17:24:42Z","published":"2024-06-10T17:24:42Z","title":"Parallelizing Linear Transformers with the Delta Rule over Sequence\n  Length","summary":"  Transformers with linear attention (i.e., linear transformers) and\nstate-space models have recently been suggested as a viable linear-time\nalternative to transformers with softmax attention. However, these models still\nunderperform transformers especially on tasks that require in-context\nretrieval. While more expressive variants of linear transformers which replace\nthe additive outer-product update in linear transformers with the delta rule\nhave been found to be more effective at associative recall, existing algorithms\nfor training such models do not parallelize over sequence length and are thus\ninefficient to train on modern hardware. This work describes a\nhardware-efficient algorithm for training linear transformers with the delta\nrule, which exploits a memory-efficient representation for computing products\nof Householder matrices. This algorithm allows us to scale up DeltaNet to\nstandard language modeling settings. We train a 1.3B model for 100B tokens and\nfind that it outperforms recent linear-time baselines such as Mamba and GLA in\nterms of perplexity and zero-shot performance on downstream tasks (including on\ntasks that focus on recall). We also experiment with two hybrid models which\ncombine DeltaNet layers with (1) sliding-window attention layers every other\nlayer or (2) two global attention layers, and find that these hybrid models\noutperform strong transformer baselines.\n","authors":["Songlin Yang","Bailin Wang","Yu Zhang","Yikang Shen","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2406.06484v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.02638v2","updated":"2024-06-10T17:22:33Z","published":"2024-06-04T09:07:58Z","title":"EchoMamba4Rec: Harmonizing Bidirectional State Space Models with\n  Spectral Filtering for Advanced Sequential Recommendation","summary":"  Predicting user preferences and sequential dependencies based on historical\nbehavior is the core goal of sequential recommendation. Although\nattention-based models have shown effectiveness in this field, they often\nstruggle with inference inefficiency due to the quadratic computational\ncomplexity inherent in attention mechanisms, especially with long-range\nbehavior sequences. Drawing inspiration from the recent advancements of state\nspace models (SSMs) in control theory, which provide a robust framework for\nmodeling and controlling dynamic systems, we introduce EchoMamba4Rec. Control\ntheory emphasizes the use of SSMs for managing long-range dependencies and\nmaintaining inferential efficiency through structured state matrices.\nEchoMamba4Rec leverages these control relationships in sequential\nrecommendation and integrates bi-directional processing with frequency-domain\nfiltering to capture complex patterns and dependencies in user interaction data\nmore effectively. Our model benefits from the ability of state space models\n(SSMs) to learn and perform parallel computations, significantly enhancing\ncomputational efficiency and scalability. It features a bi-directional Mamba\nmodule that incorporates both forward and reverse Mamba components, leveraging\ninformation from both past and future interactions. Additionally, a filter\nlayer operates in the frequency domain using learnable Fast Fourier Transform\n(FFT) and learnable filters, followed by an inverse FFT to refine item\nembeddings and reduce noise. We also integrate Gate Linear Units (GLU) to\ndynamically control information flow, enhancing the model's expressiveness and\ntraining stability. Experimental results demonstrate that EchoMamba\nsignificantly outperforms existing models, providing more accurate and\npersonalized recommendations.\n","authors":["Yuda Wang","Xuxin He","Shengxin Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.02638v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2403.03900 by other authors"},{"id":"http://arxiv.org/abs/2406.06482v1","updated":"2024-06-10T17:22:09Z","published":"2024-06-10T17:22:09Z","title":"Quantum Equilibrium Propagation for efficient training of quantum\n  systems based on Onsager reciprocity","summary":"  The widespread adoption of machine learning and artificial intelligence in\nall branches of science and technology has created a need for energy-efficient,\nalternative hardware platforms. While such neuromorphic approaches have been\nproposed and realised for a wide range of platforms, physically extracting the\ngradients required for training remains challenging as generic approaches only\nexist in certain cases. Equilibrium propagation (EP) is such a procedure that\nhas been introduced and applied to classical energy-based models which relax to\nan equilibrium. Here, we show a direct connection between EP and Onsager\nreciprocity and exploit this to derive a quantum version of EP. This can be\nused to optimize loss functions that depend on the expectation values of\nobservables of an arbitrary quantum system. Specifically, we illustrate this\nnew concept with supervised and unsupervised learning examples in which the\ninput or the solvable task is of quantum mechanical nature, e.g., the\nrecognition of quantum many-body ground states, quantum phase exploration,\nsensing and phase boundary exploration. We propose that in the future quantum\nEP may be used to solve tasks such as quantum phase discovery with a quantum\nsimulator even for Hamiltonians which are numerically hard to simulate or even\npartially unknown. Our scheme is relevant for a variety of quantum simulation\nplatforms such as ion chains, superconducting qubit arrays, neutral atom\nRydberg tweezer arrays and strongly interacting atoms in optical lattices.\n","authors":["Clara C. Wanjura","Florian Marquardt"],"pdf_url":"https://arxiv.org/pdf/2406.06482v1.pdf","comment":"10 pages, 3 figures; comments welcome!"},{"id":"http://arxiv.org/abs/2406.01539v2","updated":"2024-06-10T17:22:08Z","published":"2024-06-03T17:16:11Z","title":"Physics-informed deep learning and compressive collocation for\n  high-dimensional diffusion-reaction equations: practical existence theory and\n  numerics","summary":"  On the forefront of scientific computing, Deep Learning (DL), i.e., machine\nlearning with Deep Neural Networks (DNNs), has emerged a powerful new tool for\nsolving Partial Differential Equations (PDEs). It has been observed that DNNs\nare particularly well suited to weakening the effect of the curse of\ndimensionality, a term coined by Richard E. Bellman in the late `50s to\ndescribe challenges such as the exponential dependence of the sample\ncomplexity, i.e., the number of samples required to solve an approximation\nproblem, on the dimension of the ambient space. However, although DNNs have\nbeen used to solve PDEs since the `90s, the literature underpinning their\nmathematical efficiency in terms of numerical analysis (i.e., stability,\naccuracy, and sample complexity), is only recently beginning to emerge. In this\npaper, we leverage recent advancements in function approximation using\nsparsity-based techniques and random sampling to develop and analyze an\nefficient high-dimensional PDE solver based on DL. We show, both theoretically\nand numerically, that it can compete with a novel stable and accurate\ncompressive spectral collocation method. In particular, we demonstrate a new\npractical existence theorem, which establishes the existence of a class of\ntrainable DNNs with suitable bounds on the network architecture and a\nsufficient condition on the sample complexity, with logarithmic or, at worst,\nlinear scaling in dimension, such that the resulting networks stably and\naccurately approximate a diffusion-reaction PDE with high probability.\n","authors":["Simone Brugiapaglia","Nick Dexter","Samir Karam","Weiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2406.01539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06479v1","updated":"2024-06-10T17:20:13Z","published":"2024-06-10T17:20:13Z","title":"Graph-Based Bidirectional Transformer Decision Threshold Adjustment\n  Algorithm for Class-Imbalanced Molecular Data","summary":"  Data sets with imbalanced class sizes, often where one class size is much\nsmaller than that of others, occur extremely often in various applications,\nincluding those with biological foundations, such as drug discovery and disease\ndiagnosis. Thus, it is extremely important to be able to identify data elements\nof classes of various sizes, as a failure to detect can result in heavy costs.\nHowever, many data classification algorithms do not perform well on imbalanced\ndata sets as they often fail to detect elements belonging to underrepresented\nclasses. In this paper, we propose the BTDT-MBO algorithm, incorporating\nMerriman-Bence-Osher (MBO) techniques and a bidirectional transformer, as well\nas distance correlation and decision threshold adjustments, for data\nclassification problems on highly imbalanced molecular data sets, where the\nsizes of the classes vary greatly. The proposed method not only integrates\nadjustments in the classification threshold for the MBO algorithm in order to\nhelp deal with the class imbalance, but also uses a bidirectional transformer\nmodel based on an attention mechanism for self-supervised learning.\nAdditionally, the method implements distance correlation as a weight function\nfor the similarity graph-based framework on which the adjusted MBO algorithm\noperates. The proposed model is validated using six molecular data sets, and we\nalso provide a thorough comparison to other competing algorithms. The\ncomputational experiments show that the proposed method performs better than\ncompeting techniques even when the class imbalance ratio is very high.\n","authors":["Nicole Hayes","Ekaterina Merkurjev","Guo-Wei Wei"],"pdf_url":"https://arxiv.org/pdf/2406.06479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19442v2","updated":"2024-06-10T17:18:07Z","published":"2024-02-29T18:43:52Z","title":"Training Dynamics of Multi-Head Softmax Attention for In-Context\n  Learning: Emergence, Convergence, and Optimality","summary":"  We study the dynamics of gradient flow for training a multi-head softmax\nattention model for in-context learning of multi-task linear regression. We\nestablish the global convergence of gradient flow under suitable choices of\ninitialization. In addition, we prove that an interesting \"task allocation\"\nphenomenon emerges during the gradient flow dynamics, where each attention head\nfocuses on solving a single task of the multi-task model. Specifically, we\nprove that the gradient flow dynamics can be split into three phases -- a\nwarm-up phase where the loss decreases rather slowly and the attention heads\ngradually build up their inclination towards individual tasks, an emergence\nphase where each head selects a single task and the loss rapidly decreases, and\na convergence phase where the attention parameters converge to a limit.\nFurthermore, we prove the optimality of gradient flow in the sense that the\nlimiting model learned by gradient flow is on par with the best possible\nmulti-head softmax attention model up to a constant factor. Our analysis also\ndelineates a strict separation in terms of the prediction accuracy of ICL\nbetween single-head and multi-head attention models. The key technique for our\nconvergence analysis is to map the gradient flow dynamics in the parameter\nspace to a set of ordinary differential equations in the spectral domain, where\nthe relative magnitudes of the semi-singular values of the attention weights\ndetermines task allocation. To our best knowledge, our work provides the first\nconvergence result for the multi-head softmax attention model.\n","authors":["Siyu Chen","Heejune Sheen","Tianhao Wang","Zhuoran Yang"],"pdf_url":"https://arxiv.org/pdf/2402.19442v2.pdf","comment":"141 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.12348v2","updated":"2024-06-10T17:14:09Z","published":"2024-02-19T18:23:36Z","title":"GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via\n  Game-Theoretic Evaluations","summary":"  As Large Language Models (LLMs) are integrated into critical real-world\napplications, their strategic and logical reasoning abilities are increasingly\ncrucial. This paper evaluates LLMs' reasoning abilities in competitive\nenvironments through game-theoretic tasks, e.g., board and card games that\nrequire pure logic and strategic reasoning to compete with opponents. We first\npropose GTBench, a language-driven environment composing 10 widely recognized\ntasks, across a comprehensive game taxonomy: complete versus incomplete\ninformation, dynamic versus static, and probabilistic versus deterministic\nscenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and\n(2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that\n(1) LLMs have distinct behaviors regarding various gaming scenarios; for\nexample, LLMs fail in complete and deterministic games yet they are competitive\nin probabilistic gaming scenarios; (2) Most open-source LLMs, e.g.,\nCodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than\ncommercial LLMs, e.g., GPT-4, in complex games, yet the recently released\nLlama-3-70b-Instruct makes up for this shortcoming. In addition,\ncode-pretraining greatly benefits strategic reasoning, while advanced reasoning\nmethods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always\nhelp. We further characterize the game-theoretic properties of LLMs, such as\nequilibrium and Pareto Efficiency in repeated games. Detailed error profiles\nare provided for a better understanding of LLMs' behavior. We hope our research\nprovides standardized protocols and serves as a foundation to spur further\nexplorations in the strategic reasoning of LLMs.\n","authors":["Jinhao Duan","Renming Zhang","James Diffenderfer","Bhavya Kailkhura","Lichao Sun","Elias Stengel-Eskin","Mohit Bansal","Tianlong Chen","Kaidi Xu"],"pdf_url":"https://arxiv.org/pdf/2402.12348v2.pdf","comment":"26 pages; the first two authors contributed equally; GTBench HF\n  Leaderboard: https://huggingface.co/spaces/GTBench/GTBench"},{"id":"http://arxiv.org/abs/2405.10410v2","updated":"2024-06-10T17:13:52Z","published":"2024-05-16T19:22:49Z","title":"The fast committor machine: Interpretable prediction with kernels","summary":"  In the study of stochastic systems, the committor function describes the\nprobability that a system starting from an initial configuration $x$ will reach\na set $B$ before a set $A$. This paper introduces an efficient and\ninterpretable algorithm for approximating the committor, called the \"fast\ncommittor machine\" (FCM). The FCM uses simulated trajectory data to build a\nkernel-based model of the committor. The kernel function is constructed to\nemphasize low-dimensional subspaces which optimally describe the $A$ to $B$\ntransitions. The coefficients in the kernel model are determined using\nrandomized linear algebra, leading to a runtime that scales linearly in the\nnumber of data points. In numerical experiments involving a triple-well\npotential and alanine dipeptide, the FCM yields higher accuracy and trains more\nquickly than a neural network with the same number of parameters. The FCM is\nalso more interpretable than the neural net.\n","authors":["D. Aristoff","M. Johnson","G. Simpson","R. J. Webber"],"pdf_url":"https://arxiv.org/pdf/2405.10410v2.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.06470v1","updated":"2024-06-10T17:09:38Z","published":"2024-06-10T17:09:38Z","title":"GKAN: Graph Kolmogorov-Arnold Networks","summary":"  We introduce Graph Kolmogorov-Arnold Networks (GKAN), an innovative neural\nnetwork architecture that extends the principles of the recently proposed\nKolmogorov-Arnold Networks (KAN) to graph-structured data. By adopting the\nunique characteristics of KANs, notably the use of learnable univariate\nfunctions instead of fixed linear weights, we develop a powerful model for\ngraph-based learning tasks. Unlike traditional Graph Convolutional Networks\n(GCNs) that rely on a fixed convolutional architecture, GKANs implement\nlearnable spline-based functions between layers, transforming the way\ninformation is processed across the graph structure. We present two different\nways to incorporate KAN layers into GKAN: architecture 1 -- where the learnable\nfunctions are applied to input features after aggregation and architecture 2 --\nwhere the learnable functions are applied to input features before aggregation.\nWe evaluate GKAN empirically using a semi-supervised graph learning task on a\nreal-world dataset (Cora). We find that architecture generally performs better.\nWe find that GKANs achieve higher accuracy in semi-supervised learning tasks on\ngraphs compared to the traditional GCN model. For example, when considering 100\nfeatures, GCN provides an accuracy of 53.5 while a GKAN with a comparable\nnumber of parameters gives an accuracy of 61.76; with 200 features, GCN\nprovides an accuracy of 61.24 while a GKAN with a comparable number of\nparameters gives an accuracy of 67.66. We also present results on the impact of\nvarious parameters such as the number of hidden nodes, grid-size, and the\npolynomial-degree of the spline on the performance of GKAN.\n","authors":["Mehrdad Kiamari","Mohammad Kiamari","Bhaskar Krishnamachari"],"pdf_url":"https://arxiv.org/pdf/2406.06470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06469v1","updated":"2024-06-10T17:07:25Z","published":"2024-06-10T17:07:25Z","title":"Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning","summary":"  Language agents perform complex tasks by using tools to execute each step\nprecisely. However, most existing agents are based on proprietary models or\ndesigned to target specific tasks, such as mathematics or multi-hop question\nanswering. We introduce Husky, a holistic, open-source language agent that\nlearns to reason over a unified action space to address a diverse set of\ncomplex tasks involving numerical, tabular, and knowledge-based reasoning.\nHusky iterates between two stages: 1) generating the next action to take\ntowards solving a given task and 2) executing the action using expert models\nand updating the current solution state. We identify a thorough ontology of\nactions for addressing complex tasks and curate high-quality data to train\nexpert models for executing these actions. Our experiments show that Husky\noutperforms prior language agents across 14 evaluation datasets. Moreover, we\nintroduce HuskyQA, a new evaluation set which stress tests language agents for\nmixed-tool reasoning, with a focus on retrieving missing knowledge and\nperforming numerical reasoning. Despite using 7B models, Husky matches or even\nexceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of\nour holistic approach in addressing complex reasoning problems. Our code and\nmodels are available at https://github.com/agent-husky/Husky-v1.\n","authors":["Joongwon Kim","Bhargavi Paranjape","Tushar Khot","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2406.06469v1.pdf","comment":"50 pages, 42 figures. Project webpage available\n  [here](https://agent-husky.github.io/)"},{"id":"http://arxiv.org/abs/2406.06467v1","updated":"2024-06-10T17:05:12Z","published":"2024-06-10T17:05:12Z","title":"How Far Can Transformers Reason? The Locality Barrier and Inductive\n  Scratchpad","summary":"  Can Transformers predict new syllogisms by composing established ones? More\ngenerally, what type of targets can be learned by such models from scratch?\nRecent works show that Transformers can be Turing-complete in terms of\nexpressivity, but this does not address the learnability objective. This paper\nputs forward the notion of 'distribution locality' to capture when weak\nlearning is efficiently achievable by regular Transformers, where the locality\nmeasures the least number of tokens required in addition to the tokens\nhistogram to correlate nontrivially with the target. As shown experimentally\nand theoretically under additional assumptions, distributions with high\nlocality cannot be learned efficiently. In particular, syllogisms cannot be\ncomposed on long chains. Furthermore, we show that (i) an agnostic scratchpad\ncannot help to break the locality barrier, (ii) an educated scratchpad can help\nif it breaks the locality at each step, (iii) a notion of 'inductive\nscratchpad' can both break the locality and improve the out-of-distribution\ngeneralization, e.g., generalizing to almost double input size for some\narithmetic tasks.\n","authors":["Emmanuel Abbe","Samy Bengio","Aryo Lotfi","Colin Sandon","Omid Saremi"],"pdf_url":"https://arxiv.org/pdf/2406.06467v1.pdf","comment":"38 pages, 11 figures"},{"id":"http://arxiv.org/abs/2404.11597v2","updated":"2024-06-10T17:04:10Z","published":"2024-04-17T17:49:38Z","title":"Explainable Artificial Intelligence Techniques for Accurate Fault\n  Detection and Diagnosis: A Review","summary":"  As the manufacturing industry advances with sensor integration and\nautomation, the opaque nature of deep learning models in machine learning poses\na significant challenge for fault detection and diagnosis. And despite the\nrelated predictive insights Artificial Intelligence (AI) can deliver, advanced\nmachine learning engines often remain a black box. This paper reviews the\neXplainable AI (XAI) tools and techniques in this context. We explore various\nXAI methodologies, focusing on their role in making AI decision-making\ntransparent, particularly in critical scenarios where humans are involved. We\nalso discuss current limitations and potential future research that aims to\nbalance explainability with model performance while improving trustworthiness\nin the context of AI applications for critical industrial use cases.\n","authors":["Ahmed Maged","Salah Haridy","Herman Shen"],"pdf_url":"https://arxiv.org/pdf/2404.11597v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06465v1","updated":"2024-06-10T17:02:08Z","published":"2024-06-10T17:02:08Z","title":"AID: Adapting Image2Video Diffusion Models for Instruction-guided Video\n  Prediction","summary":"  Text-guided video prediction (TVP) involves predicting the motion of future\nframes from the initial frame according to an instruction, which has wide\napplications in virtual reality, robotics, and content creation. Previous TVP\nmethods make significant breakthroughs by adapting Stable Diffusion for this\ntask. However, they struggle with frame consistency and temporal stability\nprimarily due to the limited scale of video datasets. We observe that\npretrained Image2Video diffusion models possess good priors for video dynamics\nbut they lack textual control. Hence, transferring Image2Video models to\nleverage their video dynamic priors while injecting instruction control to\ngenerate controllable videos is both a meaningful and challenging task. To\nachieve this, we introduce the Multi-Modal Large Language Model (MLLM) to\npredict future video states based on initial frames and text instructions. More\nspecifically, we design a dual query transformer (DQFormer) architecture, which\nintegrates the instructions and frames into the conditional embeddings for\nfuture frame prediction. Additionally, we develop Long-Short Term Temporal\nAdapters and Spatial Adapters that can quickly transfer general video diffusion\nmodels to specific scenarios with minimal training costs. Experimental results\nshow that our method significantly outperforms state-of-the-art techniques on\nfour datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and\nUCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and\nSSv2 respectively, demonstrating its effectiveness in various domains. More\nexamples can be found at our website https://chenhsing.github.io/AID.\n","authors":["Zhen Xing","Qi Dai","Zejia Weng","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.06465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06462v1","updated":"2024-06-10T16:58:48Z","published":"2024-06-10T16:58:48Z","title":"VCR: Visual Caption Restoration","summary":"  We introduce Visual Caption Restoration (VCR), a novel vision-language task\nthat challenges models to accurately restore partially obscured texts using\npixel-level hints within images. This task stems from the observation that text\nembedded in images is intrinsically different from common visual elements and\nnatural language due to the need to align the modalities of vision, text, and\ntext embedded in images. While numerous works have integrated text embedded in\nimages into visual question-answering tasks, approaches to these tasks\ngenerally rely on optical character recognition or masked language modeling,\nthus reducing the task to mainly text-based processing. However, text-based\nprocessing becomes ineffective in VCR as accurate text restoration depends on\nthe combined information from provided images, context, and subtle cues from\nthe tiny exposed areas of masked texts. We develop a pipeline to generate\nsynthetic images for the VCR task using image-caption pairs, with adjustable\ncaption visibility to control the task difficulty. With this pipeline, we\nconstruct a dataset for VCR called VCR-Wiki using images with captions from\nWikipedia, comprising 2.11M English and 346K Chinese entities in both easy and\nhard split variants. Our results reveal that current vision language models\nsignificantly lag behind human performance in the VCR task, and merely\nfine-tuning the models on our dataset does not lead to notable improvements. We\nrelease VCR-Wiki and the data construction code to facilitate future research.\n","authors":["Tianyu Zhang","Suyuchen Wang","Lu Li","Ge Zhang","Perouz Taslakian","Sai Rajeswar","Jie Fu","Bang Liu","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2406.06462v1.pdf","comment":"18 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.06459v1","updated":"2024-06-10T16:53:58Z","published":"2024-06-10T16:53:58Z","title":"How Useful is Intermittent, Asynchronous Expert Feedback for Bayesian\n  Optimization?","summary":"  Bayesian optimization (BO) is an integral part of automated scientific\ndiscovery -- the so-called self-driving lab -- where human inputs are ideally\nminimal or at least non-blocking. However, scientists often have strong\nintuition, and thus human feedback is still useful. Nevertheless, prior works\nin enhancing BO with expert feedback, such as by incorporating it in an offline\nor online but blocking (arrives at each BO iteration) manner, are incompatible\nwith the spirit of self-driving labs. In this work, we study whether a small\namount of randomly arriving expert feedback that is being incorporated in a\nnon-blocking manner can improve a BO campaign. To this end, we run an\nadditional, independent computing thread on top of the BO loop to handle the\nfeedback-gathering process. The gathered feedback is used to learn a Bayesian\npreference model that can readily be incorporated into the BO thread, to steer\nits exploration-exploitation process. Experiments on toy and chemistry datasets\nsuggest that even just a few intermittent, asynchronous expert feedback can be\nuseful for improving or constraining BO. This can especially be useful for its\nimplication in improving self-driving labs, e.g. making them more\ndata-efficient and less costly.\n","authors":["Agustinus Kristiadi","Felix Strieth-Kalthoff","Sriram Ganapathi Subramanian","Vincent Fortuin","Pascal Poupart","Geoff Pleiss"],"pdf_url":"https://arxiv.org/pdf/2406.06459v1.pdf","comment":"AABI 2024. Code: https://github.com/wiseodd/bo-async-feedback"},{"id":"http://arxiv.org/abs/2406.06452v1","updated":"2024-06-10T16:40:55Z","published":"2024-06-10T16:40:55Z","title":"Estimating Heterogeneous Treatment Effects by Combining Weak Instruments\n  and Observational Data","summary":"  Accurately predicting conditional average treatment effects (CATEs) is\ncrucial in personalized medicine and digital platform analytics. Since often\nthe treatments of interest cannot be directly randomized, observational data is\nleveraged to learn CATEs, but this approach can incur significant bias from\nunobserved confounding. One strategy to overcome these limitations is to seek\nlatent quasi-experiments in instrumental variables (IVs) for the treatment, for\nexample, a randomized intent to treat or a randomized product recommendation.\nThis approach, on the other hand, can suffer from low compliance, i.e., IV\nweakness. Some subgroups may even exhibit zero compliance meaning we cannot\ninstrument for their CATEs at all. In this paper we develop a novel approach to\ncombine IV and observational data to enable reliable CATE estimation in the\npresence of unobserved confounding in the observational data and low compliance\nin the IV data, including no compliance for some subgroups. We propose a\ntwo-stage framework that first learns biased CATEs from the observational data,\nand then applies a compliance-weighted correction using IV data, effectively\nleveraging IV strength variability across covariates. We characterize the\nconvergence rates of our method and validate its effectiveness through a\nsimulation study. Additionally, we demonstrate its utility with real data by\nanalyzing the heterogeneous effects of 401(k) plan participation on wealth.\n","authors":["Miruna Oprescu","Nathan Kallus"],"pdf_url":"https://arxiv.org/pdf/2406.06452v1.pdf","comment":"20 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.06449v1","updated":"2024-06-10T16:39:39Z","published":"2024-06-10T16:39:39Z","title":"Cometh: A continuous-time discrete-state graph diffusion model","summary":"  Discrete-state denoising diffusion models led to state-of-the-art performance\nin graph generation, especially in the molecular domain. Recently, they have\nbeen transposed to continuous time, allowing more flexibility in the reverse\nprocess and a better trade-off between sampling efficiency and quality. Here,\nto leverage the benefits of both approaches, we propose Cometh, a\ncontinuous-time discrete-state graph diffusion model, integrating graph data\ninto a continuous-time diffusion model framework. Empirically, we show that\nintegrating continuous time leads to significant improvements across various\nmetrics over state-of-the-art discrete-state diffusion models on a large set of\nmolecular and non-molecular benchmark datasets.\n","authors":["Antoine Siraudin","Fragkiskos D. Malliaros","Christopher Morris"],"pdf_url":"https://arxiv.org/pdf/2406.06449v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2406.06446v1","updated":"2024-06-10T16:36:02Z","published":"2024-06-10T16:36:02Z","title":"Deep Generative Modeling Reshapes Compression and Transmission: From\n  Efficiency to Resiliency","summary":"  Information theory and machine learning are inextricably linked and have even\nbeen referred to as \"two sides of the same coin\". One particularly elegant\nconnection is the essential equivalence between probabilistic generative\nmodeling and data compression or transmission. In this article, we reveal the\ndual-functionality of deep generative models that reshapes both data\ncompression for efficiency and transmission error concealment for resiliency.\nWe present how the contextual predictive capabilities of powerful generative\nmodels can be well positioned to be strong compressors and estimators. In this\nsense, we advocate for viewing the deep generative modeling problem through the\nlens of end-to-end communications, and evaluate the compression and error\nrestoration capabilities of foundation generative models. We show that the\nkernel of many large generative models is powerful predictor that can capture\ncomplex relationships among semantic latent variables, and the communication\nviewpoints provide novel insights into semantic feature tokenization,\ncontextual learning, and usage of deep generative models. In summary, our\narticle highlights the essential connections of generative AI to source and\nchannel coding techniques, and motivates researchers to make further\nexplorations in this emerging topic.\n","authors":["Jincheng Dai","Xiaoqi Qin","Sixian Wang","Lexi Xu","Kai Niu","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06446v1.pdf","comment":"Publication in IEEE Wireless Communications"},{"id":"http://arxiv.org/abs/2406.06443v1","updated":"2024-06-10T16:34:43Z","published":"2024-06-10T16:34:43Z","title":"LLM Dataset Inference: Did you train on my dataset?","summary":"  The proliferation of large language models (LLMs) in the real world has come\nwith a rise in copyright cases against companies for training their models on\nunlicensed data from the internet. Recent works have presented methods to\nidentify if individual text sequences were members of the model's training\ndata, known as membership inference attacks (MIAs). We demonstrate that the\napparent success of these MIAs is confounded by selecting non-members (text\nsequences not used for training) belonging to a different distribution from the\nmembers (e.g., temporally shifted recent Wikipedia articles compared with ones\nused to train the model). This distribution shift makes membership inference\nappear successful. However, most MIA methods perform no better than random\nguessing when discriminating between members and non-members from the same\ndistribution (e.g., in this case, the same period of time). Even when MIAs\nwork, we find that different MIAs succeed at inferring membership of samples\nfrom different distributions. Instead, we propose a new dataset inference\nmethod to accurately identify the datasets used to train large language models.\nThis paradigm sits realistically in the modern-day copyright landscape, where\nauthors claim that an LLM is trained over multiple documents (such as a book)\nwritten by them, rather than one particular paragraph. While dataset inference\nshares many of the challenges of membership inference, we solve it by\nselectively combining the MIAs that provide positive signal for a given\ndistribution, and aggregating them to perform a statistical test on a given\ndataset. Our approach successfully distinguishes the train and test sets of\ndifferent subsets of the Pile with statistically significant p-values < 0.1,\nwithout any false positives.\n","authors":["Pratyush Maini","Hengrui Jia","Nicolas Papernot","Adam Dziedzic"],"pdf_url":"https://arxiv.org/pdf/2406.06443v1.pdf","comment":"Code is available at\n  \\href{https://github.com/pratyushmaini/llm_dataset_inference/"},{"id":"http://arxiv.org/abs/2406.06438v1","updated":"2024-06-10T16:31:34Z","published":"2024-06-10T16:31:34Z","title":"Multimodal Contextualized Semantic Parsing from Speech","summary":"  We introduce Semantic Parsing in Contextual Environments (SPICE), a task\ndesigned to enhance artificial agents' contextual awareness by integrating\nmultimodal inputs with prior contexts. SPICE goes beyond traditional semantic\nparsing by offering a structured, interpretable framework for dynamically\nupdating an agent's knowledge with new information, mirroring the complexity of\nhuman communication. We develop the VG-SPICE dataset, crafted to challenge\nagents with visual scene graph construction from spoken conversational\nexchanges, highlighting speech and visual data integration. We also present the\nAudio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE.\nThese innovations aim to improve multimodal information processing and\nintegration. Both the VG-SPICE dataset and the AViD-SP model are publicly\navailable.\n","authors":["Jordan Voas","Raymond Mooney","David Harwath"],"pdf_url":"https://arxiv.org/pdf/2406.06438v1.pdf","comment":"10 Pages, 3 figures, ACL 2024 Main"},{"id":"http://arxiv.org/abs/2308.11639v2","updated":"2024-06-10T16:29:37Z","published":"2023-08-16T08:33:50Z","title":"An Empirical Study on Fault Detection and Root Cause Analysis of Indium\n  Tin Oxide Electrodes by Processing S-parameter Patterns","summary":"  In the field of optoelectronics, indium tin oxide (ITO) electrodes play a\ncrucial role in various applications, such as displays, sensors, and solar\ncells. Effective fault diagnosis and root cause analysis of the ITO electrodes\nare essential to ensure the performance and reliability of the devices.\nHowever, traditional visual inspection is challenging with transparent ITO\nelectrodes, and existing fault diagnosis methods have limitations in\ndetermining the root causes of the defects, often requiring destructive\nevaluations and secondary material characterization techniques. In this study,\na fault diagnosis method with root cause analysis is proposed using scattering\nparameter (S-parameter) patterns, offering early detection, high diagnostic\naccuracy, and noise robustness. A comprehensive S-parameter pattern database is\nobtained according to various defect states of the ITO electrodes. Deep\nlearning (DL) approaches, including multilayer perceptron (MLP), convolutional\nneural network (CNN), and transformer, are then used to simultaneously analyze\nthe cause and severity of defects. Notably, it is demonstrated that the\ndiagnostic performance under additive noise levels can be significantly\nenhanced by combining different channels of the S-parameters as input to the\nlearning algorithms, as confirmed through the t-distributed stochastic neighbor\nembedding (t-SNE) dimension reduction visualization of the S-parameter\npatterns.\n","authors":["Tae Yeob Kang","Haebom Lee","Sungho Suh"],"pdf_url":"https://arxiv.org/pdf/2308.11639v2.pdf","comment":"Accepted in IEEE Transactions on Device and Materials Reliability"},{"id":"http://arxiv.org/abs/2309.06584v4","updated":"2024-06-10T16:29:11Z","published":"2023-09-12T20:12:08Z","title":"Self-explainable Graph Neural Network for Alzheimer's Disease And\n  Related Dementias Risk Prediction","summary":"  Background:\n  Alzheimer's disease and related dementias (ADRD) ranks as the sixth leading\ncause of death in the US, underlining the importance of accurate ADRD risk\nprediction. While recent advancement in ADRD risk prediction have primarily\nrelied on imaging analysis, yet not all patients undergo medical imaging before\nan ADRD diagnosis. Merging machine learning with claims data can reveal\nadditional risk factors and uncover interconnections among diverse medical\ncodes.\n  Objective:\n  Our goal is to utilize Graph Neural Networks (GNNs) with claims data for ADRD\nrisk prediction. Addressing the lack of human-interpretable reasons behind\nthese predictions, we introduce an innovative method to evaluate relationship\nimportance and its influence on ADRD risk prediction, ensuring comprehensive\ninterpretation.\n  Methods:\n  We employed Variationally Regularized Encoder-decoder Graph Neural Network\n(VGNN) for estimating ADRD likelihood. We created three scenarios to assess the\nmodel's efficiency, using Random Forest and Light Gradient Boost Machine as\nbaselines. We further used our relation importance method to clarify the key\nrelationships for ADRD risk prediction.\n  Results:\n  VGNN surpassed other baseline models by 10% in the area under the receiver\noperating characteristic. The integration of the GNN model and relation\nimportance interpretation could potentially play an essential role in providing\nvaluable insight into factors that may contribute to or delay ADRD progression.\n  Conclusions:\n  Employing a GNN approach with claims data enhances ADRD risk prediction and\nprovides insights into the impact of interconnected medical code relationships.\nThis methodology not only enables ADRD risk modeling but also shows potential\nfor other image analysis predictions using claims data.\n","authors":["Xinyue Hu","Zenan Sun","Yi Nian","Yichen Wang","Yifang Dang","Fang Li","Jingna Feng","Evan Yu","Cui Tao"],"pdf_url":"https://arxiv.org/pdf/2309.06584v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04626v2","updated":"2024-06-10T16:28:15Z","published":"2024-06-07T04:22:32Z","title":"Adaptive Interface-PINNs (AdaI-PINNs): An Efficient Physics-informed\n  Neural Networks Framework for Interface Problems","summary":"  We present an efficient physics-informed neural networks (PINNs) framework,\ntermed Adaptive Interface-PINNs (AdaI-PINNs), to improve the modeling of\ninterface problems with discontinuous coefficients and/or interfacial jumps.\nThis framework is an enhanced version of its predecessor, Interface PINNs or\nI-PINNs (Sarma et al.; https://dx.doi.org/10.2139/ssrn.4766623), which involves\ndomain decomposition and assignment of different predefined activation\nfunctions to the neural networks in each subdomain across a sharp interface,\nwhile keeping all other parameters of the neural networks identical. In\nAdaI-PINNs, the activation functions vary solely in their slopes, which are\ntrained along with the other parameters of the neural networks. This makes the\nAdaI-PINNs framework fully automated without requiring preset activation\nfunctions. Comparative studies on one-dimensional, two-dimensional, and\nthree-dimensional benchmark elliptic interface problems reveal that AdaI-PINNs\noutperform I-PINNs, reducing computational costs by 2-6 times while producing\nsimilar or better accuracy.\n","authors":["Sumanta Roy","Chandrasekhar Annavarapu","Pratanu Roy","Antareep Kumar Sarma"],"pdf_url":"https://arxiv.org/pdf/2406.04626v2.pdf","comment":"17 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2402.14860v4","updated":"2024-06-10T16:25:30Z","published":"2024-02-21T00:49:43Z","title":"Ranking Large Language Models without Ground Truth","summary":"  Evaluation and ranking of large language models (LLMs) has become an\nimportant problem with the proliferation of these models and their impact.\nEvaluation methods either require human responses which are expensive to\nacquire or use pairs of LLMs to evaluate each other which can be unreliable. In\nthis paper, we provide a novel perspective where, given a dataset of prompts\n(viz. questions, instructions, etc.) and a set of LLMs, we rank them without\naccess to any ground truth or reference responses. Inspired by real life where\nboth an expert and a knowledgeable person can identify a novice our main idea\nis to consider triplets of models, where each one of them evaluates the other\ntwo, correctly identifying the worst model in the triplet with high\nprobability. We also analyze our idea and provide sufficient conditions for it\nto succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.\nIn experiments on different generative tasks (summarization, multiple-choice,\nand dialog), our methods reliably recover close to true rankings without\nreference data. This points to a viable low-resource mechanism for practical\nuse.\n","authors":["Amit Dhurandhar","Rahul Nair","Moninder Singh","Elizabeth Daly","Karthikeyan Natesan Ramamurthy"],"pdf_url":"https://arxiv.org/pdf/2402.14860v4.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.06433v1","updated":"2024-06-10T16:24:35Z","published":"2024-06-10T16:24:35Z","title":"DISCO: An End-to-End Bandit Framework for Personalised Discount\n  Allocation","summary":"  Personalised discount codes provide a powerful mechanism for managing\ncustomer relationships and operational spend in e-commerce. Bandits are well\nsuited for this product area, given the partial information nature of the\nproblem, as well as the need for adaptation to the changing business\nenvironment. Here, we introduce DISCO, an end-to-end contextual bandit\nframework for personalised discount code allocation at ASOS.com. DISCO adapts\nthe traditional Thompson Sampling algorithm by integrating it within an integer\nprogram, thereby allowing for operational cost control. Because bandit learning\nis often worse with high dimensional actions, we focused on building low\ndimensional action and context representations that were nonetheless capable of\ngood accuracy. Additionally, we sought to build a model that preserved the\nrelationship between price and sales, in which customers increasing their\npurchasing in response to lower prices (\"negative price elasticity\"). These\naims were achieved by using radial basis functions to represent the continuous\n(i.e. infinite armed) action space, in combination with context embeddings\nextracted from a neural network. These feature representations were used within\na Thompson Sampling framework to facilitate exploration, and further integrated\nwith an integer program to allocate discount codes across ASOS's customer base.\nThese modelling decisions result in a reward model that (a) enables pooled\nlearning across similar actions, (b) is highly accurate, including in\nextrapolation, and (c) preserves the expected negative price elasticity.\nThrough offline analysis, we show that DISCO is able to effectively enact\nexploration and improves its performance over time, despite the global\nconstraint. Finally, we subjected DISCO to a rigorous online A/B test, and find\nthat it achieves a significant improvement of >1% in average basket value,\nrelative to the legacy systems.\n","authors":["Jason Shuo Zhang","Benjamin Howson","Panayiota Savva","Eleanor Loh"],"pdf_url":"https://arxiv.org/pdf/2406.06433v1.pdf","comment":"Accepted at ECML/PKDD 2024"},{"id":"http://arxiv.org/abs/2406.06425v1","updated":"2024-06-10T16:14:50Z","published":"2024-06-10T16:14:50Z","title":"Multivariate Stochastic Dominance via Optimal Transport and Applications\n  to Models Benchmarking","summary":"  Stochastic dominance is an important concept in probability theory,\neconometrics and social choice theory for robustly modeling agents' preferences\nbetween random outcomes. While many works have been dedicated to the univariate\ncase, little has been done in the multivariate scenario, wherein an agent has\nto decide between different multivariate outcomes. By exploiting a\ncharacterization of multivariate first stochastic dominance in terms of\ncouplings, we introduce a statistic that assesses multivariate almost\nstochastic dominance under the framework of Optimal Transport with a smooth\ncost. Further, we introduce an entropic regularization of this statistic, and\nestablish a central limit theorem (CLT) and consistency of the bootstrap\nprocedure for the empirical statistic. Armed with this CLT, we propose a\nhypothesis testing framework as well as an efficient implementation using the\nSinkhorn algorithm. We showcase our method in comparing and benchmarking Large\nLanguage Models that are evaluated on multiple metrics. Our multivariate\nstochastic dominance test allows us to capture the dependencies between the\nmetrics in order to make an informed and statistically significant decision on\nthe relative performance of the models.\n","authors":["Gabriel Rioux","Apoorva Nitsure","Mattia Rigotti","Kristjan Greenewald","Youssef Mroueh"],"pdf_url":"https://arxiv.org/pdf/2406.06425v1.pdf","comment":"27 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.06420v1","updated":"2024-06-10T16:12:32Z","published":"2024-06-10T16:12:32Z","title":"An Improved Empirical Fisher Approximation for Natural Gradient Descent","summary":"  Approximate Natural Gradient Descent (NGD) methods are an important family of\noptimisers for deep learning models, which use approximate Fisher information\nmatrices to pre-condition gradients during training. The empirical Fisher (EF)\nmethod approximates the Fisher information matrix empirically by reusing the\nper-sample gradients collected during back-propagation. Despite its ease of\nimplementation, the EF approximation has its theoretical and practical\nlimitations. This paper first investigates the inversely-scaled projection\nissue of EF, which is shown to be a major cause of the poor empirical\napproximation quality. An improved empirical Fisher (iEF) method, motivated as\na generalised NGD method from a loss reduction perspective, is proposed to\naddress this issue, meanwhile retaining the practical convenience of EF. The\nexact iEF and EF methods are experimentally evaluated using practical deep\nlearning setups, including widely-used setups for parameter-efficient\nfine-tuning of pre-trained models (T5-base with LoRA and Prompt-Tuning on GLUE\ntasks, and ViT with LoRA for CIFAR100). Optimisation experiments show that\napplying exact iEF as an optimiser provides strong convergence and\ngeneralisation. It achieves the best test performance and the lowest training\nloss for majority of the tasks, even when compared with well-tuned\nAdamW/Adafactor baselines. Additionally, under a novel empirical evaluation\nframework, the proposed iEF method shows consistently better approximation\nquality to the exact Natural Gradient updates than both EF and the more\nexpensive sampled Fisher (SF). Further investigation also shows that the\nsuperior approximation quality of iEF is robust to damping across tasks and\ntraining stages. Improving existing approximate NGD optimisers with iEF is\nexpected to lead to better convergence ability and stronger robustness to\nchoice of damping.\n","authors":["Xiaodong Wu","Wenyi Yu","Chao Zhang","Philip Woodland"],"pdf_url":"https://arxiv.org/pdf/2406.06420v1.pdf","comment":"33 pages, 11 figures, 7 tables"},{"id":"http://arxiv.org/abs/2406.06419v1","updated":"2024-06-10T16:12:00Z","published":"2024-06-10T16:12:00Z","title":"Foundation Inference Models for Markov Jump Processes","summary":"  Markov jump processes are continuous-time stochastic processes which describe\ndynamical systems evolving in discrete state spaces. These processes find wide\napplication in the natural sciences and machine learning, but their inference\nis known to be far from trivial. In this work we introduce a methodology for\nzero-shot inference of Markov jump processes (MJPs), on bounded state spaces,\nfrom noisy and sparse observations, which consists of two components. First, a\nbroad probability distribution over families of MJPs, as well as over possible\nobservation times and noise mechanisms, with which we simulate a synthetic\ndataset of hidden MJPs and their noisy observation process. Second, a neural\nnetwork model that processes subsets of the simulated observations, and that is\ntrained to output the initial condition and rate matrix of the target MJP in a\nsupervised way. We empirically demonstrate that one and the same (pretrained)\nmodel can infer, in a zero-shot fashion, hidden MJPs evolving in state spaces\nof different dimensionalities. Specifically, we infer MJPs which describe (i)\ndiscrete flashing ratchet systems, which are a type of Brownian motors, and the\nconformational dynamics in (ii) molecular simulations, (iii) experimental ion\nchannel data and (iv) simple protein folding models. What is more, we show that\nour model performs on par with state-of-the-art models which are finetuned to\nthe target datasets.\n","authors":["David Berghaus","Kostadin Cvejoski","Patrick Seifner","Cesar Ojeda","Ramses J. Sanchez"],"pdf_url":"https://arxiv.org/pdf/2406.06419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06417v1","updated":"2024-06-10T16:09:16Z","published":"2024-06-10T16:09:16Z","title":"Explainable Graph Neural Networks Under Fire","summary":"  Predictions made by graph neural networks (GNNs) usually lack\ninterpretability due to their complex computational behavior and the abstract\nnature of graphs. In an attempt to tackle this, many GNN explanation methods\nhave emerged. Their goal is to explain a model's predictions and thereby obtain\ntrust when GNN models are deployed in decision critical applications. Most GNN\nexplanation methods work in a post-hoc manner and provide explanations in the\nform of a small subset of important edges and/or nodes. In this paper we\ndemonstrate that these explanations can unfortunately not be trusted, as common\nGNN explanation methods turn out to be highly susceptible to adversarial\nperturbations. That is, even small perturbations of the original graph\nstructure that preserve the model's predictions may yield drastically different\nexplanations. This calls into question the trustworthiness and practical\nutility of post-hoc explanation methods for GNNs. To be able to attack GNN\nexplanation models, we devise a novel attack method dubbed \\textit{GXAttack},\nthe first \\textit{optimization-based} adversarial attack method for post-hoc\nGNN explanations under such settings. Due to the devastating effectiveness of\nour attack, we call for an adversarial evaluation of future GNN explainers to\ndemonstrate their robustness.\n","authors":["Zhong Li","Simon Geisler","Yuhang Wang","Stephan Günnemann","Matthijs van Leeuwen"],"pdf_url":"https://arxiv.org/pdf/2406.06417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03314v2","updated":"2024-06-10T16:09:03Z","published":"2024-06-05T14:26:45Z","title":"Reproducibility study of FairAC","summary":"  This work aims to reproduce the findings of the paper \"Fair Attribute\nCompletion on Graph with Missing Attributes\" written by Guo, Chu, and Li\narXiv:2302.12977 by investigating the claims made in the paper. This paper\nsuggests that the results of the original paper are reproducible and thus, the\nclaims hold. However, the claim that FairAC is a generic framework for many\ndownstream tasks is very broad and could therefore only be partially tested.\nMoreover, we show that FairAC is generalizable to various datasets and\nsensitive attributes and show evidence that the improvement in group fairness\nof the FairAC framework does not come at the expense of individual fairness.\nLastly, the codebase of FairAC has been refactored and is now easily applicable\nfor various datasets and models.\n","authors":["Gijs de Jong","Macha J. Meijer","Derck W. E. Prinzhorn","Harold Ruiter"],"pdf_url":"https://arxiv.org/pdf/2406.03314v2.pdf","comment":"14 pages, 2 figures, accepted at TMLR"},{"id":"http://arxiv.org/abs/2406.06408v1","updated":"2024-06-10T16:02:48Z","published":"2024-06-10T16:02:48Z","title":"Differentially Private Best-Arm Identification","summary":"  Best Arm Identification (BAI) problems are progressively used for\ndata-sensitive applications, such as designing adaptive clinical trials, tuning\nhyper-parameters, and conducting user studies. Motivated by the data privacy\nconcerns invoked by these applications, we study the problem of BAI with fixed\nconfidence in both the local and central models, i.e. $\\epsilon$-local and\n$\\epsilon$-global Differential Privacy (DP). First, to quantify the cost of\nprivacy, we derive lower bounds on the sample complexity of any\n$\\delta$-correct BAI algorithm satisfying $\\epsilon$-global DP or\n$\\epsilon$-local DP. Our lower bounds suggest the existence of two privacy\nregimes. In the high-privacy regime, the hardness depends on a coupled effect\nof privacy and novel information-theoretic quantities involving the Total\nVariation. In the low-privacy regime, the lower bounds reduce to the\nnon-private lower bounds. We propose $\\epsilon$-local DP and $\\epsilon$-global\nDP variants of a Top Two algorithm, namely CTB-TT and AdaP-TT*, respectively.\nFor $\\epsilon$-local DP, CTB-TT is asymptotically optimal by plugging in a\nprivate estimator of the means based on Randomised Response. For\n$\\epsilon$-global DP, our private estimator of the mean runs in arm-dependent\nadaptive episodes and adds Laplace noise to ensure a good privacy-utility\ntrade-off. By adapting the transportation costs, the expected sample complexity\nof AdaP-TT* reaches the asymptotic lower bound up to multiplicative constants.\n","authors":["Achraf Azize","Marc Jourdan","Aymen Al Marjani","Debabrota Basu"],"pdf_url":"https://arxiv.org/pdf/2406.06408v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2309.02202"},{"id":"http://arxiv.org/abs/2406.06407v1","updated":"2024-06-10T15:59:08Z","published":"2024-06-10T15:59:08Z","title":"A Taxonomy of Challenges to Curating Fair Datasets","summary":"  Despite extensive efforts to create fairer machine learning (ML) datasets,\nthere remains a limited understanding of the practical aspects of dataset\ncuration. Drawing from interviews with 30 ML dataset curators, we present a\ncomprehensive taxonomy of the challenges and trade-offs encountered throughout\nthe dataset curation lifecycle. Our findings underscore overarching issues\nwithin the broader fairness landscape that impact data curation. We conclude\nwith recommendations aimed at fostering systemic changes to better facilitate\nfair dataset curation practices.\n","authors":["Dora Zhao","Morgan Klaus Scheuerman","Pooja Chitre","Jerone T. A. Andrews","Georgia Panagiotidou","Shawn Walker","Kathleen H. Pine","Alice Xiang"],"pdf_url":"https://arxiv.org/pdf/2406.06407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06403v1","updated":"2024-06-10T15:56:52Z","published":"2024-06-10T15:56:52Z","title":"Meta Learning Text-to-Speech Synthesis in over 7000 Languages","summary":"  In this work, we take on the challenging task of building a single\ntext-to-speech synthesis system that is capable of generating speech in over\n7000 languages, many of which lack sufficient data for traditional TTS\ndevelopment. By leveraging a novel integration of massively multilingual\npretraining and meta learning to approximate language representations, our\napproach enables zero-shot speech synthesis in languages without any available\ndata. We validate our system's performance through objective measures and human\nevaluation across a diverse linguistic landscape. By releasing our code and\nmodels publicly, we aim to empower communities with limited linguistic\nresources and foster further innovation in the field of speech technology.\n","authors":["Florian Lux","Sarina Meyer","Lyonel Behringer","Frank Zalkow","Phat Do","Matt Coler","Emanuël A. P. Habets","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2406.06403v1.pdf","comment":"accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2306.05862v2","updated":"2024-06-10T15:52:35Z","published":"2023-06-09T12:53:24Z","title":"Lessons from Generalization Error Analysis of Federated Learning: You\n  May Communicate Less Often!","summary":"  We investigate the generalization error of statistical learning models in a\nFederated Learning (FL) setting. Specifically, we study the evolution of the\ngeneralization error with the number of communication rounds $R$ between $K$\nclients and a parameter server (PS), i.e., the effect on the generalization\nerror of how often the clients' local models are aggregated at PS. In our\nsetup, the more the clients communicate with PS the less data they use for\nlocal training in each round, such that the amount of training data per client\nis identical for distinct values of $R$. We establish PAC-Bayes and\nrate-distortion theoretic bounds on the generalization error that account\nexplicitly for the effect of the number of rounds $R$, in addition to the\nnumber of participating devices $K$ and individual datasets size $n$. The\nbounds, which apply to a large class of loss functions and learning algorithms,\nappear to be the first of their kind for the FL setting. Furthermore, we apply\nour bounds to FL-type Support Vector Machines (FSVM); and derive (more)\nexplicit bounds in this case. In particular, we show that the generalization\nbound of FSVM increases with $R$, suggesting that more frequent communication\nwith PS diminishes the generalization power. This implies that the population\nrisk decreases less fast with $R$ than does the empirical risk. Moreover, our\nbound suggests that the generalization error of FSVM decreases faster than that\nof centralized learning by a factor of $\\mathcal{O}(\\sqrt{\\log(K)/K})$.\nFinally, we provide experimental results obtained using neural networks\n(ResNet-56) which show evidence that not only may our observations for FSVM\nhold more generally but also that the population risk may even start to\nincrease beyond some value of $R$.\n","authors":["Milad Sefidgaran","Romain Chor","Abdellatif Zaidi","Yijun Wan"],"pdf_url":"https://arxiv.org/pdf/2306.05862v2.pdf","comment":"Accepted at ICML 2024"},{"id":"http://arxiv.org/abs/2406.06397v1","updated":"2024-06-10T15:50:45Z","published":"2024-06-10T15:50:45Z","title":"Contrastive learning of T cell receptor representations","summary":"  Computational prediction of the interaction of T cell receptors (TCRs) and\ntheir ligands is a grand challenge in immunology. Despite advances in\nhigh-throughput assays, specificity-labelled TCR data remains sparse. In other\ndomains, the pre-training of language models on unlabelled data has been\nsuccessfully used to address data bottlenecks. However, it is unclear how to\nbest pre-train protein language models for TCR specificity prediction. Here we\nintroduce a TCR language model called SCEPTR (Simple Contrastive Embedding of\nthe Primary sequence of T cell Receptors), capable of data-efficient transfer\nlearning. Through our model, we introduce a novel pre-training strategy\ncombining autocontrastive learning and masked-language modelling, which enables\nSCEPTR to achieve its state-of-the-art performance. In contrast, existing\nprotein language models and a variant of SCEPTR pre-trained without\nautocontrastive learning are outperformed by sequence alignment-based methods.\nWe anticipate that contrastive learning will be a useful paradigm to decode the\nrules of TCR specificity.\n","authors":["Yuta Nagano","Andrew Pyo","Martina Milighetti","James Henderson","John Shawe-Taylor","Benny Chain","Andreas Tiffeau-Mayer"],"pdf_url":"https://arxiv.org/pdf/2406.06397v1.pdf","comment":"19 pages, 17 figures"},{"id":"http://arxiv.org/abs/2406.06391v1","updated":"2024-06-10T15:46:25Z","published":"2024-06-10T15:46:25Z","title":"Towards Lifelong Learning of Large Language Models: A Survey","summary":"  As the applications of large language models (LLMs) expand across diverse\nfields, the ability of these models to adapt to ongoing changes in data, tasks,\nand user preferences becomes crucial. Traditional training methods, relying on\nstatic datasets, are increasingly inadequate for coping with the dynamic nature\nof real-world information. Lifelong learning, also known as continual or\nincremental learning, addresses this challenge by enabling LLMs to learn\ncontinuously and adaptively over their operational lifetime, integrating new\nknowledge while retaining previously learned information and preventing\ncatastrophic forgetting. This survey delves into the sophisticated landscape of\nlifelong learning, categorizing strategies into two primary groups: Internal\nKnowledge and External Knowledge. Internal Knowledge includes continual\npretraining and continual finetuning, each enhancing the adaptability of LLMs\nin various scenarios. External Knowledge encompasses retrieval-based and\ntool-based lifelong learning, leveraging external data sources and\ncomputational tools to extend the model's capabilities without modifying core\nparameters. The key contributions of our survey are: (1) Introducing a novel\ntaxonomy categorizing the extensive literature of lifelong learning into 12\nscenarios; (2) Identifying common techniques across all lifelong learning\nscenarios and classifying existing literature into various technique groups\nwithin each scenario; (3) Highlighting emerging techniques such as model\nexpansion and data selection, which were less explored in the pre-LLM era.\nThrough a detailed examination of these groups and their respective categories,\nthis survey aims to enhance the adaptability, reliability, and overall\nperformance of LLMs in real-world applications.\n","authors":["Junhao Zheng","Shengjie Qiu","Chengming Shi","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2406.06391v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2406.06385v1","updated":"2024-06-10T15:44:22Z","published":"2024-06-10T15:44:22Z","title":"Low-Rank Quantization-Aware Training for LLMs","summary":"  Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to the LLaMA-2/3 and Mistral model\nfamilies and validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory.\n","authors":["Yelysei Bondarenko","Riccardo Del Chiaro","Markus Nagel"],"pdf_url":"https://arxiv.org/pdf/2406.06385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07937v2","updated":"2024-06-10T15:42:50Z","published":"2024-05-13T17:13:32Z","title":"Active Learning with Simple Questions","summary":"  We consider an active learning setting where a learner is presented with a\npool S of n unlabeled examples belonging to a domain X and asks queries to find\nthe underlying labeling that agrees with a target concept h^* \\in H.\n  In contrast to traditional active learning that queries a single example for\nits label, we study more general region queries that allow the learner to pick\na subset of the domain T \\subset X and a target label y and ask a labeler\nwhether h^*(x) = y for every example in the set T \\cap S.\n  Such more powerful queries allow us to bypass the limitations of traditional\nactive learning and use significantly fewer rounds of interactions to learn but\ncan potentially lead to a significantly more complex query language. Our main\ncontribution is quantifying the trade-off between the number of queries and the\ncomplexity of the query language used by the learner.\n  We measure the complexity of the region queries via the VC dimension of the\nfamily of regions. We show that given any hypothesis class H with VC dimension\nd, one can design a region query family Q with VC dimension O(d) such that for\nevery set of n examples S \\subset X and every h^* \\in H, a learner can submit\nO(d log n) queries from Q to a labeler and perfectly label S. We show a\nmatching lower bound by designing a hypothesis class H with VC dimension d and\na dataset S \\subset X of size n such that any learning algorithm using any\nquery class with VC dimension less than O(d) must make poly(n) queries to label\nS perfectly.\n  Finally, we focus on well-studied hypothesis classes including unions of\nintervals, high-dimensional boxes, and d-dimensional halfspaces, and obtain\nstronger results. In particular, we design learning algorithms that (i) are\ncomputationally efficient and (ii) work even when the queries are not answered\nbased on the learner's pool of examples S but on some unknown superset L of S\n","authors":["Vasilis Kontonis","Mingchen Ma","Christos Tzamos"],"pdf_url":"https://arxiv.org/pdf/2405.07937v2.pdf","comment":"To appear at COLT 2024"},{"id":"http://arxiv.org/abs/2406.06382v1","updated":"2024-06-10T15:42:03Z","published":"2024-06-10T15:42:03Z","title":"Diffusion-RPO: Aligning Diffusion Models through Relative Preference\n  Optimization","summary":"  Aligning large language models with human preferences has emerged as a\ncritical focus in language modeling research. Yet, integrating preference\nlearning into Text-to-Image (T2I) generative models is still relatively\nuncharted territory. The Diffusion-DPO technique made initial strides by\nemploying pairwise preference learning in diffusion models tailored for\nspecific text prompts. We introduce Diffusion-RPO, a new method designed to\nalign diffusion-based T2I models with human preferences more effectively. This\napproach leverages both prompt-image pairs with identical prompts and those\nwith semantically related content across various modalities. Furthermore, we\nhave developed a new evaluation metric, style alignment, aimed at overcoming\nthe challenges of high costs, low reproducibility, and limited interpretability\nprevalent in current evaluations of human preference alignment. Our findings\ndemonstrate that Diffusion-RPO outperforms established methods such as\nSupervised Fine-Tuning and Diffusion-DPO in tuning Stable Diffusion versions\n1.5 and XL-1.0, achieving superior results in both automated evaluations of\nhuman preferences and style alignment. Our code is available at\nhttps://github.com/yigu1008/Diffusion-RPO\n","authors":["Yi Gu","Zhendong Wang","Yueqin Yin","Yujia Xie","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.06382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00425v2","updated":"2024-06-10T15:21:41Z","published":"2024-03-01T10:21:52Z","title":"HALC: Object Hallucination Reduction via Adaptive Focal-Contrast\n  Decoding","summary":"  While large vision-language models (LVLMs) have demonstrated impressive\ncapabilities in interpreting multi-modal contexts, they invariably suffer from\nobject hallucinations (OH). We introduce HALC, a novel decoding algorithm\ndesigned to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal\nvisual information in vision-language tasks and operates on both local and\nglobal contexts simultaneously. Specifically, HALC integrates a robust\nauto-focal grounding mechanism (locally) to correct hallucinated tokens on the\nfly, and a specialized beam search algorithm (globally) to significantly reduce\nOH while preserving text generation quality. Additionally, HALC can be\nintegrated into any LVLMs as a plug-and-play module without extra training.\nExtensive experimental studies demonstrate the effectiveness of HALC in\nreducing OH, outperforming state-of-the-arts across four benchmarks.\n","authors":["Zhaorun Chen","Zhuokai Zhao","Hongyin Luo","Huaxiu Yao","Bo Li","Jiawei Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.00425v2.pdf","comment":"ICML camera-ready version. Code is released at\n  https://github.com/BillChan226/HALC"},{"id":"http://arxiv.org/abs/2406.06354v1","updated":"2024-06-10T15:14:33Z","published":"2024-06-10T15:14:33Z","title":"On the Minimal Degree Bias in Generalization on the Unseen for\n  non-Boolean Functions","summary":"  We investigate the out-of-domain generalization of random feature (RF) models\nand Transformers. We first prove that in the `generalization on the unseen\n(GOTU)' setting, where training data is fully seen in some part of the domain\nbut testing is made on another part, and for RF models in the small feature\nregime, the convergence takes place to interpolators of minimal degree as in\nthe Boolean case (Abbe et al., 2023). We then consider the sparse target regime\nand explain how this regime relates to the small feature regime, but with a\ndifferent regularization term that can alter the picture in the non-Boolean\ncase. We show two different outcomes for the sparse regime with q-ary data\ntokens: (1) if the data is embedded with roots of unities, then a min-degree\ninterpolator is learned like in the Boolean case for RF models, (2) if the data\nis not embedded as such, e.g., simply as integers, then RF models and\nTransformers may not learn minimal degree interpolators. This shows that the\nBoolean setting and its roots of unities generalization are special cases where\nthe minimal degree interpolator offers a rare characterization of how learning\ntakes place. For more general integer and real-valued settings, a more nuanced\npicture remains to be fully characterized.\n","authors":["Denys Pushkin","Raphaël Berthier","Emmanuel Abbe"],"pdf_url":"https://arxiv.org/pdf/2406.06354v1.pdf","comment":"9 pages of main body, 24 pages in total. 7 figures Proceedings of the\n  41-st International Conference on Machine Learning, Vienna, Austria. PMLR\n  235, 2024"},{"id":"http://arxiv.org/abs/2406.06351v1","updated":"2024-06-10T15:13:07Z","published":"2024-06-10T15:13:07Z","title":"Cascading Unknown Detection with Known Classification for Open Set\n  Recognition","summary":"  Deep learners tend to perform well when trained under the closed set\nassumption but struggle when deployed under open set conditions. This motivates\nthe field of Open Set Recognition in which we seek to give deep learners the\nability to recognize whether a data sample belongs to the known classes trained\non or comes from the surrounding infinite world. Existing open set recognition\nmethods typically rely upon a single function for the dual task of\ndistinguishing between knowns and unknowns as well as making known class\ndistinction. This dual process leaves performance on the table as the function\nis not specialized for either task. In this work, we introduce Cascading\nUnknown Detection with Known Classification (Cas-DC), where we instead learn\nspecialized functions in a cascading fashion for both known/unknown detection\nand fine class classification amongst the world of knowns. Our experiments and\nanalysis demonstrate that Cas-DC handily outperforms modern methods in open set\nrecognition when compared using AUROC scores and correct classification rate at\nvarious true positive rates.\n","authors":["Daniel Brignac","Abhijit Mahalanobis"],"pdf_url":"https://arxiv.org/pdf/2406.06351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06350v1","updated":"2024-06-10T15:12:53Z","published":"2024-06-10T15:12:53Z","title":"Error Analysis and Numerical Algorithm for PDE Approximation with\n  Hidden-Layer Concatenated Physics Informed Neural Networks","summary":"  We present the hidden-layer concatenated physics informed neural network\n(HLConcPINN) method, which combines hidden-layer concatenated feed-forward\nneural networks, a modified block time marching strategy, and a physics\ninformed approach for approximating partial differential equations (PDEs). We\nanalyze the convergence properties and establish the error bounds of this\nmethod for two types of PDEs: parabolic (exemplified by the heat and Burgers'\nequations) and hyperbolic (exemplified by the wave and nonlinear Klein-Gordon\nequations). We show that its approximation error of the solution can be\neffectively controlled by the training loss for dynamic simulations with long\ntime horizons. The HLConcPINN method in principle allows an arbitrary number of\nhidden layers not smaller than two and any of the commonly-used smooth\nactivation functions for the hidden layers beyond the first two, with\ntheoretical guarantees. This generalizes several recent neural-network\ntechniques, which have theoretical guarantees but are confined to two hidden\nlayers in the network architecture and the $\\tanh$ activation function. Our\ntheoretical analyses subsequently inform the formulation of appropriate\ntraining loss functions for these PDEs, leading to physics informed neural\nnetwork (PINN) type computational algorithms that differ from the standard PINN\nformulation. Ample numerical experiments are presented based on the proposed\nalgorithm to validate the effectiveness of this method and confirm aspects of\nthe theoretical analyses.\n","authors":["Yianxia Qian","Yongchao Zhang","Suchuan Dong"],"pdf_url":"https://arxiv.org/pdf/2406.06350v1.pdf","comment":"40 pages, 10 tables, 18 figures"},{"id":"http://arxiv.org/abs/2310.18329v2","updated":"2024-06-10T15:09:24Z","published":"2023-10-19T23:55:00Z","title":"Unveiling Energy Efficiency in Deep Learning: Measurement, Prediction,\n  and Scoring across Edge Devices","summary":"  Today, deep learning optimization is primarily driven by research focused on\nachieving high inference accuracy and reducing latency. However, the energy\nefficiency aspect is often overlooked, possibly due to a lack of sustainability\nmindset in the field and the absence of a holistic energy dataset. In this\npaper, we conduct a threefold study, including energy measurement, prediction,\nand efficiency scoring, with an objective to foster transparency in power and\nenergy consumption within deep learning across various edge devices. Firstly,\nwe present a detailed, first-of-its-kind measurement study that uncovers the\nenergy consumption characteristics of on-device deep learning. This study\nresults in the creation of three extensive energy datasets for edge devices,\ncovering a wide range of kernels, state-of-the-art DNN models, and popular AI\napplications. Secondly, we design and implement the first kernel-level energy\npredictors for edge devices based on our kernel-level energy dataset.\nEvaluation results demonstrate the ability of our predictors to provide\nconsistent and accurate energy estimations on unseen DNN models. Lastly, we\nintroduce two scoring metrics, PCS and IECS, developed to convert complex power\nand energy consumption data of an edge device into an easily understandable\nmanner for edge device end-users. We hope our work can help shift the mindset\nof both end-users and the research community towards sustainability in edge\ncomputing, a principle that drives our research. Find data, code, and more\nup-to-date information at https://amai-gsu.github.io/DeepEn2023.\n","authors":["Xiaolong Tu","Anik Mallik","Dawei Chen","Kyungtae Han","Onur Altintas","Haoxin Wang","Jiang Xie"],"pdf_url":"https://arxiv.org/pdf/2310.18329v2.pdf","comment":"This paper has been accepted by ACM/IEEE Symposium on Edge Computing\n  (SEC '23)"},{"id":"http://arxiv.org/abs/2406.06348v1","updated":"2024-06-10T15:08:14Z","published":"2024-06-10T15:08:14Z","title":"Causal Discovery over High-Dimensional Structured Hypothesis Spaces with\n  Causal Graph Partitioning","summary":"  The aim in many sciences is to understand the mechanisms that underlie the\nobserved distribution of variables, starting from a set of initial hypotheses.\nCausal discovery allows us to infer mechanisms as sets of cause and effect\nrelationships in a generalized way -- without necessarily tailoring to a\nspecific domain. Causal discovery algorithms search over a structured\nhypothesis space, defined by the set of directed acyclic graphs, to find the\ngraph that best explains the data. For high-dimensional problems, however, this\nsearch becomes intractable and scalable algorithms for causal discovery are\nneeded to bridge the gap. In this paper, we define a novel causal graph\npartition that allows for divide-and-conquer causal discovery with theoretical\nguarantees. We leverage the idea of a superstructure -- a set of learned or\nexisting candidate hypotheses -- to partition the search space. We prove under\ncertain assumptions that learning with a causal graph partition always yields\nthe Markov Equivalence Class of the true causal graph. We show our algorithm\nachieves comparable accuracy and a faster time to solution for\nbiologically-tuned synthetic networks and networks up to ${10^4}$ variables.\nThis makes our method applicable to gene regulatory network inference and other\ndomains with high-dimensional structured hypothesis spaces.\n","authors":["Ashka Shah","Adela DePavia","Nathaniel Hudson","Ian Foster","Rick Stevens"],"pdf_url":"https://arxiv.org/pdf/2406.06348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20482v2","updated":"2024-06-10T15:06:41Z","published":"2024-05-30T21:08:14Z","title":"Sparsity regularization via tree-structured environments for\n  disentangled representations","summary":"  Many causal systems such as biological processes in cells can only be\nobserved indirectly via measurements, such as gene expression. Causal\nrepresentation learning -- the task of correctly mapping low-level observations\nto latent causal variables -- could advance scientific understanding by\nenabling inference of latent variables such as pathway activation. In this\npaper, we develop methods for inferring latent variables from multiple related\ndatasets (environments) and tasks. As a running example, we consider the task\nof predicting a phenotype from gene expression, where we often collect data\nfrom multiple cell types or organisms that are related in known ways. The key\ninsight is that the mapping from latent variables driven by gene expression to\nthe phenotype of interest changes sparsely across closely related environments.\nTo model sparse changes, we introduce Tree-Based Regularization (TBR), an\nobjective that minimizes both prediction error and regularizes closely related\nenvironments to learn similar predictors. We prove that under assumptions about\nthe degree of sparse changes, TBR identifies the true latent variables up to\nsome simple transformations. We evaluate the theory empirically with both\nsimulations and ground-truth gene expression data. We find that TBR recovers\nthe latent causal variables better than related methods across these settings,\neven under settings that violate some assumptions of the theory.\n","authors":["Elliot Layne","Jason Hartford","Sébastien Lachapelle","Mathieu Blanchette","Dhanya Sridhar"],"pdf_url":"https://arxiv.org/pdf/2405.20482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06989v4","updated":"2024-06-10T15:05:39Z","published":"2023-05-11T17:23:29Z","title":"Neural Wave Functions for Superfluids","summary":"  Understanding superfluidity remains a major goal of condensed matter physics.\nHere we tackle this challenge utilizing the recently developed Fermionic neural\nnetwork (FermiNet) wave function Ansatz [D. Pfau et al., Phys. Rev. Res. 2,\n033429 (2020).] for variational Monte Carlo calculations. We study the unitary\nFermi gas, a system with strong, short-range, two-body interactions known to\npossess a superfluid ground state but difficult to describe quantitatively. We\ndemonstrate key limitations of the FermiNet Ansatz in studying the unitary\nFermi gas and propose a simple modification based on the idea of an\nantisymmetric geminal power singlet (AGPs) wave function. The new AGPs FermiNet\noutperforms the original FermiNet significantly in paired systems, giving\nresults which are more accurate than fixed-node diffusion Monte Carlo and are\nconsistent with experiment. We prove mathematically that the new Ansatz, which\nonly differs from the original Ansatz by the method of antisymmetrization, is a\nstrict generalization of the original FermiNet architecture, despite the use of\nfewer parameters. Our approach shares several advantages with the original\nFermiNet: the use of a neural network removes the need for an underlying basis\nset; and the flexibility of the network yields extremely accurate results\nwithin a variational quantum Monte Carlo framework that provides access to\nunbiased estimates of arbitrary ground-state expectation values. We discuss how\nthe method can be extended to study other superfluids.\n","authors":["Wan Tong Lou","Halvard Sutterud","Gino Cassella","W. M. C. Foulkes","Johannes Knolle","David Pfau","James S. Spencer"],"pdf_url":"https://arxiv.org/pdf/2305.06989v4.pdf","comment":"19 pages, 8 figures. Talk presented at the 2023 APS March Meeting,\n  March 5-10, 2023, Las Vegas, Nevada, United States"},{"id":"http://arxiv.org/abs/2310.14968v3","updated":"2024-06-10T15:02:31Z","published":"2023-10-23T14:13:27Z","title":"Bayesian Active Learning in the Presence of Nuisance Parameters","summary":"  In many settings, such as scientific inference, optimization, and transfer\nlearning, the learner has a well-defined objective, which can be treated as\nestimation of a target parameter, and no intrinsic interest in characterizing\nthe entire data-generating process. Usually, the learner must also contend with\nadditional sources of uncertainty or variables -- with nuisance parameters.\nBayesian active learning, or sequential optimal experimental design, can\nstraightforwardly accommodate the presence of nuisance parameters, and so is a\nnatural active learning framework for such problems. However, the introduction\nof nuisance parameters can lead to bias in the Bayesian learner's estimate of\nthe target parameters, a phenomenon we refer to as negative interference. We\ncharacterize the threat of negative interference and how it fundamentally\nchanges the nature of the Bayesian active learner's task. We show that the\nextent of negative interference can be extremely large, and that accurate\nestimation of the nuisance parameters is critical to reducing it. The Bayesian\nactive learner is confronted with a dilemma: whether to spend a finite\nacquisition budget in pursuit of estimation of the target or of the nuisance\nparameters. Our setting encompasses Bayesian transfer learning as a special\ncase, and our results shed light on the phenomenon of negative transfer between\nlearning environments.\n","authors":["Sabina J. Sloman","Ayush Bharti","Julien Martinelli","Samuel Kaski"],"pdf_url":"https://arxiv.org/pdf/2310.14968v3.pdf","comment":"Accepted for UAI 2024"},{"id":"http://arxiv.org/abs/2406.06340v1","updated":"2024-06-10T15:01:03Z","published":"2024-06-10T15:01:03Z","title":"Optimisation of federated learning settings under statistical\n  heterogeneity variations","summary":"  Federated Learning (FL) enables local devices to collaboratively learn a\nshared predictive model by only periodically sharing model parameters with a\ncentral aggregator. However, FL can be disadvantaged by statistical\nheterogeneity produced by the diversity in each local devices data\ndistribution, which creates different levels of Independent and Identically\nDistributed (IID) data. Furthermore, this can be more complex when optimising\ndifferent combinations of FL parameters and choosing optimal aggregation. In\nthis paper, we present an empirical analysis of different FL training\nparameters and aggregators over various levels of statistical heterogeneity on\nthree datasets. We propose a systematic data partition strategy to simulate\ndifferent levels of statistical heterogeneity and a metric to measure the level\nof IID. Additionally, we empirically identify the best FL model and key\nparameters for datasets of different characteristics. On the basis of these, we\npresent recommended guidelines for FL parameters and aggregators to optimise\nmodel performance under different levels of IID and with different datasets\n","authors":["Basem Suleiman","Muhammad Johan Alibasa","Rizka Widyarini Purwanto","Lewis Jeffries","Ali Anaissi","Jacky Song"],"pdf_url":"https://arxiv.org/pdf/2406.06340v1.pdf","comment":"27 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.02630v4","updated":"2024-06-10T14:57:11Z","published":"2024-03-05T03:40:39Z","title":"FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal\n  Decoupling","summary":"  In recent years, Cross-Domain Recommendation (CDR) has drawn significant\nattention, which utilizes user data from multiple domains to enhance the\nrecommendation performance. However, current CDR methods require sharing user\ndata across domains, thereby violating the General Data Protection Regulation\n(GDPR). Consequently, numerous approaches have been proposed for Federated\nCross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity\nacross different domains inevitably influences the overall performance of\nfederated learning. In this study, we propose FedHCDR, a novel Federated\nCross-Domain Recommendation framework with Hypergraph signal decoupling.\nSpecifically, to address the data heterogeneity across domains, we introduce an\napproach called hypergraph signal decoupling (HSD) to decouple the user\nfeatures into domain-exclusive and domain-shared features. The approach employs\nhigh-pass and low-pass hypergraph filters to decouple domain-exclusive and\ndomain-shared user representations, which are trained by the local-global\nbi-directional transfer algorithm. In addition, a hypergraph contrastive\nlearning (HCL) module is devised to enhance the learning of domain-shared user\nrelationship information by perturbing the user hypergraph. Extensive\nexperiments conducted on three real-world scenarios demonstrate that FedHCDR\noutperforms existing baselines significantly.\n","authors":["Hongyu Zhang","Dongyi Zheng","Lin Zhong","Xu Yang","Jiyuan Feng","Yunqing Feng","Qing Liao"],"pdf_url":"https://arxiv.org/pdf/2403.02630v4.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.06966v2","updated":"2024-06-10T14:56:21Z","published":"2024-03-11T17:49:18Z","title":"Acquiring Diverse Skills using Curriculum Reinforcement Learning with\n  Mixture of Experts","summary":"  Reinforcement learning (RL) is a powerful approach for acquiring a\ngood-performing policy. However, learning diverse skills is challenging in RL\ndue to the commonly used Gaussian policy parameterization. We propose\n\\textbf{Di}verse \\textbf{Skil}l \\textbf{L}earning (Di-SkilL\\footnote{Videos and\ncode are available on the project webpage:\n\\url{https://alrhub.github.io/di-skill-website/}}), an RL method for learning\ndiverse skills using Mixture of Experts, where each expert formalizes a skill\nas a contextual motion primitive. Di-SkilL optimizes each expert and its\nassociate context distribution to a maximum entropy objective that incentivizes\nlearning diverse skills in similar contexts. The per-expert context\ndistribution enables automatic curricula learning, allowing each expert to\nfocus on its best-performing sub-region of the context space. To overcome hard\ndiscontinuities and multi-modalities without any prior knowledge of the\nenvironment's unknown context probability space, we leverage energy-based\nmodels to represent the per-expert context distributions and demonstrate how we\ncan efficiently train them using the standard policy gradient objective. We\nshow on challenging robot simulation tasks that Di-SkilL can learn diverse and\nperformant skills.\n","authors":["Onur Celik","Aleksandar Taranovic","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2403.06966v2.pdf","comment":"International conference on machine learning (ICML)"},{"id":"http://arxiv.org/abs/2209.03450v3","updated":"2024-06-10T14:54:23Z","published":"2022-09-07T20:11:17Z","title":"Seeking Interpretability and Explainability in Binary Activated Neural\n  Networks","summary":"  We study the use of binary activated neural networks as interpretable and\nexplainable predictors in the context of regression tasks on tabular data; more\nspecifically, we provide guarantees on their expressiveness, present an\napproach based on the efficient computation of SHAP values for quantifying the\nrelative importance of the features, hidden neurons and even weights. As the\nmodel's simplicity is instrumental in achieving interpretability, we propose a\ngreedy algorithm for building compact binary activated networks. This approach\ndoesn't need to fix an architecture for the network in advance: it is built one\nlayer at a time, one neuron at a time, leading to predictors that aren't\nneedlessly complex for a given task.\n","authors":["Benjamin Leblanc","Pascal Germain"],"pdf_url":"https://arxiv.org/pdf/2209.03450v3.pdf","comment":"2nd World Conference on eXplainable Artificial Intelligence\n  (xAI-2024)"},{"id":"http://arxiv.org/abs/2403.20287v2","updated":"2024-06-10T14:47:46Z","published":"2024-03-29T16:58:13Z","title":"Benchmarking Counterfactual Image Generation","summary":"  Generative AI has revolutionised visual content editing, empowering users to\neffortlessly modify images and videos. However, not all edits are equal. To\nperform realistic edits in domains such as natural image or medical imaging,\nmodifications must respect causal relationships inherent to the data generation\nprocess. Such image editing falls into the counterfactual image generation\nregime. Evaluating counterfactual image generation is substantially complex:\nnot only it lacks observable ground truths, but also requires adherence to\ncausal constraints. Although several counterfactual image generation methods\nand evaluation metrics exist, a comprehensive comparison within a unified\nsetting is lacking. We present a comparison framework to thoroughly benchmark\ncounterfactual image generation methods. We integrate all models that have been\nused for the task at hand and expand them to novel datasets and causal graphs,\ndemonstrating the superiority of Hierarchical VAEs across most datasets and\nmetrics. Our framework is implemented in a user-friendly Python package that\ncan be extended to incorporate additional SCMs, causal methods, generative\nmodels, and datasets for the community to build on.\n","authors":["Thomas Melistas","Nikos Spyrou","Nefeli Gkouti","Pedro Sanchez","Athanasios Vlontzos","Yannis Panagakis","Giorgos Papanastasiou","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2403.20287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17234v2","updated":"2024-06-10T14:43:34Z","published":"2023-09-29T13:33:06Z","title":"Cooperation, Competition, and Maliciousness: LLM-Stakeholders\n  Interactive Negotiation","summary":"  There is an growing interest in using Large Language Models (LLMs) in\nmulti-agent systems to tackle interactive real-world tasks that require\neffective collaboration and assessing complex situations. Yet, we still have a\nlimited understanding of LLMs' communication and decision-making abilities in\nmulti-agent setups. The fundamental task of negotiation spans many key features\nof communication, such as cooperation, competition, and manipulation\npotentials. Thus, we propose using scorable negotiation to evaluate LLMs. We\ncreate a testbed of complex multi-agent, multi-issue, and semantically rich\nnegotiation games. To reach an agreement, agents must have strong arithmetic,\ninference, exploration, and planning capabilities while integrating them in a\ndynamic and multi-turn setup. We propose multiple metrics to rigorously\nquantify agents' performance and alignment with the assigned role. We provide\nprocedures to create new games and increase games' difficulty to have an\nevolving benchmark. Importantly, we evaluate critical safety aspects such as\nthe interaction dynamics between agents influenced by greedy and adversarial\nplayers. Our benchmark is highly challenging; GPT-3.5 and small models mostly\nfail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform.\n","authors":["Sahar Abdelnabi","Amr Gomaa","Sarath Sivaprasad","Lea Schönherr","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2309.17234v2.pdf","comment":"Updated version with major additions (new experiments, evaluation,\n  and attacks)"},{"id":"http://arxiv.org/abs/2310.05264v4","updated":"2024-06-10T14:37:45Z","published":"2023-10-08T19:02:46Z","title":"The Emergence of Reproducibility and Generalizability in Diffusion\n  Models","summary":"  In this work, we investigate an intriguing and prevalent phenomenon of\ndiffusion models which we term as \"consistent model reproducibility\": given the\nsame starting noise input and a deterministic sampler, different diffusion\nmodels often yield remarkably similar outputs. We confirm this phenomenon\nthrough comprehensive experiments, implying that different diffusion models\nconsistently reach the same data distribution and scoring function regardless\nof diffusion model frameworks, model architectures, or training procedures.\nMore strikingly, our further investigation implies that diffusion models are\nlearning distinct distributions affected by the training data size. This is\nsupported by the fact that the model reproducibility manifests in two distinct\ntraining regimes: (i) \"memorization regime\", where the diffusion model overfits\nto the training data distribution, and (ii) \"generalization regime\", where the\nmodel learns the underlying data distribution. Our study also finds that this\nvaluable property generalizes to many variants of diffusion models, including\nthose for conditional use, solving inverse problems, and model fine-tuning.\nFinally, our work raises numerous intriguing theoretical questions for future\ninvestigation and highlights practical implications regarding training\nefficiency, model privacy, and the controlled generation of diffusion models.\n","authors":["Huijie Zhang","Jinfan Zhou","Yifu Lu","Minzhe Guo","Peng Wang","Liyue Shen","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2310.05264v4.pdf","comment":"NeurIPS Diffusion Model Workshop 2023 (best paper award), the\n  Forty-first International Conference on Machine Learning (ICML 2024)"},{"id":"http://arxiv.org/abs/2406.06316v1","updated":"2024-06-10T14:33:02Z","published":"2024-06-10T14:33:02Z","title":"Tx-LLM: A Large Language Model for Therapeutics","summary":"  Developing therapeutics is a lengthy and expensive process that requires the\nsatisfaction of many different criteria, and AI models capable of expediting\nthe process would be invaluable. However, the majority of current AI approaches\naddress only a narrowly defined set of tasks, often circumscribed within a\nparticular domain. To bridge this gap, we introduce Tx-LLM, a generalist large\nlanguage model (LLM) fine-tuned from PaLM-2 which encodes knowledge about\ndiverse therapeutic modalities. Tx-LLM is trained using a collection of 709\ndatasets that target 66 tasks spanning various stages of the drug discovery\npipeline. Using a single set of weights, Tx-LLM simultaneously processes a wide\nvariety of chemical or biological entities(small molecules, proteins, nucleic\nacids, cell lines, diseases) interleaved with free-text, allowing it to predict\na broad range of associated properties, achieving competitive with\nstate-of-the-art (SOTA) performance on 43 out of 66 tasks and exceeding SOTA on\n22. Among these, Tx-LLM is particularly powerful and exceeds best-in-class\nperformance on average for tasks combining molecular SMILES representations\nwith text such as cell line names or disease names, likely due to context\nlearned during pretraining. We observe evidence of positive transfer between\ntasks with diverse drug types (e.g.,tasks involving small molecules and tasks\ninvolving proteins), and we study the impact of model size, domain finetuning,\nand prompting strategies on performance. We believe Tx-LLM represents an\nimportant step towards LLMs encoding biochemical knowledge and could have a\nfuture role as an end-to-end tool across the drug discovery development\npipeline.\n","authors":["Juan Manuel Zambrano Chaves","Eric Wang","Tao Tu","Eeshit Dhaval Vaishnav","Byron Lee","S. Sara Mahdavi","Christopher Semturs","David Fleet","Vivek Natarajan","Shekoofeh Azizi"],"pdf_url":"https://arxiv.org/pdf/2406.06316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06313v1","updated":"2024-06-10T14:31:38Z","published":"2024-06-10T14:31:38Z","title":"ProAct: Progressive Training for Hybrid Clipped Activation Function to\n  Enhance Resilience of DNNs","summary":"  Deep Neural Networks (DNNs) are extensively employed in safety-critical\napplications where ensuring hardware reliability is a primary concern. To\nenhance the reliability of DNNs against hardware faults, activation restriction\ntechniques significantly mitigate the fault effects at the DNN structure level,\nirrespective of accelerator architectures. State-of-the-art methods offer\neither neuron-wise or layer-wise clipping activation functions. They attempt to\ndetermine optimal clipping thresholds using heuristic and learning-based\napproaches. Layer-wise clipped activation functions cannot preserve DNNs\nresilience at high bit error rates. On the other hand, neuron-wise clipping\nactivation functions introduce considerable memory overhead due to the addition\nof parameters, which increases their vulnerability to faults. Moreover, the\nheuristic-based optimization approach demands numerous fault injections during\nthe search process, resulting in time-consuming threshold identification. On\nthe other hand, learning-based techniques that train thresholds for entire\nlayers concurrently often yield sub-optimal results. In this work, first, we\ndemonstrate that it is not essential to incorporate neuron-wise activation\nfunctions throughout all layers in DNNs. Then, we propose a hybrid clipped\nactivation function that integrates neuron-wise and layer-wise methods that\napply neuron-wise clipping only in the last layer of DNNs. Additionally, to\nattain optimal thresholds in the clipping activation function, we introduce\nProAct, a progressive training methodology. This approach iteratively trains\nthe thresholds on a layer-by-layer basis, aiming to obtain optimal threshold\nvalues in each layer separately.\n","authors":["Seyedhamidreza Mousavi","Mohammad Hasan Ahmadilivani","Jaan Raik","Maksim Jenihhin","Masoud Daneshtalab"],"pdf_url":"https://arxiv.org/pdf/2406.06313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14623v2","updated":"2024-06-10T14:30:19Z","published":"2024-05-23T14:31:53Z","title":"U-TELL: Unsupervised Task Expert Lifelong Learning","summary":"  Continual learning (CL) models are designed to learn new tasks arriving\nsequentially without re-training the network. However, real-world ML\napplications have very limited label information and these models suffer from\ncatastrophic forgetting. To address these issues, we propose an unsupervised CL\nmodel with task experts called Unsupervised Task Expert Lifelong Learning\n(U-TELL) to continually learn the data arriving in a sequence addressing\ncatastrophic forgetting. During training of U-TELL, we introduce a new expert\non arrival of a new task. Our proposed architecture has task experts, a\nstructured data generator and a task assigner. Each task expert is composed of\n3 blocks; i) a variational autoencoder to capture the task distribution and\nperform data abstraction, ii) a k-means clustering module, and iii) a structure\nextractor to preserve latent task data signature. During testing, task assigner\nselects a suitable expert to perform clustering. U-TELL does not store or\nreplay task samples, instead, we use generated structured samples to train the\ntask assigner. We compared U-TELL with five SOTA unsupervised CL methods.\nU-TELL outperformed all baselines on seven benchmarks and one industry dataset\nfor various CL scenarios with a training time over 6 times faster than the best\nperforming baseline.\n","authors":["Indu Solomon","Aye Phyu Phyu Aung","Uttam Kumar","Senthilnath Jayavelu"],"pdf_url":"https://arxiv.org/pdf/2405.14623v2.pdf","comment":"Accepted by International Conference on Image Processing 2024\n  (ICIP2024)"},{"id":"http://arxiv.org/abs/2206.06929v2","updated":"2024-06-10T14:28:26Z","published":"2022-06-14T15:49:10Z","title":"Scaling ResNets in the Large-depth Regime","summary":"  Deep ResNets are recognized for achieving state-of-the-art results in complex\nmachine learning tasks. However, the remarkable performance of these\narchitectures relies on a training procedure that needs to be carefully crafted\nto avoid vanishing or exploding gradients, particularly as the depth $L$\nincreases. No consensus has been reached on how to mitigate this issue,\nalthough a widely discussed strategy consists in scaling the output of each\nlayer by a factor $\\alpha_L$. We show in a probabilistic setting that with\nstandard i.i.d.~initializations, the only non-trivial dynamics is for $\\alpha_L\n= \\frac{1}{\\sqrt{L}}$; other choices lead either to explosion or to identity\nmapping. This scaling factor corresponds in the continuous-time limit to a\nneural stochastic differential equation, contrarily to a widespread\ninterpretation that deep ResNets are discretizations of neural ordinary\ndifferential equations. By contrast, in the latter regime, stability is\nobtained with specific correlated initializations and $\\alpha_L = \\frac{1}{L}$.\nOur analysis suggests a strong interplay between scaling and regularity of the\nweights as a function of the layer index. Finally, in a series of experiments,\nwe exhibit a continuous range of regimes driven by these two parameters, which\njointly impact performance before and after training.\n","authors":["Pierre Marion","Adeline Fermanian","Gérard Biau","Jean-Philippe Vert"],"pdf_url":"https://arxiv.org/pdf/2206.06929v2.pdf","comment":"44 pages, 9 figures. Updated with clarifications and additional\n  references"},{"id":"http://arxiv.org/abs/2406.06309v1","updated":"2024-06-10T14:25:11Z","published":"2024-06-10T14:25:11Z","title":"Is Value Functions Estimation with Classification Plug-and-play for\n  Offline Reinforcement Learning?","summary":"  In deep Reinforcement Learning (RL), value functions are typically\napproximated using deep neural networks and trained via mean squared error\nregression objectives to fit the true value functions. Recent research has\nproposed an alternative approach, utilizing the cross-entropy classification\nobjective, which has demonstrated improved performance and scalability of RL\nalgorithms. However, existing study have not extensively benchmarked the\neffects of this replacement across various domains, as the primary objective\nwas to demonstrate the efficacy of the concept across a broad spectrum of\ntasks, without delving into in-depth analysis. Our work seeks to empirically\ninvestigate the impact of such a replacement in an offline RL setup and analyze\nthe effects of different aspects on performance. Through large-scale\nexperiments conducted across a diverse range of tasks using different\nalgorithms, we aim to gain deeper insights into the implications of this\napproach. Our results reveal that incorporating this change can lead to\nsuperior performance over state-of-the-art solutions for some algorithms in\ncertain tasks, while maintaining comparable performance levels in other tasks,\nhowever for other algorithms this modification might lead to the dramatic\nperformance drop. This findings are crucial for further application of\nclassification approach in research and practical tasks.\n","authors":["Denis Tarasov","Kirill Brilliantov","Dmitrii Kharlapenko"],"pdf_url":"https://arxiv.org/pdf/2406.06309v1.pdf","comment":"https://github.com/DT6A/ClORL"},{"id":"http://arxiv.org/abs/2406.06307v1","updated":"2024-06-10T14:23:25Z","published":"2024-06-10T14:23:25Z","title":"Building Continuous Quantum-Classical Bayesian Neural Networks for a\n  Classical Clinical Dataset","summary":"  In this work, we are introducing a Quantum-Classical Bayesian Neural Network\n(QCBNN) that is capable to perform uncertainty-aware classification of\nclassical medical dataset. This model is a symbiosis of a classical\nConvolutional NN that performs ultra-sound image processing and a quantum\ncircuit that generates its stochastic weights, within a Bayesian learning\nframework. To test the utility of this idea for the possible future deployment\nin the medical sector we track multiple behavioral metrics that capture both\npredictive performance as well as model's uncertainty. It is our ambition to\ncreate a hybrid model that is capable to classify samples in a more uncertainty\naware fashion, which will advance the trustworthiness of these models and thus\nbring us step closer to utilizing them in the industry. We test multiple setups\nfor quantum circuit for this task, and our best architectures display bigger\nuncertainty gap between correctly and incorrectly identified samples than its\nclassical benchmark at an expense of a slight drop in predictive performance.\nThe innovation of this paper is two-fold: (1) combining of different approaches\nthat allow the stochastic weights from the quantum circuit to be continues thus\nallowing the model to classify application-driven dataset; (2) studying\narchitectural features of quantum circuit that make-or-break these models,\nwhich pave the way into further investigation of more informed architectural\ndesigns.\n","authors":["Alona Sakhnenko","Julian Sikora","Jeanette Miriam Lorenz"],"pdf_url":"https://arxiv.org/pdf/2406.06307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07087v3","updated":"2024-06-10T14:22:45Z","published":"2024-02-11T02:34:42Z","title":"Self-Correcting Self-Consuming Loops for Generative Model Training","summary":"  As synthetic data becomes higher quality and proliferates on the internet,\nmachine learning models are increasingly trained on a mix of human- and\nmachine-generated data. Despite the successful stories of using synthetic data\nfor representation learning, using synthetic data for generative model training\ncreates \"self-consuming loops\" which may lead to training instability or even\ncollapse, unless certain conditions are met. Our paper aims to stabilize\nself-consuming generative model training. Our theoretical results demonstrate\nthat by introducing an idealized correction function, which maps a data point\nto be more likely under the true data distribution, self-consuming loops can be\nmade exponentially more stable. We then propose self-correction functions,\nwhich rely on expert knowledge (e.g. the laws of physics programmed in a\nsimulator), and aim to approximate the idealized corrector automatically and at\nscale. We empirically validate the effectiveness of self-correcting\nself-consuming loops on the challenging human motion synthesis task, and\nobserve that it successfully avoids model collapse, even when the ratio of\nsynthetic data to real data is as high as 100%.\n","authors":["Nate Gillman","Michael Freeman","Daksh Aggarwal","Chia-Hong Hsu","Calvin Luo","Yonglong Tian","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2402.07087v3.pdf","comment":"Camera ready version (ICML 2024). Code at\n  https://nategillman.com/sc-sc.html"},{"id":"http://arxiv.org/abs/2406.06290v1","updated":"2024-06-10T14:12:33Z","published":"2024-06-10T14:12:33Z","title":"Geometric sparsification in recurrent neural networks","summary":"  A common technique for ameliorating the computational costs of running large\nneural models is sparsification, or the removal of neural connections during\ntraining. Sparse models are capable of maintaining the high accuracy of state\nof the art models, while functioning at the cost of more parsimonious models.\nThe structures which underlie sparse architectures are, however, poorly\nunderstood and not consistent between differently trained models and\nsparsification schemes. In this paper, we propose a new technique for\nsparsification of recurrent neural nets (RNNs), called moduli regularization,\nin combination with magnitude pruning. Moduli regularization leverages the\ndynamical system induced by the recurrent structure to induce a geometric\nrelationship between neurons in the hidden state of the RNN. By making our\nregularizing term explicitly geometric, we provide the first, to our knowledge,\na priori description of the desired sparse architecture of our neural net. We\nverify the effectiveness of our scheme for navigation and natural language\nprocessing RNNs. Navigation is a structurally geometric task, for which there\nare known moduli spaces, and we show that regularization can be used to reach\n90% sparsity while maintaining model performance only when coefficients are\nchosen in accordance with a suitable moduli space. Natural language processing,\nhowever, has no known moduli space in which computations are performed.\nNevertheless, we show that moduli regularization induces more stable recurrent\nneural nets with a variety of moduli regularizers, and achieves high fidelity\nmodels at 98% sparsity.\n","authors":["Wyatt Mackey","Ioannis Schizas","Jared Deighton","David L. Boothe, Jr.","Vasileios Maroulas"],"pdf_url":"https://arxiv.org/pdf/2406.06290v1.pdf","comment":"25 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.06287v1","updated":"2024-06-10T14:11:15Z","published":"2024-06-10T14:11:15Z","title":"VS-PINN: A Fast and efficient training of physics-informed neural\n  networks using variable-scaling methods for solving PDEs with stiff behavior","summary":"  Physics-informed neural networks (PINNs) have recently emerged as a promising\nway to compute the solutions of partial differential equations (PDEs) using\ndeep neural networks. However, despite their significant success in various\nfields, it remains unclear in many aspects how to effectively train PINNs if\nthe solutions of PDEs exhibit stiff behaviors or high frequencies. In this\npaper, we propose a new method for training PINNs using variable-scaling\ntechniques. This method is simple and it can be applied to a wide range of\nproblems including PDEs with rapidly-varying solutions. Throughout various\nnumerical experiments, we will demonstrate the effectiveness of the proposed\nmethod for these problems and confirm that it can significantly improve the\ntraining efficiency and performance of PINNs. Furthermore, based on the\nanalysis of the neural tangent kernel (NTK), we will provide theoretical\nevidence for this phenomenon and show that our methods can indeed improve the\nperformance of PINNs.\n","authors":["Seungchan Ko","Sang Hyeon Park"],"pdf_url":"https://arxiv.org/pdf/2406.06287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15374v2","updated":"2024-06-10T14:10:38Z","published":"2024-02-23T15:19:37Z","title":"Outlier detection by ensembling uncertainty with negative objectness","summary":"  Outlier detection is an essential capability in safety-critical applications\nof supervised visual recognition. Most of the existing methods deliver best\nresults by encouraging standard closed-set models to produce low-confidence\npredictions in negative training data. However, that approach conflates\nprediction uncertainty with recognition of the negative class. We therefore\nreconsider direct prediction of K+1 logits that correspond to K groundtruth\nclasses and one outlier class. This setup allows us to formulate a novel\nanomaly score as an ensemble of in-distribution uncertainty and the posterior\nof the outlier class which we term negative objectness. Now outliers can be\nindependently detected due to i) high prediction uncertainty or ii) similarity\nwith negative data. We embed our method into a dense prediction architecture\nwith mask-level recognition over K+2 classes. The training procedure encourages\nthe novel K+2-th class to learn negative objectness at pasted negative\ninstances. Our models outperform the current state-of-the art on standard\nbenchmarks for image-wide and pixel-level outlier detection with and without\ntraining on real negative data.\n","authors":["Anja Delić","Matej Grcić","Siniša Šegvić"],"pdf_url":"https://arxiv.org/pdf/2402.15374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03849v4","updated":"2024-06-10T14:06:05Z","published":"2024-03-06T16:49:33Z","title":"MedMamba: Vision Mamba for Medical Image Classification","summary":"  Since the era of deep learning, convolutional neural networks (CNNs) and\nvision transformers (ViTs) have been extensively studied and widely used in\nmedical image classification tasks. Unfortunately, CNN's limitations in\nmodeling long-range dependencies result in poor classification performances. In\ncontrast, ViTs are hampered by the quadratic computational complexity of their\nself-attention mechanism, making them difficult to deploy in real-world\nsettings with limited computational resources. Recent studies have shown that\nstate space models (SSMs) represented by Mamba can effectively model long-range\ndependencies while maintaining linear computational complexity. Inspired by it,\nwe proposed MedMamba, the first vision Mamba for generalized medical image\nclassification. Concretely, we introduced a novel hybrid basic block named\nSS-Conv-SSM, which integrates the convolutional layers for extracting local\nfeatures with the abilities of SSM to capture long-range dependencies, aiming\nto model medical images from different image modalities efficiently. By\nemploying the grouped convolution strategy and channel-shuffle operation,\nMedMamba successfully provides fewer model parameters and a lower computational\nburden for efficient applications. To demonstrate the potential of MedMamba, we\nconducted extensive experiments using 16 datasets containing ten imaging\nmodalities and 411,007 images. Experimental results show that the proposed\nMedMamba demonstrates competitive performance in classifying various medical\nimages compared with the state-of-the-art methods. Our work is aims to\nestablish a new baseline for medical image classification and provide valuable\ninsights for developing more powerful SSM-based artificial intelligence\nalgorithms and application systems in the medical field. The source codes and\nall pre-trained weights of MedMamba are available at\nhttps://github.com/YubiaoYue/MedMamba.\n","authors":["Yubiao Yue","Zhenzhang Li"],"pdf_url":"https://arxiv.org/pdf/2403.03849v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06282v1","updated":"2024-06-10T14:01:21Z","published":"2024-06-10T14:01:21Z","title":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone","summary":"  This paper introduces PowerInfer-2, a framework designed for high-speed\ninference of Large Language Models (LLMs) on smartphones, particularly\neffective for models whose sizes exceed the device's memory capacity. The key\ninsight of PowerInfer-2 is to utilize the heterogeneous computation, memory,\nand I/O resources in smartphones by decomposing traditional matrix computations\ninto fine-grained neuron cluster computations. Specifically, PowerInfer-2\nfeatures a polymorphic neuron engine that adapts computational strategies for\nvarious stages of LLM inference. Additionally, it introduces segmented neuron\ncaching and fine-grained neuron-cluster-level pipelining, which effectively\nminimize and conceal the overhead caused by I/O operations. The implementation\nand evaluation of PowerInfer-2 demonstrate its capability to support a wide\narray of LLM models on two smartphones, achieving up to a 29.2x speed increase\ncompared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first\nsystem to serve the TurboSparse-Mixtral-47B model with a generation rate of\n11.68 tokens per second on a smartphone. For models that fit entirely within\nthe memory, PowerInfer-2 can achieve approximately a 40% reduction in memory\nusage while maintaining inference speeds comparable to llama.cpp and MLC-LLM.\nFor more details, including a demonstration video, please visit the project\nsite at www.powerinfer.ai/v2.\n","authors":["Zhenliang Xue","Yixin Song","Zeyu Mi","Le Chen","Yubin Xia","Haibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.06282v1.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.07245v3","updated":"2024-06-10T13:55:22Z","published":"2024-03-12T02:05:06Z","title":"Dataset Condensation for Time Series Classification via Dual Domain\n  Matching","summary":"  Time series data has been demonstrated to be crucial in various research\nfields. The management of large quantities of time series data presents\nchallenges in terms of deep learning tasks, particularly for training a deep\nneural network. Recently, a technique named \\textit{Dataset Condensation} has\nemerged as a solution to this problem. This technique generates a smaller\nsynthetic dataset that has comparable performance to the full real dataset in\ndownstream tasks such as classification. However, previous methods are\nprimarily designed for image and graph datasets, and directly adapting them to\nthe time series dataset leads to suboptimal performance due to their inability\nto effectively leverage the rich information inherent in time series data,\nparticularly in the frequency domain. In this paper, we propose a novel\nframework named Dataset \\textit{\\textbf{Cond}}ensation for\n\\textit{\\textbf{T}}ime \\textit{\\textbf{S}}eries\n\\textit{\\textbf{C}}lassification via Dual Domain Matching (\\textbf{CondTSC})\nwhich focuses on the time series classification dataset condensation task.\nDifferent from previous methods, our proposed framework aims to generate a\ncondensed dataset that matches the surrogate objectives in both the time and\nfrequency domains. Specifically, CondTSC incorporates multi-view data\naugmentation, dual domain training, and dual surrogate objectives to enhance\nthe dataset condensation process in the time and frequency domains. Through\nextensive experiments, we demonstrate the effectiveness of our proposed\nframework, which outperforms other baselines and learns a condensed synthetic\ndataset that exhibits desirable characteristics such as conforming to the\ndistribution of the original data.\n","authors":["Zhanyu Liu","Ke Hao","Guanjie Zheng","Yanwei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.07245v3.pdf","comment":"Accepted by KDD 2024 research track"},{"id":"http://arxiv.org/abs/2406.04519v2","updated":"2024-06-10T13:52:32Z","published":"2024-06-06T21:26:30Z","title":"Multifidelity digital twin for real-time monitoring of structural\n  dynamics in aquaculture net cages","summary":"  As the global population grows and climate change intensifies, sustainable\nfood production is critical. Marine aquaculture offers a viable solution,\nproviding a sustainable protein source. However, the industry's expansion\nrequires novel technologies for remote management and autonomous operations.\nDigital twin technology can advance the aquaculture industry, but its adoption\nhas been limited. Fish net cages, which are flexible floating structures, are\ncritical yet vulnerable components of aquaculture farms. Exposed to harsh and\ndynamic marine environments, the cages experience significant loads and risk\ndamage, leading to fish escapes, environmental impacts, and financial losses.\nWe propose a multifidelity surrogate modeling framework for integration into a\ndigital twin for real-time monitoring of aquaculture net cage structural\ndynamics under stochastic marine conditions. Central to this framework is the\nnonlinear autoregressive Gaussian process method, which learns complex,\nnonlinear cross-correlations between models of varying fidelity. It combines\nlow-fidelity simulation data with a small set of high-fidelity field sensor\nmeasurements, which offer the real dynamics but are costly and spatially\nsparse. Validated at the SINTEF ACE fish farm in Norway, our digital twin\nreceives online metocean data and accurately predicts net cage displacements\nand mooring line loads, aligning closely with field measurements. The proposed\nframework is beneficial where application-specific data are scarce, offering\nrapid predictions and real-time system representation. The developed digital\ntwin prevents potential damages by assessing structural integrity and\nfacilitates remote operations with unmanned underwater vehicles. Our work also\ncompares GP and GCNs for predicting net cage deformation, highlighting the\nlatter's effectiveness in complex structural applications.\n","authors":["Eirini Katsidoniotaki","Biao Su","Eleni Kelasidi","Themistoklis P. Sapsis"],"pdf_url":"https://arxiv.org/pdf/2406.04519v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08189v4","updated":"2024-06-10T13:46:22Z","published":"2024-01-16T08:04:50Z","title":"PRewrite: Prompt Rewriting with Reinforcement Learning","summary":"  Prompt engineering is critical for the development of LLM-based applications.\nHowever, it is usually done manually in a \"trial and error\" fashion that can be\ntime consuming, ineffective, and sub-optimal. Even for the prompts which\nseemingly work well, there is always a lingering question: can the prompts be\nmade better with further modifications?\n  To address these problems, we investigate automated prompt engineering in\nthis paper. Specifically, we propose PRewrite, an automated method to rewrite\nan under-optimized prompt to a more effective prompt. We instantiate the prompt\nrewriter using a LLM. The rewriter LLM is trained using reinforcement learning\nto optimize the performance on a given downstream task. We conduct experiments\non diverse benchmark datasets, which demonstrates the effectiveness of\nPRewrite.\n","authors":["Weize Kong","Spurthi Amba Hombaiah","Mingyang Zhang","Qiaozhu Mei","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2401.08189v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12780v2","updated":"2024-06-10T13:43:21Z","published":"2024-02-20T07:40:11Z","title":"Byzantine-Robust Federated Learning: Impact of Client Subsampling and\n  Local Updates","summary":"  The possibility of adversarial (a.k.a., {\\em Byzantine}) clients makes\nfederated learning (FL) prone to arbitrary manipulation. The natural approach\nto robustify FL against adversarial clients is to replace the simple averaging\noperation at the server in the standard $\\mathsf{FedAvg}$ algorithm by a\n\\emph{robust averaging rule}. While a significant amount of work has been\ndevoted to studying the convergence of federated {\\em robust averaging} (which\nwe denote by $\\mathsf{FedRo}$), prior work has largely ignored the impact of\n{\\em client subsampling} and {\\em local steps}, two fundamental FL\ncharacteristics. While client subsampling increases the effective fraction of\nByzantine clients, local steps increase the drift between the local updates\ncomputed by honest (i.e., non-Byzantine) clients. Consequently, a careless\ndeployment of $\\mathsf{FedRo}$ could yield poor performance. We validate this\nobservation by presenting an in-depth analysis of $\\mathsf{FedRo}$ tightly\nanalyzing the impact of client subsampling and local steps. Specifically, we\npresent a sufficient condition on client subsampling for nearly-optimal\nconvergence of $\\mathsf{FedRo}$ (for smooth non-convex loss). Also, we show\nthat the rate of improvement in learning accuracy {\\em diminishes} with respect\nto the number of clients subsampled, as soon as the sample size exceeds a\nthreshold value. Interestingly, we also observe that under a careful choice of\nstep-sizes, the learning error due to Byzantine clients decreases with the\nnumber of local steps. We validate our theory by experiments on the FEMNIST and\nCIFAR-$10$ image classification tasks.\n","authors":["Youssef Allouah","Sadegh Farhadkhani","Rachid GuerraouI","Nirupam Gupta","Rafael Pinot","Geovani Rizk","Sasha Voitovych"],"pdf_url":"https://arxiv.org/pdf/2402.12780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06248v1","updated":"2024-06-10T13:25:43Z","published":"2024-06-10T13:25:43Z","title":"Compute Better Spent: Replacing Dense Layers with Structured Matrices","summary":"  Dense linear layers are the dominant computational bottleneck in foundation\nmodels. Identifying more efficient alternatives to dense matrices has enormous\npotential for building more compute-efficient models, as exemplified by the\nsuccess of convolutional networks in the image domain. In this work, we\nsystematically explore structured matrices as replacements for dense matrices.\nWe show that different structures often require drastically different\ninitialization scales and learning rates, which are crucial to performance,\nespecially as models scale. Using insights from the Maximal Update\nParameterization, we determine the optimal scaling for initialization and\nlearning rates of these unconventional layers. Finally, we measure the scaling\nlaws of different structures to compare how quickly their performance improves\nwith compute. We propose a novel matrix family containing Monarch matrices, the\nBlock Tensor-Train (BTT), which we show performs better than dense matrices for\nthe same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT\nachieves exponentially lower training loss than dense when training MLPs and\nViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less\ncompute and is more efficient than dense for training small GPT-2 language\nmodels.\n","authors":["Shikai Qiu","Andres Potapczynski","Marc Finzi","Micah Goldblum","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2406.06248v1.pdf","comment":"ICML 24. Code available at\n  https://github.com/shikaiqiu/compute-better-spent"},{"id":"http://arxiv.org/abs/2406.06246v1","updated":"2024-06-10T13:23:00Z","published":"2024-06-10T13:23:00Z","title":"Data-Efficient Learning with Neural Programs","summary":"  Many computational tasks can be naturally expressed as a composition of a DNN\nfollowed by a program written in a traditional programming language or an API\ncall to an LLM. We call such composites \"neural programs\" and focus on the\nproblem of learning the DNN parameters when the training data consist of\nend-to-end input-output labels for the composite. When the program is written\nin a differentiable logic programming language, techniques from neurosymbolic\nlearning are applicable, but in general, the learning for neural programs\nrequires estimating the gradients of black-box components. We present an\nalgorithm for learning neural programs, called ISED, that only relies on\ninput-output samples of black-box components. For evaluation, we introduce new\nbenchmarks that involve calls to modern LLMs such as GPT-4 and also consider\nbenchmarks from the neurosymolic learning literature. Our evaluation shows that\nfor the latter benchmarks, ISED has comparable performance to state-of-the-art\nneurosymbolic frameworks. For the former, we use adaptations of prior work on\ngradient approximations of black-box components as a baseline, and show that\nISED achieves comparable accuracy but in a more data- and sample-efficient\nmanner.\n","authors":["Alaia Solko-Breslin","Seewon Choi","Ziyang Li","Neelay Velingker","Rajeev Alur","Mayur Naik","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2406.06246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02632v3","updated":"2024-06-10T13:22:42Z","published":"2023-09-06T00:44:29Z","title":"Deep Reinforcement Learning from Hierarchical Preference Design","summary":"  Reward design is a fundamental, yet challenging aspect of reinforcement\nlearning (RL). Researchers typically utilize feedback signals from the\nenvironment to handcraft a reward function, but this process is not always\neffective due to the varying scale and intricate dependencies of the feedback\nsignals. This paper shows by exploiting certain structures, one can ease the\nreward design process. Specifically, we propose a hierarchical reward modeling\nframework -- HERON for scenarios: (I) The feedback signals naturally present\nhierarchy; (II) The reward is sparse, but with less important surrogate\nfeedback to help policy learning. Both scenarios allow us to design a\nhierarchical decision tree induced by the importance ranking of the feedback\nsignals to compare RL trajectories. With such preference data, we can then\ntrain a reward model for policy learning. We apply HERON to several RL\napplications, and we find that our framework can not only train high performing\nagents on a variety of difficult tasks, but also provide additional benefits\nsuch as improved sample efficiency and robustness. Our code is available at\n\\url{https://github.com/abukharin3/HERON}.\n","authors":["Alexander Bukharin","Yixiao Li","Pengcheng He","Tuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2309.02632v3.pdf","comment":"28 Pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.15769v3","updated":"2024-06-10T13:09:53Z","published":"2024-03-23T08:54:03Z","title":"FusionINN: Decomposable Image Fusion for Brain Tumor Monitoring","summary":"  Image fusion typically employs non-invertible neural networks to merge\nmultiple source images into a single fused image. However, for clinical\nexperts, solely relying on fused images may be insufficient for making\ndiagnostic decisions, as the fusion mechanism blends features from source\nimages, thereby making it difficult to interpret the underlying tumor\npathology. We introduce FusionINN, a novel decomposable image fusion framework,\ncapable of efficiently generating fused images and also decomposing them back\nto the source images. FusionINN is designed to be bijective by including a\nlatent image alongside the fused image, while ensuring minimal transfer of\ninformation from the source images to the latent representation. To the best of\nour knowledge, we are the first to investigate the decomposability of fused\nimages, which is particularly crucial for life-sensitive applications such as\nmedical image fusion compared to other tasks like multi-focus or multi-exposure\nimage fusion. Our extensive experimentation validates FusionINN over existing\ndiscriminative and generative fusion methods, both subjectively and\nobjectively. Moreover, compared to a recent denoising diffusion-based fusion\nmodel, our approach offers faster and qualitatively better fusion results.\n","authors":["Nishant Kumar","Ziyan Tao","Jaikirat Singh","Yang Li","Peiwen Sun","Binghui Zhao","Stefan Gumhold"],"pdf_url":"https://arxiv.org/pdf/2403.15769v3.pdf","comment":"Accepted at IJCAI Workshop 2024. Source code available at\n  https://github.com/nish03/FusionINN"},{"id":"http://arxiv.org/abs/2406.06237v1","updated":"2024-06-10T13:07:13Z","published":"2024-06-10T13:07:13Z","title":"Efficient Neural Compression with Inference-time Decoding","summary":"  This paper explores the combination of neural network quantization and\nentropy coding for memory footprint minimization. Edge deployment of quantized\nmodels is hampered by the harsh Pareto frontier of the accuracy-to-bitwidth\ntradeoff, causing dramatic accuracy loss below a certain bitwidth. This\naccuracy loss can be alleviated thanks to mixed precision quantization,\nallowing for more flexible bitwidth allocation. However, standard mixed\nprecision benefits remain limited due to the 1-bit frontier, that forces each\nparameter to be encoded on at least 1 bit of data. This paper introduces an\napproach that combines mixed precision, zero-point quantization and entropy\ncoding to push the compression boundary of Resnets beyond the 1-bit frontier\nwith an accuracy drop below 1% on the ImageNet benchmark. From an\nimplementation standpoint, a compact decoder architecture features reduced\nlatency, thus allowing for inference-compatible decoding.\n","authors":["C. Metz","O. Bichler","A. Dupret"],"pdf_url":"https://arxiv.org/pdf/2406.06237v1.pdf","comment":"5 pages, 5 figures, to be published in ISCAS 2024"},{"id":"http://arxiv.org/abs/2301.12776v3","updated":"2024-06-10T12:53:36Z","published":"2023-01-30T10:44:15Z","title":"PAC-Bayesian Soft Actor-Critic Learning","summary":"  Actor-critic algorithms address the dual goals of reinforcement learning\n(RL), policy evaluation and improvement via two separate function\napproximators. The practicality of this approach comes at the expense of\ntraining instability, caused mainly by the destructive effect of the\napproximation errors of the critic on the actor. We tackle this bottleneck by\nemploying an existing Probably Approximately Correct (PAC) Bayesian bound for\nthe first time as the critic training objective of the Soft Actor-Critic (SAC)\nalgorithm. We further demonstrate that online learning performance improves\nsignificantly when a stochastic actor explores multiple futures by\ncritic-guided random search. We observe our resulting algorithm to compare\nfavorably against the state-of-the-art SAC implementation on multiple classical\ncontrol and locomotion tasks in terms of both sample efficiency and regret.\n","authors":["Bahareh Tasdighi","Abdullah Akgül","Manuel Haussmann","Kenny Kazimirzak Brink","Melih Kandemir"],"pdf_url":"https://arxiv.org/pdf/2301.12776v3.pdf","comment":"19 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.06227v1","updated":"2024-06-10T12:53:13Z","published":"2024-06-10T12:53:13Z","title":"PAC-Bayes Analysis for Recalibration in Classification","summary":"  Nonparametric estimation with binning is widely employed in the calibration\nerror evaluation and the recalibration of machine learning models. Recently,\ntheoretical analyses of the bias induced by this estimation approach have been\nactively pursued; however, the understanding of the generalization of the\ncalibration error to unknown data remains limited. In addition, although many\nrecalibration algorithms have been proposed, their generalization performance\nlacks theoretical guarantees. To address this problem, we conduct a\ngeneralization analysis of the calibration error under the probably\napproximately correct (PAC) Bayes framework. This approach enables us to derive\na first optimizable upper bound for the generalization error in the calibration\ncontext. We then propose a generalization-aware recalibration algorithm based\non our generalization theory. Numerical experiments show that our algorithm\nimproves the Gaussian-process-based recalibration performance on various\nbenchmark datasets and models.\n","authors":["Masahiro Fujisawa","Futoshi Futami"],"pdf_url":"https://arxiv.org/pdf/2406.06227v1.pdf","comment":"27 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.06225v1","updated":"2024-06-10T12:47:49Z","published":"2024-06-10T12:47:49Z","title":"Siren -- Advancing Cybersecurity through Deception and Adaptive Analysis","summary":"  Siren represents a pioneering research effort aimed at fortifying\ncybersecurity through strategic integration of deception, machine learning, and\nproactive threat analysis. Drawing inspiration from mythical sirens, this\nproject employs sophisticated methods to lure potential threats into controlled\nenvironments. The system features a dynamic machine learning model for\nreal-time analysis and classification, ensuring continuous adaptability to\nemerging cyber threats. The architectural framework includes a link monitoring\nproxy, a purpose-built machine learning model for dynamic link analysis, and a\nhoneypot enriched with simulated user interactions to intensify threat\nengagement. Data protection within the honeypot is fortified with probabilistic\nencryption. Additionally, the incorporation of simulated user activity extends\nthe system's capacity to capture and learn from potential attackers even after\nuser disengagement. Siren introduces a paradigm shift in cybersecurity,\ntransforming traditional defense mechanisms into proactive systems that\nactively engage and learn from potential adversaries. The research strives to\nenhance user protection while yielding valuable insights for ongoing refinement\nin response to the evolving landscape of cybersecurity threats.\n","authors":["Girish Kulathumani","Samruth Ananthanarayanan","Ganesh Narayanan"],"pdf_url":"https://arxiv.org/pdf/2406.06225v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.00173v2","updated":"2024-06-10T12:46:22Z","published":"2024-03-29T22:05:26Z","title":"Comparing Hyper-optimized Machine Learning Models for Predicting\n  Efficiency Degradation in Organic Solar Cells","summary":"  This work presents a set of optimal machine learning (ML) models to represent\nthe temporal degradation suffered by the power conversion efficiency (PCE) of\npolymeric organic solar cells (OSCs) with a multilayer structure\nITO/PEDOT:PSS/P3HT:PCBM/Al. To that aim, we generated a database with 996\nentries, which includes up to 7 variables regarding both the manufacturing\nprocess and environmental conditions for more than 180 days. Then, we relied on\na software framework that brings together a conglomeration of automated ML\nprotocols that execute sequentially against our database by simply command-line\ninterface. This easily permits hyper-optimizing and randomizing seeds of the ML\nmodels through exhaustive benchmarking so that optimal models are obtained. The\naccuracy achieved reaches values of the coefficient determination (R2) widely\nexceeding 0.90, whereas the root mean squared error (RMSE), sum of squared\nerror (SSE), and mean absolute error (MAE)>1% of the target value, the PCE.\nAdditionally, we contribute with validated models able to screen the behavior\nof OSCs never seen in the database. In that case, R2~0.96-0.97 and RMSE~1%,\nthus confirming the reliability of the proposal to predict. For comparative\npurposes, classical Bayesian regression fitting based on non-linear mean\nsquares (LMS) are also presented, which only perform sufficiently for\nunivariate cases of single OSCs. Hence they fail to outperform the breadth of\nthe capabilities shown by the ML models. Finally, thanks to the standardized\nresults offered by the ML framework, we study the dependencies between the\nvariables of the dataset and their implications for the optimal performance and\nstability of the OSCs. Reproducibility is ensured by a standardized report\naltogether with the dataset, which are publicly available at Github.\n","authors":["David Valiente","Fernando Rodríguez-Mas","Juan V. Alegre-Requena","David Dalmau","Juan C. Ferrer"],"pdf_url":"https://arxiv.org/pdf/2404.00173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01975v2","updated":"2024-06-10T12:43:11Z","published":"2024-02-03T00:58:41Z","title":"Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks","summary":"  A molecule's 2D representation consists of its atoms, their attributes, and\nthe molecule's covalent bonds. A 3D (geometric) representation of a molecule is\ncalled a conformer and consists of its atom types and Cartesian coordinates.\nEvery conformer has a potential energy, and the lower this energy, the more\nlikely it occurs in nature. Most existing machine learning methods for\nmolecular property prediction consider either 2D molecular graphs or 3D\nconformer structure representations in isolation. Inspired by recent work on\nusing ensembles of conformers in conjunction with 2D graph representations, we\npropose $\\mathrm{E}$(3)-invariant molecular conformer aggregation networks. The\nmethod integrates a molecule's 2D representation with that of multiple of its\nconformers. Contrary to prior work, we propose a novel 2D-3D aggregation\nmechanism based on a differentiable solver for the \\emph{Fused\nGromov-Wasserstein Barycenter} problem and the use of an efficient conformer\ngeneration method based on distance geometry. We show that the proposed\naggregation mechanism is $\\mathrm{E}$(3) invariant and propose an efficient GPU\nimplementation. Moreover, we demonstrate that the aggregation mechanism helps\nto significantly outperform state-of-the-art molecule property prediction\nmethods on established datasets.\n","authors":["Duy M. H. Nguyen","Nina Lukashina","Tai Nguyen","An T. Le","TrungTin Nguyen","Nhat Ho","Jan Peters","Daniel Sonntag","Viktor Zaverkin","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2402.01975v2.pdf","comment":"Accepted at ICML 2024"},{"id":"http://arxiv.org/abs/2312.13985v2","updated":"2024-06-10T12:41:43Z","published":"2023-12-21T16:18:33Z","title":"Rényi Pufferfish Privacy: General Additive Noise Mechanisms and\n  Privacy Amplification by Iteration","summary":"  Pufferfish privacy is a flexible generalization of differential privacy that\nallows to model arbitrary secrets and adversary's prior knowledge about the\ndata. Unfortunately, designing general and tractable Pufferfish mechanisms that\ndo not compromise utility is challenging. Furthermore, this framework does not\nprovide the composition guarantees needed for a direct use in iterative machine\nlearning algorithms. To mitigate these issues, we introduce a R\\'enyi\ndivergence-based variant of Pufferfish and show that it allows us to extend the\napplicability of the Pufferfish framework. We first generalize the Wasserstein\nmechanism to cover a wide range of noise distributions and introduce several\nways to improve its utility. We also derive stronger guarantees against\nout-of-distribution adversaries. Finally, as an alternative to composition, we\nprove privacy amplification results for contractive noisy iterations and\nshowcase the first use of Pufferfish in private convex optimization. A common\ningredient underlying our results is the use and extension of shift reduction\nlemmas.\n","authors":["Clément Pierquin","Aurélien Bellet","Marc Tommasi","Matthieu Boussard"],"pdf_url":"https://arxiv.org/pdf/2312.13985v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06220v1","updated":"2024-06-10T12:34:38Z","published":"2024-06-10T12:34:38Z","title":"Label-Looping: Highly Efficient Decoding for Transducers","summary":"  This paper introduces a highly efficient greedy decoding algorithm for\nTransducer inference. We propose a novel data structure using CUDA tensors to\nrepresent partial hypotheses in a batch that supports parallelized hypothesis\nmanipulations. During decoding, our algorithm maximizes GPU parallelism by\nadopting a nested-loop design, where the inner loop consumes all blank\npredictions, while non-blank predictions are handled in the outer loop. Our\nalgorithm is general-purpose and can work with both conventional Transducers\nand Token-and-Duration Transducers. Experiments show that the label-looping\nalgorithm can bring a speedup up to 2.0X compared to conventional batched\ndecoding algorithms when using batch size 32, and can be combined with other\ncompiler or GPU call-related techniques to bring more speedup. We will\nopen-source our implementation to benefit the research community.\n","authors":["Vladimir Bataev","Hainan Xu","Daniel Galvez","Vitaly Lavrukhin","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2406.06220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06213v1","updated":"2024-06-10T12:25:13Z","published":"2024-06-10T12:25:13Z","title":"A Statistical Theory of Regularization-Based Continual Learning","summary":"  We provide a statistical analysis of regularization-based continual learning\non a sequence of linear regression tasks, with emphasis on how different\nregularization terms affect the model performance. We first derive the\nconvergence rate for the oracle estimator obtained as if all data were\navailable simultaneously. Next, we consider a family of generalized\n$\\ell_2$-regularization algorithms indexed by matrix-valued hyperparameters,\nwhich includes the minimum norm estimator and continual ridge regression as\nspecial cases. As more tasks are introduced, we derive an iterative update\nformula for the estimation error of generalized $\\ell_2$-regularized\nestimators, from which we determine the hyperparameters resulting in the\noptimal algorithm. Interestingly, the choice of hyperparameters can effectively\nbalance the trade-off between forward and backward knowledge transfer and\nadjust for data heterogeneity. Moreover, the estimation error of the optimal\nalgorithm is derived explicitly, which is of the same order as that of the\noracle estimator. In contrast, our lower bounds for the minimum norm estimator\nand continual ridge regression show their suboptimality. A byproduct of our\ntheoretical analysis is the equivalence between early stopping and generalized\n$\\ell_2$-regularization in continual learning, which may be of independent\ninterest. Finally, we conduct experiments to complement our theory.\n","authors":["Xuyang Zhao","Huiyuan Wang","Weiran Huang","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2406.06213v1.pdf","comment":"Accepted by ICML 2024"},{"id":"http://arxiv.org/abs/2406.06207v1","updated":"2024-06-10T12:14:05Z","published":"2024-06-10T12:14:05Z","title":"Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against\n  Personalized Federated Learning","summary":"  Federated Learning (FL) is a collaborative machine learning technique where\nmultiple clients work together with a central server to train a global model\nwithout sharing their private data. However, the distribution shift across\nnon-IID datasets of clients poses a challenge to this one-model-fits-all method\nhindering the ability of the global model to effectively adapt to each client's\nunique local data. To echo this challenge, personalized FL (PFL) is designed to\nallow each client to create personalized local models tailored to their private\ndata. While extensive research has scrutinized backdoor risks in FL, it has\nremained underexplored in PFL applications. In this study, we delve deep into\nthe vulnerabilities of PFL to backdoor attacks. Our analysis showcases a tale\nof two cities. On the one hand, the personalization process in PFL can dilute\nthe backdoor poisoning effects injected into the personalized local models.\nFurthermore, PFL systems can also deploy both server-end and client-end defense\nmechanisms to strengthen the barrier against backdoor attacks. On the other\nhand, our study shows that PFL fortified with these defense methods may offer a\nfalse sense of security. We propose \\textit{PFedBA}, a stealthy and effective\nbackdoor attack strategy applicable to PFL systems. \\textit{PFedBA} ingeniously\naligns the backdoor learning task with the main learning task of PFL by\noptimizing the trigger generation process. Our comprehensive experiments\ndemonstrate the effectiveness of \\textit{PFedBA} in seamlessly embedding\ntriggers into personalized local models. \\textit{PFedBA} yields outstanding\nattack performance across 10 state-of-the-art PFL algorithms, defeating the\nexisting 6 defense mechanisms. Our study sheds light on the subtle yet potent\nbackdoor threats to PFL systems, urging the community to bolster defenses\nagainst emerging backdoor challenges.\n","authors":["Xiaoting Lyu","Yufei Han","Wei Wang","Jingkai Liu","Yongsheng Zhu","Guangquan Xu","Jiqiang Liu","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06207v1.pdf","comment":"Accepted by Usenix Security 2024"},{"id":"http://arxiv.org/abs/2406.03586v2","updated":"2024-06-10T12:09:37Z","published":"2024-06-05T19:05:08Z","title":"CountCLIP -- [Re] Teaching CLIP to Count to Ten","summary":"  Large vision-language models (VLMs) are shown to learn rich joint image-text\nrepresentations enabling high performances in relevant downstream tasks.\nHowever, they fail to showcase their quantitative understanding of objects, and\nthey lack good counting-aware representation. This paper conducts a\nreproducibility study of 'Teaching CLIP to Count to Ten' (Paiss et al., 2023),\nwhich presents a method to finetune a CLIP model (Radford et al., 2021) to\nimprove zero-shot counting accuracy in an image while maintaining the\nperformance for zero-shot classification by introducing a counting-contrastive\nloss term. We improve the model's performance on a smaller subset of their\ntraining data with lower computational resources. We verify these claims by\nreproducing their study with our own code. The implementation can be found at\nhttps://github.com/SforAiDl/CountCLIP.\n","authors":["Harshvardhan Mestha","Tejas Agrawal","Karan Bania","Shreyas V","Yash Bhisikar"],"pdf_url":"https://arxiv.org/pdf/2406.03586v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06202v1","updated":"2024-06-10T11:58:11Z","published":"2024-06-10T11:58:11Z","title":"Federated learning in food research","summary":"  Research in the food domain is at times limited due to data sharing\nobstacles, such as data ownership, privacy requirements, and regulations. While\nimportant, these obstacles can restrict data-driven methods such as machine\nlearning. Federated learning, the approach of training models on locally kept\ndata and only sharing the learned parameters, is a potential technique to\nalleviate data sharing obstacles. This systematic review investigates the use\nof federated learning within the food domain, structures included papers in a\nfederated learning framework, highlights knowledge gaps, and discusses\npotential applications. A total of 41 papers were included in the review. The\ncurrent applications include solutions to water and milk quality assessment,\ncybersecurity of water processing, pesticide residue risk analysis, weed\ndetection, and fraud detection, focusing on centralized horizontal federated\nlearning. One of the gaps found was the lack of vertical or transfer federated\nlearning and decentralized architectures.\n","authors":["Zuzanna Fendor","Bas H. M. van der Velden","Xinxin Wang","Andrea Jr. Carnoli","Osman Mutlu","Ali Hürriyetoğlu"],"pdf_url":"https://arxiv.org/pdf/2406.06202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17303v2","updated":"2024-06-10T11:46:34Z","published":"2023-10-26T10:54:47Z","title":"Demonstration-Regularized RL","summary":"  Incorporating expert demonstrations has empirically helped to improve the\nsample efficiency of reinforcement learning (RL). This paper quantifies\ntheoretically to what extent this extra information reduces RL's sample\ncomplexity. In particular, we study the demonstration-regularized reinforcement\nlearning that leverages the expert demonstrations by KL-regularization for a\npolicy learned by behavior cloning. Our findings reveal that using\n$N^{\\mathrm{E}}$ expert demonstrations enables the identification of an optimal\npolicy at a sample complexity of order\n$\\widetilde{O}(\\mathrm{Poly}(S,A,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in finite\nand $\\widetilde{O}(\\mathrm{Poly}(d,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in\nlinear Markov decision processes, where $\\varepsilon$ is the target precision,\n$H$ the horizon, $A$ the number of action, $S$ the number of states in the\nfinite case and $d$ the dimension of the feature space in the linear case. As a\nby-product, we provide tight convergence guarantees for the behaviour cloning\nprocedure under general assumptions on the policy classes. Additionally, we\nestablish that demonstration-regularized methods are provably efficient for\nreinforcement learning from human feedback (RLHF). In this respect, we provide\ntheoretical evidence showing the benefits of KL-regularization for RLHF in\ntabular and linear MDPs. Interestingly, we avoid pessimism injection by\nemploying computationally feasible regularization to handle reward estimation\nuncertainty, thus setting our approach apart from the prior works.\n","authors":["Daniil Tiapkin","Denis Belomestny","Daniele Calandriello","Eric Moulines","Alexey Naumov","Pierre Perrault","Michal Valko","Pierre Menard"],"pdf_url":"https://arxiv.org/pdf/2310.17303v2.pdf","comment":"This revision fixes an error due to use of some incorrect results\n  (Lemma 32, Corollary 11 by Talebi & Maillard, 2018) in the proof of Theorem\n  8. The condition for the RLHF results have slightly changed"},{"id":"http://arxiv.org/abs/2403.04847v2","updated":"2024-06-10T11:43:17Z","published":"2024-03-07T19:02:13Z","title":"Solving Inverse Problems with Model Mismatch using Untrained Neural\n  Networks within Model-based Architectures","summary":"  Model-based deep learning methods such as loop unrolling (LU) and deep\nequilibrium model}(DEQ) extensions offer outstanding performance in solving\ninverse problems (IP). These methods unroll the optimization iterations into a\nsequence of neural networks that in effect learn a regularization function from\ndata. While these architectures are currently state-of-the-art in numerous\napplications, their success heavily relies on the accuracy of the forward\nmodel. This assumption can be limiting in many physical applications due to\nmodel simplifications or uncertainties in the apparatus. To address forward\nmodel mismatch, we introduce an untrained forward model residual block within\nthe model-based architecture to match the data consistency in the measurement\ndomain for each instance. We propose two variants in well-known model-based\narchitectures (LU and DEQ) and prove convergence under mild conditions. Our\napproach offers a unified solution that is less parameter-sensitive, requires\nno additional data, and enables simultaneous fitting of the forward model and\nreconstruction in a single pass, benefiting both linear and nonlinear inverse\nproblems. The experiments show significant quality improvement in removing\nartifacts and preserving details across three distinct applications,\nencompassing both linear and nonlinear inverse problems. Moreover, we highlight\nreconstruction effectiveness in intermediate steps and showcase robustness to\nrandom initialization of the residual block and a higher number of iterations\nduring evaluation. Code is available at\n\\texttt{https://github.com/InvProbs/A-adaptive-model-based-methods}.\n","authors":["Peimeng Guan","Naveed Iqbal","Mark A. Davenport","Mudassir Masood"],"pdf_url":"https://arxiv.org/pdf/2403.04847v2.pdf","comment":"Published in Transactions in Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2402.04012v2","updated":"2024-06-10T11:40:40Z","published":"2024-02-05T09:59:57Z","title":"Quantized Approximately Orthogonal Recurrent Neural Networks","summary":"  In recent years, Orthogonal Recurrent Neural Networks (ORNNs) have gained\npopularity due to their ability to manage tasks involving long-term\ndependencies, such as the copy-task, and their linear complexity. However,\nexisting ORNNs utilize full precision weights and activations, which prevents\ntheir deployment on compact devices.In this paper, we explore the quantization\nof the weight matrices in ORNNs, leading to Quantized approximately Orthogonal\nRNNs (QORNNs). The construction of such networks remained an open problem,\nacknowledged for its inherent instability. We propose and investigate two\nstrategies to learn QORNN by combining quantization-aware training (QAT) and\northogonal projections. We also study post-training quantization of the\nactivations for pure integer computation of the recurrent loop. The most\nefficient models achieve results similar to state-of-the-art full-precision\nORNN, LSTM and FastRNN on a variety of standard benchmarks, even with 4-bits\nquantization.\n","authors":["Armand Foucault","Franck Mamalet","François Malgouyres"],"pdf_url":"https://arxiv.org/pdf/2402.04012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.15437v2","updated":"2024-06-10T11:35:42Z","published":"2022-05-30T21:23:22Z","title":"AMED: Automatic Mixed-Precision Quantization for Edge Devices","summary":"  Quantized neural networks are well known for reducing the latency, power\nconsumption, and model size without significant harm to the performance. This\nmakes them highly appropriate for systems with limited resources and low power\ncapacity. Mixed-precision quantization offers better utilization of customized\nhardware that supports arithmetic operations at different bitwidths.\nQuantization methods either aim to minimize the compression loss given a\ndesired reduction or optimize a dependent variable for a specified property of\nthe model (such as FLOPs or model size); both make the performance inefficient\nwhen deployed on specific hardware, but more importantly, quantization methods\nassume that the loss manifold holds a global minimum for a quantized model that\ncopes with the global minimum of the full precision counterpart. Challenging\nthis assumption, we argue that the optimal minimum changes as the precision\nchanges, and thus, it is better to look at quantization as a random process,\nplacing the foundation for a different approach to quantize neural networks,\nwhich, during the training procedure, quantizes the model to a different\nprecision, looks at the bit allocation as a Markov Decision Process, and then,\nfinds an optimal bitwidth allocation for measuring specified behaviors on a\nspecific device via direct signals from the particular hardware architecture.\nBy doing so, we avoid the basic assumption that the loss behaves the same way\nfor a quantized model. Automatic Mixed-Precision Quantization for Edge Devices\n(dubbed AMED) demonstrates its superiority over current state-of-the-art\nschemes in terms of the trade-off between neural network accuracy and hardware\nefficiency, backed by a comprehensive evaluation.\n","authors":["Moshe Kimhi","Tal Rozen","Avi Mendelson","Chaim Baskin"],"pdf_url":"https://arxiv.org/pdf/2205.15437v2.pdf","comment":"Published in special issue From Edge Devices to Cloud Computing and\n  Datacenters: Emerging Machine Learning Applications, Algorithms, and\n  Optimizations by MPDI mathematics"},{"id":"http://arxiv.org/abs/2302.13696v4","updated":"2024-06-10T11:32:24Z","published":"2023-02-27T11:55:24Z","title":"Moderate Adaptive Linear Units (MoLU)","summary":"  We propose a new high-performance activation function, Moderate Adaptive\nLinear Units (MoLU), for the deep neural network. The MoLU is a simple,\nbeautiful and powerful activation function that can be a good main activation\nfunction among hundreds of activation functions. Because the MoLU is made up of\nthe elementary functions, not only it is a infinite diffeomorphism (i.e. smooth\nand infinitely differentiable over whole domains), but also it decreases\ntraining time.\n","authors":["Hankyul Koh","Joon-hyuk Ko","Wonho Jhe"],"pdf_url":"https://arxiv.org/pdf/2302.13696v4.pdf","comment":"4 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.14425v2","updated":"2024-06-10T11:30:28Z","published":"2024-05-23T10:48:30Z","title":"When predict can also explain: few-shot prediction to select better\n  neural latents","summary":"  Latent variable models serve as powerful tools to infer underlying dynamics\nfrom observed neural activity. However, due to the absence of ground truth\ndata, prediction benchmarks are often employed as proxies. In this study, we\nreveal the limitations of the widely-used 'co-smoothing' prediction framework\nand propose an improved few-shot prediction approach that encourages more\naccurate latent dynamics. Utilizing a student-teacher setup with Hidden Markov\nModels, we demonstrate that the high co-smoothing model space can encompass\nmodels with arbitrary extraneous dynamics within their latent representations.\nTo address this, we introduce a secondary metric -- a few-shot version of\nco-smoothing. This involves performing regression from the latent variables to\nheld-out channels in the data using fewer trials. Our results indicate that\namong models with near-optimal co-smoothing, those with extraneous dynamics\nunderperform in the few-shot co-smoothing compared to 'minimal' models devoid\nof such dynamics. We also provide analytical insights into the origin of this\nphenomenon. We further validate our findings on real neural data using two\nstate-of-the-art methods: LFADS and STNDT. In the absence of ground truth, we\nsuggest a proxy measure to quantify extraneous dynamics. By cross-decoding the\nlatent variables of all model pairs with high co-smoothing, we identify models\nwith minimal extraneous dynamics. We find a correlation between few-shot\nco-smoothing performance and this new measure. In summary, we present a novel\nprediction metric designed to yield latent variables that more accurately\nreflect the ground truth, offering a significant improvement for latent\ndynamics inference.\n","authors":["Kabir Dabholkar","Omri Barak"],"pdf_url":"https://arxiv.org/pdf/2405.14425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06185v1","updated":"2024-06-10T11:28:29Z","published":"2024-06-10T11:28:29Z","title":"EARS: An Anechoic Fullband Speech Dataset Benchmarked for Speech\n  Enhancement and Dereverberation","summary":"  We release the EARS (Expressive Anechoic Recordings of Speech) dataset, a\nhigh-quality speech dataset comprising 107 speakers from diverse backgrounds,\ntotaling in 100 hours of clean, anechoic speech data. The dataset covers a\nlarge range of different speaking styles, including emotional speech, different\nreading styles, non-verbal sounds, and conversational freeform speech. We\nbenchmark various methods for speech enhancement and dereverberation on the\ndataset and evaluate their performance through a set of instrumental metrics.\nIn addition, we conduct a listening test with 20 participants for the speech\nenhancement task, where a generative method is preferred. We introduce a blind\ntest set that allows for automatic online evaluation of uploaded data. Dataset\ndownload links and automatic evaluation server can be found online.\n","authors":["Julius Richter","Yi-Chiao Wu","Steven Krenn","Simon Welker","Bunlong Lay","Shinji Watanabe","Alexander Richard","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2406.06185v1.pdf","comment":"Accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.06184v1","updated":"2024-06-10T11:28:25Z","published":"2024-06-10T11:28:25Z","title":"Deep Multi-Objective Reinforcement Learning for Utility-Based\n  Infrastructural Maintenance Optimization","summary":"  In this paper, we introduce Multi-Objective Deep Centralized Multi-Agent\nActor-Critic (MO- DCMAC), a multi-objective reinforcement learning (MORL)\nmethod for infrastructural maintenance optimization, an area traditionally\ndominated by single-objective reinforcement learning (RL) approaches. Previous\nsingle-objective RL methods combine multiple objectives, such as probability of\ncollapse and cost, into a singular reward signal through reward-shaping. In\ncontrast, MO-DCMAC can optimize a policy for multiple objectives directly, even\nwhen the utility function is non-linear. We evaluated MO-DCMAC using two\nutility functions, which use probability of collapse and cost as input. The\nfirst utility function is the Threshold utility, in which MO-DCMAC should\nminimize cost so that the probability of collapse is never above the threshold.\nThe second is based on the Failure Mode, Effects, and Criticality Analysis\n(FMECA) methodology used by asset managers to asses maintenance plans. We\nevaluated MO-DCMAC, with both utility functions, in multiple maintenance\nenvironments, including ones based on a case study of the historical quay walls\nof Amsterdam. The performance of MO-DCMAC was compared against multiple\nrule-based policies based on heuristics currently used for constructing\nmaintenance plans. Our results demonstrate that MO-DCMAC outperforms\ntraditional rule-based policies across various environments and utility\nfunctions.\n","authors":["Jesse van Remmerden","Maurice Kenter","Diederik M. Roijers","Charalampos Andriotis","Yingqian Zhang","Zaharah Bukhsh"],"pdf_url":"https://arxiv.org/pdf/2406.06184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13570v2","updated":"2024-06-10T11:26:14Z","published":"2024-01-24T16:31:50Z","title":"Guided Diffusion for Fast Inverse Design of Density-based Mechanical\n  Metamaterials","summary":"  Mechanical metamaterial is a synthetic material that can possess\nextraordinary physical characteristics, such as abnormal elasticity, stiffness,\nand stability, by carefully designing its internal structure. To make\nmetamaterials contain delicate local structures with unique mechanical\nproperties, it is a potential method to represent them through high-resolution\nvoxels. However, it brings a substantial computational burden. To this end,\nthis paper proposes a fast inverse design method, whose core is an advanced\ndeep generative AI algorithm, to generate voxel-based mechanical metamaterials.\nSpecifically, we use the self-conditioned diffusion model, capable of\ngenerating a microstructure with a resolution of $128^3$ to approach the\nspecified homogenized tensor matrix in just 3 seconds. Accordingly, this rapid\nreverse design tool facilitates the exploration of extreme metamaterials, the\nsequence interpolation in metamaterials, and the generation of diverse\nmicrostructures for multi-scale design. This flexible and adaptive generative\ntool is of great value in structural engineering or other mechanical systems\nand can stimulate more subsequent research.\n","authors":["Yanyan Yang","Lili Wang","Xiaoya Zhai","Kai Chen","Wenming Wu","Yunkai Zhao","Ligang Liu","Xiao-Ming Fu"],"pdf_url":"https://arxiv.org/pdf/2401.13570v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.15362v3","updated":"2024-06-10T11:24:06Z","published":"2024-05-24T08:54:36Z","title":"Pipeline Parallelism with Controllable Memory","summary":"  Pipeline parallelism has been widely explored, but most existing schedules\nlack a systematic methodology. In this paper, we propose a framework to\ndecompose pipeline schedules as repeating a building block and we show that the\nlifespan of the building block decides the peak activation memory of the\npipeline schedule. Guided by the observations, we find that almost all existing\npipeline schedules, to the best of our knowledge, are memory inefficient. To\naddress this, we introduce a family of memory efficient building blocks with\ncontrollable activation memory, which can reduce the peak activation memory to\n1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable\nthroughput. We can also achieve almost zero pipeline bubbles while maintaining\nthe same activation memory as 1F1B. Our evaluations demonstrate that in pure\npipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in\nterms of throughput. When employing a grid search over hybrid parallelism\nhyperparameters in practical scenarios, our proposed methods demonstrate a 16%\nthroughput improvement over the 1F1B baseline for large language models.\n","authors":["Penghui Qi","Xinyi Wan","Nyamdavaa Amar","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2405.15362v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12044v3","updated":"2024-06-10T11:13:06Z","published":"2023-12-19T10:57:12Z","title":"XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX","summary":"  Inspired by the diversity and depth of XLand and the simplicity and\nminimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and\ngrid-world environments for meta-reinforcement learning research. Written in\nJAX, XLand-MiniGrid is designed to be highly scalable and can potentially run\non GPU or TPU accelerators, democratizing large-scale experimentation with\nlimited resources. Along with the environments, XLand-MiniGrid provides\npre-sampled benchmarks with millions of unique tasks of varying difficulty and\neasy-to-use baselines that allow users to quickly start training adaptive\nagents. In addition, we have conducted a preliminary analysis of scaling and\ngeneralization, showing that our baselines are capable of reaching millions of\nsteps per second during training and validating that the proposed benchmarks\nare challenging.\n","authors":["Alexander Nikulin","Vladislav Kurenkov","Ilya Zisman","Artem Agarkov","Viacheslav Sinii","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2312.12044v3.pdf","comment":"NeurIPS 2023, Workshop, Source code:\n  https://github.com/corl-team/xland-minigrid"},{"id":"http://arxiv.org/abs/2405.03262v2","updated":"2024-06-10T11:04:04Z","published":"2024-05-06T08:34:15Z","title":"End-to-End Reinforcement Learning of Curative Curtailment with Partial\n  Measurement Availability","summary":"  In the course of the energy transition, the expansion of generation and\nconsumption will change, and many of these technologies, such as PV systems,\nelectric cars and heat pumps, will influence the power flow, especially in the\ndistribution grids. Scalable methods that can make decisions for each grid\nconnection are needed to enable congestion-free grid operation in the\ndistribution grids. This paper presents a novel end-to-end approach to\nresolving congestion in distribution grids with deep reinforcement learning.\nOur architecture learns to curtail power and set appropriate reactive power to\ndetermine a non-congested and, thus, feasible grid state. State-of-the-art\nmethods such as the optimal power flow (OPF) demand high computational costs\nand detailed measurements of every bus in a grid. In contrast, the presented\nmethod enables decisions under sparse information with just some buses\nobservable in the grid. Distribution grids are generally not yet fully\ndigitized and observable, so this method can be used for decision-making on the\nmajority of low-voltage grids. On a real low-voltage grid the approach resolves\n100\\% of violations in the voltage band and 98.8\\% of asset overloads. The\nresults show that decisions can also be made on real grids that guarantee\nsufficient quality for congestion-free grid operation.\n","authors":["Hinrikus Wolf","Luis Böttcher","Sarra Bouchkati","Philipp Lutat","Jens Breitung","Bastian Jung","Tina Möllemann","Viktor Todosijević","Jan Schiefelbein-Lach","Oliver Pohl","Andreas Ulbig","Martin Grohe"],"pdf_url":"https://arxiv.org/pdf/2405.03262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06165v1","updated":"2024-06-10T11:00:26Z","published":"2024-06-10T11:00:26Z","title":"Generalized Nested Latent Variable Models for Lossy Coding applied to\n  Wind Turbine Scenarios","summary":"  Rate-distortion optimization through neural networks has accomplished\ncompetitive results in compression efficiency and image quality. This\nlearning-based approach seeks to minimize the compromise between compression\nrate and reconstructed image quality by automatically extracting and retaining\ncrucial information, while discarding less critical details. A successful\ntechnique consists in introducing a deep hyperprior that operates within a\n2-level nested latent variable model, enhancing compression by capturing\ncomplex data dependencies. This paper extends this concept by designing a\ngeneralized L-level nested generative model with a Markov chain structure. We\ndemonstrate as L increases that a trainable prior is detrimental and explore a\ncommon dimensionality along the distinct latent variables to boost compression\nperformance. As this structured framework can represent autoregressive coders,\nwe outperform the hyperprior model and achieve state-of-the-art performance\nwhile reducing substantially the computational cost. Our experimental\nevaluation is performed on wind turbine scenarios to study its application on\nvisual inspections\n","authors":["Raül Pérez-Gonzalo","Andreas Espersen","Antonio Agudo"],"pdf_url":"https://arxiv.org/pdf/2406.06165v1.pdf","comment":"Accepted to ICIP 2024"},{"id":"http://arxiv.org/abs/2404.18624v2","updated":"2024-06-10T10:43:20Z","published":"2024-04-29T11:52:20Z","title":"Do Vision & Language Decoders use Images and Text equally? How\n  Self-consistent are their Explanations?","summary":"  Vision and language model (VLM) decoders are currently the best-performing\narchitectures on multimodal tasks. Next to predictions, they can also produce\nexplanations, either in post-hoc or CoT settings. However, it is not clear how\nmuch they use the vision and text modalities when generating predictions or\nexplanations. In this work, we investigate if VLMs rely on modalities\ndifferently when they produce explanations as opposed to providing answers. We\nalso evaluate the self-consistency of VLM decoders in both post-hoc and CoT\nexplanation settings, by extending existing unimodal tests and measures to VLM\ndecoders. We find that VLMs are less self-consistent than LLMs. Text\ncontributions in VL decoders are more important than image contributions in all\nexamined tasks. Moreover, the contributions of images are significantly\nstronger for explanation generation compared to answer generation. This\ndifference is even larger in CoT compared to post-hoc explanations. Lastly, we\nprovide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE\nbenchmark, which before only covered VL encoders. We find that VL decoders\nstill struggle with most phenomena tested by VALSE.\n","authors":["Letitia Parcalabescu","Anette Frank"],"pdf_url":"https://arxiv.org/pdf/2404.18624v2.pdf","comment":"25 pages, 8 figures, 11 tables"},{"id":"http://arxiv.org/abs/2406.06158v1","updated":"2024-06-10T10:42:37Z","published":"2024-06-10T10:42:37Z","title":"Get rich quick: exact solutions reveal how unbalanced initializations\n  promote rapid feature learning","summary":"  While the impressive performance of modern neural networks is often\nattributed to their capacity to efficiently extract task-relevant features from\ndata, the mechanisms underlying this rich feature learning regime remain\nelusive, with much of our theoretical understanding stemming from the opposing\nlazy regime. In this work, we derive exact solutions to a minimal model that\ntransitions between lazy and rich learning, precisely elucidating how\nunbalanced layer-specific initialization variances and learning rates determine\nthe degree of feature learning. Our analysis reveals that they conspire to\ninfluence the learning regime through a set of conserved quantities that\nconstrain and modify the geometry of learning trajectories in parameter and\nfunction space. We extend our analysis to more complex linear models with\nmultiple neurons, outputs, and layers and to shallow nonlinear networks with\npiecewise linear activation functions. In linear networks, rapid feature\nlearning only occurs with balanced initializations, where all layers learn at\nsimilar speeds. While in nonlinear networks, unbalanced initializations that\npromote faster learning in earlier layers can accelerate rich learning. Through\na series of experiments, we provide evidence that this unbalanced rich regime\ndrives feature learning in deep finite-width networks, promotes\ninterpretability of early layers in CNNs, reduces the sample complexity of\nlearning hierarchical data, and decreases the time to grokking in modular\narithmetic. Our theory motivates further exploration of unbalanced\ninitializations to enhance efficient feature learning.\n","authors":["Daniel Kunin","Allan Raventós","Clémentine Dominé","Feng Chen","David Klindt","Andrew Saxe","Surya Ganguli"],"pdf_url":"https://arxiv.org/pdf/2406.06158v1.pdf","comment":"40 pages, 12 figures"},{"id":"http://arxiv.org/abs/2402.06535v2","updated":"2024-06-10T10:38:59Z","published":"2024-02-09T16:49:13Z","title":"Bandit Convex Optimisation","summary":"  Bandit convex optimisation is a fundamental framework for studying\nzeroth-order convex optimisation. These notes cover the many tools used for\nthis problem, including cutting plane methods, interior point methods,\ncontinuous exponential weights, gradient descent and online Newton step. The\nnuances between the many assumptions and setups are explained. Although there\nis not much truly new here, some existing tools are applied in novel ways to\nobtain new algorithms. A few bounds are improved in minor ways.\n","authors":["Tor Lattimore"],"pdf_url":"https://arxiv.org/pdf/2402.06535v2.pdf","comment":"200 pages. More polished and some new results"},{"id":"http://arxiv.org/abs/2402.07626v2","updated":"2024-06-10T10:25:14Z","published":"2024-02-12T13:11:11Z","title":"Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution\n  for Weak Features","summary":"  We investigate the test risk of continuous-time stochastic gradient flow\ndynamics in learning theory. Using a path integral formulation we provide, in\nthe regime of a small learning rate, a general formula for computing the\ndifference between test risk curves of pure gradient and stochastic gradient\nflows. We apply the general theory to a simple model of weak features, which\ndisplays the double descent phenomenon, and explicitly compute the corrections\nbrought about by the added stochastic term in the dynamics, as a function of\ntime and model parameters. The analytical results are compared to simulations\nof discrete-time stochastic gradient descent and show good agreement.\n","authors":["Rodrigo Veiga","Anastasia Remizova","Nicolas Macris"],"pdf_url":"https://arxiv.org/pdf/2402.07626v2.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2402.02479v2","updated":"2024-06-10T10:18:46Z","published":"2024-02-04T13:16:29Z","title":"BRAIn: Bayesian Reward-conditioned Amortized Inference for natural\n  language generation from feedback","summary":"  Distribution matching methods for language model alignment such as Generation\nwith Distributional Control (GDC) and Distributional Policy Gradient (DPG) have\nnot received the same level of attention in reinforcement learning from human\nfeedback (RLHF) as contrastive methods such as Sequence Likelihood Calibration\n(SLiC), Direct Preference Optimization (DPO) and its variants. We identify high\nvariance of the gradient estimate as the primary reason for the lack of success\nof these methods and propose a self-normalized baseline to reduce the variance.\nWe further generalize the target distribution in DPG, GDC and DPO by using\nBayes' rule to define the reward-conditioned posterior. The resulting approach,\nreferred to as BRAIn - Bayesian Reward-conditioned Amortized Inference acts as\na bridge between distribution matching methods and DPO and significantly\noutperforms prior art in summarization and Antropic HH tasks.\n","authors":["Gaurav Pandey","Yatin Nandwani","Tahira Naseem","Mayank Mishra","Guangxuan Xu","Dinesh Raghu","Sachindra Joshi","Asim Munawar","Ramón Fernandez Astudillo"],"pdf_url":"https://arxiv.org/pdf/2402.02479v2.pdf","comment":"Accepted at ICML 2024 (main conference)"},{"id":"http://arxiv.org/abs/2406.06150v1","updated":"2024-06-10T10:17:06Z","published":"2024-06-10T10:17:06Z","title":"Physics-Informed Bayesian Optimization of Variational Quantum Circuits","summary":"  In this paper, we propose a novel and powerful method to harness Bayesian\noptimization for Variational Quantum Eigensolvers (VQEs) -- a hybrid\nquantum-classical protocol used to approximate the ground state of a quantum\nHamiltonian. Specifically, we derive a VQE-kernel which incorporates important\nprior information about quantum circuits: the kernel feature map of the\nVQE-kernel exactly matches the known functional form of the VQE's objective\nfunction and thereby significantly reduces the posterior uncertainty. Moreover,\nwe propose a novel acquisition function for Bayesian optimization called\nExpected Maximum Improvement over Confident Regions (EMICoRe) which can\nactively exploit the inductive bias of the VQE-kernel by treating regions with\nlow predictive uncertainty as indirectly ``observed''. As a result,\nobservations at as few as three points in the search domain are sufficient to\ndetermine the complete objective function along an entire one-dimensional\nsubspace of the optimization landscape. Our numerical experiments demonstrate\nthat our approach improves over state-of-the-art baselines.\n","authors":["Kim A. Nicoli","Christopher J. Anders","Lena Funcke","Tobias Hartung","Karl Jansen","Stefan Kühn","Klaus-Robert Müller","Paolo Stornati","Pan Kessel","Shinichi Nakajima"],"pdf_url":"https://arxiv.org/pdf/2406.06150v1.pdf","comment":"36 pages, 17 figures, 37th Conference on Neural Information\n  Processing Systems (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2402.13999v2","updated":"2024-06-10T10:16:19Z","published":"2024-02-21T18:35:27Z","title":"Asymptotics of Learning with Deep Structured (Random) Features","summary":"  For a large class of feature maps we provide a tight asymptotic\ncharacterisation of the test error associated with learning the readout layer,\nin the high-dimensional limit where the input dimension, hidden layer widths,\nand number of training samples are proportionally large. This characterization\nis formulated in terms of the population covariance of the features. Our work\nis partially motivated by the problem of learning with Gaussian rainbow neural\nnetworks, namely deep non-linear fully-connected networks with random but\nstructured weights, whose row-wise covariances are further allowed to depend on\nthe weights of previous layers. For such networks we also derive a closed-form\nformula for the feature covariance in terms of the weight matrices. We further\nfind that in some cases our results can capture feature maps learned by deep,\nfinite-width neural networks trained under gradient descent.\n","authors":["Dominik Schröder","Daniil Dmitriev","Hugo Cui","Bruno Loureiro"],"pdf_url":"https://arxiv.org/pdf/2402.13999v2.pdf","comment":"ICML camera-ready version"},{"id":"http://arxiv.org/abs/2406.06149v1","updated":"2024-06-10T10:15:32Z","published":"2024-06-10T10:15:32Z","title":"Decoupled Marked Temporal Point Process using Neural Ordinary\n  Differential Equations","summary":"  A Marked Temporal Point Process (MTPP) is a stochastic process whose\nrealization is a set of event-time data. MTPP is often used to understand\ncomplex dynamics of asynchronous temporal events such as money transaction,\nsocial media, healthcare, etc. Recent studies have utilized deep neural\nnetworks to capture complex temporal dependencies of events and generate\nembedding that aptly represent the observed events. While most previous studies\nfocus on the inter-event dependencies and their representations, how individual\nevents influence the overall dynamics over time has been under-explored. In\nthis regime, we propose a Decoupled MTPP framework that disentangles\ncharacterization of a stochastic process into a set of evolving influences from\ndifferent events. Our approach employs Neural Ordinary Differential Equations\n(Neural ODEs) to learn flexible continuous dynamics of these influences while\nsimultaneously addressing multiple inference problems, such as density\nestimation and survival rate computation. We emphasize the significance of\ndisentangling the influences by comparing our framework with state-of-the-art\nmethods on real-life datasets, and provide analysis on the model behavior for\npotential applications.\n","authors":["Yujee Song","Donghyun Lee","Rui Meng","Won Hwa Kim"],"pdf_url":"https://arxiv.org/pdf/2406.06149v1.pdf","comment":"18 pages, 8 figures, The Twelfth International Conference on Learning\n  Representations (ICLR 2024)"},{"id":"http://arxiv.org/abs/2404.12172v2","updated":"2024-06-10T10:05:01Z","published":"2024-04-18T13:27:29Z","title":"How to Benchmark Vision Foundation Models for Semantic Segmentation?","summary":"  Recent vision foundation models (VFMs) have demonstrated proficiency in\nvarious tasks but require supervised fine-tuning to perform the task of\nsemantic segmentation effectively. Benchmarking their performance is essential\nfor selecting current models and guiding future model developments for this\ntask. The lack of a standardized benchmark complicates comparisons. Therefore,\nthe primary objective of this paper is to study how VFMs should be benchmarked\nfor semantic segmentation. To do so, various VFMs are fine-tuned under various\nsettings, and the impact of individual settings on the performance ranking and\ntraining time is assessed. Based on the results, the recommendation is to\nfine-tune the ViT-B variants of VFMs with a 16x16 patch size and a linear\ndecoder, as these settings are representative of using a larger model, more\nadvanced decoder and smaller patch size, while reducing training time by more\nthan 13 times. Using multiple datasets for training and evaluation is also\nrecommended, as the performance ranking across datasets and domain shifts\nvaries. Linear probing, a common practice for some VFMs, is not recommended, as\nit is not representative of end-to-end fine-tuning. The benchmarking setup\nrecommended in this paper enables a performance analysis of VFMs for semantic\nsegmentation. The findings of such an analysis reveal that pretraining with\npromptable segmentation is not beneficial, whereas masked image modeling (MIM)\nwith abstract representations is crucial, even more important than the type of\nsupervision used. The code for efficiently fine-tuning VFMs for semantic\nsegmentation can be accessed through the project page at:\nhttps://tue-mps.github.io/benchmark-vfm-ss/.\n","authors":["Tommie Kerssies","Daan de Geus","Gijs Dubbelman"],"pdf_url":"https://arxiv.org/pdf/2404.12172v2.pdf","comment":"CVPR 2024 Workshop Proceedings for the Second Workshop on Foundation\n  Models. v2 updates image normalization preprocessing for linear probing with\n  EVA-02, EVA-02-CLIP, SigLIP, DFN (the impact on end-to-end fine-tuning is\n  negligible; no changes made)"},{"id":"http://arxiv.org/abs/2402.05602v2","updated":"2024-06-10T09:58:55Z","published":"2024-02-08T12:01:24Z","title":"AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for\n  Transformers","summary":"  Large Language Models are prone to biased predictions and hallucinations,\nunderlining the paramount importance of understanding their model-internal\nreasoning process. However, achieving faithful attributions for the entirety of\na black-box transformer model and maintaining computational efficiency is an\nunsolved challenge. By extending the Layer-wise Relevance Propagation\nattribution method to handle attention layers, we address these challenges\neffectively. While partial solutions exist, our method is the first to\nfaithfully and holistically attribute not only input but also latent\nrepresentations of transformer models with the computational efficiency similar\nto a single backward pass. Through extensive evaluations against existing\nmethods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures,\nwe demonstrate that our proposed approach surpasses alternative methods in\nterms of faithfulness and enables the understanding of latent representations,\nopening up the door for concept-based explanations. We provide an LRP library\nat https://github.com/rachtibat/LRP-eXplains-Transformers.\n","authors":["Reduan Achtibat","Sayed Mohammad Vakilzadeh Hatefi","Maximilian Dreyer","Aakriti Jain","Thomas Wiegand","Sebastian Lapuschkin","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2402.05602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06140v1","updated":"2024-06-10T09:53:54Z","published":"2024-06-10T09:53:54Z","title":"Can I understand what I create? Self-Knowledge Evaluation of Large\n  Language Models","summary":"  Large language models (LLMs) have achieved remarkable progress in linguistic\ntasks, necessitating robust evaluation frameworks to understand their\ncapabilities and limitations. Inspired by Feynman's principle of understanding\nthrough creation, we introduce a self-knowledge evaluation framework that is\neasy to implement, evaluating models on their ability to comprehend and respond\nto self-generated questions. Our findings, based on testing multiple models\nacross diverse tasks, reveal significant gaps in the model's self-knowledge\nability. Further analysis indicates these gaps may be due to misalignment with\nhuman attention mechanisms. Additionally, fine-tuning on self-generated math\ntask may enhance the model's math performance, highlighting the potential of\nthe framework for efficient and insightful model evaluation and may also\ncontribute to the improvement of LLMs.\n","authors":["Zhiquan Tan","Lai Wei","Jindong Wang","Xing Xie","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2406.06140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06136v1","updated":"2024-06-10T09:48:13Z","published":"2024-06-10T09:48:13Z","title":"A Comparative Survey of Vision Transformers for Feature Extraction in\n  Texture Analysis","summary":"  Texture, a significant visual attribute in images, has been extensively\ninvestigated across various image recognition applications. Convolutional\nNeural Networks (CNNs), which have been successful in many computer vision\ntasks, are currently among the best texture analysis approaches. On the other\nhand, Vision Transformers (ViTs) have been surpassing the performance of CNNs\non tasks such as object recognition, causing a paradigm shift in the field.\nHowever, ViTs have so far not been scrutinized for texture recognition,\nhindering a proper appreciation of their potential in this specific setting.\nFor this reason, this work explores various pre-trained ViT architectures when\ntransferred to tasks that rely on textures. We review 21 different ViT variants\nand perform an extensive evaluation and comparison with CNNs and\nhand-engineered models on several tasks, such as assessing robustness to\nchanges in texture rotation, scale, and illumination, and distinguishing color\ntextures, material textures, and texture attributes. The goal is to understand\nthe potential and differences among these models when directly applied to\ntexture recognition, using pre-trained ViTs primarily for feature extraction\nand employing linear classifiers for evaluation. We also evaluate their\nefficiency, which is one of the main drawbacks in contrast to other methods.\nOur results show that ViTs generally outperform both CNNs and hand-engineered\nmodels, especially when using stronger pre-training and tasks involving\nin-the-wild textures (images from the internet). We highlight the following\npromising models: ViT-B with DINO pre-training, BeiTv2, and the Swin\narchitecture, as well as the EfficientFormer as a low-cost alternative. In\nterms of efficiency, although having a higher number of GFLOPs and parameters,\nViT-B and BeiT(v2) can achieve a lower feature extraction time on GPUs compared\nto ResNet50.\n","authors":["Leonardo Scabini","Andre Sacilotti","Kallil M. Zielinski","Lucas C. Ribas","Bernard De Baets","Odemir M. Bruno"],"pdf_url":"https://arxiv.org/pdf/2406.06136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06134v1","updated":"2024-06-10T09:45:38Z","published":"2024-06-10T09:45:38Z","title":"DiffInject: Revisiting Debias via Synthetic Data Generation using\n  Diffusion-based Style Injection","summary":"  Dataset bias is a significant challenge in machine learning, where specific\nattributes, such as texture or color of the images are unintentionally learned\nresulting in detrimental performance. To address this, previous efforts have\nfocused on debiasing models either by developing novel debiasing algorithms or\nby generating synthetic data to mitigate the prevalent dataset biases. However,\ngenerative approaches to date have largely relied on using bias-specific\nsamples from the dataset, which are typically too scarce. In this work, we\npropose, DiffInject, a straightforward yet powerful method to augment synthetic\nbias-conflict samples using a pretrained diffusion model. This approach\nsignificantly advances the use of diffusion models for debiasing purposes by\nmanipulating the latent space. Our framework does not require any explicit\nknowledge of the bias types or labelling, making it a fully unsupervised\nsetting for debiasing. Our methodology demonstrates substantial result in\neffectively reducing dataset bias.\n","authors":["Donggeun Ko","Sangwoo Jo","Dongjun Lee","Namjun Park","Jaekwang Kim"],"pdf_url":"https://arxiv.org/pdf/2406.06134v1.pdf","comment":"10 pages (including supplementary), 3 figures, SynData4CV@CVPR 24\n  (Workshop)"},{"id":"http://arxiv.org/abs/2312.15289v2","updated":"2024-06-10T09:45:32Z","published":"2023-12-23T16:10:53Z","title":"Fréchet Wavelet Distance: A Domain-Agnostic Metric for Image\n  Generation","summary":"  Modern metrics for generative learning like Fr\\'echet Inception Distance\n(FID) demonstrate impressive performance. However, they suffer from various\nshortcomings, like a bias towards specific generators and datasets. To address\nthis problem, we propose the Fr\\'echet Wavelet Distance (FWD) as a\ndomain-agnostic metric based on Wavelet Packet Transform ($W_p$). FWD provides\na sight across a broad spectrum of frequencies in images with a high\nresolution, along with preserving both spatial and textural aspects.\nSpecifically, we use Wp to project generated and dataset images to packet\ncoefficient space. Further, we compute Fr\\'echet distance with the resultant\ncoefficients to evaluate the quality of a generator. This metric is\ngeneral-purpose and dataset-domain agnostic, as it does not rely on any\npre-trained network while being more interpretable because of frequency band\ntransparency. We conclude with an extensive evaluation of a wide variety of\ngenerators across various datasets that the proposed FWD is able to generalize\nand improve robustness to domain shift and various corruptions compared to\nother metrics.\n","authors":["Lokesh Veeramacheneni","Moritz Wolter","Hildegard Kuehne","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2312.15289v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09748v2","updated":"2024-06-10T09:35:57Z","published":"2023-12-15T12:39:27Z","title":"VNN: Verification-Friendly Neural Networks with Hard Robustness\n  Guarantees","summary":"  Machine learning techniques often lack formal correctness guarantees,\nevidenced by the widespread adversarial examples that plague most deep-learning\napplications. This lack of formal guarantees resulted in several research\nefforts that aim at verifying Deep Neural Networks (DNNs), with a particular\nfocus on safety-critical applications. However, formal verification techniques\nstill face major scalability and precision challenges. The over-approximation\nintroduced during the formal verification process to tackle the scalability\nchallenge often results in inconclusive analysis. To address this challenge, we\npropose a novel framework to generate Verification-Friendly Neural Networks\n(VNNs). We present a post-training optimization framework to achieve a balance\nbetween preserving prediction performance and verification-friendliness. Our\nproposed framework results in VNNs that are comparable to the original DNNs in\nterms of prediction performance, while amenable to formal verification\ntechniques. This essentially enables us to establish robustness for more VNNs\nthan their DNN counterparts, in a time-efficient manner.\n","authors":["Anahita Baninajjar","Ahmed Rezine","Amir Aminifar"],"pdf_url":"https://arxiv.org/pdf/2312.09748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04012v2","updated":"2024-06-10T09:32:49Z","published":"2024-06-06T12:38:59Z","title":"Theoretical Guarantees for Variational Inference with Fixed-Variance\n  Mixture of Gaussians","summary":"  Variational inference (VI) is a popular approach in Bayesian inference, that\nlooks for the best approximation of the posterior distribution within a\nparametric family, minimizing a loss that is typically the (reverse)\nKullback-Leibler (KL) divergence. Despite its empirical success, the\ntheoretical properties of VI have only received attention recently, and mostly\nwhen the parametric family is the one of Gaussians. This work aims to\ncontribute to the theoretical study of VI in the non-Gaussian case by\ninvestigating the setting of Mixture of Gaussians with fixed covariance and\nconstant weights. In this view, VI over this specific family can be casted as\nthe minimization of a Mollified relative entropy, i.e. the KL between the\nconvolution (with respect to a Gaussian kernel) of an atomic measure supported\non Diracs, and the target distribution. The support of the atomic measure\ncorresponds to the localization of the Gaussian components. Hence, solving\nvariational inference becomes equivalent to optimizing the positions of the\nDiracs (the particles), which can be done through gradient descent and takes\nthe form of an interacting particle system. We study two sources of error of\nvariational inference in this context when optimizing the mollified relative\nentropy. The first one is an optimization result, that is a descent lemma\nestablishing that the algorithm decreases the objective at each iteration. The\nsecond one is an approximation error, that upper bounds the objective between\nan optimal finite mixture and the target distribution.\n","authors":["Tom Huix","Anna Korba","Alain Durmus","Eric Moulines"],"pdf_url":"https://arxiv.org/pdf/2406.04012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11439v2","updated":"2024-06-10T09:29:21Z","published":"2023-10-17T17:50:22Z","title":"From Alexnet to Transformers: Measuring the Non-linearity of Deep Neural\n  Networks with Affine Optimal Transport","summary":"  In the last decade, we have witnessed the introduction of several novel deep\nneural network (DNN) architectures exhibiting ever-increasing performance\nacross diverse tasks. Explaining the upward trend of their performance,\nhowever, remains difficult as different DNN architectures of comparable depth\nand width -- common factors associated with their expressive power -- may\nexhibit a drastically different performance even when trained on the same\ndataset. In this paper, we introduce the concept of the non-linearity signature\nof DNN, the first theoretically sound solution for approximately measuring the\nnon-linearity of deep neural networks. Built upon a score derived from\nclosed-form optimal transport mappings, this signature provides a better\nunderstanding of the inner workings of a wide range of DNN architectures and\nlearning paradigms, with a particular emphasis on the computer vision task. We\nprovide extensive experimental results that highlight the practical usefulness\nof the proposed non-linearity signature and its potential for long-reaching\nimplications. The code for our work is available at\nhttps://github.com/qbouniot/AffScoreDeep\n","authors":["Quentin Bouniot","Ievgen Redko","Anton Mallasto","Charlotte Laclau","Karol Arndt","Oliver Struckmeier","Markus Heinonen","Ville Kyrki","Samuel Kaski"],"pdf_url":"https://arxiv.org/pdf/2310.11439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02722v2","updated":"2024-06-10T09:13:29Z","published":"2024-04-03T13:22:47Z","title":"On-line conformalized neural networks ensembles for probabilistic\n  forecasting of day-ahead electricity prices","summary":"  Probabilistic electricity price forecasting (PEPF) is subject of increasing\ninterest, following the demand for proper quantification of prediction\nuncertainty, to support the operation in complex power markets with increasing\nshare of renewable generation. Distributional neural networks ensembles have\nbeen recently shown to outperform state of the art PEPF benchmarks. Still, they\nrequire critical reliability enhancements, as fail to pass the coverage tests\nat various steps on the prediction horizon. In this work, we propose a novel\napproach to PEPF, extending the state of the art neural networks ensembles\nbased methods through conformal inference based techniques, deployed within an\non-line recalibration procedure. Experiments have been conducted on multiple\nmarket regions, achieving day-ahead forecasts with improved hourly coverage and\nstable probabilistic scores.\n","authors":["Alessandro Brusaferri","Andrea Ballarino","Luigi Grossi","Fabrizio Laurini"],"pdf_url":"https://arxiv.org/pdf/2404.02722v2.pdf","comment":"48 pages"},{"id":"http://arxiv.org/abs/2406.06119v1","updated":"2024-06-10T09:11:30Z","published":"2024-06-10T09:11:30Z","title":"A Survey on Incomplete Multi-label Learning: Recent Advances and Future\n  Trends","summary":"  In reality, data often exhibit associations with multiple labels, making\nmulti-label learning (MLL) become a prominent research topic. The last two\ndecades have witnessed the success of MLL, which is indispensable from complete\nand accurate supervised information. However, obtaining such information in\npractice is always laborious and sometimes even impossible. To circumvent this\ndilemma, incomplete multi-label learning (InMLL) has emerged, aiming to learn\nfrom incomplete labeled data. To date, enormous InMLL works have been proposed\nto narrow the performance gap with complete MLL, whereas a systematic review\nfor InMLL is still absent. In this paper, we not only attempt to fill the\nlacuna but also strive to pave the way for innovative research. Specifically,\nwe retrospect the origin of InMLL, analyze the challenges of InMLL, and make a\ntaxonomy of InMLL from the data-oriented and algorithm-oriented perspectives,\nrespectively. Besides, we also present real applications of InMLL in various\ndomains. More importantly, we highlight several potential future trends,\nincluding four open problems that are more in line with practice and three\nunder-explored/unexplored techniques in addressing the challenges of InMLL,\nwhich may shed new light on developing novel research directions in the field\nof InMLL.\n","authors":["Xiang Li","Jiexi Liu","Xinrui Wang","Songcan Chen"],"pdf_url":"https://arxiv.org/pdf/2406.06119v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2304.08319v3","updated":"2024-06-10T09:09:15Z","published":"2023-04-17T14:39:56Z","title":"Towards Computational Performance Engineering for Unsupervised Concept\n  Drift Detection -- Complexities, Benchmarking, Performance Analysis","summary":"  Concept drift detection is crucial for many AI systems to ensure the system's\nreliability. These systems often have to deal with large amounts of data or\nreact in real-time. Thus, drift detectors must meet computational requirements\nor constraints with a comprehensive performance evaluation. However, so far,\nthe focus of developing drift detectors is on inference quality, e.g. accuracy,\nbut not on computational performance, such as runtime. Many of the previous\nworks consider computational performance only as a secondary objective and do\nnot have a benchmark for such evaluation. Hence, we propose and explain\nperformance engineering for unsupervised concept drift detection that reflects\non computational complexities, benchmarking, and performance analysis. We\nprovide the computational complexities of existing unsupervised drift detectors\nand discuss why further computational performance investigations are required.\nHence, we state and substantiate the aspects of a benchmark for unsupervised\ndrift detection reflecting on inference quality and computational performance.\nFurthermore, we demonstrate performance analysis practices that have proven\ntheir effectiveness in High-Performance Computing, by tracing two drift\ndetectors and displaying their performance data.\n","authors":["Elias Werner","Nishant Kumar","Matthias Lieber","Sunna Torge","Stefan Gumhold","Wolfgang E. Nagel"],"pdf_url":"https://arxiv.org/pdf/2304.08319v3.pdf","comment":"Accepted at 13th International Conference on Data Science, Technology\n  and Applications (DATA). Source code: https://github.com/elwer/Perf_DD"},{"id":"http://arxiv.org/abs/2306.07220v4","updated":"2024-06-10T09:04:11Z","published":"2023-06-12T16:26:38Z","title":"Strokes2Surface: Recovering Curve Networks From 4D Architectural Design\n  Sketches","summary":"  We present Strokes2Surface, an offline geometry reconstruction pipeline that\nrecovers well-connected curve networks from imprecise 4D sketches to bridge\nconcept design and digital modeling stages in architectural design. The input\nto our pipeline consists of 3D strokes' polyline vertices and their timestamps\nas the 4th dimension, along with additional metadata recorded throughout\nsketching. Inspired by architectural sketching practices, our pipeline combines\na classifier and two clustering models to achieve its goal. First, with a set\nof extracted hand-engineered features from the sketch, the classifier\nrecognizes the type of individual strokes between those depicting boundaries\n(Shape strokes) and those depicting enclosed areas (Scribble strokes). Next,\nthe two clustering models parse strokes of each type into distinct groups, each\nrepresenting an individual edge or face of the intended architectural object.\nCurve networks are then formed through topology recovery of consolidated Shape\nclusters and surfaced using Scribble clusters guiding the cycle discovery. Our\nevaluation is threefold: We confirm the usability of the Strokes2Surface\npipeline in architectural design use cases via a user study, we validate our\nchoice of features via statistical analysis and ablation studies on our\ncollected dataset, and we compare our outputs against a range of\nreconstructions computed using alternative methods.\n","authors":["S. Rasoulzadeh","M. Wimmer","P. Stauss","I. Kovacic"],"pdf_url":"https://arxiv.org/pdf/2306.07220v4.pdf","comment":"16 pages, 15 figures"},{"id":"http://arxiv.org/abs/2310.11130v2","updated":"2024-06-10T08:58:42Z","published":"2023-10-17T10:28:00Z","title":"Topological Expressivity of ReLU Neural Networks","summary":"  We study the expressivity of ReLU neural networks in the setting of a binary\nclassification problem from a topological perspective. Recently, empirical\nstudies showed that neural networks operate by changing topology, transforming\na topologically complicated data set into a topologically simpler one as it\npasses through the layers. This topological simplification has been measured by\nBetti numbers, which are algebraic invariants of a topological space. We use\nthe same measure to establish lower and upper bounds on the topological\nsimplification a ReLU neural network can achieve with a given architecture. We\ntherefore contribute to a better understanding of the expressivity of ReLU\nneural networks in the context of binary classification problems by shedding\nlight on their ability to capture the underlying topological structure of the\ndata. In particular the results show that deep ReLU neural networks are\nexponentially more powerful than shallow ones in terms of topological\nsimplification. This provides a mathematically rigorous explanation why deeper\nnetworks are better equipped to handle complex and topologically rich data\nsets.\n","authors":["Ekin Ergen","Moritz Grillo"],"pdf_url":"https://arxiv.org/pdf/2310.11130v2.pdf","comment":"44 pages, to appear in COLT 2024"},{"id":"http://arxiv.org/abs/2402.03781v6","updated":"2024-06-10T08:45:51Z","published":"2024-02-06T07:51:56Z","title":"MolTC: Towards Molecular Relational Modeling In Language Models","summary":"  Molecular Relational Learning (MRL), aiming to understand interactions\nbetween molecular pairs, plays a pivotal role in advancing biochemical\nresearch. Recently, the adoption of large language models (LLMs), known for\ntheir vast knowledge repositories and advanced logical inference capabilities,\nhas emerged as a promising way for efficient and effective MRL. Despite their\npotential, these methods predominantly rely on the textual data, thus not fully\nharnessing the wealth of structural information inherent in molecular graphs.\nMoreover, the absence of a unified framework exacerbates the issue of\ninformation underutilization, as it hinders the sharing of interaction\nmechanism learned across diverse datasets. To address these challenges, this\nwork proposes a novel LLM-based multi-modal framework for Molecular inTeraction\nprediction following Chain-of-Thought (CoT) theory, termed MolTC, which\neffectively integrate graphical information of two molecules in pair. To train\nMolTC efficiently, we introduce a Multi-hierarchical CoT concept to refine its\ntraining paradigm, and conduct a comprehensive Molecular Interactive\nInstructions dataset for the development of biochemical LLMs involving MRL. Our\nexperiments, conducted across various datasets involving over 4,000,000\nmolecular pairs, exhibit the superiority of our method over current GNN and\nLLM-based baselines. Code is available at https://github.com/MangoKiller/MolTC.\n","authors":["Junfeng Fang","Shuai Zhang","Chang Wu","Zhengyi Yang","Zhiyuan Liu","Sihang Li","Kun Wang","Wenjie Du","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.03781v6.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2406.06106v1","updated":"2024-06-10T08:42:48Z","published":"2024-06-10T08:42:48Z","title":"Testably Learning Polynomial Threshold Functions","summary":"  Rubinfeld & Vasilyan recently introduced the framework of testable learning\nas an extension of the classical agnostic model. It relaxes distributional\nassumptions which are difficult to verify by conditions that can be checked\nefficiently by a tester. The tester has to accept whenever the data truly\nsatisfies the original assumptions, and the learner has to succeed whenever the\ntester accepts. We focus on the setting where the tester has to accept standard\nGaussian data. There, it is known that basic concept classes such as halfspaces\ncan be learned testably with the same time complexity as in the\n(distribution-specific) agnostic model. In this work, we ask whether there is a\nprice to pay for testably learning more complex concept classes. In particular,\nwe consider polynomial threshold functions (PTFs), which naturally generalize\nhalfspaces. We show that PTFs of arbitrary constant degree can be testably\nlearned up to excess error $\\varepsilon > 0$ in time\n$n^{\\mathrm{poly}(1/\\varepsilon)}$. This qualitatively matches the best known\nguarantees in the agnostic model. Our results build on a connection between\ntestable learning and fooling. In particular, we show that distributions that\napproximately match at least $\\mathrm{poly}(1/\\varepsilon)$ moments of the\nstandard Gaussian fool constant-degree PTFs (up to error $\\varepsilon$). As a\nsecondary result, we prove that a direct approach to show testable learning\n(without fooling), which was successfully used for halfspaces, cannot work for\nPTFs.\n","authors":["Lucas Slot","Stefan Tiegel","Manuel Wiedmer"],"pdf_url":"https://arxiv.org/pdf/2406.06106v1.pdf","comment":"53 pages"},{"id":"http://arxiv.org/abs/2406.06101v1","updated":"2024-06-10T08:35:01Z","published":"2024-06-10T08:35:01Z","title":"On the Consistency of Kernel Methods with Dependent Observations","summary":"  The consistency of a learning method is usually established under the\nassumption that the observations are a realization of an independent and\nidentically distributed (i.i.d.) or mixing process. Yet, kernel methods such as\nsupport vector machines (SVMs), Gaussian processes, or conditional kernel mean\nembeddings (CKMEs) all give excellent performance under sampling schemes that\nare obviously non-i.i.d., such as when data comes from a dynamical system. We\npropose the new notion of empirical weak convergence (EWC) as a general\nassumption explaining such phenomena for kernel methods. It assumes the\nexistence of a random asymptotic data distribution and is a strict weakening of\nprevious assumptions in the field. Our main results then establish consistency\nof SVMs, kernel mean embeddings, and general Hilbert-space valued empirical\nexpectations with EWC data. Our analysis holds for both finite- and\ninfinite-dimensional outputs, as we extend classical results of statistical\nlearning to the latter case. In particular, it is also applicable to CKMEs.\nOverall, our results open new classes of processes to statistical learning and\ncan serve as a foundation for a theory of learning beyond i.i.d. and mixing.\n","authors":["Pierre-François Massiani","Sebastian Trimpe","Friedrich Solowjow"],"pdf_url":"https://arxiv.org/pdf/2406.06101v1.pdf","comment":"26 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.06099v1","updated":"2024-06-10T08:34:13Z","published":"2024-06-10T08:34:13Z","title":"Sequential Binary Classification for Intrusion Detection in Software\n  Defined Networks","summary":"  Software-Defined Networks (SDN) are the standard architecture for network\ndeployment. Intrusion Detection Systems (IDS) are a pivotal part of this\ntechnology as networks become more vulnerable to new and sophisticated attacks.\nMachine Learning (ML)-based IDS are increasingly seen as the most effective\napproach to handle this issue. However, IDS datasets suffer from high class\nimbalance, which impacts the performance of standard ML models. We propose\nSequential Binary Classification (SBC) - an algorithm for multi-class\nclassification to address this issue. SBC is a hierarchical cascade of base\nclassifiers, each of which can be modelled on any general binary classifier.\nExtensive experiments are reported on benchmark datasets that evaluate the\nperformance of SBC under different scenarios.\n","authors":["Ishan Chokshi","Shrihari Vasudevan","Nachiappan Sundaram","Raaghul Ranganathan"],"pdf_url":"https://arxiv.org/pdf/2406.06099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14857v2","updated":"2024-06-10T08:23:03Z","published":"2024-05-23T17:58:03Z","title":"Semantica: An Adaptable Image-Conditioned Diffusion Model","summary":"  We investigate the task of adapting image generative models to different\ndatasets without finetuneing. To this end, we introduce Semantica, an\nimage-conditioned diffusion model capable of generating images based on the\nsemantics of a conditioning image. Semantica is trained exclusively on\nweb-scale image pairs, that is it receives a random image from a webpage as\nconditional input and models another random image from the same webpage. Our\nexperiments highlight the expressivity of pretrained image encoders and\nnecessity of semantic-based data filtering in achieving high-quality image\ngeneration. Once trained, it can adaptively generate new images from a dataset\nby simply using images from that dataset as input. We study the transfer\nproperties of Semantica on ImageNet, LSUN Churches, LSUN Bedroom and SUN397.\n","authors":["Manoj Kumar","Neil Houlsby","Emiel Hoogeboom"],"pdf_url":"https://arxiv.org/pdf/2405.14857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10008v2","updated":"2024-06-10T08:11:00Z","published":"2023-12-15T18:24:28Z","title":"Movement Primitive Diffusion: Learning Gentle Robotic Manipulation of\n  Deformable Objects","summary":"  Policy learning in robot-assisted surgery (RAS) lacks data efficient and\nversatile methods that exhibit the desired motion quality for delicate surgical\ninterventions. To this end, we introduce Movement Primitive Diffusion (MPD), a\nnovel method for imitation learning (IL) in RAS that focuses on gentle\nmanipulation of deformable objects. The approach combines the versatility of\ndiffusion-based imitation learning (DIL) with the high-quality motion\ngeneration capabilities of Probabilistic Dynamic Movement Primitives (ProDMPs).\nThis combination enables MPD to achieve gentle manipulation of deformable\nobjects, while maintaining data efficiency critical for RAS applications where\ndemonstration data is scarce. We evaluate MPD across various simulated and real\nworld robotic tasks on both state and image observations. MPD outperforms\nstate-of-the-art DIL methods in success rate, motion quality, and data\nefficiency.\n  Project page: https://scheiklp.github.io/movement-primitive-diffusion/\n","authors":["Paul Maria Scheikl","Nicolas Schreiber","Christoph Haas","Niklas Freymuth","Gerhard Neumann","Rudolf Lioutikov","Franziska Mathis-Ullrich"],"pdf_url":"https://arxiv.org/pdf/2312.10008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.01377v2","updated":"2024-06-10T07:57:24Z","published":"2023-05-02T12:53:50Z","title":"Random Function Descent","summary":"  Classical worst-case optimization theory neither explains the success of\noptimization in machine learning, nor does it help with step size selection. We\nestablish a connection between Bayesian Optimization (i.e. average case\noptimization theory) and classical optimization using a 'stochastic Taylor\napproximation' to rediscover gradient descent. This rediscovery yields a step\nsize schedule we call Random Function Descent (RFD), which, in contrast to\nclassical derivations, is scale invariant. Furthermore, our analysis of RFD\nstep sizes yields a theoretical foundation for common step size heuristics such\nas gradient clipping and gradual learning rate warmup. We finally propose a\nstatistical procedure for estimating the RFD step size schedule and validate\nthis theory with a case study on the MNIST dataset.\n","authors":["Felix Benning","Leif Döring"],"pdf_url":"https://arxiv.org/pdf/2305.01377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.14195v2","updated":"2024-06-10T07:56:11Z","published":"2022-10-18T13:11:16Z","title":"Using Deep Learning to Find the Next Unicorn: A Practical Synthesis","summary":"  Startups often represent newly established business models associated with\ndisruptive innovation and high scalability. They are commonly regarded as\npowerful engines for economic and social development. Meanwhile, startups are\nheavily constrained by many factors such as limited financial funding and human\nresources. Therefore, the chance for a startup to eventually succeed is as rare\nas \"spotting a unicorn in the wild\". Venture Capital (VC) strives to identify\nand invest in unicorn startups during their early stages, hoping to gain a high\nreturn. To avoid entirely relying on human domain expertise and intuition,\ninvestors usually employ data-driven approaches to forecast the success\nprobability of startups. Over the past two decades, the industry has gone\nthrough a paradigm shift moving from conventional statistical approaches\ntowards becoming machine-learning (ML) based. Notably, the rapid growth of data\nvolume and variety is quickly ushering in deep learning (DL), a subset of ML,\nas a potentially superior approach in terms of capacity and expressivity. In\nthis work, we carry out a literature review and synthesis on DL-based\napproaches, covering the entire DL life cycle. The objective is a) to obtain a\nthorough and in-depth understanding of the methodologies for startup evaluation\nusing DL, and b) to distil valuable and actionable learning for practitioners.\nTo the best of our knowledge, our work is the first of this kind.\n","authors":["Lele Cao","Vilhelm von Ehrenheim","Sebastian Krakowski","Xiaoxue Li","Alexandra Lutz"],"pdf_url":"https://arxiv.org/pdf/2210.14195v2.pdf","comment":"A condensed version is published by IJCAI 2024 Workshop on FinNLP and\n  Muffin (48 pages, 18 figures). ACL Link:\n  https://aclanthology.org/2023.finnlp-1.6"},{"id":"http://arxiv.org/abs/2406.06081v1","updated":"2024-06-10T07:54:56Z","published":"2024-06-10T07:54:56Z","title":"An Open and Large-Scale Dataset for Multi-Modal Climate Change-aware\n  Crop Yield Predictions","summary":"  Precise crop yield predictions are of national importance for ensuring food\nsecurity and sustainable agricultural practices. While AI-for-science\napproaches have exhibited promising achievements in solving many scientific\nproblems such as drug discovery, precipitation nowcasting, etc., the\ndevelopment of deep learning models for predicting crop yields is constantly\nhindered by the lack of an open and large-scale deep learning-ready dataset\nwith multiple modalities to accommodate sufficient information. To remedy this,\nwe introduce the CropNet dataset, the first terabyte-sized, publicly available,\nand multi-modal dataset specifically targeting climate change-aware crop yield\npredictions for the contiguous United States (U.S.) continent at the county\nlevel. Our CropNet dataset is composed of three modalities of data, i.e.,\nSentinel-2 Imagery, WRF-HRRR Computed Dataset, and USDA Crop Dataset, for over\n2200 U.S. counties spanning 6 years (2017-2022), expected to facilitate\nresearchers in developing versatile deep learning models for timely and\nprecisely predicting crop yields at the county-level, by accounting for the\neffects of both short-term growing season weather variations and long-term\nclimate change on crop yields. Besides, we develop the CropNet package,\noffering three types of APIs, for facilitating researchers in downloading the\nCropNet data on the fly over the time and region of interest, and flexibly\nbuilding their deep learning models for accurate crop yield predictions.\nExtensive experiments have been conducted on our CropNet dataset via employing\nvarious types of deep learning solutions, with the results validating the\ngeneral applicability and the efficacy of the CropNet dataset in climate\nchange-aware crop yield predictions.\n","authors":["Fudong Lin","Kaleb Guillot","Summer Crawford","Yihe Zhang","Xu Yuan","Nian-Feng Tzeng"],"pdf_url":"https://arxiv.org/pdf/2406.06081v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2406.06072v1","updated":"2024-06-10T07:36:24Z","published":"2024-06-10T07:36:24Z","title":"Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor\n  Control","summary":"  Vision Transformers (ViT), when paired with large-scale pretraining, have\nshown remarkable performance across various computer vision tasks, primarily\ndue to their weak inductive bias. However, while such weak inductive bias aids\nin pretraining scalability, this may hinder the effective adaptation of ViTs\nfor visuo-motor control tasks as a result of the absence of control-centric\ninductive biases. Such absent inductive biases include spatial locality and\ntranslation equivariance bias which convolutions naturally offer. To this end,\nwe introduce Convolution Injector (CoIn), an add-on module that injects\nconvolutions which are rich in locality and equivariance biases into a\npretrained ViT for effective adaptation in visuo-motor control. We evaluate\nCoIn with three distinct types of pretrained ViTs (CLIP, MVP, VC-1) across 12\nvaried control tasks within three separate domains (Adroit, MetaWorld, DMC),\nand demonstrate that CoIn consistently enhances control task performance across\nall experimented environments and models, validating the effectiveness of\nproviding pretrained ViTs with control-centric biases.\n","authors":["Dongyoon Hwang","Byungkun Lee","Hojoon Lee","Hyunseung Kim","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.06072v1.pdf","comment":"accepted to ICML 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2406.06465v1","updated":"2024-06-10T17:02:08Z","published":"2024-06-10T17:02:08Z","title":"AID: Adapting Image2Video Diffusion Models for Instruction-guided Video\n  Prediction","summary":"  Text-guided video prediction (TVP) involves predicting the motion of future\nframes from the initial frame according to an instruction, which has wide\napplications in virtual reality, robotics, and content creation. Previous TVP\nmethods make significant breakthroughs by adapting Stable Diffusion for this\ntask. However, they struggle with frame consistency and temporal stability\nprimarily due to the limited scale of video datasets. We observe that\npretrained Image2Video diffusion models possess good priors for video dynamics\nbut they lack textual control. Hence, transferring Image2Video models to\nleverage their video dynamic priors while injecting instruction control to\ngenerate controllable videos is both a meaningful and challenging task. To\nachieve this, we introduce the Multi-Modal Large Language Model (MLLM) to\npredict future video states based on initial frames and text instructions. More\nspecifically, we design a dual query transformer (DQFormer) architecture, which\nintegrates the instructions and frames into the conditional embeddings for\nfuture frame prediction. Additionally, we develop Long-Short Term Temporal\nAdapters and Spatial Adapters that can quickly transfer general video diffusion\nmodels to specific scenarios with minimal training costs. Experimental results\nshow that our method significantly outperforms state-of-the-art techniques on\nfour datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and\nUCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and\nSSv2 respectively, demonstrating its effectiveness in various domains. More\nexamples can be found at our website https://chenhsing.github.io/AID.\n","authors":["Zhen Xing","Qi Dai","Zejia Weng","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.06465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06446v1","updated":"2024-06-10T16:36:02Z","published":"2024-06-10T16:36:02Z","title":"Deep Generative Modeling Reshapes Compression and Transmission: From\n  Efficiency to Resiliency","summary":"  Information theory and machine learning are inextricably linked and have even\nbeen referred to as \"two sides of the same coin\". One particularly elegant\nconnection is the essential equivalence between probabilistic generative\nmodeling and data compression or transmission. In this article, we reveal the\ndual-functionality of deep generative models that reshapes both data\ncompression for efficiency and transmission error concealment for resiliency.\nWe present how the contextual predictive capabilities of powerful generative\nmodels can be well positioned to be strong compressors and estimators. In this\nsense, we advocate for viewing the deep generative modeling problem through the\nlens of end-to-end communications, and evaluate the compression and error\nrestoration capabilities of foundation generative models. We show that the\nkernel of many large generative models is powerful predictor that can capture\ncomplex relationships among semantic latent variables, and the communication\nviewpoints provide novel insights into semantic feature tokenization,\ncontextual learning, and usage of deep generative models. In summary, our\narticle highlights the essential connections of generative AI to source and\nchannel coding techniques, and motivates researchers to make further\nexplorations in this emerging topic.\n","authors":["Jincheng Dai","Xiaoqi Qin","Sixian Wang","Lexi Xu","Kai Niu","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06446v1.pdf","comment":"Publication in IEEE Wireless Communications"},{"id":"http://arxiv.org/abs/2309.07773v3","updated":"2024-06-10T16:08:27Z","published":"2023-09-14T15:02:05Z","title":"Spoken Humanoid Embodied Conversational Agents in Mobile Serious Games:\n  A Usability Assessment","summary":"  This paper presents an empirical investigation of the extent to which spoken\nHumanoid Embodied Conversational Agents (HECAs) can foster usability in mobile\nserious game (MSG) applications. The aim of the research is to assess the\nimpact of multiple agents and illusion of humanness on the quality of the\ninteraction. The experiment investigates two styles of agent presentation: an\nagent of high human-likeness (HECA) and an agent of low human-likeness (text).\nThe purpose of the experiment is to assess whether and how agents of high\nhumanlikeness can evoke the illusion of humanness and affect usability. Agents\nof high human-likeness were designed by following the ECA design model that is\na proposed guide for ECA development. The results of the experiment with 90\nparticipants show that users prefer to interact with the HECAs. The difference\nbetween the two versions is statistically significant with a large effect size\n(d=1.01), with many of the participants justifying their choice by saying that\nthe human-like characteristics of the HECA made the version more appealing.\nThis research provides key information on the potential effect of HECAs on\nserious games, which can provide insight into the design of future mobile\nserious games.\n","authors":["Danai Korre","Judy Robertson"],"pdf_url":"https://arxiv.org/pdf/2309.07773v3.pdf","comment":"46 pages, 9 figures, 14 tables"},{"id":"http://arxiv.org/abs/2406.06048v1","updated":"2024-06-10T06:29:00Z","published":"2024-06-10T06:29:00Z","title":"Robust Latent Representation Tuning for Image-text Classification","summary":"  Large models have demonstrated exceptional generalization capabilities in\ncomputer vision and natural language processing. Recent efforts have focused on\nenhancing these models with multimodal processing abilities. However,\naddressing the challenges posed by scenarios where one modality is absent\nremains a significant hurdle. In response to this issue, we propose a robust\nlatent representation tuning method for large models. Specifically, our\napproach introduces a modality latent translation module to maximize the\ncorrelation between modalities. Following this, a newly designed fusion module\nis employed to facilitate information interaction between the modalities. In\nthis framework, not only are common semantics refined during training, but the\nmethod also yields robust representations in the absence of one modality.\nImportantly, our method maintains the frozen state of the image and text\nfoundation models to preserve their abilities acquired through large-scale\npretraining. We conduct experiments on several public datasets, and the results\nunderscore the effectiveness of our proposed method.\n","authors":["Hao Sun","Yu Song"],"pdf_url":"https://arxiv.org/pdf/2406.06048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.09571v8","updated":"2024-06-10T05:30:38Z","published":"2023-04-19T11:19:10Z","title":"LLIC: Large Receptive Field Transform Coding with Adaptive Weights for\n  Learned Image Compression","summary":"  The effective receptive field (ERF) plays an important role in transform\ncoding, which determines how much redundancy can be removed during transform\nand how many spatial priors can be utilized to synthesize textures during\ninverse transform. Existing methods rely on stacks of small kernels, whose ERFs\nremain insufficiently large, or heavy non-local attention mechanisms, which\nlimit the potential of high-resolution image coding. To tackle this issue, we\npropose Large Receptive Field Transform Coding with Adaptive Weights for\nLearned Image Compression (LLIC). Specifically, for the first time in the\nlearned image compression community, we introduce a few large kernelbased\ndepth-wise convolutions to reduce more redundancy while maintaining modest\ncomplexity. Due to the wide range of image diversity, we further propose a\nmechanism to augment convolution adaptability through the self-conditioned\ngeneration of weights. The large kernels cooperate with non-linear embedding\nand gate mechanisms for better expressiveness and lighter pointwise\ninteractions. Our investigation extends to refined training methods that unlock\nthe full potential of these large kernels. Moreover, to promote more dynamic\ninter-channel interactions, we introduce an adaptive channel-wise bit\nallocation strategy that autonomously generates channel importance factors in a\nself-conditioned manner. To demonstrate the effectiveness of the proposed\ntransform coding, we align the entropy model to compare with existing transform\nmethods and obtain models LLIC-STF, LLIC-ELIC, and LLIC-TCM. Extensive\nexperiments demonstrate that our proposed LLIC models have significant\nimprovements over the corresponding baselines and reduce the BD-Rate by 9.49%,\n9.47%, 10.94% on Kodak over VTM-17.0 Intra, respectively. Our LLIC models\nachieve state-of-the-art performances and better trade-offs between performance\nand complexity.\n","authors":["Wei Jiang","Peirong Ning","Jiayu Yang","Yongqi Zhai","Feng Gao","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2304.09571v8.pdf","comment":"Accepted to IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2403.14468v3","updated":"2024-06-10T18:38:00Z","published":"2024-03-21T15:15:00Z","title":"AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks","summary":"  In the dynamic field of digital content creation using generative models,\nstate-of-the-art video editing models still do not offer the level of quality\nand control that users desire. Previous works on video editing either extended\nfrom image-based generative models in a zero-shot manner or necessitated\nextensive fine-tuning, which can hinder the production of fluid video edits.\nFurthermore, these methods frequently rely on textual input as the editing\nguidance, leading to ambiguities and limiting the types of edits they can\nperform. Recognizing these challenges, we introduce AnyV2V, a novel tuning-free\nparadigm designed to simplify video editing into two primary steps: (1)\nemploying an off-the-shelf image editing model to modify the first frame, (2)\nutilizing an existing image-to-video generation model to generate the edited\nvideo through temporal feature injection. AnyV2V can leverage any existing\nimage editing tools to support an extensive array of video editing tasks,\nincluding prompt-based editing, reference-based style transfer, subject-driven\nediting, and identity manipulation, which were unattainable by previous\nmethods. AnyV2V can also support any video length. Our evaluation indicates\nthat AnyV2V significantly outperforms other baseline methods in automatic and\nhuman evaluations by significant margin, maintaining visual consistency with\nthe source video while achieving high-quality edits across all the editing\ntasks.\n","authors":["Max Ku","Cong Wei","Weiming Ren","Harry Yang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14468v3.pdf","comment":"preprint"}]},"2024-06-09T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2305.01579v3","updated":"2024-06-09T23:42:48Z","published":"2023-05-02T16:28:10Z","title":"Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models\n  against Counterfactual Noise","summary":"  Most existing retrieval-augmented language models (LMs) assume a naive\ndichotomy within a retrieved document set: query-relevance and irrelevance. Our\nwork investigates a more challenging scenario in which even the \"relevant\"\ndocuments may contain misleading or incorrect information, causing conflict\namong the retrieved documents and thereby negatively influencing model\ndecisions as noise. We observe that existing LMs are highly brittle to the\npresence of conflicting information in both the fine-tuning and in-context\nfew-shot learning scenarios. We propose approaches for handling knowledge\nconflicts among retrieved documents by explicitly fine-tuning a discriminator\nor prompting GPT-3.5 to elicit its discriminative capability. Our empirical\nresults on open-domain QA show that these approaches significantly enhance\nmodel robustness. We also provide our findings on incorporating the fine-tuned\ndiscriminator's decision into the in-context learning process, proposing a way\nto exploit the benefits of two disparate learning schemes. Alongside our\nfindings, we provide MacNoise, a machine-generated, conflict-induced dataset to\nfurther encourage research in this direction.\n","authors":["Giwon Hong","Jeonghwan Kim","Junmo Kang","Sung-Hyon Myaeng","Joyce Jiyoung Whang"],"pdf_url":"https://arxiv.org/pdf/2305.01579v3.pdf","comment":"NAACL 2024 (Findings; Long Paper)"},{"id":"http://arxiv.org/abs/2406.05930v1","updated":"2024-06-09T22:46:41Z","published":"2024-06-09T22:46:41Z","title":"Semisupervised Neural Proto-Language Reconstruction","summary":"  Existing work implementing comparative reconstruction of ancestral languages\n(proto-languages) has usually required full supervision. However, historical\nreconstruction models are only of practical value if they can be trained with a\nlimited amount of labeled data. We propose a semisupervised historical\nreconstruction task in which the model is trained on only a small amount of\nlabeled data (cognate sets with proto-forms) and a large amount of unlabeled\ndata (cognate sets without proto-forms). We propose a neural architecture for\ncomparative reconstruction (DPD-BiReconstructor) incorporating an essential\ninsight from linguists' comparative method: that reconstructed words should not\nonly be reconstructable from their daughter words, but also deterministically\ntransformable back into their daughter words. We show that this architecture is\nable to leverage unlabeled cognate sets to outperform strong semisupervised\nbaselines on this novel task.\n","authors":["Liang Lu","Peirong Xie","David R. Mortensen"],"pdf_url":"https://arxiv.org/pdf/2406.05930v1.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2403.11346v3","updated":"2024-06-09T22:10:04Z","published":"2024-03-17T21:16:17Z","title":"CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using\n  Synthetic Back-Translation Data","summary":"  Neural Machine Translation (NMT) for low-resource languages is still a\nchallenging task in front of NLP researchers. In this work, we deploy a\nstandard data augmentation methodology by back-translation to a new language\ntranslation direction Cantonese-to-English. We present the models we fine-tuned\nusing the limited amount of real data and the synthetic data we generated using\nback-translation including OpusMT, NLLB, and mBART. We carried out automatic\nevaluation using a range of different metrics including lexical-based and\nembedding-based. Furthermore. we create a user-friendly interface for the\nmodels we included in this\\textsc{ CantonMT} research project and make it\navailable to facilitate Cantonese-to-English MT research. Researchers can add\nmore models into this platform via our open-source\\textsc{ CantonMT} toolkit\n\\url{https://github.com/kenrickkung/CantoneseTranslation}.\n","authors":["Kung Yin Hong","Lifeng Han","Riza Batista-Navarro","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2403.11346v3.pdf","comment":"Accepted by: The 25th Annual Conference of The European Association\n  for Machine Translation, 24 - 27 June 2024, Sheffield, UK (forthcoming)"},{"id":"http://arxiv.org/abs/2405.16969v4","updated":"2024-06-09T22:03:49Z","published":"2024-05-27T09:06:24Z","title":"The Multi-Range Theory of Translation Quality Measurement: MQM scoring\n  models and Statistical Quality Control","summary":"  The year 2024 marks the 10th anniversary of the Multidimensional Quality\nMetrics (MQM) framework for analytic translation quality evaluation. The MQM\nerror typology has been widely used by practitioners in the translation and\nlocalization industry and has served as the basis for many derivative projects.\nThe annual Conference on Machine Translation (WMT) shared tasks on both human\nand automatic translation quality evaluations used the MQM error typology.\n  The metric stands on two pillars: error typology and the scoring model. The\nscoring model calculates the quality score from annotation data, detailing how\nto convert error type and severity counts into numeric scores to determine if\nthe content meets specifications. Previously, only the raw scoring model had\nbeen published. This April, the MQM Council published the Linear Calibrated\nScoring Model, officially presented herein, along with the Non-Linear Scoring\nModel, which had not been published before.\n  This paper details the latest MQM developments and presents a universal\napproach to translation quality measurement across three sample size ranges. It\nalso explains why Statistical Quality Control should be used for very small\nsample sizes, starting from a single sentence.\n","authors":["Arle Lommel","Serge Gladkoff","Alan Melby","Sue Ellen Wright","Ingemar Strandvik","Katerina Gasova","Angelika Vaasa","Andy Benzo","Romina Marazzato Sparano","Monica Foresi","Johani Innis","Lifeng Han","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2405.16969v4.pdf","comment":"working paper, 20 pages, under-review"},{"id":"http://arxiv.org/abs/2406.05925v1","updated":"2024-06-09T21:58:32Z","published":"2024-06-09T21:58:32Z","title":"Hello Again! LLM-powered Personalized Agent for Long-term Dialogue","summary":"  Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent.\n","authors":["Hao Li","Chenghao Yang","An Zhang","Yang Deng","Xiang Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2406.05925v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.15269v2","updated":"2024-06-09T21:45:09Z","published":"2024-04-23T17:57:47Z","title":"Aligning LLM Agents by Learning Latent Preference from User Edits","summary":"  We study interactive learning of LLM-based language agents based on user\nedits made to the agent's output. In a typical setting such as writing\nassistants, the user interacts with a language agent to generate a response\ngiven a context, and may optionally edit the agent response to personalize it\nbased on their latent preference, in addition to improving the correctness. The\nedit feedback is naturally generated, making it a suitable candidate for\nimproving the agent's alignment with the user's preference, and for reducing\nthe cost of user edits over time. We propose a learning framework, PRELUDE that\ninfers a description of the user's latent preference based on historic edit\ndata. The inferred user preference descriptions are used to define prompts for\ngenerating responses in the future. This avoids fine-tuning the agent, which is\ncostly, challenging to scale with the number of users, and may even degrade its\nperformance on other tasks. Furthermore, learning descriptive preference\nimproves interpretability, allowing the user to view and modify the learned\npreference. However, user preference can be complex, subtle, and vary based on\ncontext, making it challenging to learn. To address this, we propose a simple\nyet effective algorithm named CIPHER that leverages the LLM to infer the user\npreference for a given context based on user edits. In the future, CIPHER\nretrieves inferred preferences from the k-closest contexts in the history, and\nforms an aggregate preference for response generation. We introduce two\ninteractive environments -- summarization and email writing, and use a GPT-4\nsimulated user for evaluation. On both tasks, CIPHER outperforms several\nbaselines by achieving the lowest edit distance cost while only having a small\noverhead in LLM query cost. Our analysis reports that user preferences learned\nby CIPHER show significant similarity to the ground truth latent preferences.\n","authors":["Ge Gao","Alexey Taymanov","Eduardo Salinas","Paul Mineiro","Dipendra Misra"],"pdf_url":"https://arxiv.org/pdf/2404.15269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05918v1","updated":"2024-06-09T21:12:15Z","published":"2024-06-09T21:12:15Z","title":"Why Don't Prompt-Based Fairness Metrics Correlate?","summary":"  The widespread use of large language models has brought up essential\nquestions about the potential biases these models might learn. This led to the\ndevelopment of several metrics aimed at evaluating and mitigating these biases.\nIn this paper, we first demonstrate that prompt-based fairness metrics exhibit\npoor agreement, as measured by correlation, raising important questions about\nthe reliability of fairness assessment using prompts. Then, we outline six\nrelevant reasons why such a low correlation is observed across existing\nmetrics. Based on these insights, we propose a method called Correlated\nFairness Output (CAIRO) to enhance the correlation between fairness metrics.\nCAIRO augments the original prompts of a given fairness metric by using several\npre-trained language models and then selects the combination of the augmented\nprompts that achieves the highest correlation across metrics. We show a\nsignificant improvement in Pearson correlation from 0.3 and 0.18 to 0.90 and\n0.98 across metrics for gender and religion biases, respectively. Our code is\navailable at https://github.com/chandar-lab/CAIRO.\n","authors":["Abdelrahman Zayed","Goncalo Mordido","Ioana Baldini","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2406.05918v1.pdf","comment":"In Proceedings of ACL main 2024"},{"id":"http://arxiv.org/abs/2402.14973v2","updated":"2024-06-09T21:10:34Z","published":"2024-02-22T21:22:04Z","title":"Introducing GenCeption for Multimodal LLM Benchmarking: You May Bypass\n  Annotations","summary":"  Multimodal Large Language Models (MLLMs) are commonly evaluated using costly\nannotated multimodal benchmarks. However, these benchmarks often struggle to\nkeep pace with the rapidly advancing requirements of MLLM evaluation. We\npropose GenCeption, a novel and annotation-free MLLM evaluation framework that\nmerely requires unimodal data to assess inter-modality semantic coherence and\ninversely reflects the models' inclination to hallucinate. Analogous to the\npopular DrawCeption game, GenCeption initiates with a non-textual sample and\nundergoes a series of iterative description and generation steps. Semantic\ndrift across iterations is quantified using the GC@T metric. Our empirical\nfindings validate GenCeption's efficacy, showing strong correlations with\npopular MLLM benchmarking results. GenCeption may be extended to mitigate\ntraining data contamination by utilizing ubiquitous, previously unseen unimodal\ndata.\n","authors":["Lele Cao","Valentin Buchner","Zineb Senane","Fangkai Yang"],"pdf_url":"https://arxiv.org/pdf/2402.14973v2.pdf","comment":"Accepted by the 4th Workshop on TrustNLP (Trustworthy Natural\n  Language Processing) @ NAACL2024. Source code:\n  https://github.com/llcresearch/GenCeption. Leaderboard:\n  https://huggingface.co/spaces/valbuc/GenCeption"},{"id":"http://arxiv.org/abs/2406.05906v1","updated":"2024-06-09T20:18:58Z","published":"2024-06-09T20:18:58Z","title":"TTM-RE: Memory-Augmented Document-Level Relation Extraction","summary":"  Document-level relation extraction aims to categorize the association between\nany two entities within a document. We find that previous methods for\ndocument-level relation extraction are ineffective in exploiting the full\npotential of large amounts of training data with varied noise levels. For\nexample, in the ReDocRED benchmark dataset, state-of-the-art methods trained on\nthe large-scale, lower-quality, distantly supervised training data generally do\nnot perform better than those trained solely on the smaller, high-quality,\nhuman-annotated training data. To unlock the full potential of large-scale\nnoisy training data for document-level relation extraction, we propose TTM-RE,\na novel approach that integrates a trainable memory module, known as the Token\nTuring Machine, with a noisy-robust loss function that accounts for the\npositive-unlabeled setting. Extensive experiments on ReDocRED, a benchmark\ndataset for document-level relation extraction, reveal that TTM-RE achieves\nstate-of-the-art performance (with an absolute F1 score improvement of over\n3%). Ablation studies further illustrate the superiority of TTM-RE in other\ndomains (the ChemDisGene dataset in the biomedical domain) and under highly\nunlabeled settings.\n","authors":["Chufan Gao","Xuan Wang","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2406.05906v1.pdf","comment":"Accepted in ACL 2024 Main"},{"id":"http://arxiv.org/abs/2312.12141v3","updated":"2024-06-09T20:03:02Z","published":"2023-12-19T13:23:18Z","title":"Neuron-Level Knowledge Attribution in Large Language Models","summary":"  Identifying important neurons for final predictions is essential for\nunderstanding the mechanisms of large language models. Due to computational\nconstraints, current attribution techniques struggle to operate at neuron\nlevel. In this paper, we propose a static method for pinpointing significant\nneurons for different outputs. Compared to seven other methods, our approach\ndemonstrates superior performance across three metrics. Additionally, since\nmost static methods typically only identify \"value neurons\" directly\ncontributing to the final prediction, we introduce a static method for\nidentifying \"query neurons\" which activate these \"value neurons\". Finally, we\napply our methods to analyze the localization of six distinct types of\nknowledge across both attention and feed-forward network (FFN) layers. Our\nmethod and analysis are helpful for understanding the mechanisms of knowledge\nstorage and set the stage for future research in knowledge editing. We will\nrelease our data and code on github.\n","authors":["Zeping Yu","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2312.12141v3.pdf","comment":"Preprint (code and data will be released in final version). Update\n  version of \"Locating Factual Knowledge in Large Language Models: Exploring\n  the Residual Stream and Analyzing Subvalues in Vocabulary Space\""},{"id":"http://arxiv.org/abs/2402.16797v2","updated":"2024-06-09T19:47:15Z","published":"2024-02-26T18:10:56Z","title":"Set the Clock: Temporal Alignment of Pretrained Language Models","summary":"  Language models (LMs) are trained on web text originating from many points in\ntime and, in general, without any explicit temporal grounding. This work\ninvestigates the temporal chaos of pretrained LMs and explores various methods\nto align their internal knowledge to a target time, which we call \"temporal\nalignment.\" To do this, we first automatically construct a dataset containing\n20K time-sensitive questions and their answers for each year from 2000 to 2023.\nBased on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2),\ndespite having a recent pretraining cutoff (e.g., 2022), mostly answer\nquestions using earlier knowledge (e.g., in 2019). We then develop several\nmethods, from prompting to finetuning, to align LMs to use their most recent\nknowledge when answering questions, and investigate various factors in this\nalignment. Our experiments demonstrate that aligning LLaMa2 to the year 2022\ncan enhance its performance by up to 62% according to that year's answers. This\nimprovement occurs even without explicitly mentioning time information,\nindicating the possibility of aligning models' internal sense of time after\npretraining. Finally, we find that alignment to a historical time is also\npossible, with up to 2.8$\\times$ the performance of the unaligned LM in 2010 if\nfinetuning models to that year. These findings hint at the sophistication of\nLMs' internal knowledge organization and the necessity of tuning them properly.\n","authors":["Bowen Zhao","Zander Brumbaugh","Yizhong Wang","Hannaneh Hajishirzi","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2402.16797v2.pdf","comment":"Accepted as Findings of ACL 2024. Our code and data is available at\n  https://github.com/yizhongw/llm-temporal-alignment"},{"id":"http://arxiv.org/abs/2406.05902v1","updated":"2024-06-09T19:42:25Z","published":"2024-06-09T19:42:25Z","title":"Whose Preferences? Differences in Fairness Preferences and Their Impact\n  on the Fairness of AI Utilizing Human Feedback","summary":"  There is a growing body of work on learning from human feedback to align\nvarious aspects of machine learning systems with human values and preferences.\nWe consider the setting of fairness in content moderation, in which human\nfeedback is used to determine how two comments -- referencing different\nsensitive attribute groups -- should be treated in comparison to one another.\nWith a novel dataset collected from Prolific and MTurk, we find significant\ngaps in fairness preferences depending on the race, age, political stance,\neducational level, and LGBTQ+ identity of annotators. We also demonstrate that\ndemographics mentioned in text have a strong influence on how users perceive\nindividual fairness in moderation. Further, we find that differences also exist\nin downstream classifiers trained to predict human preferences. Finally, we\nobserve that an ensemble, giving equal weight to classifiers trained on\nannotations from different demographics, performs better for different\ndemographic intersections; compared to a single classifier that gives equal\nweight to each annotation.\n","authors":["Emilia Agis Lerner","Florian E. Dorner","Elliott Ash","Naman Goel"],"pdf_url":"https://arxiv.org/pdf/2406.05902v1.pdf","comment":"To appear in the Proceedings of the 62nd Annual Meeting of the\n  Association for Computational Linguistics, ACL 2024"},{"id":"http://arxiv.org/abs/2406.05888v1","updated":"2024-06-09T19:08:33Z","published":"2024-06-09T19:08:33Z","title":"Feriji: A French-Zarma Parallel Corpus, Glossary & Translator","summary":"  Machine translation (MT) is a rapidly expanding field that has experienced\nsignificant advancements in recent years with the development of models capable\nof translating multiple languages with remarkable accuracy. However, the\nrepresentation of African languages in this field still needs to improve due to\nlinguistic complexities and limited resources. This applies to the Zarma\nlanguage, a dialect of Songhay (of the Nilo-Saharan language family) spoken by\nover 5 million people across Niger and neighboring countries\n\\cite{lewis2016ethnologue}. This paper introduces Feriji, the first robust\nFrench-Zarma parallel corpus and glossary designed for MT. The corpus,\ncontaining 61,085 sentences in Zarma and 42,789 in French, and a glossary of\n4,062 words represent a significant step in addressing the need for more\nresources for Zarma. We fine-tune three large language models on our dataset,\nobtaining a BLEU score of 30.06 on the best-performing model. We further\nevaluate the models on human judgments of fluency, comprehension, and\nreadability and the importance and impact of the corpus and models. Our\ncontributions help to bridge a significant language gap and promote an\nessential and overlooked indigenous African language.\n","authors":["Mamadou K. Keita","Elysabhete Amadou Ibrahim","Habibatou Abdoulaye Alfari","Christopher Homan"],"pdf_url":"https://arxiv.org/pdf/2406.05888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07767v2","updated":"2024-06-09T18:48:06Z","published":"2024-02-12T16:30:41Z","title":"Text Detoxification as Style Transfer in English and Hindi","summary":"  This paper focuses on text detoxification, i.e., automatically converting\ntoxic text into non-toxic text. This task contributes to safer and more\nrespectful online communication and can be considered a Text Style Transfer\n(TST) task, where the text style changes while its content is preserved. We\npresent three approaches: knowledge transfer from a similar task, multi-task\nlearning approach, combining sequence-to-sequence modeling with various\ntoxicity classification tasks, and delete and reconstruct approach. To support\nour research, we utilize a dataset provided by Dementieva et al.(2021), which\ncontains multiple versions of detoxified texts corresponding to toxic texts. In\nour experiments, we selected the best variants through expert human annotators,\ncreating a dataset where each toxic sentence is paired with a single,\nappropriate detoxified version. Additionally, we introduced a small Hindi\nparallel dataset, aligning with a part of the English dataset, suitable for\nevaluation purposes. Our results demonstrate that our approach effectively\nbalances text detoxication while preserving the actual content and maintaining\nfluency.\n","authors":["Sourabrata Mukherjee","Akanksha Bansal","Atul Kr. Ojha","John P. McCrae","Ondřej Dušek"],"pdf_url":"https://arxiv.org/pdf/2402.07767v2.pdf","comment":"Accepted and presented at the 20th International Conference on\n  Natural Language Processing (ICON-2023) during December 14-17, 2023"},{"id":"http://arxiv.org/abs/2405.20805v2","updated":"2024-06-09T18:46:48Z","published":"2024-05-31T14:05:27Z","title":"Multilingual Text Style Transfer: Datasets & Models for Indian Languages","summary":"  Text style transfer (TST) involves altering the linguistic style of a text\nwhile preserving its core content. This paper focuses on sentiment transfer, a\nvital TST subtask (Mukherjee et al., 2022a), across a spectrum of Indian\nlanguages: Hindi, Magahi, Malayalam, Marathi, Punjabi, Odia, Telugu, and Urdu,\nexpanding upon previous work on English-Bangla sentiment transfer (Mukherjee et\nal., 2023). We introduce dedicated datasets of 1,000 positive and 1,000\nnegative style-parallel sentences for each of these eight languages. We then\nevaluate the performance of various benchmark models categorized into parallel,\nnon-parallel, cross-lingual, and shared learning approaches, including the\nLlama2 and GPT-3.5 large language models (LLMs). Our experiments highlight the\nsignificance of parallel data in TST and demonstrate the effectiveness of the\nMasked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel\ntechniques. Moreover, cross-lingual and joint multilingual learning methods\nshow promise, offering insights into selecting optimal models tailored to the\nspecific language and task requirements. To the best of our knowledge, this\nwork represents the first comprehensive exploration of the TST task as\nsentiment transfer across a diverse set of languages.\n","authors":["Sourabrata Mukherjee","Atul Kr. Ojha","Akanksha Bansal","Deepak Alok","John P. McCrae","Ondřej Dušek"],"pdf_url":"https://arxiv.org/pdf/2405.20805v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05885v1","updated":"2024-06-09T18:45:41Z","published":"2024-06-09T18:45:41Z","title":"Are Large Language Models Actually Good at Text Style Transfer?","summary":"  We analyze the performance of large language models (LLMs) on Text Style\nTransfer (TST), specifically focusing on sentiment transfer and text\ndetoxification across three languages: English, Hindi, and Bengali. Text Style\nTransfer involves modifying the linguistic style of a text while preserving its\ncore content. We evaluate the capabilities of pre-trained LLMs using zero-shot\nand few-shot prompting as well as parameter-efficient finetuning on publicly\navailable datasets. Our evaluation using automatic metrics, GPT-4 and human\nevaluations reveals that while some prompted LLMs perform well in English,\ntheir performance in on other languages (Hindi, Bengali) remains average.\nHowever, finetuning significantly improves results compared to zero-shot and\nfew-shot prompting, making them comparable to previous state-of-the-art. This\nunderscores the necessity of dedicated datasets and specialized models for\neffective TST.\n","authors":["Sourabrata Mukherjee","Atul Kr. Ojha","Ondřej Dušek"],"pdf_url":"https://arxiv.org/pdf/2406.05885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05881v1","updated":"2024-06-09T18:40:24Z","published":"2024-06-09T18:40:24Z","title":"LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical\n  Reinforcement Learning","summary":"  Developing interactive systems that leverage natural language instructions to\nsolve complex robotic control tasks has been a long-desired goal in the\nrobotics community. Large Language Models (LLMs) have demonstrated exceptional\nabilities in handling complex tasks, including logical reasoning, in-context\nlearning, and code generation. However, predicting low-level robotic actions\nusing LLMs poses significant challenges. Additionally, the complexity of such\ntasks usually demands the acquisition of policies to execute diverse subtasks\nand combine them to attain the ultimate objective. Hierarchical Reinforcement\nLearning (HRL) is an elegant approach for solving such tasks, which provides\nthe intuitive benefits of temporal abstraction and improved exploration.\nHowever, HRL faces the recurring issue of non-stationarity due to unstable\nlower primitive behaviour. In this work, we propose LGR2, a novel HRL framework\nthat leverages language instructions to generate a stationary reward function\nfor the higher-level policy. Since the language-guided reward is unaffected by\nthe lower primitive behaviour, LGR2 mitigates non-stationarity and is thus an\nelegant method for leveraging language instructions to solve robotic control\ntasks. To analyze the efficacy of our approach, we perform empirical analysis\nand demonstrate that LGR2 effectively alleviates non-stationarity in HRL. Our\napproach attains success rates exceeding 70$\\%$ in challenging, sparse-reward\nrobotic navigation and manipulation environments where the baselines fail to\nachieve any significant progress. Additionally, we conduct real-world robotic\nmanipulation experiments and demonstrate that CRISP shows impressive\ngeneralization in real-world scenarios.\n","authors":["Utsav Singh","Pramit Bhattacharyya","Vinay P. Namboodiri"],"pdf_url":"https://arxiv.org/pdf/2406.05881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10707v2","updated":"2024-06-09T18:22:33Z","published":"2023-10-16T16:18:55Z","title":"Demonstrations Are All You Need: Advancing Offensive Content\n  Paraphrasing using In-Context Learning","summary":"  Paraphrasing of offensive content is a better alternative to content removal\nand helps improve civility in a communication environment. Supervised\nparaphrasers; however, rely heavily on large quantities of labelled data to\nhelp preserve meaning and intent. They also often retain a large portion of the\noffensiveness of the original content, which raises questions on their overall\nusability. In this paper we aim to assist practitioners in developing usable\nparaphrasers by exploring In-Context Learning (ICL) with large language models\n(LLMs), i.e., using a limited number of input-label demonstration pairs to\nguide the model in generating desired outputs for specific queries. Our study\nfocuses on key factors such as - number and order of demonstrations, exclusion\nof prompt instruction, and reduction in measured toxicity. We perform\nprincipled evaluation on three datasets, including our proposed Context-Aware\nPolite Paraphrase (CAPP) dataset, comprising of dialogue-style rude utterances,\npolite paraphrases, and additional dialogue context. We evaluate our approach\nusing four closed source and one open source LLM. Our results reveal that ICL\nis comparable to supervised methods in generation quality, while being\nqualitatively better by 25% on human evaluation and attaining lower toxicity by\n76%. Also, ICL-based paraphrasers only show a slight reduction in performance\neven with just 10% training data.\n","authors":["Anirudh Som","Karan Sikka","Helen Gent","Ajay Divakaran","Andreas Kathol","Dimitra Vergyri"],"pdf_url":"https://arxiv.org/pdf/2310.10707v2.pdf","comment":"Accepted in Association for Computational Linguistics (ACL) 2024\n  Findings"},{"id":"http://arxiv.org/abs/2406.05876v1","updated":"2024-06-09T18:13:36Z","published":"2024-06-09T18:13:36Z","title":"Zero-Shot End-To-End Spoken Question Answering In Medical Domain","summary":"  In the rapidly evolving landscape of spoken question-answering (SQA), the\nintegration of large language models (LLMs) has emerged as a transformative\ndevelopment. Conventional approaches often entail the use of separate models\nfor question audio transcription and answer selection, resulting in significant\nresource utilization and error accumulation. To tackle these challenges, we\nexplore the effectiveness of end-to-end (E2E) methodologies for SQA in the\nmedical domain. Our study introduces a novel zero-shot SQA approach, compared\nto traditional cascade systems. Through a comprehensive evaluation conducted on\na new open benchmark of 8 medical tasks and 48 hours of synthetic audio, we\ndemonstrate that our approach requires up to 14.7 times fewer resources than a\ncombined 1.3B parameters LLM with a 1.55B parameters ASR model while improving\naverage accuracy by 0.5\\%. These findings underscore the potential of E2E\nmethodologies for SQA in resource-constrained contexts.\n","authors":["Yanis Labrak","Adel Moumen","Richard Dufour","Mickael Rouvier"],"pdf_url":"https://arxiv.org/pdf/2406.05876v1.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.05872v1","updated":"2024-06-09T18:07:47Z","published":"2024-06-09T18:07:47Z","title":"STARLING: Self-supervised Training of Text-based Reinforcement Learning\n  Agent with Large Language Models","summary":"  Interactive fiction games have emerged as an important application to improve\nthe generalization capabilities of language-based reinforcement learning (RL)\nagents. Existing environments for interactive fiction games are domain-specific\nor time-consuming to generate and do not train the RL agents to master a\nspecific set of skills. In this work, we introduce an interactive environment\nfor self-supervised RL, STARLING, for text-based games that bootstraps the\ntext-based RL agents with automatically generated games (based on the seed set\nof game ideas) to boost the performance and generalization capabilities to\nreach a goal of the target environment. These games let the agent hone their\nskills on a predefined set of tasks. We create and test an environment with 100\ngames, generated using this automated framework that uses large language models\n(GPT-3) and an interactive fiction game engine (based on Inform7) to provide\nthe user with the ability to generate more games under minimal human\nsupervision. Experimental results based on both the human participants and\nbaseline text-based RL agents reveal that current state-of-the-art text-based\nRL agents cannot use previously learned skills in new situations at the level\nhumans can. These results enforce STARLING's potential to serve as a sandbox\nenvironment for further research in self-supervised text-based RL.\n","authors":["Shreyas Basavatia","Keerthiram Murugesan","Shivam Ratnakar"],"pdf_url":"https://arxiv.org/pdf/2406.05872v1.pdf","comment":"ACL 2024 (Findings)"},{"id":"http://arxiv.org/abs/2405.05894v2","updated":"2024-06-09T17:56:11Z","published":"2024-05-09T16:45:27Z","title":"Efficient LLM Comparative Assessment: a Product of Experts Framework for\n  Pairwise Comparisons","summary":"  LLM-as-a-judge approaches are a practical and effective way of assessing a\nrange of text tasks, aligning with human judgements especially when applied in\na comparative assessment fashion. However, when using pairwise comparisons to\nrank a set of candidates the computational costs scale quadratically with the\nnumber of candidates, which can have practical limitations. This paper\nintroduces a Product of Expert (PoE) framework for efficient LLM Comparative\nAssessment. Here individual comparisons are considered experts that provide\ninformation on a pair's score difference. The PoE framework combines the\ninformation from these experts to yield an expression that can be maximized\nwith respect to the underlying set of candidates, and is highly flexible where\nany form of expert can be assumed. When Gaussian experts are used one can\nderive simple closed-form solutions for the optimal candidate ranking, as well\nas expressions for selecting which comparisons should be made to maximize the\nprobability of this ranking. Our approach enables efficient comparative\nassessment, where by using only a small subset of the possible comparisons, one\ncan generate score predictions that correlate as well to human judgements as\nthe predictions when all comparisons are used. We evaluate the approach on\nmultiple NLG tasks and demonstrate that our framework can yield considerable\ncomputational savings when performing pairwise comparative assessment. When N\nis large, with as few as 2% of comparisons the PoE solution can achieve similar\nperformance to when all comparisons are used.\n","authors":["Adian Liusie","Vatsal Raina","Yassir Fathullah","Mark Gales"],"pdf_url":"https://arxiv.org/pdf/2405.05894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05870v1","updated":"2024-06-09T17:55:55Z","published":"2024-06-09T17:55:55Z","title":"Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents","summary":"  Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database, then generating an answer by\napplying an LLM to the retrieved documents.\n  We demonstrate that RAG systems that operate on databases with potentially\nuntrusted content are vulnerable to a new class of denial-of-service attacks we\ncall jamming. An adversary can add a single ``blocker'' document to the\ndatabase that will be retrieved in response to a specific query and,\nfurthermore, result in the RAG system not answering the query - ostensibly\nbecause it lacks the information or because the answer is unsafe.\n  We describe and analyze several methods for generating blocker documents,\nincluding a new method based on black-box optimization that does not require\nthe adversary to know the embedding or LLM used by the target RAG system, nor\naccess to an auxiliary LLM to generate blocker documents. We measure the\nefficacy of the considered methods against several LLMs and embeddings, and\ndemonstrate that the existing safety metrics for LLMs do not capture their\nvulnerability to jamming. We then discuss defenses against blocker documents.\n","authors":["Avital Shafran","Roei Schuster","Vitaly Shmatikov"],"pdf_url":"https://arxiv.org/pdf/2406.05870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11100v2","updated":"2024-06-09T17:55:05Z","published":"2024-02-16T22:12:53Z","title":"When LLMs Meet Cunning Texts: A Fallacy Understanding Benchmark for\n  Large Language Models","summary":"  Recently, Large Language Models (LLMs) make remarkable evolutions in language\nunderstanding and generation. Following this, various benchmarks for measuring\nall kinds of capabilities of LLMs have sprung up. In this paper, we challenge\nthe reasoning and understanding abilities of LLMs by proposing a FaLlacy\nUnderstanding Benchmark (FLUB) containing cunning texts that are easy for\nhumans to understand but difficult for models to grasp. Specifically, the\ncunning texts that FLUB focuses on mainly consist of the tricky, humorous, and\nmisleading texts collected from the real internet environment. And we design\nthree tasks with increasing difficulty in the FLUB benchmark to evaluate the\nfallacy understanding ability of LLMs. Based on FLUB, we investigate the\nperformance of multiple representative and advanced LLMs, reflecting our FLUB\nis challenging and worthy of more future study. Interesting discoveries and\nvaluable insights are achieved in our extensive experiments and detailed\nanalyses. We hope that our benchmark can encourage the community to improve\nLLMs' ability to understand fallacies. Our data and codes are available at\nhttps://github.com/THUKElab/FLUB.\n","authors":["Yinghui Li","Qingyu Zhou","Yuanzhen Luo","Shirong Ma","Yangning Li","Hai-Tao Zheng","Xuming Hu","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2402.11100v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05862v1","updated":"2024-06-09T17:25:47Z","published":"2024-06-09T17:25:47Z","title":"II-Bench: An Image Implication Understanding Benchmark for Multimodal\n  Large Language Models","summary":"  The rapid advancements in the development of multimodal large language models\n(MLLMs) have consistently led to new breakthroughs on various benchmarks. In\nresponse, numerous challenging and comprehensive benchmarks have been proposed\nto more accurately assess the capabilities of MLLMs. However, there is a dearth\nof exploration of the higher-order perceptual capabilities of MLLMs. To fill\nthis gap, we propose the Image Implication understanding Benchmark, II-Bench,\nwhich aims to evaluate the model's higher-order perception of images. Through\nextensive experiments on II-Bench across multiple MLLMs, we have made\nsignificant findings. Initially, a substantial gap is observed between the\nperformance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs\nattains 74.8%, whereas human accuracy averages 90%, peaking at an impressive\n98%. Subsequently, MLLMs perform worse on abstract and complex images,\nsuggesting limitations in their ability to understand high-level semantics and\ncapture image details. Finally, it is observed that most models exhibit\nenhanced accuracy when image sentiment polarity hints are incorporated into the\nprompts. This observation underscores a notable deficiency in their inherent\nunderstanding of image sentiment. We believe that II-Bench will inspire the\ncommunity to develop the next generation of MLLMs, advancing the journey\ntowards expert artificial general intelligence (AGI). II-Bench is publicly\navailable at https://huggingface.co/datasets/m-a-p/II-Bench.\n","authors":["Ziqiang Liu","Feiteng Fang","Xi Feng","Xinrun Du","Chenhao Zhang","Zekun Wang","Yuelin Bai","Qixuan Zhao","Liyang Fan","Chengguang Gan","Hongquan Lin","Jiaming Li","Yuansheng Ni","Haihong Wu","Yaswanth Narsupalli","Zhigang Zheng","Chengming Li","Xiping Hu","Ruifeng Xu","Xiaojun Chen","Min Yang","Jiaheng Liu","Ruibo Liu","Wenhao Huang","Ge Zhang","Shiwen Ni"],"pdf_url":"https://arxiv.org/pdf/2406.05862v1.pdf","comment":"100 pages, 82 figures"},{"id":"http://arxiv.org/abs/2307.03042v3","updated":"2024-06-09T17:00:36Z","published":"2023-07-06T15:06:41Z","title":"Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain","summary":"  Adapting pretrained language models to novel domains, such as clinical\napplications, traditionally involves retraining their entire set of parameters.\nParameter-Efficient Fine-Tuning (PEFT) techniques for fine-tuning language\nmodels significantly reduce computational requirements by selectively\nfine-tuning small subsets of parameters. In this study, we propose a two-step\nPEFT framework and evaluate it in the clinical domain. Our approach combines a\nspecialised PEFT adapter layer designed for clinical domain adaptation with\nanother adapter specialised for downstream tasks. We evaluate the framework on\nmultiple clinical outcome prediction datasets, comparing it to clinically\ntrained language models. Our framework achieves a better AUROC score averaged\nacross all clinical downstream tasks compared to clinical language models. In\nparticular, we observe large improvements of 4-5% AUROC in large-scale\nmultilabel classification tasks, such as diagnoses and procedures\nclassification. To our knowledge, this study is the first to provide an\nextensive empirical analysis of the interplay between PEFT techniques and\ndomain adaptation in an important real-world domain of clinical applications.\n","authors":["Aryo Pradipta Gema","Pasquale Minervini","Luke Daines","Tom Hope","Beatrice Alex"],"pdf_url":"https://arxiv.org/pdf/2307.03042v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05845v1","updated":"2024-06-09T16:33:28Z","published":"2024-06-09T16:33:28Z","title":"MedREQAL: Examining Medical Knowledge Recall of Large Language Models\n  via Question Answering","summary":"  In recent years, Large Language Models (LLMs) have demonstrated an impressive\nability to encode knowledge during pre-training on large text corpora. They can\nleverage this knowledge for downstream tasks like question answering (QA), even\nin complex areas involving health topics. Considering their high potential for\nfacilitating clinical work in the future, understanding the quality of encoded\nmedical knowledge and its recall in LLMs is an important step forward. In this\nstudy, we examine the capability of LLMs to exhibit medical knowledge recall by\nconstructing a novel dataset derived from systematic reviews -- studies\nsynthesizing evidence-based answers for specific medical questions. Through\nexperiments on the new MedREQAL dataset, comprising question-answer pairs\nextracted from rigorous systematic reviews, we assess six LLMs, such as GPT and\nMixtral, analyzing their classification and generation performance. Our\nexperimental insights into LLM performance on the novel biomedical QA dataset\nreveal the still challenging nature of this task.\n","authors":["Juraj Vladika","Phillip Schneider","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2406.05845v1.pdf","comment":"Accepted to ACL 2024 (Findings)"},{"id":"http://arxiv.org/abs/2402.06255v2","updated":"2024-06-09T16:18:46Z","published":"2024-02-09T09:09:39Z","title":"Fight Back Against Jailbreaking via Prompt Adversarial Tuning","summary":"  While Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to jailbreak attacks. Several\nprimary defense strategies have been proposed to protect LLMs from producing\nharmful information, mostly with a particular focus on harmful content\nfiltering or heuristical defensive prompt designs. However, how to achieve\nintrinsic robustness through the prompts remains an open problem. In this\npaper, motivated by adversarial training paradigms for achieving reliable\nrobustness, we propose an approach named Prompt Adversarial Tuning (PAT) that\ntrains a prompt control attached to the user prompt as a guard prefix. To\nachieve our defense goal whilst maintaining natural performance, we optimize\nthe control prompt with both adversarial and benign prompts. Comprehensive\nexperiments show that our method is effective against both black-box and\nwhite-box attacks, reducing the success rate of advanced attacks to nearly 0\nwhile maintaining the model's utility on the benign task. The proposed defense\nstrategy incurs only negligible computational overhead, charting a new\nperspective for future explorations in LLM security. Our code is available at\nhttps://github.com/rain152/PAT.\n","authors":["Yichuan Mo","Yuji Wang","Zeming Wei","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2402.06255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13915v4","updated":"2024-06-09T16:00:37Z","published":"2023-05-23T10:39:57Z","title":"DAPR: A Benchmark on Document-Aware Passage Retrieval","summary":"  The work of neural retrieval so far focuses on ranking short texts and is\nchallenged with long documents. There are many cases where the users want to\nfind a relevant passage within a long document from a huge corpus, e.g.\nWikipedia articles, research papers, etc. We propose and name this task\n\\emph{Document-Aware Passage Retrieval} (DAPR). While analyzing the errors of\nthe State-of-The-Art (SoTA) passage retrievers, we find the major errors\n(53.5\\%) are due to missing document context. This drives us to build a\nbenchmark for this task including multiple datasets from heterogeneous domains.\nIn the experiments, we extend the SoTA passage retrievers with document context\nvia (1) hybrid retrieval with BM25 and (2) contextualized passage\nrepresentations, which inform the passage representation with document context.\nWe find despite that hybrid retrieval performs the strongest on the mixture of\nthe easy and the hard queries, it completely fails on the hard queries that\nrequire document-context understanding. On the other hand, contextualized\npassage representations (e.g. prepending document titles) achieve good\nimprovement on these hard queries, but overall they also perform rather poorly.\nOur created benchmark enables future research on developing and comparing\nretrieval systems for the new task. The code and the data are available at\nhttps://github.com/UKPLab/arxiv2023-dapr.\n","authors":["Kexin Wang","Nils Reimers","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2305.13915v4.pdf","comment":"Accepted at ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2402.10373v2","updated":"2024-06-09T15:19:09Z","published":"2024-02-15T23:39:04Z","title":"BioMistral: A Collection of Open-Source Pretrained Large Language Models\n  for Medical Domains","summary":"  Large Language Models (LLMs) have demonstrated remarkable versatility in\nrecent years, offering potential applications across specialized domains such\nas healthcare and medicine. Despite the availability of various open-source\nLLMs tailored for health contexts, adapting general-purpose LLMs to the medical\ndomain presents significant challenges. In this paper, we introduce BioMistral,\nan open-source LLM tailored for the biomedical domain, utilizing Mistral as its\nfoundation model and further pre-trained on PubMed Central. We conduct a\ncomprehensive evaluation of BioMistral on a benchmark comprising 10 established\nmedical question-answering (QA) tasks in English. We also explore lightweight\nmodels obtained through quantization and model merging approaches. Our results\ndemonstrate BioMistral's superior performance compared to existing open-source\nmedical models and its competitive edge against proprietary counterparts.\nFinally, to address the limited availability of data beyond English and to\nassess the multilingual generalization of medical LLMs, we automatically\ntranslated and evaluated this benchmark into 7 other languages. This marks the\nfirst large-scale multilingual evaluation of LLMs in the medical domain.\nDatasets, multilingual evaluation benchmarks, scripts, and all the models\nobtained during our experiments are freely released.\n","authors":["Yanis Labrak","Adrien Bazoge","Emmanuel Morin","Pierre-Antoine Gourraud","Mickael Rouvier","Richard Dufour"],"pdf_url":"https://arxiv.org/pdf/2402.10373v2.pdf","comment":"Accepted at ACL 2024 - Proceedings of the 62st Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers)"},{"id":"http://arxiv.org/abs/2402.15010v2","updated":"2024-06-09T15:11:31Z","published":"2024-02-22T23:11:08Z","title":"How Important Is Tokenization in French Medical Masked Language Models?","summary":"  Subword tokenization has become the prevailing standard in the field of\nnatural language processing (NLP) over recent years, primarily due to the\nwidespread utilization of pre-trained language models. This shift began with\nByte-Pair Encoding (BPE) and was later followed by the adoption of\nSentencePiece and WordPiece. While subword tokenization consistently\noutperforms character and word-level tokenization, the precise factors\ncontributing to its success remain unclear. Key aspects such as the optimal\nsegmentation granularity for diverse tasks and languages, the influence of data\nsources on tokenizers, and the role of morphological information in\nIndo-European languages remain insufficiently explored. This is particularly\npertinent for biomedical terminology, characterized by specific rules governing\nmorpheme combinations. Despite the agglutinative nature of biomedical\nterminology, existing language models do not explicitly incorporate this\nknowledge, leading to inconsistent tokenization strategies for common terms. In\nthis paper, we seek to delve into the complexities of subword tokenization in\nFrench biomedical domain across a variety of NLP tasks and pinpoint areas where\nfurther enhancements can be made. We analyze classical tokenization algorithms,\nincluding BPE and SentencePiece, and introduce an original tokenization\nstrategy that integrates morpheme-enriched word segmentation into existing\ntokenization methods.\n","authors":["Yanis Labrak","Adrien Bazoge","Beatrice Daille","Mickael Rouvier","Richard Dufour"],"pdf_url":"https://arxiv.org/pdf/2402.15010v2.pdf","comment":"Proceedings of the 2024 Joint International Conference on\n  Computational Linguistics, Language Resources and Evaluation (LREC-COLING\n  2024)"},{"id":"http://arxiv.org/abs/2307.12114v3","updated":"2024-06-09T15:06:57Z","published":"2023-07-22T15:58:17Z","title":"A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language\n  Models Applied to Clinical and Biomedical Tasks","summary":"  We evaluate four state-of-the-art instruction-tuned large language models\n(LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13\nreal-world clinical and biomedical natural language processing (NLP) tasks in\nEnglish, such as named-entity recognition (NER), question-answering (QA),\nrelation extraction (RE), etc. Our overall results demonstrate that the\nevaluated LLMs begin to approach performance of state-of-the-art models in\nzero- and few-shot scenarios for most tasks, and particularly well for the QA\ntask, even though they have never seen examples from these tasks before.\nHowever, we observed that the classification and RE tasks perform below what\ncan be achieved with a specifically trained model for the medical field, such\nas PubMedBERT. Finally, we noted that no LLM outperforms all the others on all\nthe studied tasks, with some models being better suited for certain tasks than\nothers.\n","authors":["Yanis Labrak","Mickael Rouvier","Richard Dufour"],"pdf_url":"https://arxiv.org/pdf/2307.12114v3.pdf","comment":"LREC-COLING 2024 - Proceedings of the 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation"},{"id":"http://arxiv.org/abs/2403.07865v4","updated":"2024-06-09T15:04:34Z","published":"2024-03-12T17:55:38Z","title":"CodeAttack: Revealing Safety Generalization Challenges of Large Language\n  Models via Code Completion","summary":"  The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable generative capabilities but also raised concerns about their\npotential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\nnew and universal safety vulnerability of these models against code input:\nCodeAttack bypasses the safety guardrails of all models more than 80\\% of the\ntime. We find that a larger distribution gap between CodeAttack and natural\nlanguage leads to weaker safety generalization, such as encoding natural\nlanguage input with data structures. Furthermore, we give our hypotheses about\nthe success of CodeAttack: the misaligned bias acquired by LLMs during code\ntraining, prioritizing code completion over avoiding the potential safety risk.\nFinally, we analyze potential mitigation measures. These findings highlight new\nsafety risks in the code domain and the need for more robust safety alignment\nalgorithms to match the code capabilities of LLMs.\n","authors":["Qibing Ren","Chang Gao","Jing Shao","Junchi Yan","Xin Tan","Wai Lam","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.07865v4.pdf","comment":"ACL Findings 2024, Code is available at\n  https://github.com/renqibing/CodeAttack"},{"id":"http://arxiv.org/abs/2406.05814v1","updated":"2024-06-09T15:00:28Z","published":"2024-06-09T15:00:28Z","title":"Unified Text-to-Image Generation and Retrieval","summary":"  How humans can efficiently and effectively acquire images has always been a\nperennial question. A typical solution is text-to-image retrieval from an\nexisting database given the text query; however, the limited database typically\nlacks creativity. By contrast, recent breakthroughs in text-to-image generation\nhave made it possible to produce fancy and diverse visual content, but it faces\nchallenges in synthesizing knowledge-intensive images. In this work, we rethink\nthe relationship between text-to-image generation and retrieval and propose a\nunified framework in the context of Multimodal Large Language Models (MLLMs).\nSpecifically, we first explore the intrinsic discriminative abilities of MLLMs\nand introduce a generative retrieval method to perform retrieval in a\ntraining-free manner. Subsequently, we unify generation and retrieval in an\nautoregressive generation way and propose an autonomous decision module to\nchoose the best-matched one between generated and retrieved images as the\nresponse to the text query. Additionally, we construct a benchmark called\nTIGeR-Bench, including creative and knowledge-intensive domains, to standardize\nthe evaluation of unified text-to-image generation and retrieval. Extensive\nexperimental results on TIGeR-Bench and two retrieval benchmarks, i.e.,\nFlickr30K and MS-COCO, demonstrate the superiority and effectiveness of our\nproposed method.\n","authors":["Leigang Qu","Haochuan Li","Tan Wang","Wenjie Wang","Yongqi Li","Liqiang Nie","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2406.05814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05812v1","updated":"2024-06-09T14:54:22Z","published":"2024-06-09T14:54:22Z","title":"Seventeenth-Century Spanish American Notary Records for Fine-Tuning\n  Spanish Large Language Models","summary":"  Large language models have gained tremendous popularity in domains such as\ne-commerce, finance, healthcare, and education. Fine-tuning is a common\napproach to customize an LLM on a domain-specific dataset for a desired\ndownstream task. In this paper, we present a valuable resource for fine-tuning\nLLMs developed for the Spanish language to perform a variety of tasks such as\nclassification, masked language modeling, clustering, and others. Our resource\nis a collection of handwritten notary records from the seventeenth century\nobtained from the National Archives of Argentina. This collection contains a\ncombination of original images and transcribed text (and metadata) of 160+\npages that were handwritten by two notaries, namely, Estenban Agreda de Vergara\nand Nicolas de Valdivia y Brisuela nearly 400 years ago. Through empirical\nevaluation, we demonstrate that our collection can be used to fine-tune Spanish\nLLMs for tasks such as classification and masked language modeling, and can\noutperform pre-trained Spanish models and ChatGPT-3.5/ChatGPT-4o. Our resource\nwill be an invaluable resource for historical text analysis and is publicly\navailable on GitHub.\n","authors":["Shraboni Sarker","Ahmad Tamim Hamad","Hulayyil Alshammari","Viviana Grieco","Praveen Rao"],"pdf_url":"https://arxiv.org/pdf/2406.05812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05806v1","updated":"2024-06-09T14:44:59Z","published":"2024-06-09T14:44:59Z","title":"Do Prompts Really Prompt? Exploring the Prompt Understanding Capability\n  of Whisper","summary":"  This research explores the interaction between Whisper, a high-performing\nspeech recognition model, and information in prompts. Our results unexpectedly\nshow that Whisper may not fully grasp textual prompts as anticipated.\nAdditionally, we find that performance improvement is not guaranteed even with\nstronger adherence to the topic information in textual prompts. It is also\nnoted that English prompts generally outperform Mandarin ones on datasets of\nboth languages, likely due to differences in training data distributions for\nthese languages. Conversely, we discover that Whisper exhibits awareness of\nmisleading information in language tokens by effectively ignoring incorrect\nlanguage tokens and focusing on the correct ones. In summary, this work raises\nquestions about Whisper's prompt understanding capability and encourages\nfurther studies.\n","authors":["Chih-Kai Yang","Kuan-Po Huang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2406.05806v1.pdf","comment":"In progress"},{"id":"http://arxiv.org/abs/2406.05804v1","updated":"2024-06-09T14:42:55Z","published":"2024-06-09T14:42:55Z","title":"A Survey on LLM-Based Agentic Workflows and LLM-Profiled Components","summary":"  Recent advancements in Large Language Models (LLMs) have catalyzed the\ndevelopment of sophisticated agentic workflows, offering improvements over\ntraditional single-path, Chain-of-Thought (CoT) prompting techniques. This\nsurvey summarize the common workflows, with the particular focus on\nLLM-Profiled Components (LMPCs) and ignorance of non-LLM components. The reason\nbehind such exploration is to facilitate a clearer understanding of LLM roles\nand see how reusabile of the LMPCs.\n","authors":["Xinzhe Li"],"pdf_url":"https://arxiv.org/pdf/2406.05804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05798v1","updated":"2024-06-09T14:25:09Z","published":"2024-06-09T14:25:09Z","title":"Hidden Holes: topological aspects of language models","summary":"  We explore the topology of representation manifolds arising in autoregressive\nneural language models trained on raw text data. In order to study their\nproperties, we introduce tools from computational algebraic topology, which we\nuse as a basis for a measure of topological complexity, that we call\nperforation.\n  Using this measure, we study the evolution of topological structure in GPT\nbased large language models across depth and time during training. We then\ncompare these to gated recurrent models, and show that the latter exhibit more\ntopological complexity, with a distinct pattern of changes common to all\nnatural languages but absent from synthetically generated data. The paper\npresents a detailed analysis of the representation manifolds derived by these\nmodels based on studying the shapes of vector clouds induced by them as they\nare conditioned on sentences from corpora of natural language text.\n  The methods developed in this paper are novel in the field and based on\nmathematical apparatus that might be unfamiliar to the target audience. To help\nwith that we introduce the minimum necessary theory, and provide additional\nvisualizations in the appendices.\n  The main contribution of the paper is a striking observation about the\ntopological structure of the transformer as compared to LSTM based neural\narchitectures. It suggests that further research into mathematical properties\nof these neural networks is necessary to understand the operation of large\ntransformer language models. We hope this work inspires further explorations in\nthis direction within the NLP community.\n","authors":["Stephen Fitz","Peter Romero","Jiyan Jonas Schneider"],"pdf_url":"https://arxiv.org/pdf/2406.05798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03348v3","updated":"2024-06-09T14:24:54Z","published":"2024-03-05T22:21:45Z","title":"Learning to Maximize Mutual Information for Chain-of-Thought\n  Distillation","summary":"  Knowledge distillation, the technique of transferring knowledge from large,\ncomplex models to smaller ones, marks a pivotal step towards efficient AI\ndeployment. Distilling Step-by-Step~(DSS), a novel method utilizing\nchain-of-thought~(CoT) distillation, has demonstrated promise by imbuing\nsmaller models with the superior reasoning capabilities of their larger\ncounterparts. In DSS, the distilled model acquires the ability to generate\nrationales and predict labels concurrently through a multi-task learning\nframework. However, DSS overlooks the intrinsic relationship between the two\ntraining tasks, leading to ineffective integration of CoT knowledge with the\ntask of label prediction. To this end, we investigate the mutual relationship\nof the two tasks from Information Bottleneck perspective and formulate it as\nmaximizing the mutual information of the representation features of the two\ntasks. We propose a variational approach to solve this optimization problem\nusing a learning-based method. Our experimental results across four datasets\ndemonstrate that our method outperforms the state-of-the-art DSS. Our findings\noffer insightful guidance for future research on language model distillation as\nwell as applications involving CoT. Codes are available at\n\\url{https://github.com/xinchen9/cot_distillation_ACL2024}.\n","authors":["Xin Chen","Hanxian Huang","Yanjun Gao","Yi Wang","Jishen Zhao","Ke Ding"],"pdf_url":"https://arxiv.org/pdf/2403.03348v3.pdf","comment":"Accepted to ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.05797v1","updated":"2024-06-09T14:20:55Z","published":"2024-06-09T14:20:55Z","title":"3D-MolT5: Towards Unified 3D Molecule-Text Modeling with 3D Molecular\n  Tokenization","summary":"  The integration of molecule and language has garnered increasing attention in\nmolecular science. Recent advancements in Language Models (LMs) have\ndemonstrated potential for the comprehensive modeling of molecule and language.\nHowever, existing works exhibit notable limitations. Most existing works\noverlook the modeling of 3D information, which is crucial for understanding\nmolecular structures and also functions. While some attempts have been made to\nleverage external structure encoding modules to inject the 3D molecular\ninformation into LMs, there exist obvious difficulties that hinder the\nintegration of molecular structure and language text, such as modality\nalignment and separate tuning. To bridge this gap, we propose 3D-MolT5, a\nunified framework designed to model both 1D molecular sequence and 3D molecular\nstructure. The key innovation lies in our methodology for mapping fine-grained\n3D substructure representations (based on 3D molecular fingerprints) to a\nspecialized 3D token vocabulary for 3D-MolT5. This 3D structure token\nvocabulary enables the seamless combination of 1D sequence and 3D structure\nrepresentations in a tokenized format, allowing 3D-MolT5 to encode molecular\nsequence (SELFIES), molecular structure, and text sequences within a unified\narchitecture. Alongside, we further introduce 1D and 3D joint pre-training to\nenhance the model's comprehension of these diverse modalities in a joint\nrepresentation space and better generalize to various tasks for our foundation\nmodel. Through instruction tuning on multiple downstream datasets, our proposed\n3D-MolT5 shows superior performance than existing methods in molecular property\nprediction, molecule captioning, and text-based molecule generation tasks. Our\ncode will be available on GitHub soon.\n","authors":["Qizhi Pei","Lijun Wu","Kaiyuan Gao","Jinhua Zhu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2406.05797v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2406.05794v1","updated":"2024-06-09T14:11:19Z","published":"2024-06-09T14:11:19Z","title":"RE-RAG: Improving Open-Domain QA Performance and Interpretability with\n  Relevance Estimator in Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) frame work is showing state-of-the-art\nperformance on open-domain question answering tasks by referencing external\nknowledge. However, the RAG system faces challenges with performance\ndegradation when it is fed contexts of low relevance or when the relative\nrelevance among the input contexts is inaccurately assessed. In this work, we\npropose a RE-RAG framework that injects an explicit context relevance estimator\n(RE) into the RAG system. RE-RAG re-evaluates the retrieved contexts with the\nproposed context RE and passes the more relevant contexts along with their\nmeasure importance to the generator. To train context RE, we propose an\nunsupervised learning method, which does not utilize any labeled document\nranking data to train the context RE. To examine the efficacy of RE-RAG, we\nexamine its performance on Natural Questions and TriviaQA datasets. RE-RAG\nachieves on-par performance compared to the FiD variants while utilizing fewer\ncontexts (0.25x). We show that the proposed context RE, which was trained with\nthe T5 model, is also applicable to RAG with LLMs(ChatGPT) by improving the\nperformance on NQ (+6.4EM) and TQA (+2.8EM), respecitvely. Lastly, we display\nthat RE can add interpretability to RAG framework as RE score highly correlates\nwith the RE-RAG accuracy. Consequently, RE can be utilized to filter out\nunanswerable scenarios where context does not contain answers with 38.9%-51.3%\naccuracy just by examining a set of retrieved contexts.\n","authors":["Kiseung Kim","Jay-Yoon Lee"],"pdf_url":"https://arxiv.org/pdf/2406.05794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17644v2","updated":"2024-06-09T13:54:09Z","published":"2024-02-27T16:15:03Z","title":"Are LLMs Capable of Data-based Statistical and Causal Reasoning?\n  Benchmarking Advanced Quantitative Reasoning with Data","summary":"  Quantitative reasoning is a critical skill to analyze data, yet the\nassessment of such ability remains limited. To address this gap, we introduce\nthe Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate\nLarge Language Models' capability in statistical and causal reasoning with\nreal-world data. The benchmark comprises a carefully constructed dataset of 411\nquestions accompanied by data sheets from textbooks, online learning materials,\nand academic papers. To compare models' quantitative reasoning abilities on\ndata and text, we enrich the benchmark with an auxiliary set of 290 text-only\nquestions, namely QRText. We evaluate natural language reasoning, program-based\nreasoning, and agent reasoning methods including Chain-of-Thought,\nProgram-of-Thoughts, ReAct, and code interpreter assistants on diverse models.\nThe strongest model GPT-4 achieves an accuracy of 58%, which has much room for\nimprovement. Among open-source models, Deepseek-coder-instruct, a code LLM\npretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals\nthat models encounter difficulties in data analysis and causal reasoning, and\nstruggle in using causal knowledge and provided data simultaneously. Code and\ndata are in https://github.com/xxxiaol/QRData.\n","authors":["Xiao Liu","Zirui Wu","Xueqing Wu","Pan Lu","Kai-Wei Chang","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2402.17644v2.pdf","comment":"Findings of ACL 2024. Project website:\n  https://xxxiaol.github.io/QRData/"},{"id":"http://arxiv.org/abs/2401.06468v3","updated":"2024-06-09T13:13:03Z","published":"2024-01-12T09:29:13Z","title":"Adapting Large Language Models for Document-Level Machine Translation","summary":"  Large language models (LLMs) have significantly advanced various natural\nlanguage processing (NLP) tasks. Recent research indicates that\nmoderately-sized LLMs often outperform larger ones after task-specific\nfine-tuning. This study focuses on adapting LLMs for document-level machine\ntranslation (DocMT) for specific language pairs. We first investigate the\nimpact of prompt strategies on translation performance and then conduct\nextensive experiments using two fine-tuning methods, three LLM backbones, and\n18 translation tasks across nine language pairs. Our results show that\nspecialized models can sometimes surpass GPT-4 in translation performance but\nstill face issues like off-target translation due to error propagation in\ndecoding. We provide an in-depth analysis of these LLMs tailored for DocMT,\nexamining translation errors, discourse phenomena, training strategies, the\nscaling law of parallel documents, recent test set evaluations, and zero-shot\ncrosslingual transfer. Our findings highlight the strengths and limitations of\nLLM-based DocMT models and provide a foundation for future research.\n","authors":["Minghao Wu","Thuy-Trang Vu","Lizhen Qu","George Foster","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2401.06468v3.pdf","comment":"work in progress; 23 pages, 19 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.01509v2","updated":"2024-06-09T13:07:50Z","published":"2024-03-03T13:14:47Z","title":"Fantastic Semantics and Where to Find Them: Investigating Which Layers\n  of Generative LLMs Reflect Lexical Semantics","summary":"  Large language models have achieved remarkable success in general language\nunderstanding tasks. However, as a family of generative methods with the\nobjective of next token prediction, the semantic evolution with the depth of\nthese models are not fully explored, unlike their predecessors, such as\nBERT-like architectures. In this paper, we specifically investigate the\nbottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by\nprobing its hidden states at the end of each layer using a contextualized word\nidentification task. Our experiments show that the representations in lower\nlayers encode lexical semantics, while the higher layers, with weaker semantic\ninduction, are responsible for prediction. This is in contrast to models with\ndiscriminative objectives, such as mask language modeling, where the higher\nlayers obtain better lexical semantics. The conclusion is further supported by\nthe monotonic increase in performance via the hidden states for the last\nmeaningless symbols, such as punctuation, in the prompting strategy. Our codes\nare available at https://github.com/RyanLiut/LLM_LexSem.\n","authors":["Zhu Liu","Cunliang Kong","Ying Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.01509v2.pdf","comment":"Accepted to Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2309.03564v3","updated":"2024-06-09T12:49:52Z","published":"2023-09-07T08:50:46Z","title":"Supervised Learning and Large Language Model Benchmarks on Mental Health\n  Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media","summary":"  On social media, users often express their personal feelings, which may\nexhibit cognitive distortions or even suicidal tendencies on certain specific\ntopics. Early recognition of these signs is critical for effective\npsychological intervention. In this paper, we introduce two novel datasets from\nChinese social media: SOS-HL-1K for suicidal risk classification and\nSocialCD-3K for cognitive distortions detection. The SOS-HL-1K dataset\ncontained 1,249 posts and SocialCD-3K dataset was a multi-label classification\ndataset that containing 3,407 posts. We propose a comprehensive evaluation\nusing two supervised learning methods and eight large language models (LLMs) on\nthe proposed datasets. From the prompt engineering perspective, we experimented\nwith two types of prompt strategies, including four zero-shot and five few-shot\nstrategies. We also evaluated the performance of the LLMs after fine-tuning on\nthe proposed tasks. The experimental results show that there is still a huge\ngap between LLMs relying only on prompt engineering and supervised learning. In\nthe suicide classification task, this gap is 6.95% points in F1-score, while in\nthe cognitive distortion task, the gap is even more pronounced, reaching 31.53%\npoints in F1-score. However, after fine-tuning, this difference is\nsignificantly reduced. In the suicide and cognitive distortion classification\ntasks, the gap decreases to 4.31% and 3.14%, respectively. This research\nhighlights the potential of LLMs in psychological contexts, but supervised\nlearning remains necessary for more challenging tasks. All datasets and code\nare made available.\n","authors":["Hongzhi Qi","Qing Zhao","Jianqiang Li","Changwei Song","Wei Zhai","Dan Luo","Shuo Liu","Yi Jing Yu","Fan Wang","Huijing Zou","Bing Xiang Yang","Guanghui Fu"],"pdf_url":"https://arxiv.org/pdf/2309.03564v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2401.03735v3","updated":"2024-06-09T12:42:01Z","published":"2024-01-08T08:54:22Z","title":"Language Models Know the Value of Numbers","summary":"  Large language models (LLMs) have exhibited impressive competence in various\ntasks, but their internal mechanisms on mathematical problems are still\nunder-explored. In this paper, we study a fundamental question: whether\nlanguage models know the value of numbers, a basic element in math. To study\nthe question, we construct a synthetic dataset comprising addition problems and\nutilize linear probes to read out input numbers from the hidden states.\nExperimental results support the existence of encoded number values in LLMs on\ndifferent layers, and these values can be extracted via linear probes. Further\nexperiments show that LLMs store their calculation results in a similar manner,\nand we can intervene the output via simple vector additions, proving the causal\nconnection between encoded numbers and language model outputs. Our research\nprovides evidence that LLMs know the value of numbers, thus offering insights\nfor better exploring, designing, and utilizing numeric information in LLMs.\n","authors":["Fangwei Zhu","Damai Dai","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2401.03735v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05766v1","updated":"2024-06-09T12:41:14Z","published":"2024-06-09T12:41:14Z","title":"Gentle-CLIP: Exploring Aligned Semantic In Low-Quality Multimodal Data\n  With Soft Alignment","summary":"  Multimodal fusion breaks through the barriers between diverse modalities and\nhas already yielded numerous impressive performances. However, in various\nspecialized fields, it is struggling to obtain sufficient alignment data for\nthe training process, which seriously limits the use of previously elegant\nmodels. Thus, semi-supervised learning attempts to achieve multimodal alignment\nwith fewer matched pairs but traditional methods like pseudo-labeling are\ndifficult to apply in domains with no label information. To address these\nproblems, we transform semi-supervised multimodal alignment into a manifold\nmatching problem and propose a new method based on CLIP, named Gentle-CLIP.\nSpecifically, we design a novel semantic density distribution loss to explore\nimplicit semantic alignment information from unpaired multimodal data by\nconstraining the latent representation distribution with fine granularity, thus\neliminating the need for numerous strictly matched pairs. Meanwhile, we\nintroduce multi-kernel maximum mean discrepancy as well as self-supervised\ncontrastive loss to pull separate modality distributions closer and enhance the\nstability of the representation distribution. In addition, the contrastive loss\nused in CLIP is employed on the supervised matched data to prevent negative\noptimization. Extensive experiments conducted on a range of tasks in various\nfields, including protein, remote sensing, and the general vision-language\nfield, demonstrate the effectiveness of our proposed Gentle-CLIP.\n","authors":["Zijia Song","Zelin Zang","Yelin Wang","Guozheng Yang","Jiangbin Zheng","Kaicheng yu","Wanyu Chen","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2406.05766v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2402.02352v4","updated":"2024-06-09T23:21:28Z","published":"2024-02-04T05:33:04Z","title":"Region-Based Representations Revisited","summary":"  We investigate whether region-based representations are effective for\nrecognition. Regions were once a mainstay in recognition approaches, but pixel\nand patch-based features are now used almost exclusively. We show that recent\nclass-agnostic segmenters like SAM can be effectively combined with strong\nunsupervised representations like DINOv2 and used for a wide variety of tasks,\nincluding semantic segmentation, object-based image retrieval, and multi-image\nanalysis. Once the masks and features are extracted, these representations,\neven with linear decoders, enable competitive performance, making them well\nsuited to applications that require custom queries. The compactness of the\nrepresentation also makes it well-suited to video analysis and other problems\nrequiring inference across many images.\n","authors":["Michal Shlapentokh-Rothman","Ansel Blume","Yao Xiao","Yuqun Wu","Sethuraman T V","Heyi Tao","Jae Yong Lee","Wilfredo Torres","Yu-Xiong Wang","Derek Hoiem"],"pdf_url":"https://arxiv.org/pdf/2402.02352v4.pdf","comment":"CVPR 2024 Camera Ready; website: https://regionreps.web.illinois.edu/"},{"id":"http://arxiv.org/abs/2406.00545v2","updated":"2024-06-09T22:50:22Z","published":"2024-06-01T19:53:25Z","title":"Memory-guided Network with Uncertainty-based Feature Augmentation for\n  Few-shot Semantic Segmentation","summary":"  The performance of supervised semantic segmentation methods highly relies on\nthe availability of large-scale training data. To alleviate this dependence,\nfew-shot semantic segmentation (FSS) is introduced to leverage the model\ntrained on base classes with sufficient data into the segmentation of novel\nclasses with few data. FSS methods face the challenge of model generalization\non novel classes due to the distribution shift between base and novel classes.\nTo overcome this issue, we propose a class-shared memory (CSM) module\nconsisting of a set of learnable memory vectors. These memory vectors learn\nelemental object patterns from base classes during training whilst re-encoding\nquery features during both training and inference, thereby improving the\ndistribution alignment between base and novel classes. Furthermore, to cope\nwith the performance degradation resulting from the intra-class variance across\nimages, we introduce an uncertainty-based feature augmentation (UFA) module to\nproduce diverse query features during training for improving the model's\nrobustness. We integrate CSM and UFA into representative FSS works, with\nexperimental results on the widely-used PASCAL-5$^i$ and COCO-20$^i$ datasets\ndemonstrating the superior performance of ours over state of the art.\n","authors":["Xinyue Chen","Miaojing Shi"],"pdf_url":"https://arxiv.org/pdf/2406.00545v2.pdf","comment":"Accepted to IEEE International Conference on Multimedia and Expo\n  (ICME) 2024 as an oral presentation"},{"id":"http://arxiv.org/abs/2406.05927v1","updated":"2024-06-09T22:14:55Z","published":"2024-06-09T22:14:55Z","title":"MeanSparse: Post-Training Robustness Enhancement Through Mean-Centered\n  Feature Sparsification","summary":"  We present a simple yet effective method to improve the robustness of\nConvolutional Neural Networks (CNNs) against adversarial examples by\npost-processing an adversarially trained model. Our technique, MeanSparse,\ncascades the activation functions of a trained model with novel operators that\nsparsify mean-centered feature vectors. This is equivalent to reducing feature\nvariations around the mean, and we show that such reduced variations merely\naffect the model's utility, yet they strongly attenuate the adversarial\nperturbations and decrease the attacker's success rate. Our experiments show\nthat, when applied to the top models in the RobustBench leaderboard, it\nachieves a new robustness record of 72.08% (from 71.07%) and 59.64% (from\n59.56%) on CIFAR-10 and ImageNet, respectively, in term of AutoAttack accuracy.\nCode is available at https://github.com/SPIN-UMass/MeanSparse\n","authors":["Sajjad Amini","Mohammadreza Teymoorianfard","Shiqing Ma","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2406.05927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15476v2","updated":"2024-06-09T21:54:18Z","published":"2024-03-20T17:29:58Z","title":"Learning to Infer Generative Template Programs for Visual Concepts","summary":"  People grasp flexible visual concepts from a few examples. We explore a\nneurosymbolic system that learns how to infer programs that capture visual\nconcepts in a domain-general fashion. We introduce Template Programs:\nprogrammatic expressions from a domain-specific language that specify\nstructural and parametric patterns common to an input concept. Our framework\nsupports multiple concept-related tasks, including few-shot generation and\nco-segmentation through parsing. We develop a learning paradigm that allows us\nto train networks that infer Template Programs directly from visual datasets\nthat contain concept groupings. We run experiments across multiple visual\ndomains: 2D layouts, Omniglot characters, and 3D shapes. We find that our\nmethod outperforms task-specific alternatives, and performs competitively\nagainst domain-specific approaches for the limited domains where they exist.\n","authors":["R. Kenny Jones","Siddhartha Chaudhuri","Daniel Ritchie"],"pdf_url":"https://arxiv.org/pdf/2403.15476v2.pdf","comment":"ICML 2024; Project page: https://rkjones4.github.io/template.html"},{"id":"http://arxiv.org/abs/2406.03645v2","updated":"2024-06-09T21:16:27Z","published":"2024-06-05T22:49:30Z","title":"Partial Label Learning with Focal Loss for Sea Ice Classification Based\n  on Ice Charts","summary":"  Sea ice, crucial to the Arctic and Earth's climate, requires consistent\nmonitoring and high-resolution mapping. Manual sea ice mapping, however, is\ntime-consuming and subjective, prompting the need for automated deep\nlearning-based classification approaches. However, training these algorithms is\nchallenging because expert-generated ice charts, commonly used as training\ndata, do not map single ice types but instead map polygons with multiple ice\ntypes. Moreover, the distribution of various ice types in these charts is\nfrequently imbalanced, resulting in a performance bias towards the dominant\nclass. In this paper, we present a novel GeoAI approach to training sea ice\nclassification by formalizing it as a partial label learning task with explicit\nconfidence scores to address multiple labels and class imbalance. We treat the\npolygon-level labels as candidate partial labels, assign the corresponding ice\nconcentrations as confidence scores to each candidate label, and integrate them\nwith focal loss to train a Convolutional Neural Network (CNN). Our proposed\napproach leads to enhanced performance for sea ice classification in Sentinel-1\ndual-polarized SAR images, improving classification accuracy (from 87% to 92%)\nand weighted average F-1 score (from 90% to 93%) compared to the conventional\ntraining approach of using one-hot encoded labels and Categorical Cross-Entropy\nloss. It also improves the F-1 score in 4 out of the 6 sea ice classes.\n","authors":["Behzad Vahedi","Benjamin Lucas","Farnoush Banaei-Kashani","Andrew P. Barrett","Walter N. Meier","Siri Jodha Khalsa","Morteza Karimzadeh"],"pdf_url":"https://arxiv.org/pdf/2406.03645v2.pdf","comment":"Updated DOI and copyright info. Accepted for publication at the IEEE\n  Journal of Selected Topics in Applied Earth Observations and Remote Sensing"},{"id":"http://arxiv.org/abs/2405.19569v2","updated":"2024-06-09T21:06:44Z","published":"2024-05-29T23:24:48Z","title":"Improved Convex Decomposition with Ensembling and Boolean Primitives","summary":"  Describing a scene in terms of primitives -- geometrically simple shapes that\noffer a parsimonious but accurate abstraction of structure -- is an established\nvision problem. This is a good model of a difficult fitting problem: different\nscenes require different numbers of primitives and primitives interact\nstrongly, but any proposed solution can be evaluated at inference time. The\nstate of the art method involves a learned regression procedure to predict a\nstart point consisting of a fixed number of primitives, followed by a descent\nmethod to refine the geometry and remove redundant primitives. Methods are\nevaluated by accuracy in depth and normal prediction and in scene segmentation.\nThis paper shows that very significant improvements in accuracy can be obtained\nby (a) incorporating a small number of negative primitives and (b) ensembling\nover a number of different regression procedures. Ensembling is by refining\neach predicted start point, then choosing the best by fitting loss. Extensive\nexperiments on a standard dataset confirm that negative primitives are useful\nin a large fraction of images, and that our refine-then-choose strategy\noutperforms choose-then-refine, confirming that the fitting problem is very\ndifficult.\n","authors":["Vaibhav Vavilala","Florian Kluger","Seemandhar Jain","Bodo Rosenhahn","David Forsyth"],"pdf_url":"https://arxiv.org/pdf/2405.19569v2.pdf","comment":"18 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2406.05915v1","updated":"2024-06-09T20:58:32Z","published":"2024-06-09T20:58:32Z","title":"Bits-to-Photon: End-to-End Learned Scalable Point Cloud Compression for\n  Direct Rendering","summary":"  Point cloud is a promising 3D representation for volumetric streaming in\nemerging AR/VR applications. Despite recent advances in point cloud\ncompression, decoding and rendering high-quality images from lossy compressed\npoint clouds is still challenging in terms of quality and complexity, making it\na major roadblock to achieve real-time 6-Degree-of-Freedom video streaming. In\nthis paper, we address this problem by developing a point cloud compression\nscheme that generates a bit stream that can be directly decoded to renderable\n3D Gaussians. The encoder and decoder are jointly optimized to consider both\nbit-rates and rendering quality. It significantly improves the rendering\nquality while substantially reducing decoding and rendering time, compared to\nexisting point cloud compression methods. Furthermore, the proposed scheme\ngenerates a scalable bit stream, allowing multiple levels of details at\ndifferent bit-rate ranges. Our method supports real-time color decoding and\nrendering of high quality point clouds, thus paving the way for interactive 3D\nstreaming applications with free view points.\n","authors":["Yueyu Hu","Ran Gong","Yao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.05915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05912v1","updated":"2024-06-09T20:54:58Z","published":"2024-06-09T20:54:58Z","title":"BD-SAT: High-resolution Land Use Land Cover Dataset & Benchmark Results\n  for Developing Division: Dhaka, BD","summary":"  Land Use Land Cover (LULC) analysis on satellite images using deep\nlearning-based methods is significantly helpful in understanding the geography,\nsocio-economic conditions, poverty levels, and urban sprawl in developing\ncountries. Recent works involve segmentation with LULC classes such as\nfarmland, built-up areas, forests, meadows, water bodies, etc. Training deep\nlearning methods on satellite images requires large sets of images annotated\nwith LULC classes. However, annotated data for developing countries are scarce\ndue to a lack of funding, absence of dedicated residential/industrial/economic\nzones, a large population, and diverse building materials. BD-SAT provides a\nhigh-resolution dataset that includes pixel-by-pixel LULC annotations for Dhaka\nmetropolitan city and surrounding rural/urban areas. Using a strict and\nstandardized procedure, the ground truth is created using Bing satellite\nimagery with a ground spatial distance of 2.22 meters per pixel. A three-stage,\nwell-defined annotation process has been followed with support from GIS experts\nto ensure the reliability of the annotations. We performed several experiments\nto establish benchmark results. The results show that the annotated BD-SAT is\nsufficient to train large deep learning models with adequate accuracy for five\nmajor LULC classes: forest, farmland, built-up areas, water bodies, and\nmeadows.\n","authors":["Ovi Paul","Abu Bakar Siddik Nayem","Anis Sarker","Amin Ahsan Ali","M Ashraful Amin","AKM Mahbubur Rahman"],"pdf_url":"https://arxiv.org/pdf/2406.05912v1.pdf","comment":"26 pages, 15 figures and 12 tables"},{"id":"http://arxiv.org/abs/2406.05897v1","updated":"2024-06-09T19:33:32Z","published":"2024-06-09T19:33:32Z","title":"InfoGaussian: Structure-Aware Dynamic Gaussians through Lightweight\n  Information Shaping","summary":"  3D Gaussians, as a low-level scene representation, typically involve\nthousands to millions of Gaussians. This makes it difficult to control the\nscene in ways that reflect the underlying dynamic structure, where the number\nof independent entities is typically much smaller. In particular, it can be\nchallenging to animate and move objects in the scene, which requires\ncoordination among many Gaussians. To address this issue, we develop a mutual\ninformation shaping technique that enforces movement resonance between\ncorrelated Gaussians in a motion network. Such correlations can be learned from\nputative 2D object masks in different views. By approximating the mutual\ninformation with the Jacobians of the motions, our method ensures consistent\nmovements of the Gaussians composing different objects under various\nperturbations. In particular, we develop an efficient contrastive training\npipeline with lightweight optimization to shape the motion network, avoiding\nthe need for re-shaping throughout the motion sequence. Notably, our training\nonly touches a small fraction of all Gaussians in the scene yet attains the\ndesired compositional behavior according to the underlying dynamic structure.\nThe proposed technique is evaluated on challenging scenes and demonstrates\nsignificant performance improvement in promoting consistent movements and 3D\nobject segmentation while inducing low computation and memory requirements.\n","authors":["Yunchao Zhang","Guandao Yang","Leonidas Guibas","Yanchao Yang"],"pdf_url":"https://arxiv.org/pdf/2406.05897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05891v1","updated":"2024-06-09T19:17:14Z","published":"2024-06-09T19:17:14Z","title":"GCtx-UNet: Efficient Network for Medical Image Segmentation","summary":"  Medical image segmentation is crucial for disease diagnosis and monitoring.\nThough effective, the current segmentation networks such as UNet struggle with\ncapturing long-range features. More accurate models such as TransUNet,\nSwin-UNet, and CS-UNet have higher computation complexity. To address this\nproblem, we propose GCtx-UNet, a lightweight segmentation architecture that can\ncapture global and local image features with accuracy better or comparable to\nthe state-of-the-art approaches. GCtx-UNet uses vision transformer that\nleverages global context self-attention modules joined with local\nself-attention to model long and short range spatial dependencies. GCtx-UNet is\nevaluated on the Synapse multi-organ abdominal CT dataset, the ACDC cardiac MRI\ndataset, and several polyp segmentation datasets. In terms of Dice Similarity\nCoefficient (DSC) and Hausdorff Distance (HD) metrics, GCtx-UNet outperformed\nCNN-based and Transformer-based approaches, with notable gains in the\nsegmentation of complex and small anatomical structures. Moreover, GCtx-UNet is\nmuch more efficient than the state-of-the-art approaches with smaller model\nsize, lower computation workload, and faster training and inference speed,\nmaking it a practical choice for clinical applications.\n","authors":["Khaled Alrfou","Tian Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.05891v1.pdf","comment":"13 pages, 7 figures, 7 tables"},{"id":"http://arxiv.org/abs/2402.02345v2","updated":"2024-06-09T18:42:20Z","published":"2024-02-04T05:03:06Z","title":"Stereographic Spherical Sliced Wasserstein Distances","summary":"  Comparing spherical probability distributions is of great interest in various\nfields, including geology, medical domains, computer vision, and deep\nrepresentation learning. The utility of optimal transport-based distances, such\nas the Wasserstein distance, for comparing probability measures has spurred\nactive research in developing computationally efficient variations of these\ndistances for spherical probability measures. This paper introduces a\nhigh-speed and highly parallelizable distance for comparing spherical measures\nusing the stereographic projection and the generalized Radon transform, which\nwe refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance.\nWe carefully address the distance distortion caused by the stereographic\nprojection and provide an extensive theoretical analysis of our proposed metric\nand its rotationally invariant variation. Finally, we evaluate the performance\nof the proposed metrics and compare them with recent baselines in terms of both\nspeed and accuracy through a wide range of numerical studies, including\ngradient flows and self-supervised learning. Our code is available at\nhttps://github.com/mint-vu/s3wd.\n","authors":["Huy Tran","Yikun Bai","Abihith Kothapalli","Ashkan Shahbazi","Xinran Liu","Rocio Diaz Martin","Soheil Kolouri"],"pdf_url":"https://arxiv.org/pdf/2402.02345v2.pdf","comment":"Published at ICML 2024 (Spotlight). Project page:\n  https://abi-kothapalli.github.io/s3w/"},{"id":"http://arxiv.org/abs/2404.00949v2","updated":"2024-06-09T18:12:34Z","published":"2024-04-01T06:22:28Z","title":"Harnessing The Power of Attention For Patch-Based Biomedical Image\n  Classification","summary":"  Biomedical image analysis is of paramount importance for the advancement of\nhealthcare and medical research. Although conventional convolutional neural\nnetworks (CNNs) are frequently employed in this domain, facing limitations in\ncapturing intricate spatial and temporal relationships at the pixel level due\nto their reliance on fixed-sized windows and immutable filter weights\npost-training. These constraints impede their ability to adapt to input\nfluctuations and comprehend extensive long-range contextual information. To\novercome these challenges, a novel architecture based on self-attention\nmechanisms as an alternative to conventional CNNs.The proposed model utilizes\nattention-based mechanisms to surpass the limitations of CNNs. The key\ncomponent of our strategy is the combination of non-overlapping (vanilla\npatching) and novel overlapped Shifted Patching Techniques (S.P.T.s), which\nenhances the model's capacity to capture local context and improves\ngeneralization. Additionally, we introduce the Lancoz5 interpolation technique,\nwhich adapts variable image sizes to higher resolutions, facilitating better\nanalysis of high-resolution biomedical images. Our methods address critical\nchallenges faced by attention-based vision models, including inductive bias,\nweight sharing, receptive field limitations, and efficient data handling.\nExperimental evidence shows the effectiveness of proposed model in generalizing\nto various biomedical imaging tasks. The attention-based model, combined with\nadvanced data augmentation methodologies, exhibits robust modeling capabilities\nand superior performance compared to existing approaches. The integration of\nS.P.T.s significantly enhances the model's ability to capture local context,\nwhile the Lancoz5 interpolation technique ensures efficient handling of\nhigh-resolution images.\n","authors":["Gousia Habib","Shaima Qureshi","Malik ishfaq"],"pdf_url":"https://arxiv.org/pdf/2404.00949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05871v1","updated":"2024-06-09T18:03:47Z","published":"2024-06-09T18:03:47Z","title":"OmniControlNet: Dual-stage Integration for Conditional Image Generation","summary":"  We provide a two-way integration for the widely adopted ControlNet by\nintegrating external condition generation algorithms into a single dense\nprediction method and incorporating its individually trained image generation\nprocesses into a single model. Despite its tremendous success, the ControlNet\nof a two-stage pipeline bears limitations in being not self-contained (e.g.\ncalls the external condition generation algorithms) with a large model\nredundancy (separately trained models for different types of conditioning\ninputs). Our proposed OmniControlNet consolidates 1) the condition generation\n(e.g., HED edges, depth maps, user scribble, and animal pose) by a single\nmulti-tasking dense prediction algorithm under the task embedding guidance and\n2) the image generation process for different conditioning types under the\ntextual embedding guidance. OmniControlNet achieves significantly reduced model\ncomplexity and redundancy while capable of producing images of comparable\nquality for conditioned text-to-image generation.\n","authors":["Yilin Wang","Haiyang Xu","Xiang Zhang","Zeyuan Chen","Zhizhou Sha","Zirui Wang","Zhuowen Tu"],"pdf_url":"https://arxiv.org/pdf/2406.05871v1.pdf","comment":"Accepted to CVPR 2024 Workshop: Generative Models for Computer Vision"},{"id":"http://arxiv.org/abs/2304.07250v4","updated":"2024-06-09T17:57:45Z","published":"2023-04-14T16:58:23Z","title":"Fusing Structure from Motion and Simulation-Augmented Pose Regression\n  from Optical Flow for Challenging Indoor Environments","summary":"  The localization of objects is a crucial task in various applications such as\nrobotics, virtual and augmented reality, and the transportation of goods in\nwarehouses. Recent advances in deep learning have enabled the localization\nusing monocular visual cameras. While structure from motion (SfM) predicts the\nabsolute pose from a point cloud, absolute pose regression (APR) methods learn\na semantic understanding of the environment through neural networks. However,\nboth fields face challenges caused by the environment such as motion blur,\nlighting changes, repetitive patterns, and feature-less structures. This study\naims to address these challenges by incorporating additional information and\nregularizing the absolute pose using relative pose regression (RPR) methods.\nRPR methods suffer under different challenges, i.e., motion blur. The optical\nflow between consecutive images is computed using the Lucas-Kanade algorithm,\nand the relative pose is predicted using an auxiliary small recurrent\nconvolutional network. The fusion of absolute and relative poses is a complex\ntask due to the mismatch between the global and local coordinate systems.\nState-of-the-art methods fusing absolute and relative poses use pose graph\noptimization (PGO) to regularize the absolute pose predictions using relative\nposes. In this work, we propose recurrent fusion networks to optimally align\nabsolute and relative pose predictions to improve the absolute pose prediction.\nWe evaluate eight different recurrent units and construct a simulation\nenvironment to pre-train the APR and RPR networks for better generalized\ntraining. Additionally, we record a large database of different scenarios in a\nchallenging large-scale indoor environment that mimics a warehouse with\ntransportation robots. We conduct hyperparameter searches and experiments to\nshow the effectiveness of our recurrent fusion method compared to PGO.\n","authors":["Felix Ott","Lucas Heublein","David Rügamer","Bernd Bischl","Christopher Mutschler"],"pdf_url":"https://arxiv.org/pdf/2304.07250v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05866v1","updated":"2024-06-09T17:44:17Z","published":"2024-06-09T17:44:17Z","title":"Procrastination Is All You Need: Exponent Indexed Accumulators for\n  Floating Point, Posits and Logarithmic Numbers","summary":"  This paper discusses a simple and effective method for the summation of long\nsequences of floating point numbers. The method comprises two phases: an\naccumulation phase where the mantissas of the floating point numbers are added\nto accumulators indexed by the exponents and a reconstruction phase where the\nactual summation result is finalised. Various architectural details are given\nfor both FPGAs and ASICs including fusing the operation with a multiplier,\ncreating efficient MACs. Some results are presented for FPGAs, including a\ntensor core capable of multiplying and accumulating two 4x4 matrices of\nbfloat16 values every clock cycle using ~6,400 LUTs + 64 DSP48 in AMD FPGAs at\n700+ MHz. The method is then extended to posits and logarithmic numbers.\n","authors":["Vincenzo Liguori"],"pdf_url":"https://arxiv.org/pdf/2406.05866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05862v1","updated":"2024-06-09T17:25:47Z","published":"2024-06-09T17:25:47Z","title":"II-Bench: An Image Implication Understanding Benchmark for Multimodal\n  Large Language Models","summary":"  The rapid advancements in the development of multimodal large language models\n(MLLMs) have consistently led to new breakthroughs on various benchmarks. In\nresponse, numerous challenging and comprehensive benchmarks have been proposed\nto more accurately assess the capabilities of MLLMs. However, there is a dearth\nof exploration of the higher-order perceptual capabilities of MLLMs. To fill\nthis gap, we propose the Image Implication understanding Benchmark, II-Bench,\nwhich aims to evaluate the model's higher-order perception of images. Through\nextensive experiments on II-Bench across multiple MLLMs, we have made\nsignificant findings. Initially, a substantial gap is observed between the\nperformance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs\nattains 74.8%, whereas human accuracy averages 90%, peaking at an impressive\n98%. Subsequently, MLLMs perform worse on abstract and complex images,\nsuggesting limitations in their ability to understand high-level semantics and\ncapture image details. Finally, it is observed that most models exhibit\nenhanced accuracy when image sentiment polarity hints are incorporated into the\nprompts. This observation underscores a notable deficiency in their inherent\nunderstanding of image sentiment. We believe that II-Bench will inspire the\ncommunity to develop the next generation of MLLMs, advancing the journey\ntowards expert artificial general intelligence (AGI). II-Bench is publicly\navailable at https://huggingface.co/datasets/m-a-p/II-Bench.\n","authors":["Ziqiang Liu","Feiteng Fang","Xi Feng","Xinrun Du","Chenhao Zhang","Zekun Wang","Yuelin Bai","Qixuan Zhao","Liyang Fan","Chengguang Gan","Hongquan Lin","Jiaming Li","Yuansheng Ni","Haihong Wu","Yaswanth Narsupalli","Zhigang Zheng","Chengming Li","Xiping Hu","Ruifeng Xu","Xiaojun Chen","Min Yang","Jiaheng Liu","Ruibo Liu","Wenhao Huang","Ge Zhang","Shiwen Ni"],"pdf_url":"https://arxiv.org/pdf/2406.05862v1.pdf","comment":"100 pages, 82 figures"},{"id":"http://arxiv.org/abs/2406.05857v1","updated":"2024-06-09T17:02:28Z","published":"2024-06-09T17:02:28Z","title":"Self-supervised Adversarial Training of Monocular Depth Estimation\n  against Physical-World Attacks","summary":"  Monocular Depth Estimation (MDE) plays a vital role in applications such as\nautonomous driving. However, various attacks target MDE models, with physical\nattacks posing significant threats to system security. Traditional adversarial\ntraining methods, which require ground-truth labels, are not directly\napplicable to MDE models that lack ground-truth depth. Some self-supervised\nmodel hardening techniques (e.g., contrastive learning) overlook the domain\nknowledge of MDE, resulting in suboptimal performance. In this work, we\nintroduce a novel self-supervised adversarial training approach for MDE models,\nleveraging view synthesis without the need for ground-truth depth. We enhance\nadversarial robustness against real-world attacks by incorporating\nL_0-norm-bounded perturbation during training. We evaluate our method against\nsupervised learning-based and contrastive learning-based approaches\nspecifically designed for MDE. Our experiments with two representative MDE\nnetworks demonstrate improved robustness against various adversarial attacks,\nwith minimal impact on benign performance.\n","authors":["Zhiyuan Cheng","Cheng Han","James Liang","Qifan Wang","Xiangyu Zhang","Dongfang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.05857v1.pdf","comment":"Accepted in TPAMI'24. Extended from our ICLR'23 publication\n  (arXiv:2301.13487). arXiv admin note: substantial text overlap with\n  arXiv:2301.13487"},{"id":"http://arxiv.org/abs/2406.05852v1","updated":"2024-06-09T16:49:39Z","published":"2024-06-09T16:49:39Z","title":"RefGaussian: Disentangling Reflections from 3D Gaussian Splatting for\n  Realistic Rendering","summary":"  3D Gaussian Splatting (3D-GS) has made a notable advancement in the field of\nneural rendering, 3D scene reconstruction, and novel view synthesis.\nNevertheless, 3D-GS encounters the main challenge when it comes to accurately\nrepresenting physical reflections, especially in the case of total reflection\nand semi-reflection that are commonly found in real-world scenes. This\nlimitation causes reflections to be mistakenly treated as independent elements\nwith physical presence, leading to imprecise reconstructions. Herein, to tackle\nthis challenge, we propose RefGaussian to disentangle reflections from 3D-GS\nfor realistically modeling reflections. Specifically, we propose to split a\nscene into transmitted and reflected components and represent these components\nusing two Spherical Harmonics (SH). Given that this decomposition is not fully\ndetermined, we employ local regularization techniques to ensure local\nsmoothness for both the transmitted and reflected components, thereby achieving\nmore plausible decomposition outcomes than 3D-GS. Experimental results\ndemonstrate that our approach achieves superior novel view synthesis and\naccurate depth estimation outcomes. Furthermore, it enables the utilization of\nscene editing applications, ensuring both high-quality results and physical\ncoherence.\n","authors":["Rui Zhang","Tianyue Luo","Weidong Yang","Ben Fei","Jingyi Xu","Qingyuan Zhou","Keyi Liu","Ying He"],"pdf_url":"https://arxiv.org/pdf/2406.05852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05850v1","updated":"2024-06-09T16:49:19Z","published":"2024-06-09T16:49:19Z","title":"Scaling Graph Convolutions for Mobile Vision","summary":"  To compete with existing mobile architectures, MobileViG introduces Sparse\nVision Graph Attention (SVGA), a fast token-mixing operator based on the\nprinciples of GNNs. However, MobileViG scales poorly with model size, falling\nat most 1% behind models with similar latency. This paper introduces Mobile\nGraph Convolution (MGC), a new vision graph neural network (ViG) module that\nsolves this scaling problem. Our proposed mobile vision architecture,\nMobileViGv2, uses MGC to demonstrate the effectiveness of our approach. MGC\nimproves on SVGA by increasing graph sparsity and introducing conditional\npositional encodings to the graph operation. Our smallest model,\nMobileViGv2-Ti, achieves a 77.7% top-1 accuracy on ImageNet-1K, 2% higher than\nMobileViG-Ti, with 0.9 ms inference latency on the iPhone 13 Mini NPU. Our\nlargest model, MobileViGv2-B, achieves an 83.4% top-1 accuracy, 0.8% higher\nthan MobileViG-B, with 2.7 ms inference latency. Besides image classification,\nwe show that MobileViGv2 generalizes well to other tasks. For object detection\nand instance segmentation on MS COCO 2017, MobileViGv2-M outperforms\nMobileViG-M by 1.2 $AP^{box}$ and 0.7 $AP^{mask}$, and MobileViGv2-B\noutperforms MobileViG-B by 1.0 $AP^{box}$ and 0.7 $AP^{mask}$. For semantic\nsegmentation on ADE20K, MobileViGv2-M achieves 42.9% $mIoU$ and MobileViGv2-B\nachieves 44.3% $mIoU$. Our code can be found at\n\\url{https://github.com/SLDGroup/MobileViGv2}.\n","authors":["William Avery","Mustafa Munir","Radu Marculescu"],"pdf_url":"https://arxiv.org/pdf/2406.05850v1.pdf","comment":"Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshops"},{"id":"http://arxiv.org/abs/2305.04923v2","updated":"2024-06-09T16:13:12Z","published":"2023-05-08T17:58:27Z","title":"Learning to Evaluate the Artness of AI-generated Images","summary":"  Assessing the artness of AI-generated images continues to be a challenge\nwithin the realm of image generation. Most existing metrics cannot be used to\nperform instance-level and reference-free artness evaluation. This paper\npresents ArtScore, a metric designed to evaluate the degree to which an image\nresembles authentic artworks by artists (or conversely photographs), thereby\noffering a novel approach to artness assessment. We first blend pre-trained\nmodels for photo and artwork generation, resulting in a series of mixed models.\nSubsequently, we utilize these mixed models to generate images exhibiting\nvarying degrees of artness with pseudo-annotations. Each photorealistic image\nhas a corresponding artistic counterpart and a series of interpolated images\nthat range from realistic to artistic. This dataset is then employed to train a\nneural network that learns to estimate quantized artness levels of arbitrary\nimages. Extensive experiments reveal that the artness levels predicted by\nArtScore align more closely with human artistic evaluation than existing\nevaluation metrics, such as Gram loss and ArtFID.\n","authors":["Junyu Chen","Jie An","Hanjia Lyu","Christopher Kanan","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2305.04923v2.pdf","comment":"Published in IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2211.12981v2","updated":"2024-06-09T16:09:56Z","published":"2022-11-23T14:40:51Z","title":"Holistic Visual-Textual Sentiment Analysis with Prior Models","summary":"  Visual-textual sentiment analysis aims to predict sentiment with the input of\na pair of image and text, which poses a challenge in learning effective\nfeatures for diverse input images. To address this, we propose a holistic\nmethod that achieves robust visual-textual sentiment analysis by exploiting a\nrich set of powerful pre-trained visual and textual prior models. The proposed\nmethod consists of four parts: (1) a visual-textual branch to learn features\ndirectly from data for sentiment analysis, (2) a visual expert branch with a\nset of pre-trained \"expert\" encoders to extract selected semantic visual\nfeatures, (3) a CLIP branch to implicitly model visual-textual correspondence,\nand (4) a multimodal feature fusion network based on BERT to fuse multimodal\nfeatures and make sentiment predictions. Extensive experiments on three\ndatasets show that our method produces better visual-textual sentiment analysis\nperformance than existing methods.\n","authors":["Junyu Chen","Jie An","Hanjia Lyu","Christopher Kanan","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2211.12981v2.pdf","comment":"Published in MIPR 2024"},{"id":"http://arxiv.org/abs/2406.05837v1","updated":"2024-06-09T15:56:35Z","published":"2024-06-09T15:56:35Z","title":"Solution for CVPR 2024 UG2+ Challenge Track on All Weather Semantic\n  Segmentation","summary":"  In this report, we present our solution for the semantic segmentation in\nadverse weather, in UG2+ Challenge at CVPR 2024. To achieve robust and accurate\nsegmentation results across various weather conditions, we initialize the\nInternImage-H backbone with pre-trained weights from the large-scale joint\ndataset and enhance it with the state-of-the-art Upernet segmentation method.\nSpecifically, we utilize offline and online data augmentation approaches to\nextend the train set, which helps us to further improve the performance of the\nsegmenter. As a result, our proposed solution demonstrates advanced performance\non the test set and achieves 3rd position in this challenge.\n","authors":["Jun Yu","Yunxiang Zhang","Fengzhao Sun","Leilei Wang","Renjie Lu"],"pdf_url":"https://arxiv.org/pdf/2406.05837v1.pdf","comment":"Solution for CVPR 2024 UG2+ Challenge Track on All Weather Semantic\n  Segmentation"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.15269v2","updated":"2024-06-09T21:45:09Z","published":"2024-04-23T17:57:47Z","title":"Aligning LLM Agents by Learning Latent Preference from User Edits","summary":"  We study interactive learning of LLM-based language agents based on user\nedits made to the agent's output. In a typical setting such as writing\nassistants, the user interacts with a language agent to generate a response\ngiven a context, and may optionally edit the agent response to personalize it\nbased on their latent preference, in addition to improving the correctness. The\nedit feedback is naturally generated, making it a suitable candidate for\nimproving the agent's alignment with the user's preference, and for reducing\nthe cost of user edits over time. We propose a learning framework, PRELUDE that\ninfers a description of the user's latent preference based on historic edit\ndata. The inferred user preference descriptions are used to define prompts for\ngenerating responses in the future. This avoids fine-tuning the agent, which is\ncostly, challenging to scale with the number of users, and may even degrade its\nperformance on other tasks. Furthermore, learning descriptive preference\nimproves interpretability, allowing the user to view and modify the learned\npreference. However, user preference can be complex, subtle, and vary based on\ncontext, making it challenging to learn. To address this, we propose a simple\nyet effective algorithm named CIPHER that leverages the LLM to infer the user\npreference for a given context based on user edits. In the future, CIPHER\nretrieves inferred preferences from the k-closest contexts in the history, and\nforms an aggregate preference for response generation. We introduce two\ninteractive environments -- summarization and email writing, and use a GPT-4\nsimulated user for evaluation. On both tasks, CIPHER outperforms several\nbaselines by achieving the lowest edit distance cost while only having a small\noverhead in LLM query cost. Our analysis reports that user preferences learned\nby CIPHER show significant similarity to the ground truth latent preferences.\n","authors":["Ge Gao","Alexey Taymanov","Eduardo Salinas","Paul Mineiro","Dipendra Misra"],"pdf_url":"https://arxiv.org/pdf/2404.15269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05898v1","updated":"2024-06-09T19:35:20Z","published":"2024-06-09T19:35:20Z","title":"Async Learned User Embeddings for Ads Delivery Optimization","summary":"  User representation is crucial for recommendation systems as it helps to\ndeliver personalized recommendations by capturing user preferences and\nbehaviors in low-dimensional vectors. High-quality user embeddings can capture\nsubtle preferences, enable precise similarity calculations, and adapt to\nchanging preferences over time to maintain relevance. The effectiveness of\nrecommendation systems depends significantly on the quality of user embedding.\nWe propose to asynchronously learn high fidelity user embeddings for billions\nof users each day from sequence based multimodal user activities in Meta\nplatforms through a Transformer-like large scale feature learning module. The\nasync learned user representations embeddings (ALURE) are further converted to\nuser similarity graphs through graph learning and then combined with user\nrealtime activities to retrieval highly related ads candidates for the entire\nads delivery system. Our method shows significant gains in both offline and\nonline experiments.\n","authors":["Mingwei Tang","Meng Liu","Hong Li","Junjie Yang","Chenglin Wei","Boyang Li","Dai Li","Rengan Xu","Yifan Xu","Zehua Zhang","Xiangyu Wang","Linfeng Liu","Yuelei Xie","Chengye Liu","Labib Fawaz","Li Li","Hongnan Wang","Bill Zhu","Sri Reddy"],"pdf_url":"https://arxiv.org/pdf/2406.05898v1.pdf","comment":"Accepted by workshop on Multimodal Representation and Retrieval at\n  SIGIR 2024, Washington DC"},{"id":"http://arxiv.org/abs/2404.08860v2","updated":"2024-06-09T18:33:00Z","published":"2024-04-13T00:20:09Z","title":"Enhancing Mobile \"How-to\" Queries with Automated Search Results\n  Verification and Reranking","summary":"  Many people use search engines to find online guidance to solve computer or\nmobile device problems. Users frequently encounter challenges in identifying\neffective solutions from search results, often wasting time trying ineffective\nsolutions that seem relevant yet fail to solve real problems. This paper\nintroduces a novel approach to improving the accuracy and relevance of online\ntechnical support search results through automated search results verification\nand reranking. Taking \"How-to\" queries specific to on-device execution as a\nstarting point, we developed the first solution that allows an AI agent to\ninterpret and execute step-by-step instructions in the search results in a\ncontrolled Android environment. We further integrated the agent's findings into\na reranking mechanism that orders search results based on the success\nindicators of the tested solutions.\n  The paper details the architecture of our solution and a comprehensive\nevaluation of the system through a series of tests across various application\ndomains. The results demonstrate a significant improvement in the quality and\nreliability of the top-ranked results. Our findings suggest a paradigm shift in\nhow search engine ranking for online technical support help can be optimized,\noffering a scalable and automated solution to the pervasive challenge of\nfinding effective and reliable online help.\n","authors":["Lei Ding","Jeshwanth Bheemanpally","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.08860v2.pdf","comment":"13 pages, 3 figures, Gen-IR@SIGIR2024 workshop"},{"id":"http://arxiv.org/abs/2201.11384v2","updated":"2024-06-09T17:38:26Z","published":"2022-01-27T09:06:43Z","title":"Phase Retrieval for Radar Waveform Design","summary":"  The ability of a radar to discriminate in both range and Doppler velocity is\ncompletely characterized by the ambiguity function (AF) of its transmit\nwaveform. Mathematically, it is obtained by correlating the waveform with its\nDoppler-shifted and delayed replicas. We consider the inverse problem of\ndesigning a radar transmit waveform that satisfies the specified AF magnitude.\nThis process may be viewed as a signal reconstruction with some variation of\nphase retrieval methods. We provide a trust-region algorithm that minimizes a\nsmoothed non-convex least-squares objective function to iteratively recover the\nunderlying signal-of-interest for either time- or band-limited support. The\nmethod first approximates the signal using an iterative spectral algorithm and\nthen refines the attained initialization based on a sequence of gradient\niterations. Our theoretical analysis shows that unique signal reconstruction is\npossible using signal samples no more than thrice the number of signal\nfrequencies or time samples. Numerical experiments demonstrate that our method\nrecovers both time- and band-limited signals from sparsely and randomly\nsampled, noisy, and noiseless AFs.\n","authors":["Samuel Pinilla","Kumar Vijay Mishra","Brian M. Sadler","Henry Arguello"],"pdf_url":"https://arxiv.org/pdf/2201.11384v2.pdf","comment":"40 pages, 13 figures, 1 table"},{"id":"http://arxiv.org/abs/2305.13915v4","updated":"2024-06-09T16:00:37Z","published":"2023-05-23T10:39:57Z","title":"DAPR: A Benchmark on Document-Aware Passage Retrieval","summary":"  The work of neural retrieval so far focuses on ranking short texts and is\nchallenged with long documents. There are many cases where the users want to\nfind a relevant passage within a long document from a huge corpus, e.g.\nWikipedia articles, research papers, etc. We propose and name this task\n\\emph{Document-Aware Passage Retrieval} (DAPR). While analyzing the errors of\nthe State-of-The-Art (SoTA) passage retrievers, we find the major errors\n(53.5\\%) are due to missing document context. This drives us to build a\nbenchmark for this task including multiple datasets from heterogeneous domains.\nIn the experiments, we extend the SoTA passage retrievers with document context\nvia (1) hybrid retrieval with BM25 and (2) contextualized passage\nrepresentations, which inform the passage representation with document context.\nWe find despite that hybrid retrieval performs the strongest on the mixture of\nthe easy and the hard queries, it completely fails on the hard queries that\nrequire document-context understanding. On the other hand, contextualized\npassage representations (e.g. prepending document titles) achieve good\nimprovement on these hard queries, but overall they also perform rather poorly.\nOur created benchmark enables future research on developing and comparing\nretrieval systems for the new task. The code and the data are available at\nhttps://github.com/UKPLab/arxiv2023-dapr.\n","authors":["Kexin Wang","Nils Reimers","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2305.13915v4.pdf","comment":"Accepted at ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2303.05103v3","updated":"2024-06-09T12:14:12Z","published":"2023-03-09T08:23:56Z","title":"Algorithmic neutrality","summary":"  Algorithms wield increasing control over our lives: over the jobs we get, the\nloans we're granted, the information we see online. Algorithms can and often do\nwield their power in a biased way, and much work has been devoted to\nalgorithmic bias. In contrast, algorithmic neutrality has been largely\nneglected. I investigate algorithmic neutrality, tackling three questions: What\nis algorithmic neutrality? Is it possible? And when we have it in mind, what\ncan we learn about algorithmic bias?\n","authors":["Milo Phillips-Brown"],"pdf_url":"https://arxiv.org/pdf/2303.05103v3.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2305.03972v4","updated":"2024-06-09T12:01:15Z","published":"2023-05-06T08:12:11Z","title":"Category-Oriented Representation Learning for Image to Multi-Modal\n  Retrieval","summary":"  The rise of multi-modal search requests from users has highlighted the\nimportance of multi-modal retrieval (i.e. image-to-text or text-to-image\nretrieval), yet the more complex task of image-to-multi-modal retrieval,\ncrucial for many industry applications, remains under-explored. To address this\ngap and promote further research, we introduce and define the concept of\nImage-to-Multi-Modal Retrieval (IMMR), a process designed to retrieve rich\nmulti-modal (i.e. image and text) documents based on image queries. We focus on\nrepresentation learning for IMMR and analyze three key challenges for it: 1)\nskewed data and noisy label in real-world industrial data, 2) the\ninformation-inequality between image and text modality of documents when\nlearning representations, 3) effective and efficient training in large-scale\nindustrial contexts. To tackle the above challenges, we propose a novel\nframework named organizing categories and learning by classification for\nretrieval (OCLEAR). It consists of three components: 1) a novel\ncategory-oriented data governance scheme coupled with a large-scale\nclassification-based learning paradigm, which handles the skewed and noisy data\nfrom a data perspective. 2) model architecture specially designed for\nmulti-modal learning, where information-inequality between image and text\nmodality of documents is considered for modality fusion. 3) a hybrid parallel\ntraining approach for tackling large-scale training in industrial scenario. The\nproposed framework achieves SOTA performance on public datasets and has been\ndeployed in a real-world industrial e-commence system, leading to significant\nbusiness growth. Code will be made publicly available.\n","authors":["Zida Cheng","Chen Ju","Shuai Xiao","Xu Chen","Zhonghua Zhai","Xiaoyi Zeng","Weilin Huang","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2305.03972v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05666v1","updated":"2024-06-09T06:49:22Z","published":"2024-06-09T06:49:22Z","title":"General Distribution Learning: A theoretical framework for Deep Learning","summary":"  There remain numerous unanswered research questions on deep learning (DL)\nwithin the classical learning theory framework. These include the remarkable\ngeneralization capabilities of overparametrized neural networks (NNs), the\nefficient optimization performance despite non-convexity of objectives, the\nmechanism of flat minima in generalization, and the exceptional performance of\ndeep architectures, among others. This paper introduces a novel theoretical\nlearning framework known as General Distribution Learning (GD Learning), which\nis designed to address a comprehensive range of machine learning and\nstatistical tasks, including classification, regression and parameter\nestimation. Departing from statistical machine learning, GD Learning focuses on\nthe true underlying distribution. In GD Learning, learning error, corresponding\nto the expected error in classical statistical learning framework, is divided\ninto fitting errors caused by models and fitting algorithms, as well as\nsampling errors introduced by limited sampling data. The framework\nsignificantly incorporates prior knowledge, especially in scenarios\ncharacterized by data scarcity. This integration of external knowledge helps to\nminimize learning errors across the entire dataset, thereby enhancing\nperformance. Within the GD Learning framework, we demonstrate that the global\noptimal solution to non-convex optimization problems, such as minimizing\nfitting error, can be approached by minimizing the gradient norm and the\nnon-uniformity of the eigenvalues of the model's Jacobian matrix. This insight\nhas led to the development of the gradient structure control algorithm. GD\nLearning also offers a fresh perspective on the questions on deep learning,\nincluding overparameterization and non-convex optimizations, bias-variance\ntrade-off, and the mechanism of flat minima.\n","authors":["Binchuan Qi","Li Li","Wei Gong"],"pdf_url":"https://arxiv.org/pdf/2406.05666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05654v1","updated":"2024-06-09T05:33:51Z","published":"2024-06-09T05:33:51Z","title":"DomainRAG: A Chinese Benchmark for Evaluating Domain-specific\n  Retrieval-Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) offers a promising solution to address\nvarious limitations of Large Language Models (LLMs), such as hallucination and\ndifficulties in keeping up with real-time updates. This approach is\nparticularly critical in expert and domain-specific applications where LLMs\nstruggle to cover expert knowledge. Therefore, evaluating RAG models in such\nscenarios is crucial, yet current studies often rely on general knowledge\nsources like Wikipedia to assess the models' abilities in solving common-sense\nproblems. In this paper, we evaluated LLMs by RAG settings in a domain-specific\ncontext, college enrollment. We identified six required abilities for RAG\nmodels, including the ability in conversational RAG, analyzing structural\ninformation, faithfulness to external knowledge, denoising, solving\ntime-sensitive problems, and understanding multi-document interactions. Each\nability has an associated dataset with shared corpora to evaluate the RAG\nmodels' performance. We evaluated popular LLMs such as Llama, Baichuan,\nChatGLM, and GPT models. Experimental results indicate that existing\nclosed-book LLMs struggle with domain-specific questions, highlighting the need\nfor RAG models to solve expert problems. Moreover, there is room for RAG models\nto improve their abilities in comprehending conversational history, analyzing\nstructural information, denoising, processing multi-document interactions, and\nfaithfulness in expert knowledge. We expect future studies could solve these\nproblems better.\n","authors":["Shuting Wang","Jiongnan Liu Shiren Song","Jiehan Cheng","Yuqi Fu","Peidong Guo","Kun Fang","Yutao Zhu","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2406.05654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05629v1","updated":"2024-06-09T03:38:21Z","published":"2024-06-09T03:38:21Z","title":"Separating the \"Chirp\" from the \"Chat\": Self-supervised Visual Grounding\n  of Sound and Language","summary":"  We present DenseAV, a novel dual encoder grounding architecture that learns\nhigh-resolution, semantically meaningful, and audio-visually aligned features\nsolely through watching videos. We show that DenseAV can discover the\n``meaning'' of words and the ``location'' of sounds without explicit\nlocalization supervision. Furthermore, it automatically discovers and\ndistinguishes between these two types of associations without supervision. We\nshow that DenseAV's localization abilities arise from a new multi-head feature\naggregation operator that directly compares dense image and audio\nrepresentations for contrastive learning. In contrast, many other systems that\nlearn ``global'' audio and video representations cannot localize words and\nsound. Finally, we contribute two new datasets to improve the evaluation of AV\nrepresentations through speech and sound prompted semantic segmentation. On\nthese and other datasets we show DenseAV dramatically outperforms the prior art\non speech and sound prompted semantic segmentation. DenseAV outperforms the\nprevious state-of-the-art, ImageBind, on cross-modal retrieval using fewer than\nhalf of the parameters. Project Page:\n\\href{https://aka.ms/denseav}{https://aka.ms/denseav}\n","authors":["Mark Hamilton","Andrew Zisserman","John R. Hershey","William T. Freeman"],"pdf_url":"https://arxiv.org/pdf/2406.05629v1.pdf","comment":"Computer Vision and Pattern Recognition 2024"},{"id":"http://arxiv.org/abs/2403.12388v2","updated":"2024-06-09T00:58:25Z","published":"2024-03-19T02:57:07Z","title":"Interpretable User Satisfaction Estimation for Conversational Systems\n  with Large Language Models","summary":"  Accurate and interpretable user satisfaction estimation (USE) is critical for\nunderstanding, evaluating, and continuously improving conversational systems.\nUsers express their satisfaction or dissatisfaction with diverse conversational\npatterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented\n(customer service chatbot) conversational systems. Existing approaches based on\nfeaturized ML models or text embeddings fall short in extracting generalizable\npatterns and are hard to interpret. In this work, we show that LLMs can extract\ninterpretable signals of user satisfaction from their natural language\nutterances more effectively than embedding-based approaches. Moreover, an LLM\ncan be tailored for USE via an iterative prompting framework using supervision\nfrom labeled examples. The resulting method, Supervised Prompting for User\nsatisfaction Rubrics (SPUR), not only has higher accuracy but is more\ninterpretable as it scores user satisfaction via learned rubrics with a\ndetailed breakdown.\n","authors":["Ying-Chun Lin","Jennifer Neville","Jack W. Stokes","Longqi Yang","Tara Safavi","Mengting Wan","Scott Counts","Siddharth Suri","Reid Andersen","Xiaofeng Xu","Deepak Gupta","Sujay Kumar Jauhar","Xia Song","Georg Buscher","Saurabh Tiwary","Brent Hecht","Jaime Teevan"],"pdf_url":"https://arxiv.org/pdf/2403.12388v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2211.12981v2","updated":"2024-06-09T16:09:56Z","published":"2022-11-23T14:40:51Z","title":"Holistic Visual-Textual Sentiment Analysis with Prior Models","summary":"  Visual-textual sentiment analysis aims to predict sentiment with the input of\na pair of image and text, which poses a challenge in learning effective\nfeatures for diverse input images. To address this, we propose a holistic\nmethod that achieves robust visual-textual sentiment analysis by exploiting a\nrich set of powerful pre-trained visual and textual prior models. The proposed\nmethod consists of four parts: (1) a visual-textual branch to learn features\ndirectly from data for sentiment analysis, (2) a visual expert branch with a\nset of pre-trained \"expert\" encoders to extract selected semantic visual\nfeatures, (3) a CLIP branch to implicitly model visual-textual correspondence,\nand (4) a multimodal feature fusion network based on BERT to fuse multimodal\nfeatures and make sentiment predictions. Extensive experiments on three\ndatasets show that our method produces better visual-textual sentiment analysis\nperformance than existing methods.\n","authors":["Junyu Chen","Jie An","Hanjia Lyu","Christopher Kanan","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2211.12981v2.pdf","comment":"Published in MIPR 2024"},{"id":"http://arxiv.org/abs/2406.05814v1","updated":"2024-06-09T15:00:28Z","published":"2024-06-09T15:00:28Z","title":"Unified Text-to-Image Generation and Retrieval","summary":"  How humans can efficiently and effectively acquire images has always been a\nperennial question. A typical solution is text-to-image retrieval from an\nexisting database given the text query; however, the limited database typically\nlacks creativity. By contrast, recent breakthroughs in text-to-image generation\nhave made it possible to produce fancy and diverse visual content, but it faces\nchallenges in synthesizing knowledge-intensive images. In this work, we rethink\nthe relationship between text-to-image generation and retrieval and propose a\nunified framework in the context of Multimodal Large Language Models (MLLMs).\nSpecifically, we first explore the intrinsic discriminative abilities of MLLMs\nand introduce a generative retrieval method to perform retrieval in a\ntraining-free manner. Subsequently, we unify generation and retrieval in an\nautoregressive generation way and propose an autonomous decision module to\nchoose the best-matched one between generated and retrieved images as the\nresponse to the text query. Additionally, we construct a benchmark called\nTIGeR-Bench, including creative and knowledge-intensive domains, to standardize\nthe evaluation of unified text-to-image generation and retrieval. Extensive\nexperimental results on TIGeR-Bench and two retrieval benchmarks, i.e.,\nFlickr30K and MS-COCO, demonstrate the superiority and effectiveness of our\nproposed method.\n","authors":["Leigang Qu","Haochuan Li","Tan Wang","Wenjie Wang","Yongqi Li","Liqiang Nie","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2406.05814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05756v1","updated":"2024-06-09T12:23:14Z","published":"2024-06-09T12:23:14Z","title":"EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks\n  with Large Vision-Language Models","summary":"  The recent rapid development of Large Vision-Language Models (LVLMs) has\nindicated their potential for embodied tasks.However, the critical skill of\nspatial understanding in embodied environments has not been thoroughly\nevaluated, leaving the gap between current LVLMs and qualified embodied\nintelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for\nevaluating embodied spatial understanding of LVLMs.The benchmark is\nautomatically derived from embodied scenes and covers 6 spatial relationships\nfrom an egocentric perspective.Experiments expose the insufficient capacity of\ncurrent LVLMs (even GPT-4V). We further present EmbSpatial-SFT, an\ninstruction-tuning dataset designed to improve LVLMs' embodied spatial\nunderstanding.\n","authors":["Mengfei Du","Binhao Wu","Zejun Li","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2406.05756v1.pdf","comment":"Accepted by ACL 2024 Main"},{"id":"http://arxiv.org/abs/2405.19334v2","updated":"2024-06-09T11:34:12Z","published":"2024-05-29T17:59:20Z","title":"LLMs Meet Multimodal Generation and Editing: A Survey","summary":"  With the recent advancement in large language models (LLMs), there is a\ngrowing interest in combining LLMs with multimodal learning. Previous surveys\nof multimodal large language models (MLLMs) mainly focus on multimodal\nunderstanding. This survey elaborates on multimodal generation and editing\nacross various domains, comprising image, video, 3D, and audio. Specifically,\nwe summarize the notable advancements with milestone works in these fields and\ncategorize these studies into LLM-based and CLIP/T5-based methods. Then, we\nsummarize the various roles of LLMs in multimodal generation and exhaustively\ninvestigate the critical technical components behind these methods and the\nmultimodal datasets utilized in these studies. Additionally, we dig into\ntool-augmented multimodal agents that can leverage existing generative models\nfor human-computer interaction. Lastly, we discuss the advancements in the\ngenerative AI safety field, investigate emerging applications, and discuss\nfuture prospects. Our work provides a systematic and insightful overview of\nmultimodal generation and processing, which is expected to advance the\ndevelopment of Artificial Intelligence for Generative Content (AIGC) and world\nmodels. A curated list of all related papers can be found at\nhttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation\n","authors":["Yingqing He","Zhaoyang Liu","Jingye Chen","Zeyue Tian","Hongyu Liu","Xiaowei Chi","Runtao Liu","Ruibin Yuan","Yazhou Xing","Wenhai Wang","Jifeng Dai","Yong Zhang","Wei Xue","Qifeng Liu","Yike Guo","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2405.19334v2.pdf","comment":"52 Pages with 16 Figures, 12 Tables, and 545 References. GitHub\n  Repository at:\n  https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation"},{"id":"http://arxiv.org/abs/2405.09152v4","updated":"2024-06-09T07:18:58Z","published":"2024-05-15T07:31:48Z","title":"Scalable Image Coding for Humans and Machines Using Feature Fusion\n  Network","summary":"  As image recognition models become more prevalent, scalable coding methods\nfor machines and humans gain more importance. Applications of image recognition\nmodels include traffic monitoring and farm management. In these use cases, the\nscalable coding method proves effective because the tasks require occasional\nimage checking by humans. Existing image compression methods for humans and\nmachines meet these requirements to some extent. However, these compression\nmethods are effective solely for specific image recognition models. We propose\na learning-based scalable image coding method for humans and machines that is\ncompatible with numerous image recognition models. We combine an image\ncompression model for machines with a compression model, providing additional\ninformation to facilitate image decoding for humans. The features in these\ncompression models are fused using a feature fusion network to achieve\nefficient image compression. Our method's additional information compression\nmodel is adjusted to reduce the number of parameters by enabling combinations\nof features of different sizes in the feature fusion network. Our approach\nconfirms that the feature fusion network efficiently combines image compression\nmodels while reducing the number of parameters. Furthermore, we demonstrate the\neffectiveness of the proposed scalable coding method by evaluating the image\ncompression performance in terms of decoded image quality and bitrate.\n","authors":["Takahiro Shindo","Taiju Watanabe","Yui Tatsumi","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2405.09152v4.pdf","comment":null}]},"2024-06-11T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.07546v1","updated":"2024-06-11T17:59:48Z","published":"2024-06-11T17:59:48Z","title":"Commonsense-T2I Challenge: Can Text-to-Image Generation Models\n  Understand Commonsense?","summary":"  We present a novel task and benchmark for evaluating the ability of\ntext-to-image(T2I) generation models to produce images that fit commonsense in\nreal life, which we call Commonsense-T2I. Given two adversarial text prompts\ncontaining an identical set of action words with minor differences, such as \"a\nlightbulb without electricity\" v.s. \"a lightbulb with electricity\", we evaluate\nwhether T2I models can conduct visual-commonsense reasoning, e.g. produce\nimages that fit \"the lightbulb is unlit\" vs. \"the lightbulb is lit\"\ncorrespondingly. Commonsense-T2I presents an adversarial challenge, providing\npairwise text prompts along with expected outputs. The dataset is carefully\nhand-curated by experts and annotated with fine-grained labels, such as\ncommonsense type and likelihood of the expected outputs, to assist analyzing\nmodel behavior. We benchmark a variety of state-of-the-art (sota) T2I models\nand surprisingly find that, there is still a large gap between image synthesis\nand real life photos--even the DALL-E 3 model could only achieve 48.92% on\nCommonsense-T2I, and the stable diffusion XL model only achieves 24.92%\naccuracy. Our experiments show that GPT-enriched prompts cannot solve this\nchallenge, and we include a detailed analysis about possible reasons for such\ndeficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation\nbenchmark for T2I commonsense checking, fostering advancements in real life\nimage generation.\n","authors":["Xingyu Fu","Muyu He","Yujie Lu","William Yang Wang","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.07546v1.pdf","comment":"Text-to-Image Generation, Commonsense, Project Url:\n  https://zeyofu.github.io/CommonsenseT2I/"},{"id":"http://arxiv.org/abs/2406.07545v1","updated":"2024-06-11T17:59:47Z","published":"2024-06-11T17:59:47Z","title":"Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs\n  Evaluation, Benchmark, and Arena","summary":"  Multiple-choice questions (MCQ) are frequently used to assess large language\nmodels (LLMs). Typically, an LLM is given a question and selects the answer\ndeemed most probable after adjustments for factors like length. Unfortunately,\nLLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to\ninherent biases of priori unbalanced probabilities, influencing the prediction\nof answers based on these IDs. Previous research has introduced methods to\nreduce this ''selection bias'' by simply permutating options on a few test\nsamples and applying to new ones. Another problem of MCQ is the lottery ticket\nchoice by ''random guessing''. The LLM does not learn particular knowledge, but\nthe option is guessed correctly. This situation is especially serious for those\nsmall-scale LLMs. To address them, a more thorough approach involves shifting\nfrom MCQ to open-style questions, which can fundamentally eliminate selection\nbias and random guessing issues. However, transitioning causes its own set of\nchallenges in (1) identifying suitable open-style questions and (2) validating\nthe correctness of LLM open-style responses against human-annotated\nground-truths. This work aims to tackle these significant difficulties, and\nestablish a new LLM evaluation benchmark through entirely open-style questions.\nConsequently, we introduce the Open-LLM-Leaderboard to track various LLMs'\nperformance and reflect true capability of them, such as GPT-4o/4/3.5, Claude\n3, Gemini, etc. Our code and dataset are available at\nhttps://github.com/VILA-Lab/Open-LLM-Leaderboard.\n","authors":["Aidar Myrzakhan","Sondos Mahmoud Bsharat","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2406.07545v1.pdf","comment":"Code and dataset are available at\n  https://github.com/VILA-Lab/Open-LLM-Leaderboard"},{"id":"http://arxiv.org/abs/2406.07544v1","updated":"2024-06-11T17:59:45Z","published":"2024-06-11T17:59:45Z","title":"Situational Awareness Matters in 3D Vision Language Reasoning","summary":"  Being able to carry out complicated vision language reasoning tasks in 3D\nspace represents a significant milestone in developing household robots and\nhuman-centered embodied AI. In this work, we demonstrate that a critical and\ndistinct challenge in 3D vision language reasoning is situational awareness,\nwhich incorporates two key components: (1) The autonomous agent grounds its\nself-location based on a language prompt. (2) The agent answers open-ended\nquestions from the perspective of its calculated position. To address this\nchallenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D\nvision language reasoning. We tokenize the 3D scene into sparse voxel\nrepresentation and propose a language-grounded situation estimator, followed by\na situated question answering module. Experiments on the SQA3D and ScanQA\ndatasets show that SIG3D outperforms state-of-the-art models in situation\nestimation and question answering by a large margin (e.g., an enhancement of\nover 30% on situation estimation accuracy). Subsequent analysis corroborates\nour architectural design choices, explores the distinct functions of visual and\ntextual tokens, and highlights the importance of situational awareness in the\ndomain of 3D question answering.\n","authors":["Yunze Man","Liang-Yan Gui","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07544v1.pdf","comment":"CVPR 2024. Project Page: https://yunzeman.github.io/situation3d"},{"id":"http://arxiv.org/abs/2312.07540v3","updated":"2024-06-11T17:57:15Z","published":"2023-12-12T18:59:30Z","title":"diff History for Neural Language Agents","summary":"  Neural Language Models (LMs) offer an exciting solution for general-purpose\nembodied control. However, a key technical issue arises when using an LM-based\ncontroller: environment observations must be converted to text, which coupled\nwith history, results in long and verbose textual prompts. As a result, prior\nwork in LM agents is limited to restricted domains with small observation size\nas well as minimal needs for interaction history or instruction tuning. In this\npaper, we introduce diff history, a simple and highly effective solution to\nthese issues. By applying the Unix diff command on consecutive text\nobservations in the interaction histories used to prompt LM policies, we can\nboth abstract away redundant information and focus the content of textual\ninputs on the salient changes in the environment. On NetHack, an unsolved video\ngame that requires long-horizon reasoning for decision-making, LMs tuned with\ndiff history match state-of-the-art performance for neural agents while needing\n1800x fewer training examples compared to prior work. Even on the simpler\nBabyAI-Text environment with concise text observations, we find that although\ndiff history increases the length of prompts, the representation it provides\noffers a 25% improvement in the efficiency of low-sample instruction tuning.\nFurther, we show that diff history scales favorably across different tuning\ndataset sizes. We open-source our code and data to\nhttps://diffhistory.github.io.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2312.07540v3.pdf","comment":"ICML 2024 version"},{"id":"http://arxiv.org/abs/2406.07524v1","updated":"2024-06-11T17:51:40Z","published":"2024-06-11T17:51:40Z","title":"Simple and Effective Masked Diffusion Language Models","summary":"  While diffusion models excel at generating high-quality images, prior work\nreports a significant performance gap between diffusion and autoregressive (AR)\nmethods in language modeling. In this work, we show that simple masked discrete\ndiffusion is more performant than previously thought. We apply an effective\ntraining recipe that improves the performance of masked diffusion models and\nderive a simplified, Rao-Blackwellized objective that results in additional\nimprovements. Our objective has a simple form -- it is a mixture of classical\nmasked language modeling losses -- and can be used to train encoder-only\nlanguage models that admit efficient samplers, including ones that can generate\narbitrary lengths of text semi-autoregressively like a traditional language\nmodel. On language modeling benchmarks, a range of masked diffusion models\ntrained with modern engineering practices achieves a new state-of-the-art among\ndiffusion models, and approaches AR perplexity. We release our code at:\nhttps://github.com/kuleshov-group/mdlm\n","authors":["Subham Sekhar Sahoo","Marianne Arriola","Yair Schiff","Aaron Gokaslan","Edgar Marroquin","Justin T Chiu","Alexander Rush","Volodymyr Kuleshov"],"pdf_url":"https://arxiv.org/pdf/2406.07524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07522v1","updated":"2024-06-11T17:50:51Z","published":"2024-06-11T17:50:51Z","title":"Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling","summary":"  Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.\n","authors":["Liliang Ren","Yang Liu","Yadong Lu","Yelong Shen","Chen Liang","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.07522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07844v4","updated":"2024-06-11T17:44:56Z","published":"2024-02-12T17:53:22Z","title":"Mercury: A Code Efficiency Benchmark for Code Large Language Models","summary":"  Amidst the recent strides in evaluating Large Language Models for Code (Code\nLLMs), existing benchmarks have mainly focused on the functional correctness of\ngenerated code, neglecting the importance of their computational efficiency. To\nfill the gap, we present Mercury, the first code efficiency benchmark for Code\nLLMs. It comprises 1,889 Python tasks, each accompanied by adequate solutions\nthat serve as real-world efficiency baselines, enabling a comprehensive\nanalysis of the runtime distribution. Based on the distribution, we introduce a\nnew metric Beyond, which computes a runtime-percentile-weighted Pass score to\nreflect functional correctness and code efficiency simultaneously. On Mercury,\nleading Code LLMs can achieve 65% on Pass, while less than 50% on Beyond. Given\nthat an ideal Beyond score would be aligned with the Pass score, it indicates\nthat while Code LLMs exhibit impressive capabilities in generating functionally\ncorrect code, there remains a notable gap in their efficiency. Finally, our\nempirical experiments reveal that Direct Preference Optimization (DPO) serves\nas a robust baseline for enhancing code efficiency compared with Supervised\nFine Tuning (SFT), which paves a promising avenue for future exploration of\nefficient code generation. Our code and data are available on GitHub:\nhttps://github.com/Elfsong/Mercury.\n","authors":["Mingzhe Du","Anh Tuan Luu","Bin Ji","Qian Liu","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2402.07844v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07505v1","updated":"2024-06-11T17:40:00Z","published":"2024-06-11T17:40:00Z","title":"THaLLE: Text Hyperlocally Augmented Large Language Extension --\n  Technical Report","summary":"  Recent advancements in Large Language Models (LLMs) have revealed new\ncapabilities and opportunities across the technological landscape. However, the\npracticality of very large LLMs is challenged by their high compute cost, which\ndoes not justify the benefits given their limited capability compared to\nhumans. While smaller, more practical LLMs have shown potential in financial\nanalysis, though they are not yet fully proficient, as evidenced by their\nnear-passing performance on the Chartered Financial Analyst (CFA) exam. In this\nwork, we present Financial Analyst Extension to our Text Hyperlocally Augmented\nLarge Language Extension (THaLLE), a series of 8B LLMs consistently achieving\nhighest performance on mock CFA exams against models of comparable size. We\nthoroughly document the fine-tuning techniques used to facilitate future\nresearch. Additionally, we introduce the use of Flare CFA, a publicly available\ndataset for evaluating LLMs as a financial advisor.\n","authors":["KBTG Labs","Danupat Khamnuansin","Atthakorn Petchsod","Anuruth Lertpiya","Pornchanan Balee","Thanawat Lodkaew","Tawunrat Chalothorn","Thadpong Pongthawornkamol","Monchai Lertsutthiwong"],"pdf_url":"https://arxiv.org/pdf/2406.07505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07504v1","updated":"2024-06-11T17:39:46Z","published":"2024-06-11T17:39:46Z","title":"Just Because We Camp, Doesn't Mean We Should: The Ethics of Modelling\n  Queer Voices","summary":"  Modern voice cloning models claim to be able to capture a diverse range of\nvoices. We test the ability of a typical pipeline to capture the style known\ncolloquially as \"gay voice\" and notice a homogenisation effect: synthesised\nspeech is rated as sounding significantly \"less gay\" (by LGBTQ+ participants)\nthan its corresponding ground-truth for speakers with \"gay voice\", but ratings\nactually increase for control speakers. Loss of \"gay voice\" has implications\nfor accessibility. We also find that for speakers with \"gay voice\", loss of\n\"gay voice\" corresponds to lower similarity ratings.\n  However, we caution that improving the ability of such models to synthesise\n``gay voice'' comes with a great number of risks. We use this pipeline as a\nstarting point for a discussion on the ethics of modelling queer voices more\nbroadly. Collecting \"clean\" queer data has safety and fairness ramifications,\nand the resulting technology may cause harms from mockery to death.\n","authors":["Atli Sigurgeirsson","Eddie L. Ungless"],"pdf_url":"https://arxiv.org/pdf/2406.07504v1.pdf","comment":"4 pages (+1 page references). To be presented at Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.07502v1","updated":"2024-06-11T17:37:45Z","published":"2024-06-11T17:37:45Z","title":"Image Textualization: An Automatic Framework for Creating Accurate and\n  Detailed Image Descriptions","summary":"  Image description datasets play a crucial role in the advancement of various\napplications such as image understanding, text-to-image generation, and\ntext-image retrieval. Currently, image description datasets primarily originate\nfrom two sources. One source is the scraping of image-text pairs from the web.\nDespite their abundance, these descriptions are often of low quality and noisy.\nAnother is through human labeling. Datasets such as COCO are generally very\nshort and lack details. Although detailed image descriptions can be annotated\nby humans, the high annotation cost limits the feasibility. These limitations\nunderscore the need for more efficient and scalable methods to generate\naccurate and detailed image descriptions. In this paper, we propose an\ninnovative framework termed Image Textualization (IT), which automatically\nproduces high-quality image descriptions by leveraging existing multi-modal\nlarge language models (MLLMs) and multiple vision expert models in a\ncollaborative manner, which maximally convert the visual information into text.\nTo address the current lack of benchmarks for detailed descriptions, we propose\nseveral benchmarks for comprehensive evaluation, which verifies the quality of\nimage descriptions created by our framework. Furthermore, we show that\nLLaVA-7B, benefiting from training on IT-curated descriptions, acquire improved\ncapability to generate richer image descriptions, substantially increasing the\nlength and detail of their output with less hallucination.\n","authors":["Renjie Pi","Jianshu Zhang","Jipeng Zhang","Rui Pan","Zhekai Chen","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06228v2","updated":"2024-06-11T17:33:52Z","published":"2024-04-09T11:39:53Z","title":"Understanding Cross-Lingual Alignment -- A Survey","summary":"  Cross-lingual alignment, the meaningful similarity of representations across\nlanguages in multilingual language models, has been an active field of research\nin recent years. We survey the literature of techniques to improve\ncross-lingual alignment, providing a taxonomy of methods and summarising\ninsights from throughout the field. We present different understandings of\ncross-lingual alignment and their limitations. We provide a qualitative summary\nof results from a large number of surveyed papers. Finally, we discuss how\nthese insights may be applied not only to encoder models, where this topic has\nbeen heavily studied, but also to encoder-decoder or even decoder-only models,\nand argue that an effective trade-off between language-neutral and\nlanguage-specific information is key.\n","authors":["Katharina Hämmerl","Jindřich Libovický","Alexander Fraser"],"pdf_url":"https://arxiv.org/pdf/2404.06228v2.pdf","comment":"Camera-ready version, ACL Findings 2024"},{"id":"http://arxiv.org/abs/2406.07496v1","updated":"2024-06-11T17:32:21Z","published":"2024-06-11T17:32:21Z","title":"TextGrad: Automatic \"Differentiation\" via Text","summary":"  AI is undergoing a paradigm shift, with breakthroughs achieved by systems\norchestrating multiple large language models (LLMs) and other complex\ncomponents. As a result, developing principled and automated optimization\nmethods for compound AI systems is one of the most important new challenges.\nNeural networks faced a similar challenge in its early days until\nbackpropagation and automatic differentiation transformed the field by making\noptimization turn-key. Inspired by this, we introduce TextGrad, a powerful\nframework performing automatic ``differentiation'' via text. TextGrad\nbackpropagates textual feedback provided by LLMs to improve individual\ncomponents of a compound AI system. In our framework, LLMs provide rich,\ngeneral, natural language suggestions to optimize variables in computation\ngraphs, ranging from code snippets to molecular structures. TextGrad follows\nPyTorch's syntax and abstraction and is flexible and easy-to-use. It works\nout-of-the-box for a variety of tasks, where the users only provide the\nobjective function without tuning components or prompts of the framework. We\nshowcase TextGrad's effectiveness and generality across a diverse range of\napplications, from question answering and molecule optimization to radiotherapy\ntreatment planning. Without modifying the framework, TextGrad improves the\nzero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\\%$ to\n$55\\%$, yields $20\\%$ relative performance gain in optimizing LeetCode-Hard\ncoding problem solutions, improves prompts for reasoning, designs new druglike\nsmall molecules with desirable in silico binding, and designs radiation\noncology treatment plans with high specificity. TextGrad lays a foundation to\naccelerate the development of the next-generation of AI systems.\n","authors":["Mert Yuksekgonul","Federico Bianchi","Joseph Boen","Sheng Liu","Zhi Huang","Carlos Guestrin","James Zou"],"pdf_url":"https://arxiv.org/pdf/2406.07496v1.pdf","comment":"41 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.07494v1","updated":"2024-06-11T17:30:22Z","published":"2024-06-11T17:30:22Z","title":"CADS: A Systematic Literature Review on the Challenges of Abstractive\n  Dialogue Summarization","summary":"  Abstractive dialogue summarization is the task of distilling conversations\ninto informative and concise summaries. Although reviews have been conducted on\nthis topic, there is a lack of comprehensive work detailing the challenges of\ndialogue summarization, unifying the differing understanding of the task, and\naligning proposed techniques, datasets, and evaluation metrics with the\nchallenges. This article summarizes the research on Transformer-based\nabstractive summarization for English dialogues by systematically reviewing\n1262 unique research papers published between 2019 and 2024, relying on the\nSemantic Scholar and DBLP databases. We cover the main challenges present in\ndialog summarization (i.e., language, structure, comprehension, speaker,\nsalience, and factuality) and link them to corresponding techniques such as\ngraph-based approaches, additional training tasks, and planning strategies,\nwhich typically overly rely on BART-based encoder-decoder models. We find that\nwhile some challenges, like language, have seen considerable progress, mainly\ndue to training methods, others, such as comprehension, factuality, and\nsalience, remain difficult and hold significant research opportunities. We\ninvestigate how these approaches are typically assessed, covering the datasets\nfor the subdomains of dialogue (e.g., meeting, medical), the established\nautomatic metrics and human evaluation approaches for assessing scores and\nannotator agreement. We observe that only a few datasets span across all\nsubdomains. The ROUGE metric is the most used, while human evaluation is\nfrequently reported without sufficient detail on inner-annotator agreement and\nannotation guidelines. Additionally, we discuss the possible implications of\nthe recently explored large language models and conclude that despite a\npotential shift in relevance and difficulty, our described challenge taxonomy\nremains relevant.\n","authors":["Frederic Kirstein","Jan Philip Wahle","Bela Gipp","Terry Ruas"],"pdf_url":"https://arxiv.org/pdf/2406.07494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07492v1","updated":"2024-06-11T17:30:03Z","published":"2024-06-11T17:30:03Z","title":"Paraphrasing in Affirmative Terms Improves Negation Understanding","summary":"  Negation is a common linguistic phenomenon. Yet language models face\nchallenges with negation in many natural language understanding tasks such as\nquestion answering and natural language inference. In this paper, we experiment\nwith seamless strategies that incorporate affirmative interpretations (i.e.,\nparaphrases without negation) to make models more robust against negation.\nCrucially, our affirmative interpretations are obtained automatically. We show\nimprovements with CondaQA, a large corpus requiring reasoning with negation,\nand five natural language understanding tasks.\n","authors":["MohammadHossein Rezaei","Eduardo Blanco"],"pdf_url":"https://arxiv.org/pdf/2406.07492v1.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2305.01713v3","updated":"2024-06-11T17:29:22Z","published":"2023-05-02T18:27:13Z","title":"Learning Disentangled Semantic Spaces of Explanations via Invertible\n  Neural Networks","summary":"  Disentangled latent spaces usually have better semantic separability and\ngeometrical properties, which leads to better interpretability and more\ncontrollable data generation. While this has been well investigated in Computer\nVision, in tasks such as image disentanglement, in the NLP domain sentence\ndisentanglement is still comparatively under-investigated. Most previous work\nhave concentrated on disentangling task-specific generative factors, such as\nsentiment, within the context of style transfer. In this work, we focus on a\nmore general form of sentence disentanglement, targeting the localised\nmodification and control of more general sentence semantic features. To achieve\nthis, we contribute to a novel notion of sentence semantic disentanglement and\nintroduce a flow-based invertible neural network (INN) mechanism integrated\nwith a transformer-based language Autoencoder (AE) in order to deliver latent\nspaces with better separability properties. Experimental results demonstrate\nthat the model can conform the distributed latent space into a better\nsemantically disentangled sentence space, leading to improved language\ninterpretability and controlled generation when compared to the recent\nstate-of-the-art language VAE models.\n","authors":["Yingji Zhang","Danilo S. Carvalho","André Freitas"],"pdf_url":"https://arxiv.org/pdf/2305.01713v3.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2406.07483v1","updated":"2024-06-11T17:26:07Z","published":"2024-06-11T17:26:07Z","title":"Advancing Annotation of Stance in Social Media Posts: A Comparative\n  Analysis of Large Language Models and Crowd Sourcing","summary":"  In the rapidly evolving landscape of Natural Language Processing (NLP), the\nuse of Large Language Models (LLMs) for automated text annotation in social\nmedia posts has garnered significant interest. Despite the impressive\ninnovations in developing LLMs like ChatGPT, their efficacy, and accuracy as\nannotation tools are not well understood. In this paper, we analyze the\nperformance of eight open-source and proprietary LLMs for annotating the stance\nexpressed in social media posts, benchmarking their performance against human\nannotators' (i.e., crowd-sourced) judgments. Additionally, we investigate the\nconditions under which LLMs are likely to disagree with human judgment. A\nsignificant finding of our study is that the explicitness of text expressing a\nstance plays a critical role in how faithfully LLMs' stance judgments match\nhumans'. We argue that LLMs perform well when human annotators do, and when\nLLMs fail, it often corresponds to situations in which human annotators\nstruggle to reach an agreement. We conclude with recommendations for a\ncomprehensive approach that combines the precision of human expertise with the\nscalability of LLM predictions. This study highlights the importance of\nimproving the accuracy and comprehensiveness of automated stance detection,\naiming to advance these technologies for more efficient and unbiased analysis\nof social media.\n","authors":["Mao Li","Frederick Conrad"],"pdf_url":"https://arxiv.org/pdf/2406.07483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07476v1","updated":"2024-06-11T17:22:23Z","published":"2024-06-11T17:22:23Z","title":"VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio\n  Understanding in Video-LLMs","summary":"  In this paper, we present the VideoLLaMA 2, a set of Video Large Language\nModels (Video-LLMs) designed to enhance spatial-temporal modeling and audio\nunderstanding in video and audio-oriented tasks. Building upon its predecessor,\nVideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC)\nconnector, which effectively captures the intricate spatial and temporal\ndynamics of video data. Additionally, we integrate an Audio Branch into the\nmodel through joint training, thereby enriching the multimodal understanding\ncapabilities of the model by seamlessly incorporating audio cues. Comprehensive\nevaluations on multiple-choice video question answering (MC-VQA), open-ended\nvideo question answering (OE-VQA), and video captioning (VC) tasks demonstrate\nthat VideoLLaMA 2 consistently achieves competitive results among open-source\nmodels and even gets close to some proprietary models on several benchmarks.\nFurthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and\naudio-video question-answering (AQA & OE-AVQA) benchmarks over existing models.\nThese advancements underline VideoLLaMA 2's superior performance in multimodal\ncomprehension, setting a new standard for intelligent video analysis systems.\nAll models are public to facilitate further research.\n","authors":["Zesen Cheng","Sicong Leng","Hang Zhang","Yifei Xin","Xin Li","Guanzheng Chen","Yongxin Zhu","Wenqi Zhang","Ziyang Luo","Deli Zhao","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2406.07476v1.pdf","comment":"ZC, SL, HZ, YX, and XL contributed equally to this project"},{"id":"http://arxiv.org/abs/2210.06230v2","updated":"2024-06-11T17:15:02Z","published":"2022-10-12T14:20:33Z","title":"Formal Semantic Geometry over Transformer-based Variational AutoEncoder","summary":"  Formal/symbolic semantics can provide canonical, rigid controllability and\ninterpretability to sentence representations due to their \\textit{localisation}\nor \\textit{composition} property. How can we deliver such property to the\ncurrent distributional sentence representations to control and interpret the\ngeneration of language models (LMs)? In this work, we theoretically frame the\nsentence semantics as the composition of \\textit{semantic role - word content}\nfeatures and propose the formal semantic geometry. To inject such geometry into\nTransformer-based LMs (i.e. GPT2), we deploy Transformer-based Variational\nAutoEncoder with a supervision approach, where the sentence generation can be\nmanipulated and explained over low-dimensional latent Gaussian space. In\naddition, we propose a new probing algorithm to guide the movement of sentence\nvectors over such geometry. Experimental results reveal that the formal\nsemantic geometry can potentially deliver better control and interpretation to\nsentence generation.\n","authors":["Yingji Zhang","Danilo S. Carvalho","Ian Pratt-Hartmann","André Freitas"],"pdf_url":"https://arxiv.org/pdf/2210.06230v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07466v1","updated":"2024-06-11T17:12:41Z","published":"2024-06-11T17:12:41Z","title":"Multimodal Belief Prediction","summary":"  Recognizing a speaker's level of commitment to a belief is a difficult task;\nhumans do not only interpret the meaning of the words in context, but also\nunderstand cues from intonation and other aspects of the audio signal. Many\npapers and corpora in the NLP community have approached the belief prediction\ntask using text-only approaches. We are the first to frame and present results\non the multimodal belief prediction task. We use the CB-Prosody corpus (CBP),\ncontaining aligned text and audio with speaker belief annotations. We first\nreport baselines and significant features using acoustic-prosodic features and\ntraditional machine learning methods. We then present text and audio baselines\nfor the CBP corpus fine-tuning on BERT and Whisper respectively. Finally, we\npresent our multimodal architecture which fine-tunes on BERT and Whisper and\nuses multiple fusion methods, improving on both modalities alone.\n","authors":["John Murzaku","Adil Soubki","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2406.07466v1.pdf","comment":"John Murzaku and Adil Soubki contributed equally to this work"},{"id":"http://arxiv.org/abs/2406.07444v1","updated":"2024-06-11T16:51:14Z","published":"2024-06-11T16:51:14Z","title":"On the Robustness of Document-Level Relation Extraction Models to Entity\n  Name Variations","summary":"  Driven by the demand for cross-sentence and large-scale relation extraction,\ndocument-level relation extraction (DocRE) has attracted increasing research\ninterest. Despite the continuous improvement in performance, we find that\nexisting DocRE models which initially perform well may make more mistakes when\nmerely changing the entity names in the document, hindering the generalization\nto novel entity names. To this end, we systematically investigate the\nrobustness of DocRE models to entity name variations in this work. We first\npropose a principled pipeline to generate entity-renamed documents by replacing\nthe original entity names with names from Wikidata. By applying the pipeline to\nDocRED and Re-DocRED datasets, we construct two novel benchmarks named\nEnv-DocRED and Env-Re-DocRED for robustness evaluation. Experimental results\nshow that both three representative DocRE models and two in-context learned\nlarge language models consistently lack sufficient robustness to entity name\nvariations, particularly on cross-sentence relation instances and documents\nwith more entities. Finally, we propose an entity variation robust training\nmethod which not only improves the robustness of DocRE models but also enhances\ntheir understanding and reasoning capabilities. We further verify that the\nbasic idea of this method can be effectively transferred to in-context learning\nfor DocRE as well.\n","authors":["Shiao Meng","Xuming Hu","Aiwei Liu","Fukun Ma","Yawen Yang","Shuang Li","Lijie Wen"],"pdf_url":"https://arxiv.org/pdf/2406.07444v1.pdf","comment":"Accepted to ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.07440v1","updated":"2024-06-11T16:48:17Z","published":"2024-06-11T16:48:17Z","title":"Textual Similarity as a Key Metric in Machine Translation Quality\n  Estimation","summary":"  Machine Translation (MT) Quality Estimation (QE) assesses translation\nreliability without reference texts. This study introduces \"textual similarity\"\nas a new metric for QE, using sentence transformers and cosine similarity to\nmeasure semantic closeness. Analyzing data from the MLQE-PE dataset, we found\nthat textual similarity exhibits stronger correlations with human scores than\ntraditional metrics (hter, model evaluation etc.). Employing GAMMs as a\nstatistical tool, we demonstrated that textual similarity consistently\noutperforms other metrics across multiple language pairs in predicting human\nscores. We also found that \"hter\" actually failed to predict human scores in\nQE. Our findings highlight the effectiveness of textual similarity as a robust\nQE metric, recommending its integration with other metrics into QE frameworks\nand MT system training for improved accuracy and usability.\n","authors":["Kun Sun","Rong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05423v2","updated":"2024-06-11T16:43:11Z","published":"2023-09-11T12:50:28Z","title":"Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of\n  SSWP","summary":"  In expressive and controllable Text-to-Speech (TTS), explicit prosodic\nfeatures significantly improve the naturalness and controllability of\nsynthesised speech. However, manual prosody annotation is labor-intensive and\ninconsistent. To address this issue, a two-stage automatic annotation pipeline\nis novelly proposed in this paper. In the first stage, we use contrastive\npretraining of Speech-Silence and Word-Punctuation (SSWP) pairs to enhance\nprosodic information in latent representations. In the second stage, we build a\nmulti-modal prosody annotator, comprising pretrained encoders, a text-speech\nfusing scheme, and a sequence classifier. Experiments on English prosodic\nboundaries demonstrate that our method achieves state-of-the-art (SOTA)\nperformance with 0.72 and 0.93 f1 score for Prosodic Word and Prosodic Phrase\nboundary respectively, while bearing remarkable robustness to data scarcity.\n","authors":["Jinzuomu Zhong","Yang Li","Hui Huang","Korin Richmond","Jie Liu","Zhiba Su","Jing Guo","Benlai Tang","Fengjie Zhu"],"pdf_url":"https://arxiv.org/pdf/2309.05423v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05232v2","updated":"2024-06-11T16:41:52Z","published":"2024-06-07T19:38:05Z","title":"Improving Logits-based Detector without Logits from Black-box LLMs","summary":"  The advent of Large Language Models (LLMs) has revolutionized text\ngeneration, producing outputs that closely mimic human writing. This blurring\nof lines between machine- and human-written text presents new challenges in\ndistinguishing one from the other a task further complicated by the frequent\nupdates and closed nature of leading proprietary LLMs. Traditional logits-based\ndetection methods leverage surrogate models for identifying LLM-generated\ncontent when the exact logits are unavailable from black-box LLMs. However,\nthese methods grapple with the misalignment between the distributions of the\nsurrogate and the often undisclosed target models, leading to performance\ndegradation, particularly with the introduction of new, closed-source models.\nFurthermore, while current methodologies are generally effective when the\nsource model is identified, they falter in scenarios where the model version\nremains unknown, or the test set comprises outputs from various source models.\nTo address these limitations, we present Distribution-Aligned LLMs Detection\n(DALD), an innovative framework that redefines the state-of-the-art performance\nin black-box text detection even without logits from source LLMs. DALD is\ndesigned to align the surrogate model's distribution with that of unknown\ntarget LLMs, ensuring enhanced detection capability and resilience against\nrapid model iterations with minimal training investment. By leveraging corpus\nsamples from publicly accessible outputs of advanced models such as ChatGPT,\nGPT-4 and Claude-3, DALD fine-tunes surrogate models to synchronize with\nunknown source model distributions effectively.\n","authors":["Cong Zeng","Shengkun Tang","Xianjun Yang","Yuanzhou Chen","Yiyou Sun","zhiqiang xu","Yao Li","Haifeng Chen","Wei Cheng","Dongkuan Xu"],"pdf_url":"https://arxiv.org/pdf/2406.05232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07430v1","updated":"2024-06-11T16:34:02Z","published":"2024-06-11T16:34:02Z","title":"Learning Domain-Invariant Features for Out-of-Context News Detection","summary":"  Multimodal out-of-context news is a common type of misinformation on online\nmedia platforms. This involves posting a caption, alongside an invalid\nout-of-context news image. Reflecting its importance, researchers have\ndeveloped models to detect such misinformation. However, a common limitation of\nthese models is that they only consider the scenario where pre-labeled data is\navailable for each domain, failing to address the out-of-context news detection\non unlabeled domains (e.g., unverified news on new topics or agencies). In this\nwork, we therefore focus on domain adaptive out-of-context news detection. In\norder to effectively adapt the detection model to unlabeled news topics or\nagencies, we propose ConDA-TTA (Contrastive Domain Adaptation with Test-Time\nAdaptation) which applies contrastive learning and maximum mean discrepancy\n(MMD) to learn the domain-invariant feature. In addition, it leverages target\ndomain statistics during test-time to further assist domain adaptation.\nExperimental results show that our approach outperforms baselines in 5 out of 7\ndomain adaptation settings on two public datasets, by as much as 2.93% in F1\nand 2.08% in accuracy.\n","authors":["Yimeng Gu","Mengqi Zhang","Ignacio Castro","Shu Wu","Gareth Tyson"],"pdf_url":"https://arxiv.org/pdf/2406.07430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07424v1","updated":"2024-06-11T16:26:18Z","published":"2024-06-11T16:26:18Z","title":"MINERS: Multilingual Language Models as Semantic Retrievers","summary":"  Words have been represented in a high-dimensional vector space that encodes\ntheir semantic similarities, enabling downstream applications such as\nretrieving synonyms, antonyms, and relevant contexts. However, despite recent\nadvances in multilingual language models (LMs), the effectiveness of these\nmodels' representations in semantic retrieval contexts has not been\ncomprehensively explored. To fill this gap, this paper introduces the MINERS, a\nbenchmark designed to evaluate the ability of multilingual LMs in semantic\nretrieval tasks, including bitext mining and classification via\nretrieval-augmented contexts. We create a comprehensive framework to assess the\nrobustness of LMs in retrieving samples across over 200 diverse languages,\nincluding extremely low-resource languages in challenging cross-lingual and\ncode-switching settings. Our results demonstrate that by solely retrieving\nsemantically similar embeddings yields performance competitive with\nstate-of-the-art approaches, without requiring any fine-tuning.\n","authors":["Genta Indra Winata","Ruochen Zhang","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2406.07424v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2305.12307v3","updated":"2024-06-11T16:16:19Z","published":"2023-05-21T00:32:37Z","title":"OntoType: Ontology-Guided and Pre-Trained Language Model Assisted\n  Fine-Grained Entity Typing","summary":"  Fine-grained entity typing (FET), which assigns entities in text with\ncontext-sensitive, fine-grained semantic types, is a basic but important task\nfor knowledge extraction from unstructured text. FET has been studied\nextensively in natural language processing and typically relies on\nhuman-annotated corpora for training, which is costly and difficult to scale.\nRecent studies explore the utilization of pre-trained language models (PLMs) as\na knowledge base to generate rich and context-aware weak supervision for FET.\nHowever, a PLM still requires direction and guidance to serve as a knowledge\nbase as they often generate a mixture of rough and fine-grained types, or\ntokens unsuitable for typing. In this study, we vision that an ontology\nprovides a semantics-rich, hierarchical structure, which will help select the\nbest results generated by multiple PLM models and head words. Specifically, we\npropose a novel annotation-free, ontology-guided FET method, OntoType, which\nfollows a type ontological structure, from coarse to fine, ensembles multiple\nPLM prompting results to generate a set of type candidates, and refines its\ntype resolution, under the local context with a natural language inference\nmodel. Our experiments on the Ontonotes, FIGER, and NYT datasets using their\nassociated ontological structures demonstrate that our method outperforms the\nstate-of-the-art zero-shot fine-grained entity typing methods as well as a\ntypical LLM method, ChatGPT. Our error analysis shows that refinement of the\nexisting ontology structures will further improve fine-grained entity typing.\n","authors":["Tanay Komarlu","Minhao Jiang","Xuan Wang","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2305.12307v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07411v1","updated":"2024-06-11T16:15:06Z","published":"2024-06-11T16:15:06Z","title":"VersiCode: Towards Version-controllable Code Generation","summary":"  Significant research has focused on improving the performance of large\nlanguage model on code-related tasks due to their practical importance.\nAlthough performance is typically evaluated using public benchmark datasets,\nthe existing datasets do not account for the concept of \\emph{version}, which\nis crucial in professional software development. In this paper, we introduce\nVersiCode, the first comprehensive dataset designed to assess the ability of\nlarge language models to generate verifiable code for specific library\nversions. VersiCode encompasses 300 libraries across more than 2,000 versions\nspanning 9 years. We design two dedicated evaluation tasks: version-specific\ncode completion (VSCC) and version-aware code editing (VACE). Comprehensive\nexperiments are conducted to benchmark the performance of LLMs, revealing the\nchallenging nature of these tasks and VersiCode, that even state-of-the-art\nLLMs struggle to generate version-correct code. This dataset, together with the\nproposed tasks, sheds light on LLMs' capabilities and limitations in handling\nversion-specific code generation, and opens up an important new area of\nresearch for further investigation. The resources can be found at\nhttps://github.com/wutong8023/VersiCode.\n","authors":["Tongtong Wu","Weigang Wu","Xingyu Wang","Kang Xu","Suyu Ma","Bo Jiang","Ping Yang","Zhenchang Xing","Yuan-Fang Li","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.07411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07393v1","updated":"2024-06-11T15:58:59Z","published":"2024-06-11T15:58:59Z","title":"Limited Out-of-Context Knowledge Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated strong capabilities as\nknowledge bases and significant in-context reasoning capabilities. However,\nprevious work challenges their out-of-context reasoning ability, i.e., the\nability to infer information from their training data, instead of from the\ncontext or prompt. This paper focuses on a significant facet of out-of-context\nreasoning: Out-of-Context Knowledge Reasoning (OCKR), which is to combine\nmultiple knowledge to infer new knowledge. We designed a synthetic dataset with\nseven representative OCKR tasks to systematically assess the OCKR capabilities\nof LLMs. Using this dataset, we evaluated the LLaMA2-13B-chat model and\ndiscovered that its proficiency in this aspect is limited, regardless of\nwhether the knowledge is trained in a separate or adjacent training settings.\nMoreover, training the model to reason with complete reasoning data did not\nresult in significant improvement. Training the model to perform explicit\nknowledge retrieval helps in only one of the tasks, indicating that the model's\nlimited OCKR capabilities are due to difficulties in retrieving relevant\nknowledge. Furthermore, we treat cross-lingual knowledge transfer as a distinct\nform of OCKR, and evaluate this ability. Our results show that the evaluated\nmodel also exhibits limited ability in transferring knowledge across languages.\nThe dataset used in this study is available at\nhttps://github.com/NJUNLP/ID-OCKR.\n","authors":["Peng Hu","Changjiang Gao","Ruiqi Gao","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2406.07393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03329v3","updated":"2024-06-11T15:47:39Z","published":"2024-03-05T21:19:06Z","title":"Guardrail Baselines for Unlearning in LLMs","summary":"  Recent work has demonstrated that finetuning is a promising approach to\n'unlearn' concepts from large language models. However, finetuning can be\nexpensive, as it requires both generating a set of examples and running\niterations of finetuning to update the model. In this work, we show that simple\nguardrail-based approaches such as prompting and filtering can achieve\nunlearning results comparable to finetuning. We recommend that researchers\ninvestigate these lightweight baselines when evaluating the performance of more\ncomputationally intensive finetuning methods. While we do not claim that\nmethods such as prompting or filtering are universal solutions to the problem\nof unlearning, our work suggests the need for evaluation metrics that can\nbetter separate the power of guardrails vs. finetuning, and highlights\nscenarios where guardrails expose possible unintended behavior in existing\nmetrics and benchmarks.\n","authors":["Pratiksha Thaker","Yash Maurya","Shengyuan Hu","Zhiwei Steven Wu","Virginia Smith"],"pdf_url":"https://arxiv.org/pdf/2403.03329v3.pdf","comment":"Preliminary work, accepted to ICLR workshop SeT-LLM 2024"},{"id":"http://arxiv.org/abs/2406.07378v1","updated":"2024-06-11T15:45:24Z","published":"2024-06-11T15:45:24Z","title":"Large Language Models for Constrained-Based Causal Discovery","summary":"  Causality is essential for understanding complex systems, such as the\neconomy, the brain, and the climate. Constructing causal graphs often relies on\neither data-driven or expert-driven approaches, both fraught with challenges.\nThe former methods, like the celebrated PC algorithm, face issues with data\nrequirements and assumptions of causal sufficiency, while the latter demand\nsubstantial time and domain knowledge. This work explores the capabilities of\nLarge Language Models (LLMs) as an alternative to domain experts for causal\ngraph generation. We frame conditional independence queries as prompts to LLMs\nand employ the PC algorithm with the answers. The performance of the LLM-based\nconditional independence oracle on systems with known causal graphs shows a\nhigh degree of variability. We improve the performance through a proposed\nstatistical-inspired voting schema that allows some control over false-positive\nand false-negative rates. Inspecting the chain-of-thought argumentation, we\nfind causal reasoning to justify its answer to a probabilistic query. We show\nevidence that knowledge-based CIT could eventually become a complementary tool\nfor data-driven causal discovery.\n","authors":["Kai-Hendrik Cohrs","Gherardo Varando","Emiliano Diaz","Vasileios Sitokonstantinou","Gustau Camps-Valls"],"pdf_url":"https://arxiv.org/pdf/2406.07378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16630v2","updated":"2024-06-11T15:40:43Z","published":"2023-07-31T13:08:16Z","title":"Text-CRS: A Generalized Certified Robustness Framework against Textual\n  Adversarial Attacks","summary":"  The language models, especially the basic text classification models, have\nbeen shown to be susceptible to textual adversarial attacks such as synonym\nsubstitution and word insertion attacks. To defend against such attacks, a\ngrowing body of research has been devoted to improving the model robustness.\nHowever, providing provable robustness guarantees instead of empirical\nrobustness is still widely unexplored. In this paper, we propose Text-CRS, a\ngeneralized certified robustness framework for natural language processing\n(NLP) based on randomized smoothing. To our best knowledge, existing certified\nschemes for NLP can only certify the robustness against $\\ell_0$ perturbations\nin synonym substitution attacks. Representing each word-level adversarial\noperation (i.e., synonym substitution, word reordering, insertion, and\ndeletion) as a combination of permutation and embedding transformation, we\npropose novel smoothing theorems to derive robustness bounds in both\npermutation and embedding space against such adversarial operations. To further\nimprove certified accuracy and radius, we consider the numerical relationships\nbetween discrete words and select proper noise distributions for the randomized\nsmoothing. Finally, we conduct substantial experiments on multiple language\nmodels and datasets. Text-CRS can address all four different word-level\nadversarial operations and achieve a significant accuracy improvement. We also\nprovide the first benchmark on certified accuracy and radius of four word-level\noperations, besides outperforming the state-of-the-art certification against\nsynonym substitution attacks.\n","authors":["Xinyu Zhang","Hanbin Hong","Yuan Hong","Peng Huang","Binghui Wang","Zhongjie Ba","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2307.16630v2.pdf","comment":"Published in the 2024 IEEE Symposium on Security and Privacy (SP)"},{"id":"http://arxiv.org/abs/2406.07368v1","updated":"2024-06-11T15:34:43Z","published":"2024-06-11T15:34:43Z","title":"When Linear Attention Meets Autoregressive Decoding: Towards More\n  Effective and Efficient Linearized Large Language Models","summary":"  Autoregressive Large Language Models (LLMs) have achieved impressive\nperformance in language tasks but face two significant bottlenecks: (1)\nquadratic complexity in the attention module as the number of tokens increases,\nand (2) limited efficiency due to the sequential processing nature of\nautoregressive LLMs during generation. While linear attention and speculative\ndecoding offer potential solutions, their applicability and synergistic\npotential for enhancing autoregressive LLMs remain uncertain. We conduct the\nfirst comprehensive study on the efficacy of existing linear attention methods\nfor autoregressive LLMs, integrating them with speculative decoding. We\nintroduce an augmentation technique for linear attention that ensures\ncompatibility with speculative decoding, enabling more efficient training and\nserving of LLMs. Extensive experiments and ablation studies involving seven\nexisting linear attention models and five encoder/decoder-based LLMs\nconsistently validate the effectiveness of our augmented linearized LLMs.\nNotably, our approach achieves up to a 6.67 reduction in perplexity on the\nLLaMA model and up to a 2$\\times$ speedup during generation compared to prior\nlinear attention methods. Codes and models are available at\nhttps://github.com/GATECH-EIC/Linearized-LLM.\n","authors":["Haoran You","Yichao Fu","Zheng Wang","Amir Yazdanbakhsh"," Yingyan"," Lin"],"pdf_url":"https://arxiv.org/pdf/2406.07368v1.pdf","comment":"Accepted by ICML 2024; 17 pages; 10 figures; 16 tables"},{"id":"http://arxiv.org/abs/2304.01666v2","updated":"2024-06-11T15:33:13Z","published":"2023-04-04T09:50:19Z","title":"A Survey on Contextualised Semantic Shift Detection","summary":"  Semantic Shift Detection (SSD) is the task of identifying, interpreting, and\nassessing the possible change over time in the meanings of a target word.\nTraditionally, SSD has been addressed by linguists and social scientists\nthrough manual and time-consuming activities. In the recent years,\ncomputational approaches based on Natural Language Processing and word\nembeddings gained increasing attention to automate SSD as much as possible. In\nparticular, over the past three years, significant advancements have been made\nalmost exclusively based on word contextualised embedding models, which can\nhandle the multiple usages/meanings of the words and better capture the related\nsemantic shifts. In this paper, we survey the approaches based on\ncontextualised embeddings for SSD (i.e., CSSDetection) and we propose a\nclassification framework characterised by meaning representation,\ntime-awareness, and learning modality dimensions. The framework is exploited i)\nto review the measures for shift assessment, ii) to compare the approaches on\nperformance, and iii) to discuss the current issues in terms of scalability,\ninterpretability, and robustness. Open challenges and future research\ndirections about CSSDetection are finally outlined.\n","authors":["Stefano Montanelli","Francesco Periti"],"pdf_url":"https://arxiv.org/pdf/2304.01666v2.pdf","comment":"Acceted at ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2406.07365v1","updated":"2024-06-11T15:32:32Z","published":"2024-06-11T15:32:32Z","title":"BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad\n  Prediction","summary":"  Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based\nelements, including aspect term, opinion term, aspect category, and sentiment\npolarity. In practice, unseen aspects, due to distinct data distribution,\nimpose many challenges for a trained neural model. Motivated by this, this work\nformulates ASQP into the few-shot scenario, which aims for fast adaptation in\nreal applications. Therefore, we first construct a few-shot ASQP dataset (FSQP)\nthat contains richer categories and is more balanced for the few-shot study.\nMoreover, recent methods extract quads through a generation paradigm, which\ninvolves converting the input sentence into a templated target sequence.\nHowever, they primarily focus on the utilization of a single template or the\nconsideration of different template orders, thereby overlooking the\ncorrelations among various templates. To tackle this issue, we further propose\na Broadview Soft Prompting (BvSP) method that aggregates multiple templates\nwith a broader view by taking into account the correlation between the\ndifferent templates. Specifically, BvSP uses the pre-trained language model to\nselect the most relevant k templates with Jensen-Shannon divergence. BvSP\nfurther introduces soft prompts to guide the pre-trained language model using\nthe selected templates. Then, we aggregate the results of multi-templates by\nvoting mechanism. Empirical results demonstrate that BvSP significantly\noutperforms the stateof-the-art methods under four few-shot settings and other\npublic datasets. Our code and dataset are available at\nhttps://github.com/byinhao/BvSP.\n","authors":["Yinhao Bai","Yalan Xie","Xiaoyi Liu","Yuhua Zhao","Zhixin Han","Mengting Hu","Hang Gao","Renhong Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.07365v1.pdf","comment":"Accepted to ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2406.07359v1","updated":"2024-06-11T15:27:01Z","published":"2024-06-11T15:27:01Z","title":"GLIMPSE: Pragmatically Informative Multi-Document Summarization for\n  Scholarly Reviews","summary":"  Scientific peer review is essential for the quality of academic publications.\nHowever, the increasing number of paper submissions to conferences has strained\nthe reviewing process. This surge poses a burden on area chairs who have to\ncarefully read an ever-growing volume of reviews and discern each reviewer's\nmain arguments as part of their decision process. In this paper, we introduce\n\\sys, a summarization method designed to offer a concise yet comprehensive\noverview of scholarly reviews. Unlike traditional consensus-based methods, \\sys\nextracts both common and unique opinions from the reviews. We introduce novel\nuniqueness scores based on the Rational Speech Act framework to identify\nrelevant sentences in the reviews. Our method aims to provide a pragmatic\nglimpse into all reviews, offering a balanced perspective on their opinions.\nOur experimental results with both automatic metrics and human evaluation show\nthat \\sys generates more discriminative summaries than baseline methods in\nterms of human evaluation while achieving comparable performance with these\nmethods in terms of automatic metrics.\n","authors":["Maxime Darrin","Ines Arous","Pablo Piantanida","Jackie CK Cheung"],"pdf_url":"https://arxiv.org/pdf/2406.07359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07358v1","updated":"2024-06-11T15:26:57Z","published":"2024-06-11T15:26:57Z","title":"AI Sandbagging: Language Models can Strategically Underperform on\n  Evaluations","summary":"  Trustworthy capability evaluations are crucial for ensuring the safety of AI\nsystems, and are becoming a key component of AI regulation. However, the\ndevelopers of an AI system, or the AI system itself, may have incentives for\nevaluations to understate the AI's actual capability. These conflicting\ninterests lead to the problem of sandbagging $\\unicode{x2013}$ which we define\nas \"strategic underperformance on an evaluation\". In this paper we assess\nsandbagging capabilities in contemporary language models (LMs). We prompt\nfrontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on\ndangerous capability evaluations, while maintaining performance on general\n(harmless) capability evaluations. Moreover, we find that models can be\nfine-tuned, on a synthetic dataset, to hide specific capabilities unless given\na password. This behaviour generalizes to high-quality, held-out benchmarks\nsuch as WMDP. In addition, we show that both frontier and smaller models can be\nprompted, or password-locked, to target specific scores on a capability\nevaluation. Even more, we found that a capable password-locked model (Llama 3\n70b) is reasonably able to emulate a less capable model (Llama 2 7b). Overall,\nour results suggest that capability evaluations are vulnerable to sandbagging.\nThis vulnerability decreases the trustworthiness of evaluations, and thereby\nundermines important safety decisions regarding the development and deployment\nof advanced AI systems.\n","authors":["Teun van der Weij","Felix Hofstätter","Ollie Jaffe","Samuel F. Brown","Francis Rhys Ward"],"pdf_url":"https://arxiv.org/pdf/2406.07358v1.pdf","comment":"We publish our code and results\n  $\\href{https://github.com/your-repo/your-project}{here}$"},{"id":"http://arxiv.org/abs/2406.07353v1","updated":"2024-06-11T15:22:48Z","published":"2024-06-11T15:22:48Z","title":"Toxic Memes: A Survey of Computational Perspectives on the Detection and\n  Explanation of Meme Toxicities","summary":"  Internet memes, channels for humor, social commentary, and cultural\nexpression, are increasingly used to spread toxic messages. Studies on the\ncomputational analyses of toxic memes have significantly grown over the past\nfive years, and the only three surveys on computational toxic meme analysis\ncover only work published until 2022, leading to inconsistent terminology and\nunexplored trends. Our work fills this gap by surveying content-based\ncomputational perspectives on toxic memes, and reviewing key developments until\nearly 2024. Employing the PRISMA methodology, we systematically extend the\npreviously considered papers, achieving a threefold result. First, we survey\n119 new papers, analyzing 158 computational works focused on content-based\ntoxic meme analysis. We identify over 30 datasets used in toxic meme analysis\nand examine their labeling systems. Second, after observing the existence of\nunclear definitions of meme toxicity in computational works, we introduce a new\ntaxonomy for categorizing meme toxicity types. We also note an expansion in\ncomputational tasks beyond the simple binary classification of memes as toxic\nor non-toxic, indicating a shift towards achieving a nuanced comprehension of\ntoxicity. Third, we identify three content-based dimensions of meme toxicity\nunder automatic study: target, intent, and conveyance tactics. We develop a\nframework illustrating the relationships between these dimensions and meme\ntoxicities. The survey analyzes key challenges and recent trends, such as\nenhanced cross-modal reasoning, integrating expert and cultural knowledge, the\ndemand for automatic toxicity explanations, and handling meme toxicity in\nlow-resource languages. Also, it notes the rising use of Large Language Models\n(LLMs) and generative AI for detecting and generating toxic memes. Finally, it\nproposes pathways for advancing toxic meme detection and interpretation.\n","authors":["Delfina Sol Martinez Pandiani","Erik Tjong Kim Sang","Davide Ceolin"],"pdf_url":"https://arxiv.org/pdf/2406.07353v1.pdf","comment":"39 pages, 12 figures, 9 tables"},{"id":"http://arxiv.org/abs/2310.02174v5","updated":"2024-06-11T15:22:07Z","published":"2023-10-03T16:08:41Z","title":"Ask Again, Then Fail: Large Language Models' Vacillations in Judgment","summary":"  We observe that current conversational language models often waver in their\njudgments when faced with follow-up questions, even if the original judgment\nwas correct. This wavering presents a significant challenge for generating\nreliable responses and building user trust. To comprehensively assess this\nissue, we introduce a \\textsc{Follow-up Questioning Mechanism} along with two\nmetrics to quantify this inconsistency, confirming its widespread presence in\ncurrent language models. To mitigate this issue, we explore various prompting\nstrategies for closed-source models; moreover, we develop a training-based\nframework \\textsc{Unwavering-FQ} that teaches language models to maintain their\noriginally correct judgments through synthesized high-quality preference data.\nOur experimental results confirm the effectiveness of our framework and its\nability to enhance the general capabilities of models.\n","authors":["Qiming Xie","Zengzhi Wang","Yi Feng","Rui Xia"],"pdf_url":"https://arxiv.org/pdf/2310.02174v5.pdf","comment":"Accepted by ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2406.06464v2","updated":"2024-06-11T15:17:43Z","published":"2024-06-10T17:00:54Z","title":"Transforming Wearable Data into Health Insights using Large Language\n  Model Agents","summary":"  Despite the proliferation of wearable health trackers and the importance of\nsleep and exercise to health, deriving actionable personalized insights from\nwearable data remains a challenge because doing so requires non-trivial\nopen-ended analysis of these data. The recent rise of large language model\n(LLM) agents, which can use tools to reason about and interact with the world,\npresents a promising opportunity to enable such personalized analysis at scale.\nYet, the application of LLM agents in analyzing personal health is still\nlargely untapped. In this paper, we introduce the Personal Health Insights\nAgent (PHIA), an agent system that leverages state-of-the-art code generation\nand information retrieval tools to analyze and interpret behavioral health data\nfrom wearables. We curate two benchmark question-answering datasets of over\n4000 health insights questions. Based on 650 hours of human and expert\nevaluation we find that PHIA can accurately address over 84% of factual\nnumerical questions and more than 83% of crowd-sourced open-ended questions.\nThis work has implications for advancing behavioral health across the\npopulation, potentially enabling individuals to interpret their own wearable\ndata, and paving the way for a new era of accessible, personalized wellness\nregimens that are informed by data-driven insights.\n","authors":["Mike A. Merrill","Akshay Paruchuri","Naghmeh Rezaei","Geza Kovacs","Javier Perez","Yun Liu","Erik Schenck","Nova Hammerquist","Jake Sunshine","Shyam Tailor","Kumar Ayush","Hao-Wei Su","Qian He","Cory Y. McLean","Mark Malhotra","Shwetak Patel","Jiening Zhan","Tim Althoff","Daniel McDuff","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2406.06464v2.pdf","comment":"38 pages"},{"id":"http://arxiv.org/abs/2406.07348v1","updated":"2024-06-11T15:15:33Z","published":"2024-06-11T15:15:33Z","title":"DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented\n  Generation for Question-Answering","summary":"  Retrieval-Augmented Generation (RAG) has significantly demonstrated the\nperformance of Large Language Models (LLMs) in the knowledge-intensive tasks,\nsuch as Question-Answering (QA). RAG expands the query context by incorporating\nexternal knowledge bases to enhance the response accuracy. However, it would be\ninefficient to access LLMs multiple times for each query and unreliable to\nretrieve all the relevant documents by a single query. We find that even though\nthere is low relevance between some critical documents and query, it is\npossible to retrieve the remaining documents by combining parts of the\ndocuments with the query. To mine the relevance, a two-stage retrieval\nframework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is\nproposed to improve document retrieval recall and the accuracy of answers while\nmaintaining efficiency. Also, a small classifier is applied to two different\nselection strategies to determine the contribution of the retrieved documents\nto answering the query and retrieve the relatively relevant documents.\nMeanwhile, DR-RAG call the LLMs only once, which significantly improves the\nefficiency of the experiment. The experimental results on multi-hop QA datasets\nshow that DR-RAG can significantly improve the accuracy of the answers and\nachieve new progress in QA systems.\n","authors":["Zijian Hei","Weiling Wei","Wenjie Ou","Juyi Qiao","Junming Jiao","Zhiqing Zhu","Guowen Song"],"pdf_url":"https://arxiv.org/pdf/2406.07348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05981v2","updated":"2024-06-11T15:14:30Z","published":"2024-06-10T02:47:55Z","title":"ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training\n  Multiplication-Less Reparameterization","summary":"  Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM.\n","authors":["Haoran You","Yipin Guo","Yichao Fu","Wei Zhou","Huihong Shi","Xiaofan Zhang","Souvik Kundu","Amir Yazdanbakhsh"," Yingyan"," Lin"],"pdf_url":"https://arxiv.org/pdf/2406.05981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16854v3","updated":"2024-06-11T15:12:09Z","published":"2024-03-25T15:17:05Z","title":"An Expert is Worth One Token: Synergizing Multiple Expert LLMs as\n  Generalist via Expert Token Routing","summary":"  We present Expert-Token-Routing, a unified generalist framework that\nfacilitates seamless integration of multiple expert LLMs. Our framework\nrepresents expert LLMs as special expert tokens within the vocabulary of a meta\nLLM. The meta LLM can route to an expert LLM like generating new tokens.\nExpert-Token-Routing not only supports learning the implicit expertise of\nexpert LLMs from existing instruction dataset but also allows for dynamic\nextension of new expert LLMs in a plug-and-play manner. It also conceals the\ndetailed collaboration process from the user's perspective, facilitating\ninteraction as though it were a singular LLM. Our framework outperforms various\nexisting multi-LLM collaboration paradigms across benchmarks that incorporate\nsix diverse expert domains, demonstrating effectiveness and robustness in\nbuilding generalist LLM system via synergizing multiple expert LLMs.\n","authors":["Ziwei Chai","Guoyin Wang","Jing Su","Tianjie Zhang","Xuanwen Huang","Xuwu Wang","Jingjing Xu","Jianbo Yuan","Hongxia Yang","Fei Wu","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16854v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06326v2","updated":"2024-06-11T15:03:43Z","published":"2024-06-10T14:42:20Z","title":"Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge\n  through Self-Teaching","summary":"  Large language models (LLMs) often struggle to provide up-to-date information\ndue to their one-time training and the constantly evolving nature of the world.\nTo keep LLMs current, existing approaches typically involve continued\npre-training on new documents. However, they frequently face difficulties in\nextracting stored knowledge. Motivated by the remarkable success of the Feynman\nTechnique in efficient human learning, we introduce Self-Tuning, a learning\nframework aimed at improving an LLM's ability to effectively acquire new\nknowledge from raw documents through self-teaching. Specifically, we develop a\nSelf-Teaching strategy that augments the documents with a set of\nknowledge-intensive tasks created in a self-supervised manner, focusing on\nthree crucial aspects: memorization, comprehension, and self-reflection.\nAdditionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate\nan in-depth analysis of an LLM's knowledge acquisition ability concerning\nmemorization, extraction, and reasoning. Extensive experimental results on\nLlama2 family models reveal that Self-Tuning consistently exhibits superior\nperformance across all knowledge acquisition tasks and excels in preserving\nprevious knowledge.\n","authors":["Xiaoying Zhang","Baolin Peng","Ye Tian","Jingyan Zhou","Yipeng Zhang","Haitao Mi","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2406.06326v2.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2406.07330v1","updated":"2024-06-11T15:00:33Z","published":"2024-06-11T15:00:33Z","title":"CTC-based Non-autoregressive Textless Speech-to-Speech Translation","summary":"  Direct speech-to-speech translation (S2ST) has achieved impressive\ntranslation quality, but it often faces the challenge of slow decoding due to\nthe considerable length of speech sequences. Recently, some research has turned\nto non-autoregressive (NAR) models to expedite decoding, yet the translation\nquality typically lags behind autoregressive (AR) models significantly. In this\npaper, we investigate the performance of CTC-based NAR models in S2ST, as these\nmodels have shown impressive results in machine translation. Experimental\nresults demonstrate that by combining pretraining, knowledge distillation, and\nadvanced NAR training techniques such as glancing training and non-monotonic\nlatent alignments, CTC-based NAR models achieve translation quality comparable\nto the AR model, while preserving up to 26.81$\\times$ decoding speedup.\n","authors":["Qingkai Fang","Zhengrui Ma","Yan Zhou","Min Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2406.07330v1.pdf","comment":"ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.07327v1","updated":"2024-06-11T14:59:24Z","published":"2024-06-11T14:59:24Z","title":"3D-Properties: Identifying Challenges in DPO and Charting a Path Forward","summary":"  Aligning large language models (LLMs) with human preference has recently\ngained tremendous attention, with the canonical yet costly RLHF-PPO and the\nsimple and straightforward Direct Preference Optimization (DPO) as two\nexamples. Despite the efficiency, DPO has rarely be used in the\nstate-of-the-art production-level LLMs, implying its potential pathologies. In\nthis work, we revisit DPO with a comprehensive examination of its empirical\nefficacy and a systematic comparison with RLHF-PPO. We identify the\n\\textbf{3D}-properties of DPO's learning outcomes: the \\textbf{D}rastic drop in\nthe likelihood of rejected responses, the \\textbf{D}egradation into LLM\nunlearning, and the \\textbf{D}ispersion effect on unseen responses through\nexperiments with both a carefully designed toy model and practical LLMs on\ntasks including mathematical problem-solving and instruction following. These\nfindings inherently connect to some observations made by related works and we\nadditionally contribute a plausible theoretical explanation for them.\nAccordingly, we propose easy regularization methods to mitigate the issues\ncaused by \\textbf{3D}-properties, improving the training stability and final\nperformance of DPO. Our contributions also include an investigation into how\nthe distribution of the paired preference data impacts the effectiveness of\nDPO. We hope this work could offer research directions to narrow the gap\nbetween reward-free preference learning methods and reward-based ones.\n","authors":["Yuzi Yan","Yibo Miao","Jialian Li","Yipin Zhang","Jian Xie","Zhijie Deng","Dong Yan"],"pdf_url":"https://arxiv.org/pdf/2406.07327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05967v2","updated":"2024-06-11T14:58:42Z","published":"2024-01-11T15:13:00Z","title":"Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph\n  Embedding","summary":"  The primary aim of Knowledge Graph embeddings (KGE) is to learn\nlow-dimensional representations of entities and relations for predicting\nmissing facts. While rotation-based methods like RotatE and QuatE perform well\nin KGE, they face two challenges: limited model flexibility requiring\nproportional increases in relation size with entity dimension, and difficulties\nin generalizing the model for higher-dimensional rotations. To address these\nissues, we introduce OrthogonalE, a novel KGE model employing matrices for\nentities and block-diagonal orthogonal matrices with Riemannian optimization\nfor relations. This approach enhances the generality and flexibility of KGE\nmodels. The experimental results indicate that our new KGE model, OrthogonalE,\nis both general and flexible, significantly outperforming state-of-the-art KGE\nmodels while substantially reducing the number of relation parameters.\n","authors":["Yihua Zhu","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2401.05967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07310v1","updated":"2024-06-11T14:38:29Z","published":"2024-06-11T14:38:29Z","title":"MM-KWS: Multi-modal Prompts for Multilingual User-defined Keyword\n  Spotting","summary":"  In this paper, we propose MM-KWS, a novel approach to user-defined keyword\nspotting leveraging multi-modal enrollments of text and speech templates.\nUnlike previous methods that focus solely on either text or speech features,\nMM-KWS extracts phoneme, text, and speech embeddings from both modalities.\nThese embeddings are then compared with the query speech embedding to detect\nthe target keywords. To ensure the applicability of MM-KWS across diverse\nlanguages, we utilize a feature extractor incorporating several multilingual\npre-trained models. Subsequently, we validate its effectiveness on Mandarin and\nEnglish tasks. In addition, we have integrated advanced data augmentation tools\nfor hard case mining to enhance MM-KWS in distinguishing confusable words.\nExperimental results on the LibriPhrase and WenetPhrase datasets demonstrate\nthat MM-KWS outperforms prior methods significantly.\n","authors":["Zhiqi Ai","Zhiyong Chen","Shugong Xu"],"pdf_url":"https://arxiv.org/pdf/2406.07310v1.pdf","comment":"Accepted at INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.07302v1","updated":"2024-06-11T14:30:34Z","published":"2024-06-11T14:30:34Z","title":"BertaQA: How Much Do Language Models Know About Local Culture?","summary":"  Large Language Models (LLMs) exhibit extensive knowledge about the world, but\nmost evaluations have been limited to global or anglocentric subjects. This\nraises the question of how well these models perform on topics relevant to\nother cultures, whose presence on the web is not that prominent. To address\nthis gap, we introduce BertaQA, a multiple-choice trivia dataset that is\nparallel in English and Basque. The dataset consists of a local subset with\nquestions pertinent to the Basque culture, and a global subset with questions\nof broader interest. We find that state-of-the-art LLMs struggle with local\ncultural knowledge, even as they excel on global topics. However, we show that\ncontinued pre-training in Basque significantly improves the models' performance\non Basque culture, even when queried in English. To our knowledge, this is the\nfirst solid evidence of knowledge transfer from a low-resource to a\nhigh-resource language. Our analysis sheds light on the complex interplay\nbetween language and knowledge, and reveals that some prior findings do not\nfully hold when reassessed on local topics. Our dataset and evaluation code are\navailable under open licenses at https://github.com/juletx/BertaQA.\n","authors":["Julen Etxaniz","Gorka Azkune","Aitor Soroa","Oier Lopez de Lacalle","Mikel Artetxe"],"pdf_url":"https://arxiv.org/pdf/2406.07302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07296v1","updated":"2024-06-11T14:24:45Z","published":"2024-06-11T14:24:45Z","title":"Instruct Large Language Models to Drive like Humans","summary":"  Motion planning in complex scenarios is the core challenge in autonomous\ndriving. Conventional methods apply predefined rules or learn from driving data\nto plan the future trajectory. Recent methods seek the knowledge preserved in\nlarge language models (LLMs) and apply them in the driving scenarios. Despite\nthe promising results, it is still unclear whether the LLM learns the\nunderlying human logic to drive. In this paper, we propose an InstructDriver\nmethod to transform LLM into a motion planner with explicit instruction tuning\nto align its behavior with humans. We derive driving instruction data based on\nhuman logic (e.g., do not cause collisions) and traffic rules (e.g., proceed\nonly when green lights). We then employ an interpretable InstructChain module\nto further reason the final planning reflecting the instructions. Our\nInstructDriver allows the injection of human rules and learning from driving\ndata, enabling both interpretability and data scalability. Different from\nexisting methods that experimented on closed-loop or simulated settings, we\nadopt the real-world closed-loop motion planning nuPlan benchmark for better\nevaluation. InstructDriver demonstrates the effectiveness of the LLM planner in\na real-world closed-loop setting. Our code is publicly available at\nhttps://github.com/bonbon-rj/InstructDriver.\n","authors":["Ruijun Zhang","Xianda Guo","Wenzhao Zheng","Chenming Zhang","Kurt Keutzer","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2406.07296v1.pdf","comment":"project page: https://github.com/bonbon-rj/InstructDriver"},{"id":"http://arxiv.org/abs/2406.07291v1","updated":"2024-06-11T14:22:37Z","published":"2024-06-11T14:22:37Z","title":"Joint Learning of Context and Feedback Embeddings in Spoken Dialogue","summary":"  Short feedback responses, such as backchannels, play an important role in\nspoken dialogue. So far, most of the modeling of feedback responses has focused\non their timing, often neglecting how their lexical and prosodic form influence\ntheir contextual appropriateness and conversational function. In this paper, we\ninvestigate the possibility of embedding short dialogue contexts and feedback\nresponses in the same representation space using a contrastive learning\nobjective. In our evaluation, we primarily focus on how such embeddings can be\nused as a context-feedback appropriateness metric and thus for feedback\nresponse ranking in U.S. English dialogues. Our results show that the model\noutperforms humans given the same ranking task and that the learned embeddings\ncarry information about the conversational function of feedback responses.\n","authors":["Livia Qian","Gabriel Skantze"],"pdf_url":"https://arxiv.org/pdf/2406.07291v1.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.06371v2","updated":"2024-06-11T14:19:42Z","published":"2024-06-10T15:32:42Z","title":"mHuBERT-147: A Compact Multilingual HuBERT Model","summary":"  We present mHuBERT-147, the first general-purpose massively multilingual\nHuBERT speech representation model trained on 90K hours of clean, open-license\ndata. To scale up the multi-iteration HuBERT approach, we use faiss-based\nclustering, achieving 5.2x faster label assignment than the original method. We\nalso apply a new multilingual batching up-sampling strategy, leveraging both\nlanguage and dataset diversity. After 3 training iterations, our compact 95M\nparameter mHuBERT-147 outperforms larger models trained on substantially more\ndata. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with\nSOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses\nXLS-R (300M params; 436K hours) and demonstrates strong competitiveness against\nthe much larger MMS (1B params; 491K hours). Our findings indicate that\nmHuBERT-147 is a promising model for multilingual speech tasks, offering an\nunprecedented balance between high performance and parameter efficiency.\n","authors":["Marcely Zanon Boito","Vivek Iyer","Nikolaos Lagos","Laurent Besacier","Ioan Calapodescu"],"pdf_url":"https://arxiv.org/pdf/2406.06371v2.pdf","comment":"Extended version of the Interspeech 2024 paper of same name"},{"id":"http://arxiv.org/abs/2406.07289v1","updated":"2024-06-11T14:17:12Z","published":"2024-06-11T14:17:12Z","title":"Can We Achieve High-quality Direct Speech-to-Speech Translation without\n  Parallel Speech Data?","summary":"  Recently proposed two-pass direct speech-to-speech translation (S2ST) models\ndecompose the task into speech-to-text translation (S2TT) and text-to-speech\n(TTS) within an end-to-end model, yielding promising results. However, the\ntraining of these models still relies on parallel speech data, which is\nextremely challenging to collect. In contrast, S2TT and TTS have accumulated a\nlarge amount of data and pretrained models, which have not been fully utilized\nin the development of S2ST models. Inspired by this, in this paper, we first\nintroduce a composite S2ST model named ComSpeech, which can seamlessly\nintegrate any pretrained S2TT and TTS models into a direct S2ST model.\nFurthermore, to eliminate the reliance on parallel speech data, we propose a\nnovel training method ComSpeech-ZS that solely utilizes S2TT and TTS data. It\naligns representations in the latent space through contrastive learning,\nenabling the speech synthesis capability learned from the TTS data to\ngeneralize to S2ST in a zero-shot manner. Experimental results on the CVSS\ndataset show that when the parallel speech data is available, ComSpeech\nsurpasses previous two-pass models like UnitY and Translatotron 2 in both\ntranslation quality and decoding speed. When there is no parallel speech data,\nComSpeech-ZS lags behind \\name by only 0.7 ASR-BLEU and outperforms the\ncascaded models.\n","authors":["Qingkai Fang","Shaolei Zhang","Zhengrui Ma","Min Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2406.07289v1.pdf","comment":"ACL 2024 main conference. Project Page:\n  https://ictnlp.github.io/ComSpeech-Site/"},{"id":"http://arxiv.org/abs/2406.07288v1","updated":"2024-06-11T14:16:14Z","published":"2024-06-11T14:16:14Z","title":"Fine-tuning with HED-IT: The impact of human post-editing for dialogical\n  language models","summary":"  Automatic methods for generating and gathering linguistic data have proven\neffective for fine-tuning Language Models (LMs) in languages less resourced\nthan English. Still, while there has been emphasis on data quantity, less\nattention has been given to its quality. In this work, we investigate the\nimpact of human intervention on machine-generated data when fine-tuning\ndialogical models. In particular, we study (1) whether post-edited dialogues\nexhibit higher perceived quality compared to the originals that were\nautomatically generated; (2) whether fine-tuning with post-edited dialogues\nresults in noticeable differences in the generated outputs; and (3) whether\npost-edited dialogues influence the outcomes when considering the parameter\nsize of the LMs. To this end we created HED-IT, a large-scale dataset where\nmachine-generated dialogues are paired with the version post-edited by humans.\nUsing both the edited and unedited portions of HED-IT, we fine-tuned three\ndifferent sizes of an LM. Results from both human and automatic evaluation show\nthat the different quality of training data is clearly perceived and it has an\nimpact also on the models trained on such data. Additionally, our findings\nindicate that larger models are less sensitive to data quality, whereas this\nhas a crucial impact on smaller models. These results enhance our comprehension\nof the impact of human intervention on training data in the development of\nhigh-quality LMs.\n","authors":["Daniela Occhipinti","Michele Marchi","Irene Mondella","Huiyuan Lai","Felice Dell'Orletta","Malvina Nissim","Marco Guerini"],"pdf_url":"https://arxiv.org/pdf/2406.07288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07287v1","updated":"2024-06-11T14:15:33Z","published":"2024-06-11T14:15:33Z","title":"Bilingual Sexism Classification: Fine-Tuned XLM-RoBERTa and GPT-3.5\n  Few-Shot Learning","summary":"  Sexism in online content is a pervasive issue that necessitates effective\nclassification techniques to mitigate its harmful impact. Online platforms\noften have sexist comments and posts that create a hostile environment,\nespecially for women and minority groups. This content not only spreads harmful\nstereotypes but also causes emotional harm. Reliable methods are essential to\nfind and remove sexist content, making online spaces safer and more welcoming.\nTherefore, the sEXism Identification in Social neTworks (EXIST) challenge\naddresses this issue at CLEF 2024. This study aims to improve sexism\nidentification in bilingual contexts (English and Spanish) by leveraging\nnatural language processing models. The tasks are to determine whether a text\nis sexist and what the source intention behind it is. We fine-tuned the\nXLM-RoBERTa model and separately used GPT-3.5 with few-shot learning prompts to\nclassify sexist content. The XLM-RoBERTa model exhibited robust performance in\nhandling complex linguistic structures, while GPT-3.5's few-shot learning\ncapability allowed for rapid adaptation to new data with minimal labeled\nexamples. Our approach using XLM-RoBERTa achieved 4th place in the soft-soft\nevaluation of Task 1 (sexism identification). For Task 2 (source intention), we\nachieved 2nd place in the soft-soft evaluation.\n","authors":["AmirMohammad Azadi","Baktash Ansari","Sina Zamani"],"pdf_url":"https://arxiv.org/pdf/2406.07287v1.pdf","comment":"8 pages, 6 tables"},{"id":"http://arxiv.org/abs/2404.09145v2","updated":"2024-06-11T14:05:03Z","published":"2024-04-14T05:13:37Z","title":"ToNER: Type-oriented Named Entity Recognition with Generative Language\n  Model","summary":"  In recent years, the fine-tuned generative models have been proven more\npowerful than the previous tagging-based or span-based models on named entity\nrecognition (NER) task. It has also been found that the information related to\nentities, such as entity types, can prompt a model to achieve NER better.\nHowever, it is not easy to determine the entity types indeed existing in the\ngiven sentence in advance, and inputting too many potential entity types would\ndistract the model inevitably. To exploit entity types' merit on promoting NER\ntask, in this paper we propose a novel NER framework, namely ToNER based on a\ngenerative model. In ToNER, a type matching model is proposed at first to\nidentify the entity types most likely to appear in the sentence. Then, we\nappend a multiple binary classification task to fine-tune the generative\nmodel's encoder, so as to generate the refined representation of the input\nsentence. Moreover, we add an auxiliary task for the model to discover the\nentity types which further fine-tunes the model to output more accurate\nresults. Our extensive experiments on some NER benchmarks verify the\neffectiveness of our proposed strategies in ToNER that are oriented towards\nentity types' exploitation.\n","authors":["Guochao Jiang","Ziqin Luo","Yuchen Shi","Dixuan Wang","Jiaqing Liang","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2404.09145v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2406.07277v1","updated":"2024-06-11T14:04:25Z","published":"2024-06-11T14:04:25Z","title":"Speaking Your Language: Spatial Relationships in Interpretable Emergent\n  Communication","summary":"  Effective communication requires the ability to refer to specific parts of an\nobservation in relation to others. While emergent communication literature\nshows success in developing various language properties, no research has shown\nthe emergence of such positional references. This paper demonstrates how agents\ncan communicate about spatial relationships within their observations. The\nresults indicate that agents can develop a language capable of expressing the\nrelationships between parts of their observation, achieving over 90% accuracy\nwhen trained in a referential game which requires such communication. Using a\ncollocation measure, we demonstrate how the agents create such references. This\nanalysis suggests that agents use a mixture of non-compositional and\ncompositional messages to convey spatial relationships. We also show that the\nemergent language is interpretable by humans. The translation accuracy is\ntested by communicating with the receiver agent, where the receiver achieves\nover 78% accuracy using parts of this lexicon, confirming that the\ninterpretation of the emergent language was successful.\n","authors":["Olaf Lipinski","Adam J. Sobey","Federico Cerutti","Timothy J. Norman"],"pdf_url":"https://arxiv.org/pdf/2406.07277v1.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.07268v1","updated":"2024-06-11T13:52:29Z","published":"2024-06-11T13:52:29Z","title":"Advancing Grounded Multimodal Named Entity Recognition via LLM-Based\n  Reformulation and Box-Based Segmentation","summary":"  Grounded Multimodal Named Entity Recognition (GMNER) task aims to identify\nnamed entities, entity types and their corresponding visual regions. GMNER task\nexhibits two challenging attributes: 1) The tenuous correlation between images\nand text on social media contributes to a notable proportion of named entities\nbeing ungroundable. 2) There exists a distinction between coarse-grained noun\nphrases used in similar tasks (e.g., phrase localization) and fine-grained\nnamed entities. In this paper, we propose RiVEG, a unified framework that\nreformulates GMNER into a joint MNER-VE-VG task by leveraging large language\nmodels (LLMs) as connecting bridges. This reformulation brings two benefits: 1)\nIt enables us to optimize the MNER module for optimal MNER performance and\neliminates the need to pre-extract region features using object detection\nmethods, thus naturally addressing the two major limitations of existing GMNER\nmethods. 2) The introduction of Entity Expansion Expression module and Visual\nEntailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG).\nThis endows the proposed framework with unlimited data and model scalability.\nFurthermore, to address the potential ambiguity stemming from the\ncoarse-grained bounding box output in GMNER, we further construct the new\nSegmented Multimodal Named Entity Recognition (SMNER) task and corresponding\nTwitter-SMNER dataset aimed at generating fine-grained segmentation masks, and\nexperimentally demonstrate the feasibility and effectiveness of using box\nprompt-based Segment Anything Model (SAM) to empower any GMNER model with the\nability to accomplish the SMNER task. Extensive experiments demonstrate that\nRiVEG significantly outperforms SoTA methods on four datasets across the MNER,\nGMNER, and SMNER tasks.\n","authors":["Jinyuan Li","Ziyan Li","Han Li","Jianfei Yu","Rui Xia","Di Sun","Gang Pan"],"pdf_url":"https://arxiv.org/pdf/2406.07268v1.pdf","comment":"Extension of our Findings of EMNLP 2023 & ACL 2024 paper"},{"id":"http://arxiv.org/abs/2406.07259v1","updated":"2024-06-11T13:39:07Z","published":"2024-06-11T13:39:07Z","title":"Scientific Computing with Large Language Models","summary":"  We provide an overview of the emergence of large language models for\nscientific computing applications. We highlight use cases that involve natural\nlanguage processing of scientific documents and specialized languages designed\nto describe physical systems. For the former, chatbot style applications appear\nin medicine, mathematics and physics and can be used iteratively with domain\nexperts for problem solving. We also review specialized languages within\nmolecular biology, the languages of molecules, proteins, and DNA where language\nmodels are being used to predict properties and even create novel physical\nsystems at much faster rates than traditional computing methods.\n","authors":["Christopher Culver","Peter Hicks","Mihailo Milenkovic","Sanjif Shanmugavelu","Tobias Becker"],"pdf_url":"https://arxiv.org/pdf/2406.07259v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2406.07257v1","updated":"2024-06-11T13:36:19Z","published":"2024-06-11T13:36:19Z","title":"Scholarly Question Answering using Large Language Models in the\n  NFDI4DataScience Gateway","summary":"  This paper introduces a scholarly Question Answering (QA) system on top of\nthe NFDI4DataScience Gateway, employing a Retrieval Augmented Generation-based\n(RAG) approach. The NFDI4DS Gateway, as a foundational framework, offers a\nunified and intuitive interface for querying various scientific databases using\nfederated search. The RAG-based scholarly QA, powered by a Large Language Model\n(LLM), facilitates dynamic interaction with search results, enhancing filtering\ncapabilities and fostering a conversational engagement with the Gateway search.\nThe effectiveness of both the Gateway and the scholarly QA system is\ndemonstrated through experimental analysis.\n","authors":["Hamed Babaei Giglou","Tilahun Abedissa Taffa","Rana Abdullah","Aida Usmanova","Ricardo Usbeck","Jennifer D'Souza","Sören Auer"],"pdf_url":"https://arxiv.org/pdf/2406.07257v1.pdf","comment":"13 pages main content, 16 pages overall, 3 Figures, accepted for\n  publication at NSLP 2024 workshop at ESWC 2024"},{"id":"http://arxiv.org/abs/2406.07243v1","updated":"2024-06-11T13:23:14Z","published":"2024-06-11T13:23:14Z","title":"MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in\n  Generative LLMs","summary":"  Generative large language models (LLMs) have been shown to exhibit harmful\nbiases and stereotypes. While safety fine-tuning typically takes place in\nEnglish, if at all, these models are being used by speakers of many different\nlanguages. There is existing evidence that the performance of these models is\ninconsistent across languages and that they discriminate based on demographic\nfactors of the user. Motivated by this, we investigate whether the social\nstereotypes exhibited by LLMs differ as a function of the language used to\nprompt them, while controlling for cultural differences and task accuracy. To\nthis end, we present MBBQ (Multilingual Bias Benchmark for Question-answering),\na carefully curated version of the English BBQ dataset extended to Dutch,\nSpanish, and Turkish, which measures stereotypes commonly held across these\nlanguages. We further complement MBBQ with a parallel control dataset to\nmeasure task performance on the question-answering task independently of bias.\nOur results based on several open-source and proprietary LLMs confirm that some\nnon-English languages suffer from bias more than English, even when controlling\nfor cultural shifts. Moreover, we observe significant cross-lingual differences\nin bias behaviour for all except the most accurate models. With the release of\nMBBQ, we hope to encourage further research on bias in multilingual settings.\nThe dataset and code are available at https://github.com/Veranep/MBBQ.\n","authors":["Vera Neplenbroek","Arianna Bisazza","Raquel Fernández"],"pdf_url":"https://arxiv.org/pdf/2406.07243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07239v1","updated":"2024-06-11T13:20:07Z","published":"2024-06-11T13:20:07Z","title":"On the Hallucination in Simultaneous Machine Translation","summary":"  It is widely known that hallucination is a critical issue in Simultaneous\nMachine Translation (SiMT) due to the absence of source-side information. While\nmany efforts have been made to enhance performance for SiMT, few of them\nattempt to understand and analyze hallucination in SiMT. Therefore, we conduct\na comprehensive analysis of hallucination in SiMT from two perspectives:\nunderstanding the distribution of hallucination words and the target-side\ncontext usage of them. Intensive experiments demonstrate some valuable findings\nand particularly show that it is possible to alleviate hallucination by\ndecreasing the over usage of target-side information for SiMT.\n","authors":["Meizhi Zhong","Kehai Chen","Zhengshan Xue","Lemao Liu","Mingming Yang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00046v2","updated":"2024-06-11T13:18:14Z","published":"2024-05-28T13:09:22Z","title":"Hate Speech Detection with Generalizable Target-aware Fairness","summary":"  To counter the side effect brought by the proliferation of social media\nplatforms, hate speech detection (HSD) plays a vital role in halting the\ndissemination of toxic online posts at an early stage. However, given the\nubiquitous topical communities on social media, a trained HSD classifier easily\nbecomes biased towards specific targeted groups (e.g., female and black\npeople), where a high rate of false positive/negative results can significantly\nimpair public trust in the fairness of content moderation mechanisms, and\neventually harm the diversity of online society. Although existing\nfairness-aware HSD methods can smooth out some discrepancies across targeted\ngroups, they are mostly specific to a narrow selection of targets that are\nassumed to be known and fixed. This inevitably prevents those methods from\ngeneralizing to real-world use cases where new targeted groups constantly\nemerge over time. To tackle this defect, we propose Generalizable target-aware\nFairness (GetFair), a new method for fairly classifying each post that contains\ndiverse and even unseen targets during inference. To remove the HSD\nclassifier's spurious dependence on target-related features, GetFair trains a\nseries of filter functions in an adversarial pipeline, so as to deceive the\ndiscriminator that recovers the targeted group from filtered post embeddings.\nTo maintain scalability and generalizability, we innovatively parameterize all\nfilter functions via a hypernetwork that is regularized by the semantic\naffinity among targets. Taking a target's pretrained word embedding as input,\nthe hypernetwork generates the weights used by each target-specific filter\non-the-fly without storing dedicated filter parameters. Finally, comparative\nexperiments on two HSD datasets have shown advantageous performance of GetFair\non out-of-sample targets.\n","authors":["Tong Chen","Danny Wang","Xurong Liang","Marten Risius","Gianluca Demartini","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2406.00046v2.pdf","comment":"To appear in KDD 2024"},{"id":"http://arxiv.org/abs/2310.05694v2","updated":"2024-06-11T13:13:59Z","published":"2023-10-09T13:15:23Z","title":"A Survey of Large Language Models for Healthcare: from Data, Technology,\n  and Applications to Accountability and Ethics","summary":"  The utilization of large language models (LLMs) in the Healthcare domain has\ngenerated both excitement and concern due to their ability to effectively\nrespond to freetext queries with certain professional knowledge. This survey\noutlines the capabilities of the currently developed LLMs for Healthcare and\nexplicates their development process, with the aim of providing an overview of\nthe development roadmap from traditional Pretrained Language Models (PLMs) to\nLLMs. Specifically, we first explore the potential of LLMs to enhance the\nefficiency and effectiveness of various Healthcare applications highlighting\nboth the strengths and limitations. Secondly, we conduct a comparison between\nthe previous PLMs and the latest LLMs, as well as comparing various LLMs with\neach other. Then we summarize related Healthcare training data, training\nmethods, optimization strategies, and usage. Finally, the unique concerns\nassociated with deploying LLMs in Healthcare settings are investigated,\nparticularly regarding fairness, accountability, transparency and ethics. Our\nsurvey provide a comprehensive investigation from perspectives of both computer\nscience and Healthcare specialty. Besides the discussion about Healthcare\nconcerns, we supports the computer science community by compiling a collection\nof open source resources, such as accessible datasets, the latest\nmethodologies, code implementations, and evaluation benchmarks in the Github.\nSummarily, we contend that a significant paradigm shift is underway,\ntransitioning from PLMs to LLMs. This shift encompasses a move from\ndiscriminative AI approaches to generative AI approaches, as well as a shift\nfrom model-centered methodologies to data-centered methodologies. Also, we\ndetermine that the biggest obstacle of using LLMs in Healthcare are fairness,\naccountability, transparency and ethics.\n","authors":["Kai He","Rui Mao","Qika Lin","Yucheng Ruan","Xiang Lan","Mengling Feng","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2310.05694v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07232v1","updated":"2024-06-11T13:10:39Z","published":"2024-06-11T13:10:39Z","title":"DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation\n  through Dual Learning Feedback Mechanisms","summary":"  Recently, large language models (LLMs) enhanced by self-reflection have\nachieved promising performance on machine translation. The key idea is guiding\nLLMs to generate translation with human-like feedback. However, existing\nself-reflection methods lack effective feedback information, limiting the\ntranslation performance. To address this, we introduce a DUAL-REFLECT\nframework, leveraging the dual learning of translation tasks to provide\neffective feedback, thereby enhancing the models' self-reflective abilities and\nimproving translation performance. The application of this method across\nvarious translation tasks has proven its effectiveness in improving translation\naccuracy and eliminating ambiguities, especially in translation tasks with\nlow-resource language pairs.\n","authors":["Andong Chen","Lianzhang Lou","Kehai Chen","Xuefeng Bai","Yang Xiang","Muyun Yang","Tiejun Zhao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07232v1.pdf","comment":"Accepted to ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2406.07231v1","updated":"2024-06-11T13:10:30Z","published":"2024-06-11T13:10:30Z","title":"Decipherment-Aware Multilingual Learning in Jointly Trained Language\n  Models","summary":"  The principle that governs unsupervised multilingual learning (UCL) in\njointly trained language models (mBERT as a popular example) is still being\ndebated. Many find it surprising that one can achieve UCL with multiple\nmonolingual corpora. In this work, we anchor UCL in the context of language\ndecipherment and show that the joint training methodology is a decipherment\nprocess pivotal for UCL. In a controlled setting, we investigate the effect of\ndifferent decipherment settings on the multilingual learning performance and\nconsolidate the existing opinions on the contributing factors to\nmultilinguality. From an information-theoretic perspective we draw a limit to\nthe UCL performance and demonstrate the importance of token alignment in\nchallenging decipherment settings caused by differences in the data domain,\nlanguage order and tokenization granularity. Lastly, we apply lexical alignment\nto mBERT and investigate the contribution of aligning different lexicon groups\nto downstream performance.\n","authors":["Grandee Lee"],"pdf_url":"https://arxiv.org/pdf/2406.07231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07229v1","updated":"2024-06-11T13:09:16Z","published":"2024-06-11T13:09:16Z","title":"Improving Commonsense Bias Classification by Mitigating the Influence of\n  Demographic Terms","summary":"  Understanding commonsense knowledge is crucial in the field of Natural\nLanguage Processing (NLP). However, the presence of demographic terms in\ncommonsense knowledge poses a potential risk of compromising the performance of\nNLP models. This study aims to investigate and propose methods for enhancing\nthe performance and effectiveness of a commonsense polarization classifier by\nmitigating the influence of demographic terms. Three methods are introduced in\nthis paper: (1) hierarchical generalization of demographic terms (2)\nthreshold-based augmentation and (3) integration of hierarchical generalization\nand threshold-based augmentation methods (IHTA). The first method involves\nreplacing demographic terms with more general ones based on a term hierarchy\nontology, aiming to mitigate the influence of specific terms. To address the\nlimited bias-related information, the second method measures the polarization\nof demographic terms by comparing the changes in the model's predictions when\nthese terms are masked versus unmasked. This method augments commonsense\nsentences containing terms with high polarization values by replacing their\npredicates with synonyms generated by ChatGPT. The third method combines the\ntwo approaches, starting with threshold-based augmentation followed by\nhierarchical generalization. The experiments show that the first method\nincreases the accuracy over the baseline by 2.33%, and the second one by 0.96%\nover standard augmentation methods. The IHTA techniques yielded an 8.82% and\n9.96% higher accuracy than threshold-based and standard augmentation methods,\nrespectively.\n","authors":["JinKyu Lee","Jihie Kim"],"pdf_url":"https://arxiv.org/pdf/2406.07229v1.pdf","comment":"10 pages, 5 figures, conference presentation, supported by MSIT\n  (Korea) under ITRC program (IITP-2024-2020-0-01789) and AI Convergence\n  Innovation HR Development (IITP-2024-RS-2023-00254592)"},{"id":"http://arxiv.org/abs/2406.07222v1","updated":"2024-06-11T13:01:50Z","published":"2024-06-11T13:01:50Z","title":"Improving Autoformalization using Type Checking","summary":"  Large language models show promise for autoformalization, the task of\nautomatically translating natural language into formal languages. However,\ncurrent autoformalization methods remain limited. The last reported\nstate-of-the-art performance on the ProofNet formalization benchmark for the\nLean proof assistant, achieved using Codex for Lean 3, only showed successful\nformalization of 16.1% of informal statements. Similarly, our evaluation of\nGPT-4o for Lean 4 only produces successful translations 34.9% of the time. Our\nanalysis shows that the performance of these models is largely limited by their\ninability to generate formal statements that successfully type-check (i.e., are\nsyntactically correct and consistent with types) - with a whopping 86.6% of\nGPT-4o errors starting from a type-check failure. In this work, we propose a\nmethod to fix this issue through decoding with type-check filtering, where we\ninitially sample a diverse set of candidate formalizations for an informal\nstatement, then use the Lean proof assistant to filter out candidates that do\nnot type-check. Using GPT-4o as a base model, and combining our method with\nself-consistency, we obtain a +18.3% absolute increase in formalization\naccuracy, and achieve a new state-of-the-art of 53.2% on ProofNet with Lean 4.\n","authors":["Auguste Poiroux","Gail Weiss","Viktor Kunčak","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2406.07222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02872v2","updated":"2024-06-11T12:58:51Z","published":"2024-02-05T10:39:32Z","title":"How do Large Language Models Learn In-Context? Query and Key Matrices of\n  In-Context Heads are Two Towers for Metric Learning","summary":"  We investigate the mechanism of in-context learning (ICL) on sentence\nclassification tasks with semantically-unrelated labels (\"foo\"/\"bar\"). We find\nintervening in only 1\\% heads (named \"in-context heads\") significantly affects\nICL accuracy from 87.6\\% to 24.4\\%. To understand this phenomenon, we analyze\nthe value-output vectors in these heads and discover that the vectors at each\nlabel position contain substantial information about the corresponding labels.\nFurthermore, we observe that the prediction shift from \"foo\" to \"bar\" is due to\nthe respective reduction and increase in these heads' attention scores at \"foo\"\nand \"bar\" positions. Therefore, we propose a hypothesis for ICL: in in-context\nheads, the value-output matrices extract label features, while the query-key\nmatrices compute the similarity between the features at the last position and\nthose at each label position. The query and key matrices can be considered as\ntwo towers that learn the similarity metric between the last position's\nfeatures and each demonstration at label positions. Using this hypothesis, we\nexplain the majority label bias and recency bias in ICL and propose two methods\nto reduce these biases by 22\\% and 17\\%, respectively.\n","authors":["Zeping Yu","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2402.02872v2.pdf","comment":"preprint (code and data will be released in final version)"},{"id":"http://arxiv.org/abs/2406.07217v1","updated":"2024-06-11T12:50:53Z","published":"2024-06-11T12:50:53Z","title":"A Synthetic Dataset for Personal Attribute Inference","summary":"  Recently, powerful Large Language Models (LLMs) have become easily accessible\nto hundreds of millions of users worldwide. However, their strong capabilities\nand vast world knowledge do not come without associated privacy risks. In this\nwork, we focus on the emerging privacy threat LLMs pose - the ability to\naccurately infer personal information from online texts. Despite the growing\nimportance of LLM-based author profiling, research in this area has been\nhampered by a lack of suitable public datasets, largely due to ethical and\nprivacy concerns associated with real personal data. In this work, we take two\nsteps to address this problem: (i) we construct a simulation framework for the\npopular social media platform Reddit using LLM agents seeded with synthetic\npersonal profiles; (ii) using this framework, we generate SynthPAI, a diverse\nsynthetic dataset of over 7800 comments manually labeled for personal\nattributes. We validate our dataset with a human study showing that humans\nbarely outperform random guessing on the task of distinguishing our synthetic\ncomments from real ones. Further, we verify that our dataset enables meaningful\npersonal attribute inference research by showing across 18 state-of-the-art\nLLMs that our synthetic comments allow us to draw the same conclusions as\nreal-world data. Together, this indicates that our dataset and pipeline provide\na strong and privacy-preserving basis for future research toward understanding\nand mitigating the inference-based privacy threats LLMs pose.\n","authors":["Hanna Yukhymenko","Robin Staab","Mark Vero","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2406.07217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07212v1","updated":"2024-06-11T12:41:54Z","published":"2024-06-11T12:41:54Z","title":"Towards Human-AI Collaboration in Healthcare: Guided Deferral Systems\n  with Large Language Models","summary":"  Large language models (LLMs) present a valuable technology for various\napplications in healthcare, but their tendency to hallucinate introduces\nunacceptable uncertainty in critical decision-making situations. Human-AI\ncollaboration (HAIC) can mitigate this uncertainty by combining human and AI\nstrengths for better outcomes. This paper presents a novel guided deferral\nsystem that provides intelligent guidance when AI defers cases to human\ndecision-makers. We leverage LLMs' verbalisation capabilities and internal\nstates to create this system, demonstrating that fine-tuning smaller LLMs with\ndata from larger models enhances performance while maintaining computational\nefficiency. A pilot study showcases the effectiveness of our deferral system.\n","authors":["Joshua Strong","Qianhui Men","Alison Noble"],"pdf_url":"https://arxiv.org/pdf/2406.07212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04467v2","updated":"2024-06-11T12:36:36Z","published":"2024-06-06T19:48:17Z","title":"Small-E: Small Language Model with Linear Attention for Efficient Speech\n  Synthesis","summary":"  Recent advancements in text-to-speech (TTS) powered by language models have\nshowcased remarkable capabilities in achieving naturalness and zero-shot voice\ncloning. Notably, the decoder-only transformer is the prominent architecture in\nthis domain. However, transformers face challenges stemming from their\nquadratic complexity in sequence length, impeding training on lengthy sequences\nand resource-constrained hardware. Moreover they lack specific inductive bias\nwith regards to the monotonic nature of TTS alignments. In response, we propose\nto replace transformers with emerging recurrent architectures and introduce\nspecialized cross-attention mechanisms for reducing repeating and skipping\nissues. Consequently our architecture can be efficiently trained on long\nsamples and achieve state-of-the-art zero-shot voice cloning against baselines\nof comparable size. Our implementation and demos are available at\nhttps://github.com/theodorblackbird/lina-speech.\n","authors":["Théodor Lemerle","Nicolas Obin","Axel Roebel"],"pdf_url":"https://arxiv.org/pdf/2406.04467v2.pdf","comment":"Interspeech"},{"id":"http://arxiv.org/abs/2405.00706v2","updated":"2024-06-11T12:35:51Z","published":"2024-04-23T14:43:35Z","title":"From Complexity to Clarity: How AI Enhances Perceptions of Scientists\n  and the Public's Understanding of Science","summary":"  This paper evaluated the effectiveness of using generative AI to simplify\nscience communication and enhance the public's understanding of science. By\ncomparing lay summaries of journal articles from PNAS, yoked to those generated\nby AI, this work first assessed linguistic simplicity across such summaries and\npublic perceptions. Study 1a analyzed simplicity features of PNAS abstracts\n(scientific summaries) and significance statements (lay summaries), observing\nthat lay summaries were indeed linguistically simpler, but effect size\ndifferences were small. Study 1b used a large language model, GPT-4, to create\nsignificance statements based on paper abstracts and this more than doubled the\naverage effect size without fine-tuning. Study 2 experimentally demonstrated\nthat simply-written GPT summaries facilitated more favorable perceptions of\nscientists (they were perceived as more credible and trustworthy, but less\nintelligent) than more complexly-written human PNAS summaries. Crucially, Study\n3 experimentally demonstrated that participants comprehended scientific writing\nbetter after reading simple GPT summaries compared to complex PNAS summaries.\nIn their own words, participants also summarized scientific papers in a more\ndetailed and concrete manner after reading GPT summaries compared to PNAS\nsummaries of the same article. AI has the potential to engage scientific\ncommunities and the public via a simple language heuristic, advocating for its\nintegration into scientific dissemination for a more informed society.\n","authors":["David M. Markowitz"],"pdf_url":"https://arxiv.org/pdf/2405.00706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05862v2","updated":"2024-06-11T12:33:42Z","published":"2024-06-09T17:25:47Z","title":"II-Bench: An Image Implication Understanding Benchmark for Multimodal\n  Large Language Models","summary":"  The rapid advancements in the development of multimodal large language models\n(MLLMs) have consistently led to new breakthroughs on various benchmarks. In\nresponse, numerous challenging and comprehensive benchmarks have been proposed\nto more accurately assess the capabilities of MLLMs. However, there is a dearth\nof exploration of the higher-order perceptual capabilities of MLLMs. To fill\nthis gap, we propose the Image Implication understanding Benchmark, II-Bench,\nwhich aims to evaluate the model's higher-order perception of images. Through\nextensive experiments on II-Bench across multiple MLLMs, we have made\nsignificant findings. Initially, a substantial gap is observed between the\nperformance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs\nattains 74.8%, whereas human accuracy averages 90%, peaking at an impressive\n98%. Subsequently, MLLMs perform worse on abstract and complex images,\nsuggesting limitations in their ability to understand high-level semantics and\ncapture image details. Finally, it is observed that most models exhibit\nenhanced accuracy when image sentiment polarity hints are incorporated into the\nprompts. This observation underscores a notable deficiency in their inherent\nunderstanding of image sentiment. We believe that II-Bench will inspire the\ncommunity to develop the next generation of MLLMs, advancing the journey\ntowards expert artificial general intelligence (AGI). II-Bench is publicly\navailable at https://huggingface.co/datasets/m-a-p/II-Bench.\n","authors":["Ziqiang Liu","Feiteng Fang","Xi Feng","Xinrun Du","Chenhao Zhang","Zekun Wang","Yuelin Bai","Qixuan Zhao","Liyang Fan","Chengguang Gan","Hongquan Lin","Jiaming Li","Yuansheng Ni","Haihong Wu","Yaswanth Narsupalli","Zhigang Zheng","Chengming Li","Xiping Hu","Ruifeng Xu","Xiaojun Chen","Min Yang","Jiaheng Liu","Ruibo Liu","Wenhao Huang","Ge Zhang","Shiwen Ni"],"pdf_url":"https://arxiv.org/pdf/2406.05862v2.pdf","comment":"100 pages, 82 figures, add citations"},{"id":"http://arxiv.org/abs/2311.07470v2","updated":"2024-06-11T12:30:02Z","published":"2023-11-13T17:03:02Z","title":"Finding and Editing Multi-Modal Neurons in Pre-Trained Transformers","summary":"  Understanding the internal mechanisms by which multi-modal large language\nmodels (LLMs) interpret different modalities and integrate cross-modal\nrepresentations is becoming increasingly critical for continuous improvements\nin both academia and industry. In this paper, we propose a novel method to\nidentify key neurons for interpretability -- how multi-modal LLMs bridge visual\nand textual concepts for captioning. Our method improves conventional works\nupon efficiency and applied range by removing needs of costly gradient\ncomputation. Based on those identified neurons, we further design a multi-modal\nknowledge editing method, beneficial to mitigate sensitive words or\nhallucination. For rationale of our design, we provide theoretical assumption.\nFor empirical evaluation, we have conducted extensive quantitative and\nqualitative experiments. The results not only validate the effectiveness of our\nmethods, but also offer insightful findings that highlight three key properties\nof multi-modal neurons: sensitivity, specificity and causal-effect, to shed\nlight for future research.\n","authors":["Haowen Pan","Yixin Cao","Xiaozhi Wang","Xun Yang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07470v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09267v2","updated":"2024-06-11T12:22:14Z","published":"2024-02-14T15:52:42Z","title":"Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via\n  Self-Evaluation","summary":"  Despite showing increasingly human-like abilities, large language models\n(LLMs) often struggle with factual inaccuracies, i.e. \"hallucinations\", even\nwhen they hold relevant knowledge. To address these hallucinations, current\napproaches typically necessitate high-quality human factuality annotations. In\nthis work, we explore Self-Alignment for Factuality, where we leverage the\nself-evaluation capability of an LLM to provide training signals that steer the\nmodel towards factuality. Specifically, we incorporate Self-Eval, a\nself-evaluation component, to prompt an LLM to validate the factuality of its\nown generated responses solely based on its internal knowledge. Additionally,\nwe design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's\nself-evaluation ability by improving the model's confidence estimation and\ncalibration. We then utilize these self-annotated responses to fine-tune the\nmodel via Direct Preference Optimization algorithm. We show that the proposed\nself-alignment approach substantially enhances factual accuracy over Llama\nfamily models across three key knowledge-intensive tasks on TruthfulQA and\nBioGEN.\n","authors":["Xiaoying Zhang","Baolin Peng","Ye Tian","Jingyan Zhou","Lifeng Jin","Linfeng Song","Haitao Mi","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2402.09267v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2310.17953v3","updated":"2024-06-11T12:06:43Z","published":"2023-10-27T08:01:55Z","title":"MADGF: Multi-Agent Data Generation Framework","summary":"  Automatic Speech Recognition (ASR) systems predominantly cater to monolingual\ninputs and struggle with the complexity introduced by mixed language audio. In\nthis paper, we present a novel Multi-Agent Data Generation Framework (MADGF) to\naddress this challenge. We finetune the open-source multilingual ASR model,\nWhisper, utilizing our generated Mixed Cantonese and English (MCE) audio\ndataset, Which achieved an impressive Mix Error Rate (MER) of 14.28%, 35.13%\nlower than the original model. Meanwhile, single language recognition ability\nis not affected, 12.6% Character Error Rate (CER) in Common voice zh-HK, 14.8%\nWord Error Rate (WER) in Common voice en. However, these metrics do not\nencompass all aspects critical to the ASR systems. Hence, we propose a novel\nevaluation metric called Fidelity to the Original Audio, Accuracy, and Latency\n(FAL).\n","authors":["Peng Xie","Kani Chen"],"pdf_url":"https://arxiv.org/pdf/2310.17953v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03452v3","updated":"2024-06-11T12:05:22Z","published":"2024-06-05T16:52:21Z","title":"Using Synchronic Definitions and Semantic Relations to Classify Semantic\n  Change Types","summary":"  There is abundant evidence of the fact that the way words change their\nmeaning can be classified in different types of change, highlighting the\nrelationship between the old and new meanings (among which generalization,\nspecialization and co-hyponymy transfer). In this paper, we present a way of\ndetecting these types of change by constructing a model that leverages\ninformation both from synchronic lexical relations and definitions of word\nmeanings. Specifically, we use synset definitions and hierarchy information\nfrom WordNet and test it on a digitized version of Blank's (1997) dataset of\nsemantic change types. Finally, we show how the sense relationships can improve\nmodels for both approximation of human judgments of semantic relatedness as\nwell as binary Lexical Semantic Change Detection.\n","authors":["Pierluigi Cassotti","Stefano De Pascale","Nina Tahmasebi"],"pdf_url":"https://arxiv.org/pdf/2406.03452v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07188v1","updated":"2024-06-11T12:01:09Z","published":"2024-06-11T12:01:09Z","title":"Merging Improves Self-Critique Against Jailbreak Attacks","summary":"  The robustness of large language models (LLMs) against adversarial\nmanipulations, such as jailbreak attacks, remains a significant challenge. In\nthis work, we propose an approach that enhances the self-critique capability of\nthe LLM and further fine-tunes it over sanitized synthetic data. This is done\nwith the addition of an external critic model that can be merged with the\noriginal, thus bolstering self-critique capabilities and improving the\nrobustness of the LLMs response to adversarial prompts. Our results demonstrate\nthat the combination of merging and self-critique can reduce the attack success\nrate of adversaries significantly, thus offering a promising defense mechanism\nagainst jailbreak attacks. Code, data and models released at\nhttps://github.com/vicgalle/merging-self-critique-jailbreaks .\n","authors":["Victor Gallego"],"pdf_url":"https://arxiv.org/pdf/2406.07188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07168v1","updated":"2024-06-11T11:20:05Z","published":"2024-06-11T11:20:05Z","title":"Teaching Language Models to Self-Improve by Learning from Language\n  Feedback","summary":"  Aligning Large Language Models (LLMs) with human intentions and values is\ncrucial yet challenging. Current methods primarily rely on human preferences,\nwhich are costly and insufficient in capturing nuanced feedback expressed in\nnatural language. In this paper, we present Self-Refinement Tuning (SRT), a\nmethod that leverages model feedback for alignment, thereby reducing reliance\non human annotations. SRT uses a base language model (e.g., Tulu2) to generate\ninitial responses, which are critiqued and refined by a more advanced model\n(e.g., GPT-4-Turbo). This process enables the base model to self-evaluate and\nimprove its outputs, facilitating continuous learning. SRT further optimizes\nthe model by learning from its self-generated feedback and refinements,\ncreating a feedback loop that promotes model improvement. Our empirical\nevaluations demonstrate that SRT significantly outperforms strong baselines\nacross diverse tasks and model sizes. When applied to a 70B parameter model,\nSRT increases the win rate from 9.6\\% to 25.8\\% on the AlpacaEval 2.0\nbenchmark, surpassing well-established systems such as GPT-4-0314, Claude 2,\nand Gemini. Our analysis highlights the crucial role of language feedback in\nthe success of SRT, suggesting potential for further exploration in this\ndirection.\n","authors":["Chi Hu","Yimin Hu","Hang Cao","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.07168v1.pdf","comment":"Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2406.07162v1","updated":"2024-06-11T11:12:51Z","published":"2024-06-11T11:12:51Z","title":"EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and\n  Benchmark","summary":"  Speech emotion recognition (SER) is an important part of human-computer\ninteraction, receiving extensive attention from both industry and academia.\nHowever, the current research field of SER has long suffered from the following\nproblems: 1) There are few reasonable and universal splits of the datasets,\nmaking comparing different models and methods difficult. 2) No commonly used\nbenchmark covers numerous corpus and languages for researchers to refer to,\nmaking reproduction a burden. In this paper, we propose EmoBox, an\nout-of-the-box multilingual multi-corpus speech emotion recognition toolkit,\nalong with a benchmark for both intra-corpus and cross-corpus settings. For\nintra-corpus settings, we carefully designed the data partitioning for\ndifferent datasets. For cross-corpus settings, we employ a foundation SER\nmodel, emotion2vec, to mitigate annotation errors and obtain a test set that is\nfully balanced in speakers and emotions distributions. Based on EmoBox, we\npresent the intra-corpus SER results of 10 pre-trained speech models on 32\nemotion datasets with 14 languages, and the cross-corpus SER results on 4\ndatasets with the fully balanced test sets. To the best of our knowledge, this\nis the largest SER benchmark, across language scopes and quantity scales. We\nhope that our toolkit and benchmark can facilitate the research of SER in the\ncommunity.\n","authors":["Ziyang Ma","Mingjie Chen","Hezhao Zhang","Zhisheng Zheng","Wenxi Chen","Xiquan Li","Jiaxin Ye","Xie Chen","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2406.07162v1.pdf","comment":"Accepted by INTERSPEECH 2024. GitHub Repository:\n  https://github.com/emo-box/EmoBox"},{"id":"http://arxiv.org/abs/2406.07155v1","updated":"2024-06-11T11:02:04Z","published":"2024-06-11T11:02:04Z","title":"Scaling Large-Language-Model-based Multi-Agent Collaboration","summary":"  Pioneering advancements in large language model-powered agents have\nunderscored the design pattern of multi-agent collaboration, demonstrating that\ncollective intelligence can surpass the capabilities of each individual.\nInspired by the neural scaling law, which posits that increasing neurons leads\nto emergent abilities, this study investigates whether a similar principle\napplies to increasing agents in multi-agent collaboration. Technically, we\npropose multi-agent collaboration networks (MacNet), which utilize directed\nacyclic graphs to organize agents and streamline their interactive reasoning\nvia topological ordering, with solutions derived from their dialogues.\nExtensive experiments show that MacNet consistently outperforms baseline\nmodels, enabling effective agent collaboration across various network\ntopologies and supporting cooperation among more than a thousand agents.\nNotably, we observed a small-world collaboration phenomenon, where topologies\nresembling small-world properties achieved superior performance. Additionally,\nwe identified a collaborative scaling law, indicating that normalized solution\nquality follows a logistic growth pattern as scaling agents, with collaborative\nemergence occurring much earlier than previously observed instances of neural\nemergence. The code and data will be available at\nhttps://github.com/OpenBMB/ChatDev.\n","authors":["Chen Qian","Zihao Xie","Yifei Wang","Wei Liu","Yufan Dang","Zhuoyun Du","Weize Chen","Cheng Yang","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2406.07155v1.pdf","comment":"Work in progress; The code and data will be available at\n  https://github.com/OpenBMB/ChatDev"},{"id":"http://arxiv.org/abs/2404.03565v2","updated":"2024-06-11T10:47:02Z","published":"2024-04-04T16:20:34Z","title":"Personalized LLM Response Generation with Parameterized Memory Injection","summary":"  Large Language Models (LLMs) have exhibited remarkable proficiency in\ncomprehending and generating natural language. On the other hand, personalized\nLLM response generation holds the potential to offer substantial benefits for\nindividuals in critical areas such as medical. Existing research has explored\nmemory-augmented methods to prompt the LLM with pre-stored user-specific\nknowledge for personalized response generation in terms of new queries. We\ncontend that such paradigm is unable to perceive fine-granularity information.\nIn this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach\nusing parameter-efficient fine-tuning (PEFT) and along with a Bayesian\nOptimisation searching strategy to achieve \\textbf{L}LM\n\\textbf{P}ersonalization(\\textbf{MiLP}).\n","authors":["Kai Zhang","Lizhi Qing","Yangyang Kang","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2404.03565v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07138v1","updated":"2024-06-11T10:35:49Z","published":"2024-06-11T10:35:49Z","title":"Never Miss A Beat: An Efficient Recipe for Context Window Extension of\n  Large Language Models with Consistent \"Middle\" Enhancement","summary":"  Recently, many methods have been developed to extend the context length of\npre-trained large language models (LLMs), but they often require fine-tuning at\nthe target length ($\\gg4K$) and struggle to effectively utilize information\nfrom the middle part of the context. To address these issues, we propose\n$\\textbf{C}$ontinuity-$\\textbf{R}$elativity ind$\\textbf{E}$xing with\ng$\\textbf{A}$ussian $\\textbf{M}$iddle (CREAM), which interpolates positional\nencodings by manipulating position indices. Apart from being simple, CREAM is\ntraining-efficient: it only requires fine-tuning at the pre-trained context\nwindow (eg, Llama 2-4K) and can extend LLMs to a much longer target context\nlength (eg, 256K). To ensure that the model focuses more on the information in\nthe middle, we introduce a truncated Gaussian to encourage sampling from the\nmiddle part of the context during fine-tuning, thus alleviating the\n``Lost-in-the-Middle'' problem faced by long-context LLMs. Experimental results\nshow that CREAM successfully extends LLMs to the target length for both Base\nand Chat versions of $\\texttt{Llama2-7B}$ with ``Never Miss A Beat''. Our code\nwill be publicly available soon.\n","authors":["Tong Wu","Yanpeng Zhao","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.07138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07133v1","updated":"2024-06-11T10:29:24Z","published":"2024-06-11T10:29:24Z","title":"Translating speech with just images","summary":"  Visually grounded speech models link speech to images. We extend this\nconnection by linking images to text via an existing image captioning system,\nand as a result gain the ability to map speech audio directly to text. This\napproach can be used for speech translation with just images by having the\naudio in a different language from the generated captions. We investigate such\na system on a real low-resource language, Yor\\`ub\\'a, and propose a\nYor\\`ub\\'a-to-English speech translation model that leverages pretrained\ncomponents in order to be able to learn in the low-resource regime. To limit\noverfitting, we find that it is essential to use a decoding scheme that\nproduces diverse image captions for training. Results show that the predicted\ntranslations capture the main semantics of the spoken audio, albeit in a\nsimpler and shorter form.\n","authors":["Dan Oneata","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2406.07133v1.pdf","comment":"Accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.06196v2","updated":"2024-06-11T10:19:52Z","published":"2024-06-10T11:50:29Z","title":"LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in\n  Low-Resource and Extinct Languages","summary":"  In this paper, we present the LingOly benchmark, a novel benchmark for\nadvanced reasoning abilities in large language models. Using challenging\nLinguistic Olympiad puzzles, we evaluate (i) capabilities for in-context\nidentification and generalisation of linguistic patterns in very low-resource\nor extinct languages, and (ii) abilities to follow complex task instructions.\nThe LingOly benchmark covers more than 90 mostly low-resource languages,\nminimising issues of data contamination, and contains 1,133 problems across 6\nformats and 5 levels of human difficulty. We assess performance with both\ndirect accuracy and comparison to a no-context baseline to penalise\nmemorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to\nbe challenging, and models perform poorly on the higher difficulty problems. On\nharder problems, even the top model only achieved 38.7% accuracy, 24.7%\nimprovement over the no-context baseline. Large closed models typically\noutperform open models, and in general, the higher resource the language, the\nbetter the scores. These results indicate, in absence of memorisation, true\nmulti-step out-of-domain reasoning remains a challenge for current language\nmodels.\n","authors":["Andrew M. Bean","Simi Hellsten","Harry Mayne","Jabez Magomere","Ethan A. Chi","Ryan Chi","Scott A. Hale","Hannah Rose Kirk"],"pdf_url":"https://arxiv.org/pdf/2406.06196v2.pdf","comment":"9 pages, 5 figures, 16 pages supplemental materials"},{"id":"http://arxiv.org/abs/2401.13478v2","updated":"2024-06-11T10:18:08Z","published":"2024-01-24T14:23:12Z","title":"SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval","summary":"  Multi-modal information retrieval (MMIR) is a rapidly evolving field, where\nsignificant progress, particularly in image-text pairing, has been made through\nadvanced representation learning and cross-modality alignment research.\nHowever, current benchmarks for evaluating MMIR performance in image-text\npairing within the scientific domain show a notable gap, where chart and table\nimages described in scholarly language usually do not play a significant role.\nTo bridge this gap, we develop a specialised scientific MMIR (SciMMIR)\nbenchmark by leveraging open-access paper collections to extract data relevant\nto the scientific domain. This benchmark comprises 530K meticulously curated\nimage-text pairs, extracted from figures and tables with detailed captions in\nscientific documents. We further annotate the image-text pairs with two-level\nsubset-subcategory hierarchy annotations to facilitate a more comprehensive\nevaluation of the baselines. We conducted zero-shot and fine-tuning evaluations\non prominent multi-modal image-captioning and visual language models, such as\nCLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific\ndomain, including the impact of pre-training and fine-tuning settings and the\ninfluence of the visual and textual encoders. All our data and checkpoints are\npublicly available at https://github.com/Wusiwei0410/SciMMIR.\n","authors":["Siwei Wu","Yizhi Li","Kang Zhu","Ge Zhang","Yiming Liang","Kaijing Ma","Chenghao Xiao","Haoran Zhang","Bohao Yang","Wenhu Chen","Wenhao Huang","Noura Al Moubayed","Jie Fu","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2401.13478v2.pdf","comment":"camera-ready version for ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.07115v1","updated":"2024-06-11T10:00:18Z","published":"2024-06-11T10:00:18Z","title":"Advancing Tool-Augmented Large Language Models: Integrating Insights\n  from Errors in Inference Trees","summary":"  Tool-augmented large language models (LLMs) leverage tools, often in the form\nof APIs, to enhance their reasoning capabilities on complex tasks, thus taking\non the role of intelligent agents interacting with the real world. The recently\nintroduced ToolLLaMA model by Qin et al. [2024] utilizes the depth-first\nsearch-based decision tree (DFSDT) method for reasoning with $16000+$\nreal-world APIs, which effectively improves the planning and inferencing\nperformance of tool-augmented LLMs compared to traditional chain reasoning\napproaches. However, their approach only employs successful paths from decision\ntrees (also called inference trees) for supervised fine-tuning (SFT) during\ntraining, which does not fully exploit the advantages of the tree of thought.\nIn this study, we propose an inference trajectory optimization framework based\non the preference data extracted from decision trees to address this\nlimitation. We first introduce a novel method for constructing preference data\nfrom the tree of thought, capitalizing on the failed explorations previously\noverlooked in the trees. Specifically, we generate an effective step-wise\npreference dataset, named ToolPreference, for tool use based on the ToolBench\ndataset. In the subsequent training phase, we first fine-tune the LLM with\ntool-usage expert trajectories and then use these step-wise preference pairs\nfor direct preference optimization (DPO) to update the policy of the LLM,\nresulting in our ToolPrefer-LLaMA (TP-LLaMA) model. Our experiments demonstrate\nthat by obtaining insights from errors in inference trees, TP-LLaMA\nsignificantly outperforms the baselines across almost all test scenarios by a\nlarge margin and exhibits better generalization capabilities with unseen APIs.\nAt the same time, TP-LLaMA has also demonstrated superior reasoning efficiency\ncompared to the baselines, making it more suitable for complex tool-usage\nreasoning tasks.\n","authors":["Sijia Chen","Yibo Wang","Yi-Feng Wu","Qing-Guo Chen","Zhao Xu","Weihua Luo","Kaifu Zhang","Lijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07096v1","updated":"2024-06-11T09:37:52Z","published":"2024-06-11T09:37:52Z","title":"Fast Context-Biasing for CTC and Transducer ASR models with CTC-based\n  Word Spotter","summary":"  Accurate recognition of rare and new words remains a pressing problem for\ncontextualized Automatic Speech Recognition (ASR) systems. Most context-biasing\nmethods involve modification of the ASR model or the beam-search decoding\nalgorithm, complicating model reuse and slowing down inference. This work\npresents a new approach to fast context-biasing with CTC-based Word Spotter\n(CTC-WS) for CTC and Transducer (RNN-T) ASR models. The proposed method matches\nCTC log-probabilities against a compact context graph to detect potential\ncontext-biasing candidates. The valid candidates then replace their greedy\nrecognition counterparts in corresponding frame intervals. A Hybrid\nTransducer-CTC model enables the CTC-WS application for the Transducer model.\nThe results demonstrate a significant acceleration of the context-biasing\nrecognition with a simultaneous improvement in F-score and WER compared to\nbaseline methods. The proposed method is publicly available in the NVIDIA NeMo\ntoolkit.\n","authors":["Andrei Andrusenko","Aleksandr Laptev","Vladimir Bataev","Vitaly Lavrukhin","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2406.07096v1.pdf","comment":"Accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2404.15650v2","updated":"2024-06-11T09:14:02Z","published":"2024-04-24T05:08:55Z","title":"Return of EM: Entity-driven Answer Set Expansion for QA Evaluation","summary":"  Recently, directly using large language models (LLMs) has been shown to be\nthe most reliable method to evaluate QA models. However, it suffers from\nlimited interpretability, high cost, and environmental harm. To address these,\nwe propose to use soft EM with entity-driven answer set expansion. Our approach\nexpands the gold answer set to include diverse surface forms, based on the\nobservation that the surface forms often follow particular patterns depending\non the entity type. The experimental results show that our method outperforms\ntraditional evaluation methods by a large margin. Moreover, the reliability of\nour evaluation method is comparable to that of LLM-based ones, while offering\nthe benefits of high interpretability and reduced environmental harm.\n","authors":["Dongryeol Lee","Minwoo Lee","Kyungmin Min","Joonsuk Park","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2404.15650v2.pdf","comment":"Under Review (9 pages, 4 figures)"},{"id":"http://arxiv.org/abs/2406.07081v1","updated":"2024-06-11T09:11:17Z","published":"2024-06-11T09:11:17Z","title":"Efficiently Exploring Large Language Models for Document-Level Machine\n  Translation with In-context Learning","summary":"  Large language models (LLMs) exhibit outstanding performance in machine\ntranslation via in-context learning. In contrast to sentence-level translation,\ndocument-level translation (DOCMT) by LLMs based on in-context learning faces\ntwo major challenges: firstly, document translations generated by LLMs are\noften incoherent; secondly, the length of demonstration for in-context learning\nis usually limited. To address these issues, we propose a Context-Aware\nPrompting method (CAP), which enables LLMs to generate more accurate, cohesive,\nand coherent translations via in-context learning. CAP takes into account\nmulti-level attention, selects the most relevant sentences to the current one\nas context, and then generates a summary from these collected sentences.\nSubsequently, sentences most similar to the summary are retrieved from the\ndatastore as demonstrations, which effectively guide LLMs in generating\ncohesive and coherent translations. We conduct extensive experiments across\nvarious DOCMT tasks, and the results demonstrate the effectiveness of our\napproach, particularly in zero pronoun translation (ZPT) and literary\ntranslation tasks.\n","authors":["Menglong Cui","Jiangcun Du","Shaolin Zhu","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.07081v1.pdf","comment":"Accepted to ACL2024 long paper (Findings)"},{"id":"http://arxiv.org/abs/2406.07080v1","updated":"2024-06-11T09:09:37Z","published":"2024-06-11T09:09:37Z","title":"DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for\n  Question Answering over Knowledge Graphs","summary":"  Answering Questions over Knowledge Graphs (KGQA) is key to well-functioning\nautonomous language agents in various real-life applications. To improve the\nneural-symbolic reasoning capabilities of language agents powered by Large\nLanguage Models (LLMs) in KGQA, we propose the DecompositionAlignment-Reasoning\nAgent (DARA) framework. DARA effectively parses questions into formal queries\nthrough a dual mechanism: high-level iterative task decomposition and low-level\ntask grounding. Importantly, DARA can be efficiently trained with a small\nnumber of high-quality reasoning trajectories. Our experimental results\ndemonstrate that DARA fine-tuned on LLMs (e.g. Llama-2-7B, Mistral) outperforms\nboth in-context learning-based agents with GPT-4 and alternative fine-tuned\nagents, across different benchmarks in zero-shot evaluation, making such models\nmore accessible for real-life applications. We also show that DARA attains\nperformance comparable to state-of-the-art enumerating-and-ranking-based\nmethods for KGQA.\n","authors":["Haishuo Fang","Xiaodan Zhu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2406.07080v1.pdf","comment":"Accepted by ACL2024 findings"},{"id":"http://arxiv.org/abs/2406.07070v1","updated":"2024-06-11T08:56:18Z","published":"2024-06-11T08:56:18Z","title":"HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level\n  Hallucination Evaluation","summary":"  Large Language Models (LLMs) have significantly advanced the field of Natural\nLanguage Processing (NLP), achieving remarkable performance across diverse\ntasks and enabling widespread real-world applications. However, LLMs are prone\nto hallucination, generating content that either conflicts with established\nknowledge or is unfaithful to the original sources. Existing hallucination\nbenchmarks primarily focus on sentence- or passage-level hallucination\ndetection, neglecting dialogue-level evaluation, hallucination localization,\nand rationale provision. They also predominantly target factuality\nhallucinations while underestimating faithfulness hallucinations, often relying\non labor-intensive or non-specialized evaluators. To address these limitations,\nwe propose HalluDial, the first comprehensive large-scale benchmark for\nautomatic dialogue-level hallucination evaluation. HalluDial encompasses both\nspontaneous and induced hallucination scenarios, covering factuality and\nfaithfulness hallucinations. The benchmark includes 4,094 dialogues with a\ntotal of 146,856 samples. Leveraging HalluDial, we conduct a comprehensive\nmeta-evaluation of LLMs' hallucination evaluation capabilities in\ninformation-seeking dialogues and introduce a specialized judge language model,\nHalluJudge. The high data quality of HalluDial enables HalluJudge to achieve\nsuperior or competitive performance in hallucination evaluation, facilitating\nthe automatic assessment of dialogue-level hallucinations in LLMs and providing\nvaluable insights into this phenomenon. The dataset and the code are available\nat https://github.com/FlagOpen/HalluDial.\n","authors":["Wen Luo","Tianshu Shen","Wei Li","Guangyue Peng","Richeng Xuan","Houfeng Wang","Xi Yang"],"pdf_url":"https://arxiv.org/pdf/2406.07070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07060v1","updated":"2024-06-11T08:41:21Z","published":"2024-06-11T08:41:21Z","title":"Reading Miscue Detection in Primary School through Automatic Speech\n  Recognition","summary":"  Automatic reading diagnosis systems can benefit both teachers for more\nefficient scoring of reading exercises and students for accessing reading\nexercises with feedback more easily. However, there are limited studies on\nAutomatic Speech Recognition (ASR) for child speech in languages other than\nEnglish, and limited research on ASR-based reading diagnosis systems. This\nstudy investigates how efficiently state-of-the-art (SOTA) pretrained ASR\nmodels recognize Dutch native children speech and manage to detect reading\nmiscues. We found that Hubert Large finetuned on Dutch speech achieves SOTA\nphoneme-level child speech recognition (PER at 23.1\\%), while Whisper (Faster\nWhisper Large-v2) achieves SOTA word-level performance (WER at 9.8\\%). Our\nfindings suggest that Wav2Vec2 Large and Whisper are the two best ASR models\nfor reading miscue detection. Specifically, Wav2Vec2 Large shows the highest\nrecall at 0.83, whereas Whisper exhibits the highest precision at 0.52 and an\nF1 score of 0.52.\n","authors":["Lingyun Gao","Cristian Tejedor-Garcia","Helmer Strik","Catia Cucchiarini"],"pdf_url":"https://arxiv.org/pdf/2406.07060v1.pdf","comment":"Proc. INTERSPEECH 2024, 1-5 September 2024. Kos Island, Greece"},{"id":"http://arxiv.org/abs/2406.07057v1","updated":"2024-06-11T08:38:13Z","published":"2024-06-11T08:38:13Z","title":"Benchmarking Trustworthiness of Multimodal Large Language Models: A\n  Comprehensive Study","summary":"  Despite the superior capabilities of Multimodal Large Language Models (MLLMs)\nacross diverse tasks, they still face significant trustworthiness challenges.\nYet, current literature on the assessment of trustworthy MLLMs remains limited,\nlacking a holistic evaluation to offer thorough insights into future\nimprovements. In this work, we establish MultiTrust, the first comprehensive\nand unified benchmark on the trustworthiness of MLLMs across five primary\naspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark\nemploys a rigorous evaluation strategy that addresses both multimodal risks and\ncross-modal impacts, encompassing 32 diverse tasks with self-curated datasets.\nExtensive experiments with 21 modern MLLMs reveal some previously unexplored\ntrustworthiness issues and risks, highlighting the complexities introduced by\nthe multimodality and underscoring the necessity for advanced methodologies to\nenhance their reliability. For instance, typical proprietary models still\nstruggle with the perception of visually confusing images and are vulnerable to\nmultimodal jailbreaking and adversarial attacks; MLLMs are more inclined to\ndisclose privacy in text and reveal ideological and cultural biases even when\npaired with irrelevant images in inference, indicating that the multimodality\namplifies the internal risks from base LLMs. Additionally, we release a\nscalable toolbox for standardized trustworthiness research, aiming to\nfacilitate future advancements in this important field. Code and resources are\npublicly available at: https://multi-trust.github.io/.\n","authors":["Yichi Zhang","Yao Huang","Yitong Sun","Chang Liu","Zhe Zhao","Zhengwei Fang","Yifan Wang","Huanran Chen","Xiao Yang","Xingxing Wei","Hang Su","Yinpeng Dong","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.07057v1.pdf","comment":"100 pages, 84 figures, 33 tables"},{"id":"http://arxiv.org/abs/2406.07056v1","updated":"2024-06-11T08:37:33Z","published":"2024-06-11T08:37:33Z","title":"Effectively Compress KV Heads for LLM","summary":"  The advent of pre-trained large language models (LLMs) has revolutionized\nvarious natural language processing tasks. These models predominantly employ an\nauto-regressive decoding mechanism that utilizes Key-Value (KV) caches to\neliminate redundant calculations for previous tokens. Nevertheless, as context\nlengths and batch sizes increase, the linear expansion in memory footprint of\nKV caches becomes a key bottleneck of LLM deployment, which decreases\ngeneration speeds significantly. To mitigate this issue, previous techniques\nlike multi-query attention (MQA) and grouped-query attention (GQA) have been\ndeveloped, in order to reduce KV heads to accelerate inference with comparable\naccuracy to multi-head attention (MHA). Despite their effectiveness, existing\nstrategies for compressing MHA often overlook the intrinsic properties of the\nKV caches. In this work, we explore the low-rank characteristics of the KV\ncaches and propose a novel approach for compressing KV heads. In particular, we\ncarefully optimize the MHA-to-GQA transformation to minimize compression error,\nand to remain compatible with rotary position embeddings (RoPE), we also\nintroduce specialized strategies for key caches with RoPE. We demonstrate that\nour method can compress half or even three-quarters of KV heads while\nmaintaining performance comparable to the original LLMs, which presents a\npromising direction for more efficient LLM deployment in resource-constrained\nenvironments.\n","authors":["Hao Yu","Zelan Yang","Shen Li","Yong Li","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2406.07056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07054v1","updated":"2024-06-11T08:35:37Z","published":"2024-06-11T08:35:37Z","title":"CoEvol: Constructing Better Responses for Instruction Finetuning through\n  Multi-Agent Cooperation","summary":"  In recent years, instruction fine-tuning (IFT) on large language models\n(LLMs) has garnered considerable attention to enhance model performance on\nunseen tasks. Attempts have been made on automatic construction and effective\nselection for IFT data. However, we posit that previous methods have not fully\nharnessed the potential of LLMs for enhancing data quality. The responses\nwithin IFT data could be further enhanced by leveraging the capabilities of\nLLMs themselves. In this paper, we propose CoEvol, an LLM-based multi-agent\ncooperation framework for the improvement of responses to instructions. To\neffectively refine the responses, we develop an iterative framework following a\ndebate-advise-edit-judge paradigm. A two-stage multi-agent debate strategy is\nfurther devised to ensure the diversity and reliability of editing suggestions\nwithin the framework. Empirically, models equipped with CoEvol outperform\ncompetitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its\neffectiveness in enhancing instruction-following capabilities for LLMs.\n","authors":["Renhao Li","Minghuan Tan","Derek F. Wong","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2406.07054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11457v2","updated":"2024-06-11T08:08:47Z","published":"2024-02-18T04:57:19Z","title":"When Do LLMs Need Retrieval Augmentation? Mitigating LLMs'\n  Overconfidence Helps Retrieval Augmentation","summary":"  Large Language Models (LLMs) have been found to have difficulty knowing they\ndo not possess certain knowledge and tend to provide specious answers in such\ncases. Retrieval Augmentation (RA) has been extensively studied to mitigate\nLLMs' hallucinations. However, due to the extra overhead and unassured quality\nof retrieval, it may not be optimal to conduct RA all the time. A\nstraightforward idea is to only conduct retrieval when LLMs are uncertain about\na question. This motivates us to enhance the LLMs' ability to perceive their\nknowledge boundaries to help RA. In this paper, we first quantitatively measure\nLLMs' such ability and confirm their overconfidence. Then, we study how LLMs'\ncertainty about a question correlates with their dependence on external\nretrieved information. We propose several methods to enhance LLMs' perception\nof knowledge boundaries and show that they are effective in reducing\noverconfidence. Additionally, equipped with these methods, LLMs can achieve\ncomparable or even better performance of RA with much fewer retrieval calls.\n","authors":["Shiyu Ni","Keping Bi","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2402.11457v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07036v1","updated":"2024-06-11T07:49:04Z","published":"2024-06-11T07:49:04Z","title":"Paying More Attention to Source Context: Mitigating Unfaithful\n  Translations from Large Language Model","summary":"  Large language models (LLMs) have showcased impressive multilingual machine\ntranslation ability. However, unlike encoder-decoder style models, decoder-only\nLLMs lack an explicit alignment between source and target contexts. Analyzing\ncontribution scores during generation processes revealed that LLMs can be\nbiased towards previously generated tokens over corresponding source tokens,\nleading to unfaithful translations. To address this issue, we propose to\nencourage LLMs to pay more attention to the source context from both source and\ntarget perspectives in zeroshot prompting: 1) adjust source context attention\nweights; 2) suppress irrelevant target prefix influence; Additionally, we\npropose 3) avoiding over-reliance on the target prefix in instruction tuning.\nExperimental results from both human-collected unfaithfulness test sets\nfocusing on LLM-generated unfaithful translations and general test sets, verify\nour methods' effectiveness across multiple language pairs. Further human\nevaluation shows our method's efficacy in reducing hallucinatory translations\nand facilitating faithful translation generation.\n","authors":["Hongbin Zhang","Kehai Chen","Xuefeng Bai","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07036v1.pdf","comment":"Accepted by ACL2024 Findings"},{"id":"http://arxiv.org/abs/2406.07034v1","updated":"2024-06-11T07:48:20Z","published":"2024-06-11T07:48:20Z","title":"Improving Multi-hop Logical Reasoning in Knowledge Graphs with\n  Context-Aware Query Representation Learning","summary":"  Multi-hop logical reasoning on knowledge graphs is a pivotal task in natural\nlanguage processing, with numerous approaches aiming to answer First-Order\nLogic (FOL) queries. Recent geometry (e.g., box, cone) and probability (e.g.,\nbeta distribution)-based methodologies have effectively addressed complex FOL\nqueries. However, a common challenge across these methods lies in determining\naccurate geometric bounds or probability parameters for these queries. The\nchallenge arises because existing methods rely on linear sequential operations\nwithin their computation graphs, overlooking the logical structure of the query\nand the relation-induced information that can be gleaned from the relations of\nthe query, which we call the context of the query. To address the problem, we\npropose a model-agnostic methodology that enhances the effectiveness of\nexisting multi-hop logical reasoning approaches by fully integrating the\ncontext of the FOL query graph. Our approach distinctively discerns (1) the\nstructural context inherent to the query structure and (2) the relation-induced\ncontext unique to each node in the query graph as delineated in the\ncorresponding knowledge graph. This dual-context paradigm helps nodes within a\nquery graph attain refined internal representations throughout the multi-hop\nreasoning steps. Through experiments on two datasets, our method consistently\nenhances the three multi-hop reasoning foundation models, achieving performance\nimprovements of up to 19.5%. Our code is available at\nhttps://github.com/kjh9503/caqr.\n","authors":["Jeonghoon Kim","Heesoo Jung","Hyeju Jang","Hogun Park"],"pdf_url":"https://arxiv.org/pdf/2406.07034v1.pdf","comment":"Accepted to ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2405.18357v2","updated":"2024-06-11T07:41:03Z","published":"2024-05-28T16:55:33Z","title":"Faithful Logical Reasoning via Symbolic Chain-of-Thought","summary":"  While the recent Chain-of-Thought (CoT) technique enhances the reasoning\nability of large language models (LLMs) with the theory of mind, it might still\nstruggle in handling logical reasoning that relies much on symbolic expressions\nand rigid deducing rules. To strengthen the logical reasoning capability of\nLLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully\nLLM-based framework that integrates symbolic expressions and logic rules with\nCoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates\nthe natural language context into the symbolic format, and then 2) derives a\nstep-by-step plan to solve the problem with symbolic logical rules, 3) followed\nby a verifier to check the translation and reasoning chain. Via thorough\nevaluations on 5 standard datasets with both First-Order Logic and Constraint\nOptimization symbolic expressions, SymbCoT shows striking improvements over the\nCoT method consistently, meanwhile refreshing the current state-of-the-art\nperformances. We further demonstrate that our system advances in more faithful,\nflexible, and explainable logical reasoning. To our knowledge, this is the\nfirst to combine symbolic expressions and rules into CoT for logical reasoning\nwith LLMs. Code is open at https://github.com/Aiden0526/SymbCoT.\n","authors":["Jundong Xu","Hao Fei","Liangming Pan","Qian Liu","Mong-Li Lee","Wynne Hsu"],"pdf_url":"https://arxiv.org/pdf/2405.18357v2.pdf","comment":"Accepted by ACL 2024 (main proceeding)"},{"id":"http://arxiv.org/abs/2405.06708v5","updated":"2024-06-11T07:31:13Z","published":"2024-05-09T10:04:05Z","title":"LangCell: Language-Cell Pre-training for Cell Identity Understanding","summary":"  Cell identity encompasses various semantic aspects of a cell, including cell\ntype, pathway information, disease information, and more, which are essential\nfor biologists to gain insights into its biological characteristics.\nUnderstanding cell identity from the transcriptomic data, such as annotating\ncell types, has become an important task in bioinformatics. As these semantic\naspects are determined by human experts, it is impossible for AI models to\neffectively carry out cell identity understanding tasks without the supervision\nsignals provided by single-cell and label pairs. The single-cell pre-trained\nlanguage models (PLMs) currently used for this task are trained only on a\nsingle modality, transcriptomics data, lack an understanding of cell identity\nknowledge. As a result, they have to be fine-tuned for downstream tasks and\nstruggle when lacking labeled data with the desired semantic labels. To address\nthis issue, we propose an innovative solution by constructing a unified\nrepresentation of single-cell data and natural language during the pre-training\nphase, allowing the model to directly incorporate insights related to cell\nidentity. More specifically, we introduce $\\textbf{LangCell}$, the first\n$\\textbf{Lang}$uage-$\\textbf{Cell}$ pre-training framework. LangCell utilizes\ntexts enriched with cell identity information to gain a profound comprehension\nof cross-modal knowledge. Results from experiments conducted on different\nbenchmarks show that LangCell is the only single-cell PLM that can work\neffectively in zero-shot cell identity understanding scenarios, and also\nsignificantly outperforms existing models in few-shot and fine-tuning cell\nidentity understanding scenarios.\n","authors":["Suyuan Zhao","Jiahuan Zhang","Yushuai Wu","Yizhen Luo","Zaiqing Nie"],"pdf_url":"https://arxiv.org/pdf/2405.06708v5.pdf","comment":"Accpeted by ICML 2024, code released"},{"id":"http://arxiv.org/abs/2406.07017v1","updated":"2024-06-11T07:19:04Z","published":"2024-06-11T07:19:04Z","title":"MoreauPruner: Robust Pruning of Large Language Models against Weight\n  Perturbations","summary":"  Few-shot gradient methods have been extensively utilized in existing model\npruning methods, where the model weights are regarded as static values and the\neffects of potential weight perturbations are not considered. However, the\nwidely used large language models (LLMs) have several billion model parameters,\nwhich could increase the fragility of few-shot gradient pruning. In this work,\nwe experimentally show that one-shot gradient pruning algorithms could lead to\nunstable results under perturbations to model weights. And the minor error of\nswitching between data formats bfloat16 and float16 could result in drastically\ndifferent outcomes. To address such instabilities, we leverage optimization\nanalysis and propose an LLM structural pruning method, called MoreauPruner,\nwith provable robustness against weight perturbations. In MoreauPruner, the\nmodel weight importance is estimated based on the neural network's Moreau\nenvelope, which can be flexibly combined with $\\ell_1$-norm regularization\ntechniques to induce the sparsity required in the pruning task. We extensively\nevaluate the MoreauPruner algorithm on several well-known LLMs, including\nLLaMA-7B, LLaMA-13B, LLaMA3-8B, and Vicuna-7B. Our numerical results suggest\nthe robustness of MoreauPruner against weight perturbations, and indicate the\nMoreauPruner's successful accuracy-based scores in comparison to several\nexisting pruning methods. We have released the code in\n\\url{https://github.com/ShiningSord/MoreauPruner}.\n","authors":["Zixiao Wang","Jingwei Zhang","Wenqian Zhao","Farzan Farnia","Bei Yu"],"pdf_url":"https://arxiv.org/pdf/2406.07017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07016v1","updated":"2024-06-11T07:16:34Z","published":"2024-06-11T07:16:34Z","title":"Delving into ChatGPT usage in academic writing through excess vocabulary","summary":"  Recent large language models (LLMs) can generate and revise text with\nhuman-level performance, and have been widely commercialized in systems like\nChatGPT. These models come with clear limitations: they can produce inaccurate\ninformation, reinforce existing biases, and be easily misused. Yet, many\nscientists have been using them to assist their scholarly writing. How\nwide-spread is LLM usage in the academic literature currently? To answer this\nquestion, we use an unbiased, large-scale approach, free from any assumptions\non academic LLM usage. We study vocabulary changes in 14 million PubMed\nabstracts from 2010-2024, and show how the appearance of LLMs led to an abrupt\nincrease in the frequency of certain style words. Our analysis based on excess\nwords usage suggests that at least 10% of 2024 abstracts were processed with\nLLMs. This lower bound differed across disciplines, countries, and journals,\nand was as high as 30% for some PubMed sub-corpora. We show that the appearance\nof LLM-based writing assistants has had an unprecedented impact in the\nscientific literature, surpassing the effect of major world events such as the\nCovid pandemic.\n","authors":["Dmitry Kobak","Rita González Márquez","Emőke-Ágnes Horvát","Jan Lause"],"pdf_url":"https://arxiv.org/pdf/2406.07016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11197v2","updated":"2024-06-11T07:14:46Z","published":"2024-02-17T05:15:12Z","title":"Centroid-Based Efficient Minimum Bayes Risk Decoding","summary":"  Minimum Bayes risk (MBR) decoding achieved state-of-the-art translation\nperformance by using COMET, a neural metric that has a high correlation with\nhuman evaluation. However, MBR decoding requires quadratic time since it\ncomputes the expected score between a translation hypothesis and all reference\ntranslations. We propose centroid-based MBR (CBMBR) decoding to improve the\nspeed of MBR decoding. Our method clusters the reference translations in the\nfeature space, and then calculates the score using the centroids of each\ncluster. The experimental results show that our CBMBR not only improved the\ndecoding speed of the expected score calculation 5.7 times, but also\noutperformed vanilla MBR decoding in translation quality by up to 0.5 COMET in\nthe WMT'22 En$\\leftrightarrow$Ja, En$\\leftrightarrow$De, En$\\leftrightarrow$Zh,\nand WMT'23 En$\\leftrightarrow$Ja translation tasks.\n","authors":["Hiroyuki Deguchi","Yusuke Sakai","Hidetaka Kamigaito","Taro Watanabe","Hideki Tanaka","Masao Utiyama"],"pdf_url":"https://arxiv.org/pdf/2402.11197v2.pdf","comment":"Accepted at Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2406.07012v1","updated":"2024-06-11T07:12:12Z","published":"2024-06-11T07:12:12Z","title":"Bridging Language Gaps in Audio-Text Retrieval","summary":"  Audio-text retrieval is a challenging task, requiring the search for an audio\nclip or a text caption within a database. The predominant focus of existing\nresearch on English descriptions poses a limitation on the applicability of\nsuch models, given the abundance of non-English content in real-world data. To\naddress these linguistic disparities, we propose a language enhancement (LE),\nusing a multilingual text encoder (SONAR) to encode the text data with\nlanguage-specific information. Additionally, we optimize the audio encoder\nthrough the application of consistent ensemble distillation (CED), enhancing\nsupport for variable-length audio-text retrieval. Our methodology excels in\nEnglish audio-text retrieval, demonstrating state-of-the-art (SOTA) performance\non commonly used datasets such as AudioCaps and Clotho. Simultaneously, the\napproach exhibits proficiency in retrieving content in seven other languages\nwith only 10% of additional language-enhanced training data, yielding promising\nresults. The source code is publicly available\nhttps://github.com/zyyan4/ml-clap.\n","authors":["Zhiyong Yan","Heinrich Dinkel","Yongqing Wang","Jizhong Liu","Junbo Zhang","Yujun Wang","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07012v1.pdf","comment":"interspeech2024"},{"id":"http://arxiv.org/abs/2406.03202v2","updated":"2024-06-11T07:06:34Z","published":"2024-06-05T12:35:00Z","title":"ChatLang-8: An LLM-Based Synthetic Data Generation Framework for\n  Grammatical Error Correction","summary":"  We explore and improve the capabilities of LLMs to generate data for\ngrammatical error correction (GEC). When merely producing parallel sentences,\ntheir patterns are too simplistic to be valuable as a corpus. To address this\nissue, we propose an automated framework that includes a Subject Selector,\nGrammar Selector, Prompt Manager, and Evaluator. Additionally, we introduce a\nnew dataset for GEC tasks, named ChatLang-8, which encompasses eight types of\nsubject nouns and 23 types of grammar. It consists of 1 million pairs featuring\nhuman-like grammatical errors. Our experiments reveal that ChatLang-8 exhibits\na more uniform pattern composition compared to existing GEC datasets.\nFurthermore, we observe improved model performance when using ChatLang-8\ninstead of existing GEC datasets. The experimental results suggest that our\nframework and ChatLang-8 are valuable resources for enhancing ChatGPT's data\ngeneration capabilities.\n","authors":["Jeiyoon Park","Chanjun Park","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2406.03202v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2406.07007v1","updated":"2024-06-11T07:00:08Z","published":"2024-06-11T07:00:08Z","title":"Crayon: Customized On-Device LLM via Instant Adapter Blending and\n  Edge-Server Hybrid Inference","summary":"  The customization of large language models (LLMs) for user-specified tasks\ngets important. However, maintaining all the customized LLMs on cloud servers\nincurs substantial memory and computational overheads, and uploading user data\ncan also lead to privacy concerns. On-device LLMs can offer a promising\nsolution by mitigating these issues. Yet, the performance of on-device LLMs is\ninherently constrained by the limitations of small-scaled models. To overcome\nthese restrictions, we first propose Crayon, a novel approach for on-device LLM\ncustomization. Crayon begins by constructing a pool of diverse base adapters,\nand then we instantly blend them into a customized adapter without extra\ntraining. In addition, we develop a device-server hybrid inference strategy,\nwhich deftly allocates more demanding queries or non-customized tasks to a\nlarger, more capable LLM on a server. This ensures optimal performance without\nsacrificing the benefits of on-device customization. We carefully craft a novel\nbenchmark from multiple question-answer datasets, and show the efficacy of our\nmethod in the LLM customization.\n","authors":["Jihwan Bang","Juntae Lee","Kyuhong Shim","Seunghan Yang","Simyung Chang"],"pdf_url":"https://arxiv.org/pdf/2406.07007v1.pdf","comment":"ACL 2024 Main"},{"id":"http://arxiv.org/abs/2312.06722v3","updated":"2024-06-11T06:53:44Z","published":"2023-12-11T03:35:58Z","title":"EgoPlan-Bench: Benchmarking Multimodal Large Language Models for\n  Human-Level Planning","summary":"  The pursuit of artificial general intelligence (AGI) has been accelerated by\nMultimodal Large Language Models (MLLMs), which exhibit superior reasoning,\ngeneralization capabilities, and proficiency in processing multimodal inputs. A\ncrucial milestone in the evolution of AGI is the attainment of human-level\nplanning, a fundamental ability for making informed decisions in complex\nenvironments, and solving a wide range of real-world problems. Despite the\nimpressive advancements in MLLMs, a question remains: How far are current MLLMs\nfrom achieving human-level planning? To shed light on this question, we\nintroduce EgoPlan-Bench, a comprehensive benchmark to evaluate the planning\nabilities of MLLMs in real-world scenarios from an egocentric perspective,\nmirroring human perception. EgoPlan-Bench emphasizes the evaluation of planning\ncapabilities of MLLMs, featuring realistic tasks, diverse action plans, and\nintricate visual observations. Our rigorous evaluation of a wide range of MLLMs\nreveals that EgoPlan-Bench poses significant challenges, highlighting a\nsubstantial scope for improvement in MLLMs to achieve human-level task\nplanning. To facilitate this advancement, we further present EgoPlan-IT, a\nspecialized instruction-tuning dataset that effectively enhances model\nperformance on EgoPlan-Bench. We have made all codes, data, and a maintained\nbenchmark leaderboard available to advance future research.\n","authors":["Yi Chen","Yuying Ge","Yixiao Ge","Mingyu Ding","Bohao Li","Rui Wang","Ruifeng Xu","Ying Shan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2312.06722v3.pdf","comment":"Project released at: https://github.com/ChenYi99/EgoPlan"},{"id":"http://arxiv.org/abs/2406.07001v1","updated":"2024-06-11T06:53:19Z","published":"2024-06-11T06:53:19Z","title":"Mitigating Boundary Ambiguity and Inherent Bias for Text Classification\n  in the Era of Large Language Models","summary":"  Text classification is a crucial task encountered frequently in practical\nscenarios, yet it is still under-explored in the era of large language models\n(LLMs). This study shows that LLMs are vulnerable to changes in the number and\narrangement of options in text classification. Our extensive empirical analyses\nreveal that the key bottleneck arises from ambiguous decision boundaries and\ninherent biases towards specific tokens and positions. To mitigate these\nissues, we make the first attempt and propose a novel two-stage classification\nframework for LLMs. Our approach is grounded in the empirical observation that\npairwise comparisons can effectively alleviate boundary ambiguity and inherent\nbias. Specifically, we begin with a self-reduction technique to efficiently\nnarrow down numerous options, which contributes to reduced decision space and a\nfaster comparison process. Subsequently, pairwise contrastive comparisons are\nemployed in a chain-of-thought manner to draw out nuances and distinguish\nconfusable options, thus refining the ambiguous decision boundary. Extensive\nexperiments on four datasets (Banking77, HWU64, LIU54, and Clinic150) verify\nthe effectiveness of our framework. Furthermore, benefitting from our\nframework, various LLMs can achieve consistent improvements. Our code and data\nare available in \\url{https://github.com/Chuge0335/PC-CoT}.\n","authors":["Zhenyi Lu","Jie Tian","Wei Wei","Xiaoye Qu","Yu Cheng","Wenfeng xie","Dangyang Chen"],"pdf_url":"https://arxiv.org/pdf/2406.07001v1.pdf","comment":"ACL2024 findings"},{"id":"http://arxiv.org/abs/2402.04788v3","updated":"2024-06-11T06:21:46Z","published":"2024-02-07T12:28:32Z","title":"MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with\n  Vision-Language Benchmark","summary":"  Multimodal Large Language Models (MLLMs) have gained significant attention\nrecently, showing remarkable potential in artificial general intelligence.\nHowever, assessing the utility of MLLMs presents considerable challenges,\nprimarily due to the absence of multimodal benchmarks that align with human\npreferences. Drawing inspiration from the concept of LLM-as-a-Judge within\nLLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to\nassess the ability of MLLMs in assisting judges across diverse modalities,\nencompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and\nBatch Ranking. Our study reveals that, while MLLMs demonstrate remarkable\nhuman-like discernment in Pair Comparison, there is a significant divergence\nfrom human preferences in Scoring Evaluation and Batch Ranking. Furthermore, a\ncloser examination reveals persistent challenges in the judgment capacities of\nLLMs, including diverse biases, hallucinatory responses, and inconsistencies in\njudgment, even in advanced models such as GPT-4V. These findings emphasize the\npressing need for enhancements and further research efforts to be undertaken\nbefore regarding MLLMs as fully reliable evaluators. In light of this, we\nadvocate for additional efforts dedicated to supporting the continuous\ndevelopment within the domain of MLLM functioning as judges. The code and\ndataset are publicly available at our project homepage:\n\\url{https://mllm-judge.github.io/}.\n","authors":["Dongping Chen","Ruoxi Chen","Shilin Zhang","Yinuo Liu","Yaochen Wang","Huichi Zhou","Qihui Zhang","Yao Wan","Pan Zhou","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2402.04788v3.pdf","comment":"ICML 2024 (Oral)"},{"id":"http://arxiv.org/abs/2406.02528v3","updated":"2024-06-11T06:18:28Z","published":"2024-06-04T17:50:34Z","title":"Scalable MatMul-free Language Modeling","summary":"  Matrix multiplication (MatMul) typically dominates the overall computational\ncost of large language models (LLMs). This cost only grows as LLMs scale to\nlarger embedding dimensions and context lengths. In this work, we show that\nMatMul operations can be completely eliminated from LLMs while maintaining\nstrong performance at billion-parameter scales. Our experiments show that our\nproposed MatMul-free models achieve performance on-par with state-of-the-art\nTransformers that require far more memory during inference at a scale up to at\nleast 2.7B parameters. We investigate the scaling laws and find that the\nperformance gap between our MatMul-free models and full precision Transformers\nnarrows as the model size increases. We also provide a GPU-efficient\nimplementation of this model which reduces memory usage by up to 61% over an\nunoptimized baseline during training. By utilizing an optimized kernel during\ninference, our model's memory consumption can be reduced by more than 10x\ncompared to unoptimized models. To properly quantify the efficiency of our\narchitecture, we build a custom hardware solution on an FPGA which exploits\nlightweight operations beyond what GPUs are capable of. We processed\nbillion-parameter scale models at 13W beyond human readable throughput, moving\nLLMs closer to brain-like efficiency. This work not only shows how far LLMs can\nbe stripped back while still performing effectively, but also points at the\ntypes of operations future accelerators should be optimized for in processing\nthe next generation of lightweight LLMs. Our code implementation is available\nat https://github.com/ridgerchu/matmulfreellm.\n","authors":["Rui-Jie Zhu","Yu Zhang","Ethan Sifferman","Tyler Sheaves","Yiqiao Wang","Dustin Richmond","Peng Zhou","Jason K. Eshraghian"],"pdf_url":"https://arxiv.org/pdf/2406.02528v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06964v1","updated":"2024-06-11T05:47:16Z","published":"2024-06-11T05:47:16Z","title":"Missingness-resilient Video-enhanced Multimodal Disfluency Detection","summary":"  Most existing speech disfluency detection techniques only rely upon acoustic\ndata. In this work, we present a practical multimodal disfluency detection\napproach that leverages available video data together with audio. We curate an\naudiovisual dataset and propose a novel fusion technique with unified\nweight-sharing modality-agnostic encoders to learn the temporal and semantic\ncontext. Our resilient design accommodates real-world scenarios where the video\nmodality may sometimes be missing during inference. We also present alternative\nfusion strategies when both modalities are assured to be complete. In\nexperiments across five disfluency-detection tasks, our unified multimodal\napproach significantly outperforms Audio-only unimodal methods, yielding an\naverage absolute improvement of 10% (i.e., 10 percentage point increase) when\nboth video and audio modalities are always available, and 7% even when video\nmodality is missing in half of the samples.\n","authors":["Payal Mohapatra","Shamika Likhite","Subrata Biswas","Bashima Islam","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.06964v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.06962v1","updated":"2024-06-11T05:44:56Z","published":"2024-06-11T05:44:56Z","title":"Evolving Subnetwork Training for Large Language Models","summary":"  Large language models have ushered in a new era of artificial intelligence\nresearch. However, their substantial training costs hinder further development\nand widespread adoption. In this paper, inspired by the redundancy in the\nparameters of large language models, we propose a novel training paradigm:\nEvolving Subnetwork Training (EST). EST samples subnetworks from the layers of\nthe large language model and from commonly used modules within each layer,\nMulti-Head Attention (MHA) and Multi-Layer Perceptron (MLP). By gradually\nincreasing the size of the subnetworks during the training process, EST can\nsave the cost of training. We apply EST to train GPT2 model and TinyLlama\nmodel, resulting in 26.7\\% FLOPs saving for GPT2 and 25.0\\% for TinyLlama\nwithout an increase in loss on the pre-training dataset. Moreover, EST leads to\nperformance improvements in downstream tasks, indicating that it benefits\ngeneralization. Additionally, we provide intuitive theoretical studies based on\ntraining dynamics and Dropout theory to ensure the feasibility of EST. Our code\nis available at https://github.com/OpenDFM/EST.\n","authors":["Hanqi Li","Lu Chen","Da Ma","Zijian Wu","Su Zhu","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2406.06962v1.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2406.06950v1","updated":"2024-06-11T05:21:37Z","published":"2024-06-11T05:21:37Z","title":"A Probabilistic Framework for LLM Hallucination Detection via Belief\n  Tree Propagation","summary":"  This paper focuses on the task of hallucination detection, which aims to\ndetermine the truthfulness of LLM-generated statements. To address this\nproblem, a popular class of methods utilize the LLM's self-consistencies in its\nbeliefs in a set of logically related augmented statements generated by the\nLLM, which does not require external knowledge databases and can work with both\nwhite-box and black-box LLMs. However, in many existing approaches, the\naugmented statements tend to be very monotone and unstructured, which makes it\ndifficult to integrate meaningful information from the LLM beliefs in these\nstatements. Also, many methods work with the binarized version of the LLM's\nbelief, instead of the continuous version, which significantly loses\ninformation. To overcome these limitations, in this paper, we propose Belief\nTree Propagation (BTProp), a probabilistic framework for LLM hallucination\ndetection. BTProp introduces a belief tree of logically related statements by\nrecursively decomposing a parent statement into child statements with three\ndecomposition strategies, and builds a hidden Markov tree model to integrate\nthe LLM's belief scores in these statements in a principled way. Experiment\nresults show that our method improves baselines by 3%-9% (evaluated by AUROC\nand AUC-PR) on multiple hallucination detection benchmarks. Code is available\nat https://github.com/UCSB-NLP-Chang/BTProp.\n","authors":["Bairu Hou","Yang Zhang","Jacob Andreas","Shiyu Chang"],"pdf_url":"https://arxiv.org/pdf/2406.06950v1.pdf","comment":"26 pages, 18 figures"},{"id":"http://arxiv.org/abs/2406.06938v1","updated":"2024-06-11T04:31:54Z","published":"2024-06-11T04:31:54Z","title":"Post-Hoc Answer Attribution for Grounded and Trustworthy Long Document\n  Comprehension: Task, Insights, and Challenges","summary":"  Attributing answer text to its source document for information-seeking\nquestions is crucial for building trustworthy, reliable, and accountable\nsystems. We formulate a new task of post-hoc answer attribution for long\ndocument comprehension (LDC). Owing to the lack of long-form abstractive and\ninformation-seeking LDC datasets, we refactor existing datasets to assess the\nstrengths and weaknesses of existing retrieval-based and proposed answer\ndecomposition and textual entailment-based optimal selection attribution\nsystems for this task. We throw light on the limitations of existing datasets\nand the need for datasets to assess the actual performance of systems on this\ntask.\n","authors":["Abhilasha Sancheti","Koustava Goswami","Balaji Vasan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2406.06938v1.pdf","comment":"Accepted to *SEM 2024"},{"id":"http://arxiv.org/abs/2406.06937v1","updated":"2024-06-11T04:25:48Z","published":"2024-06-11T04:25:48Z","title":"A Non-autoregressive Generation Framework for End-to-End Simultaneous\n  Speech-to-Any Translation","summary":"  Simultaneous translation models play a crucial role in facilitating\ncommunication. However, existing research primarily focuses on text-to-text or\nspeech-to-text models, necessitating additional cascade components to achieve\nspeech-to-speech translation. These pipeline methods suffer from error\npropagation and accumulate delays in each cascade component, resulting in\nreduced synchronization between the speaker and listener. To overcome these\nchallenges, we propose a novel non-autoregressive generation framework for\nsimultaneous speech translation (NAST-S2X), which integrates speech-to-text and\nspeech-to-speech tasks into a unified end-to-end framework. We develop a\nnon-autoregressive decoder capable of concurrently generating multiple text or\nacoustic unit tokens upon receiving fixed-length speech chunks. The decoder can\ngenerate blank or repeated tokens and employ CTC decoding to dynamically adjust\nits latency. Experimental results show that NAST-S2X outperforms\nstate-of-the-art models in both speech-to-text and speech-to-speech tasks. It\nachieves high-quality simultaneous interpretation within a delay of less than 3\nseconds and provides a 28 times decoding speedup in offline generation.\n","authors":["Zhengrui Ma","Qingkai Fang","Shaolei Zhang","Shoutao Guo","Yang Feng","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06937v1.pdf","comment":"ACL 2024; Codes and demos are at https://github.com/ictnlp/NAST-S2x"},{"id":"http://arxiv.org/abs/2406.02921v2","updated":"2024-06-11T04:11:56Z","published":"2024-06-05T04:20:17Z","title":"Text Injection for Neural Contextual Biasing","summary":"  Neural contextual biasing effectively improves automatic speech recognition\n(ASR) for crucial phrases within a speaker's context, particularly those that\nare infrequent in the training data. This work proposes contextual text\ninjection (CTI) to enhance contextual ASR. CTI leverages not only the paired\nspeech-text data, but also a much larger corpus of unpaired text to optimize\nthe ASR model and its biasing component. Unpaired text is converted into\nspeech-like representations and used to guide the model's attention towards\nrelevant bias phrases. Moreover, we introduce a contextual text-injected (CTI)\nminimum word error rate (MWER) training, which minimizes the expected WER\ncaused by contextual biasing when unpaired text is injected into the model.\nExperiments show that CTI with 100 billion text sentences can achieve up to\n43.3% relative WER reduction from a strong neural biasing model. CTI-MWER\nprovides a further relative improvement of 23.5%.\n","authors":["Zhong Meng","Zelin Wu","Rohit Prabhavalkar","Cal Peyser","Weiran Wang","Nanxin Chen","Tara N. Sainath","Bhuvana Ramabhadran"],"pdf_url":"https://arxiv.org/pdf/2406.02921v2.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.07556v3","updated":"2024-06-11T03:53:10Z","published":"2024-03-12T11:40:44Z","title":"Truth-Aware Context Selection: Mitigating Hallucinations of Large\n  Language Models Being Misled by Untruthful Contexts","summary":"  Although Large Language Models (LLMs) have demonstrated impressive text\ngeneration capabilities, they are easily misled by untruthful contexts provided\nby users or knowledge augmentation tools, leading to hallucinations. To\nalleviate LLMs from being misled by untruthful context and take advantage of\nknowledge augmentation, we propose Truth-Aware Context Selection (TACS), a\nlightweight method to adaptively recognize and mask untruthful context from the\ninputs. TACS begins by performing truth detection on the input context,\nleveraging the parameterized knowledge within the LLM. Subsequently, it\nconstructs a corresponding attention mask based on the truthfulness of each\nposition, selecting the truthful context and discarding the untruthful context.\nAdditionally, we introduce a new evaluation metric, Disturbance Adaption Rate,\nto further study the LLMs' ability to accept truthful information and resist\nuntruthful information. Experimental results indicate that TACS can effectively\nfilter untruthful context and significantly improve the overall quality of\nLLMs' responses when presented with misleading information.\n","authors":["Tian Yu","Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2403.07556v3.pdf","comment":"Accepted to ACL 2024 Findings. Code is available at:\n  https://github.com/ictnlp/TACS"},{"id":"http://arxiv.org/abs/2406.01304v3","updated":"2024-06-11T03:52:03Z","published":"2024-06-03T13:13:35Z","title":"CodeR: Issue Resolving with Multi-Agent and Task Graphs","summary":"  GitHub issue resolving recently has attracted significant attention from\nacademia and industry. SWE-bench is proposed to measure the performance in\nresolving issues. In this paper, we propose CodeR, which adopts a multi-agent\nframework and pre-defined task graphs to Repair & Resolve reported bugs and add\nnew features within code Repository. On SWE-bench lite, CodeR is able to solve\n28.33% of issues, when submitting only once for each issue. We examine the\nperformance impact of each design of CodeR and offer insights to advance this\nresearch direction.\n","authors":["Dong Chen","Shaoxin Lin","Muhan Zeng","Daoguang Zan","Jian-Gang Wang","Anton Cheshkov","Jun Sun","Hao Yu","Guoliang Dong","Artem Aliev","Jie Wang","Xiao Cheng","Guangtai Liang","Yuchi Ma","Pan Bian","Tao Xie","Qianxiang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.01304v3.pdf","comment":"https://github.com/NL2Code/CodeR"},{"id":"http://arxiv.org/abs/2401.06034v5","updated":"2024-06-11T03:23:51Z","published":"2024-01-11T16:48:00Z","title":"LinguAlchemy: Fusing Typological and Geographical Elements for Unseen\n  Language Generalization","summary":"  Pretrained language models (PLMs) have become remarkably adept at task and\nlanguage generalization. Nonetheless, they often fail when faced with unseen\nlanguages. In this work, we present LinguAlchemy, a regularization method that\nincorporates various linguistic information covering typological, geographical,\nand phylogenetic features to align PLMs representation to the corresponding\nlinguistic information on each language. Our LinguAlchemy significantly\nimproves the performance of mBERT and XLM-R on low-resource languages in\nmultiple downstream tasks such as intent classification, news classification,\nand semantic relatedness compared to fully finetuned models and displaying a\nhigh degree of unseen language generalization. We further introduce\nAlchemyScale and AlchemyTune, extension of LinguAlchemy which adjusts the\nlinguistic regularization weights automatically, alleviating the need for\nhyperparameter search.\n","authors":["Muhammad Farid Adilazuarda","Samuel Cahyawijaya","Alham Fikri Aji","Genta Indra Winata","Ayu Purwarianti"],"pdf_url":"https://arxiv.org/pdf/2401.06034v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13049v2","updated":"2024-06-11T03:12:01Z","published":"2024-05-19T09:59:00Z","title":"SemEval-2024 Task 3: Multimodal Emotion Cause Analysis in Conversations","summary":"  The ability to understand emotions is an essential component of human-like\nartificial intelligence, as emotions greatly influence human cognition,\ndecision making, and social interactions. In addition to emotion recognition in\nconversations, the task of identifying the potential causes behind an\nindividual's emotional state in conversations, is of great importance in many\napplication scenarios. We organize SemEval-2024 Task 3, named Multimodal\nEmotion Cause Analysis in Conversations, which aims at extracting all pairs of\nemotions and their corresponding causes from conversations. Under different\nmodality settings, it consists of two subtasks: Textual Emotion-Cause Pair\nExtraction in Conversations (TECPE) and Multimodal Emotion-Cause Pair\nExtraction in Conversations (MECPE). The shared task has attracted 143\nregistrations and 216 successful submissions. In this paper, we introduce the\ntask, dataset and evaluation settings, summarize the systems of the top teams,\nand discuss the findings of the participants.\n","authors":["Fanfan Wang","Heqing Ma","Jianfei Yu","Rui Xia","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2405.13049v2.pdf","comment":"12 pages, 3 figures, 4 Tables"},{"id":"http://arxiv.org/abs/2406.06910v1","updated":"2024-06-11T03:09:20Z","published":"2024-06-11T03:09:20Z","title":"Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large\n  Language Models","summary":"  Simultaneous Machine Translation (SiMT) generates target translations while\nreading the source sentence. It relies on a policy to determine the optimal\ntiming for reading sentences and generating translations. Existing SiMT methods\ngenerally adopt the traditional Transformer architecture, which concurrently\ndetermines the policy and generates translations. While they excel at\ndetermining policies, their translation performance is suboptimal. Conversely,\nLarge Language Models (LLMs), trained on extensive corpora, possess superior\ngeneration capabilities, but it is difficult for them to acquire translation\npolicy through the training methods of SiMT. Therefore, we introduce\nAgent-SiMT, a framework combining the strengths of LLMs and traditional SiMT\nmethods. Agent-SiMT contains the policy-decision agent and the translation\nagent. The policy-decision agent is managed by a SiMT model, which determines\nthe translation policy using partial source sentence and translation. The\ntranslation agent, leveraging an LLM, generates translation based on the\npartial source sentence. The two agents collaborate to accomplish SiMT.\nExperiments demonstrate that Agent-SiMT attains state-of-the-art performance.\n","authors":["Shoutao Guo","Shaolei Zhang","Zhengrui Ma","Min Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2406.06910v1.pdf","comment":"18 pages, 8 figures, 7 tables. arXiv admin note: substantial text\n  overlap with arXiv:2402.13036"},{"id":"http://arxiv.org/abs/2406.06907v1","updated":"2024-06-11T03:00:41Z","published":"2024-06-11T03:00:41Z","title":"SignMusketeers: An Efficient Multi-Stream Approach for Sign Language\n  Translation at Scale","summary":"  A persistent challenge in sign language video processing, including the task\nof sign language to written language translation, is how we learn\nrepresentations of sign language in an effective and efficient way that can\npreserve the important attributes of these languages, while remaining invariant\nto irrelevant visual differences. Informed by the nature and linguistics of\nsigned languages, our proposed method focuses on just the most relevant parts\nin a signing video: the face, hands and body posture of the signer. However,\ninstead of using pose estimation coordinates from off-the-shelf pose tracking\nmodels, which have inconsistent performance for hands and faces, we propose to\nlearn the complex handshapes and rich facial expressions of sign languages in a\nself-supervised fashion. Our approach is based on learning from individual\nframes (rather than video sequences) and is therefore much more efficient than\nprior work on sign language pre-training. Compared to a recent model that\nestablished a new state of the art in sign language translation on the How2Sign\ndataset, our approach yields similar translation performance, using less than\n3\\% of the compute.\n","authors":["Shester Gueuwou","Xiaodan Du","Greg Shakhnarovich","Karen Livescu"],"pdf_url":"https://arxiv.org/pdf/2406.06907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06853v5","updated":"2024-06-11T02:38:27Z","published":"2024-01-12T19:00:26Z","title":"Large Language Models Can Learn Temporal Reasoning","summary":"  While large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities, they are not without their flaws and inaccuracies. Recent studies\nhave introduced various methods to mitigate these limitations. Temporal\nreasoning (TR), in particular, presents a significant challenge for LLMs due to\nits reliance on diverse temporal concepts and intricate temporal logic. In this\npaper, we propose TG-LLM, a novel framework towards language-based TR. Instead\nof reasoning over the original context, we adopt a latent representation,\ntemporal graph (TG) that enhances the learning of TR. A synthetic dataset\n(TGQA), which is fully controllable and requires minimal supervision, is\nconstructed for fine-tuning LLMs on this text-to-TG translation task. We\nconfirmed in experiments that the capability of TG translation learned on our\ndataset can be transferred to other TR tasks and benchmarks. On top of that, we\nteach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought\n(CoT) bootstrapping and graph data augmentation. We observed that those\nstrategies, which maintain a balance between usefulness and diversity, bring\nmore reliable CoTs and final results than the vanilla CoT distillation.\n","authors":["Siheng Xiong","Ali Payani","Ramana Kompella","Faramarz Fekri"],"pdf_url":"https://arxiv.org/pdf/2401.06853v5.pdf","comment":"ACL24 (main)"},{"id":"http://arxiv.org/abs/2403.16792v3","updated":"2024-06-11T02:38:20Z","published":"2024-03-25T14:07:27Z","title":"Iterative Refinement of Project-Level Code Context for Precise Code\n  Generation with Compiler Feedback","summary":"  Large Language Models (LLMs) have shown remarkable progress in automated code\ngeneration. Yet, LLM-generated code may contain errors in API usage, class,\ndata structure, or missing project-specific information. As much of this\nproject-specific context cannot fit into the prompts of LLMs, we must find ways\nto allow the model to explore the project-level code context. We present\nCoCoGen, a new code generation approach that uses compiler feedback to improve\nthe LLM-generated code. CoCoGen first leverages static analysis to identify\nmismatches between the generated code and the project's context. It then\niteratively aligns and fixes the identified errors using information extracted\nfrom the code repository. We integrate CoCoGen with two representative LLMs,\ni.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code\ngeneration. Experimental results show that CoCoGen significantly improves the\nvanilla LLMs by over 80% in generating code dependent on the project context\nand consistently outperforms the existing retrieval-based code generation\nbaselines.\n","authors":["Zhangqian Bi","Yao Wan","Zheng Wang","Hongyu Zhang","Batu Guan","Fangxin Lu","Zili Zhang","Yulei Sui","Hai Jin","Xuanhua Shi"],"pdf_url":"https://arxiv.org/pdf/2403.16792v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02073v3","updated":"2024-06-11T02:37:23Z","published":"2023-12-04T17:35:42Z","title":"A Glitch in the Matrix? Locating and Detecting Language Model Grounding\n  with Fakepedia","summary":"  Large language models (LLMs) have an impressive ability to draw on novel\ninformation supplied in their context. Yet the mechanisms underlying this\ncontextual grounding remain unknown, especially in situations where contextual\ninformation contradicts factual knowledge stored in the parameters, which LLMs\nalso excel at recalling. Favoring the contextual information is critical for\nretrieval-augmented generation methods, which enrich the context with\nup-to-date information, hoping that grounding can rectify outdated or noisy\nstored knowledge. We present a novel method to study grounding abilities using\nFakepedia, a novel dataset of counterfactual texts constructed to clash with a\nmodel's internal parametric knowledge. In this study, we introduce Fakepedia, a\ncounterfactual dataset designed to evaluate grounding abilities when the\ninternal parametric knowledge clashes with the contextual information. We\nbenchmark various LLMs with Fakepedia and conduct a causal mediation analysis\nof LLM components when answering Fakepedia queries, based on our Masked Grouped\nCausal Tracing (MGCT) method. Through this analysis, we identify distinct\ncomputational patterns between grounded and ungrounded responses. We finally\ndemonstrate that distinguishing grounded from ungrounded responses is\nachievable through computational analysis alone. Our results, together with\nexisting findings about factual recall mechanisms, provide a coherent narrative\nof how grounding and factual recall mechanisms interact within LLMs.\n","authors":["Giovanni Monea","Maxime Peyrard","Martin Josifoski","Vishrav Chaudhary","Jason Eisner","Emre Kıcıman","Hamid Palangi","Barun Patra","Robert West"],"pdf_url":"https://arxiv.org/pdf/2312.02073v3.pdf","comment":"Accepted at ACL 2024 (main conference)"},{"id":"http://arxiv.org/abs/2406.05955v2","updated":"2024-06-11T02:15:47Z","published":"2024-06-10T01:21:59Z","title":"Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated\n  Parameters","summary":"  Exploiting activation sparsity is a promising approach to significantly\naccelerating the inference process of large language models (LLMs) without\ncompromising performance. However, activation sparsity is determined by\nactivation functions, and commonly used ones like SwiGLU and GeGLU exhibit\nlimited sparsity. Simply replacing these functions with ReLU fails to achieve\nsufficient sparsity. Moreover, inadequate training data can further increase\nthe risk of performance degradation. To address these challenges, we propose a\nnovel dReLU function, which is designed to improve LLM activation sparsity,\nalong with a high-quality training data mixture ratio to facilitate effective\nsparsification. Additionally, we leverage sparse activation patterns within the\nFeed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to\nfurther boost efficiency. By applying our neuron sparsification method to the\nMistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are\nactivated per inference iteration, respectively, while achieving even more\npowerful model performance. Evaluation results demonstrate that this sparsity\nachieves a 2-5x decoding speedup. Remarkably, on mobile phones, our\nTurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second.\nOur models are available at \\url{https://huggingface.co/PowerInfer}\n","authors":["Yixin Song","Haotong Xie","Zhengyan Zhang","Bo Wen","Li Ma","Zeyu Mi","Haibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.05955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04913v2","updated":"2024-06-11T02:14:06Z","published":"2023-08-09T12:26:37Z","title":"LLaMA-E: Empowering E-commerce Authoring with Object-Interleaved\n  Instruction Following","summary":"  E-commerce authoring entails creating engaging, diverse, and targeted content\nto enhance preference elicitation and retrieval experience. While Large\nLanguage Models (LLMs) have revolutionized content generation, they often fall\nshort in e-commerce applications due to their limited memorization of\ndomain-specific features. This paper proposes LLaMA-E, the unified e-commerce\nauthoring models that address the contextual preferences of customers, sellers,\nand platforms, the essential objects in e-commerce operation. We design the\ninstruction set derived from tasks of ads generation, query-enhanced product\ntitle rewriting, product classification, purchase intent speculation, and\ngeneral e-commerce Q&A. The instruction formulation ensures the interleaved\ncover of the presented and required object features, allowing the alignment of\nbase models to parameterise e-commerce knowledge comprehensively. The proposed\nLLaMA-E models achieve state-of-the-art evaluation performance and exhibit the\nadvantage in zero-shot practical applications. To our knowledge, this is the\nfirst LLM tailored to empower authoring applications with comprehensive\nscenario understanding by integrating features focused on participated objects.\n","authors":["Kaize Shi","Xueyao Sun","Dingxian Wang","Yinlin Fu","Guandong Xu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2308.04913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06461v2","updated":"2024-06-11T02:12:17Z","published":"2024-06-10T16:55:08Z","title":"Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning\n  Strategies","summary":"  A diverse array of reasoning strategies has been proposed to elicit the\ncapabilities of large language models. However, in this paper, we point out\nthat traditional evaluations which focus solely on performance metrics miss a\nkey factor: the increased effectiveness due to additional compute. By\noverlooking this aspect, a skewed view of strategy efficiency is often\npresented. This paper introduces a framework that incorporates the compute\nbudget into the evaluation, providing a more informative comparison that takes\ninto account both performance metrics and computational cost. In this\nbudget-aware perspective, we find that complex reasoning strategies often don't\nsurpass simpler baselines purely due to algorithmic ingenuity, but rather due\nto the larger computational resources allocated. When we provide a simple\nbaseline like chain-of-thought self-consistency with comparable compute\nresources, it frequently outperforms reasoning strategies proposed in the\nliterature. In this scale-aware perspective, we find that unlike\nself-consistency, certain strategies such as multi-agent debate or Reflexion\ncan become worse if more compute budget is utilized.\n","authors":["Junlin Wang","Siddhartha Jain","Dejiao Zhang","Baishakhi Ray","Varun Kumar","Ben Athiwaratkun"],"pdf_url":"https://arxiv.org/pdf/2406.06461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06887v1","updated":"2024-06-11T02:07:18Z","published":"2024-06-11T02:07:18Z","title":"PLUM: Preference Learning Plus Test Cases Yields Better Code Language\n  Models","summary":"  Instruction-finetuned code language models (LMs) have shown promise in\nvarious programming tasks. They are trained, using a language modeling\nobjective, on natural language instructions and gold code snippet pairs. Recent\nevidence suggests that these models, never exposed to incorrect solutions\nduring training, often struggle to distinguish between correct and incorrect\nsolutions. This observation raises our inquiry: Can preference learning, which\ntrains models to prefer correct solutions over incorrect ones, help push the\nboundaries of code LMs even further? We propose PLUM, a novel\n\\textbf{p}reference \\textbf{l}earning framework a\\textbf{u}gmented with test\ncases tailored for code L\\textbf{M}s.PLUM aims to investigate the key success\nfactors and potential benefits of preference learning in code LMs, which remain\nelusive despite its success in aligning LMs with human values. PLUM consists of\nthree stages: (1) Generating test cases for natural language instructions, (2)\nsampling candidate solutions from the policy and evaluating them against the\ntest cases to create a preference dataset, which is then used to (3) train the\npolicy with a preference learning algorithm. Experiments demonstrate that PLUM\nsubstantially improves the performance of existing code LMs on established code\ngeneration benchmarks such as HumanEval (+) and MBPP (+), even for the\nstate-of-the-art open-source language model CodeQwen-1.5-7B-Chat. PLUM\ncomplements the supervised fine-tuning (SFT) stage, demonstrating synergistic\neffects.\n","authors":["Dylan Zhang","Shizhe Diao","Xueyan Zou","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2406.06887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02888v2","updated":"2024-06-11T01:51:57Z","published":"2024-06-05T03:08:46Z","title":"HYDRA: Model Factorization Framework for Black-Box LLM Personalization","summary":"  Personalization has emerged as a critical research area in modern intelligent\nsystems, focusing on mining users' behavioral history and adapting to their\npreferences for delivering tailored experiences. Despite the remarkable\nfew-shot capabilities exhibited by black-box large language models (LLMs), the\ninherent opacity of their model parameters presents significant challenges in\naligning the generated output with individual expectations. Existing solutions\nhave primarily focused on prompt design to incorporate user-specific profiles\nand behaviors; however, such approaches often struggle to generalize\neffectively due to their inability to capture shared knowledge among all users.\nTo address these challenges, we propose HYDRA, a model factorization framework\nthat captures both user-specific behavior patterns from historical data and\nshared general knowledge among all users to deliver personalized generation. In\norder to capture user-specific behavior patterns, we first train a reranker to\nprioritize the most useful information from top-retrieved relevant historical\nrecords. By combining the prioritized history with the corresponding query, we\ntrain an adapter to align the output with individual user-specific preferences,\neliminating the reliance on access to inherent model parameters of black-box\nLLMs. Both the reranker and the adapter can be decomposed into a base model\nwith multiple user-specific heads, resembling a hydra. The base model maintains\nshared knowledge across users, while the multiple personal heads capture\nuser-specific preferences. Experimental results demonstrate that HYDRA\noutperforms existing state-of-the-art prompt-based methods by an average\nrelative improvement of 9.01% across five diverse personalization tasks in the\nLaMP benchmark. Our implementation is available at\nhttps://github.com/night-chen/HYDRA.\n","authors":["Yuchen Zhuang","Haotian Sun","Yue Yu","Rushi Qiang","Qifan Wang","Chao Zhang","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2406.02888v2.pdf","comment":"24 pages, 6 figures, work in progress"},{"id":"http://arxiv.org/abs/2406.06878v1","updated":"2024-06-11T01:43:23Z","published":"2024-06-11T01:43:23Z","title":"Modeling language contact with the Iterated Learning Model","summary":"  Contact between languages has the potential to transmit vocabulary and other\nlanguage features; however, this does not always happen. Here, an iterated\nlearning model is used to examine, in a simple way, the resistance of languages\nto change during language contact. Iterated learning models are agent-based\nmodels of language change, they demonstrate that languages that are expressive\nand compositional arise spontaneously as a consequence of a language\ntransmission bottleneck. A recently introduced type of iterated learning model,\nthe Semi-Supervised ILM is used to simulate language contact. These simulations\ndo not include many of the complex factors involved in language contact and do\nnot model a population of speakers; nonetheless the model demonstrates that the\ndynamics which lead languages in the model to spontaneously become expressive\nand compositional, also cause a language to maintain its core traits even after\nmixing with another language.\n","authors":["Seth Bullock","Conor Houghton"],"pdf_url":"https://arxiv.org/pdf/2406.06878v1.pdf","comment":"to appear ALIFE24"},{"id":"http://arxiv.org/abs/2406.06870v1","updated":"2024-06-11T01:10:40Z","published":"2024-06-11T01:10:40Z","title":"What's in an embedding? Would a rose by any embedding smell as sweet?","summary":"  Large Language Models (LLMs) are often criticized for lacking true\n\"understanding\" and an ability to \"reason\" with their knowledge, being seen\nmerely as advanced autocomplete systems. We believe that this perspective might\nbe missing an important insight. We suggest that LLMs do develop a kind of\nempirical \"understanding\" that is \"geometry\"-like, which seems quite sufficient\nfor a range of applications in NLP, computer vision, coding assistance, etc.\nHowever, this \"geometric\" understanding, built from incomplete and noisy data,\nmakes them unreliable, difficult to generalize, and lacking in inference\ncapabilities and explanations, similar to the challenges faced by\nheuristics-based expert systems decades ago.\n  To overcome these limitations, we suggest that LLMs should be integrated with\nan \"algebraic\" representation of knowledge that includes symbolic AI elements\nused in expert systems. This integration aims to create large knowledge models\n(LKMs) that not only possess \"deep\" knowledge grounded in first principles, but\nalso have the ability to reason and explain, mimicking human expert\ncapabilities. To harness the full potential of generative AI safely and\neffectively, a paradigm shift from LLMs to the more comprehensive LKMs is\nneeded.\n","authors":["Venkat Venkatasubramanian"],"pdf_url":"https://arxiv.org/pdf/2406.06870v1.pdf","comment":"7 pages, 9 images"},{"id":"http://arxiv.org/abs/2406.07759v1","updated":"2024-06-11T22:48:18Z","published":"2024-06-11T22:48:18Z","title":"LT4SG@SMM4H24: Tweets Classification for Digital Epidemiology of\n  Childhood Health Outcomes Using Pre-Trained Language Models","summary":"  This paper presents our approaches for the SMM4H24 Shared Task 5 on the\nbinary classification of English tweets reporting children's medical disorders.\nOur first approach involves fine-tuning a single RoBERTa-large model, while the\nsecond approach entails ensembling the results of three fine-tuned\nBERTweet-large models. We demonstrate that although both approaches exhibit\nidentical performance on validation data, the BERTweet-large ensemble excels on\ntest data. Our best-performing system achieves an F1-score of 0.938 on test\ndata, outperforming the benchmark classifier by 1.18%.\n","authors":["Dasun Athukoralage","Thushari Atapattu","Menasha Thilakaratne","Katrina Falkner"],"pdf_url":"https://arxiv.org/pdf/2406.07759v1.pdf","comment":"Submitted for the 9th Social Media Mining for Health Research and\n  Applications Workshop and Shared Tasks- Large Language Models (LLMs) and\n  Generalizability for Social Media NLP"},{"id":"http://arxiv.org/abs/2406.07753v1","updated":"2024-06-11T22:26:20Z","published":"2024-06-11T22:26:20Z","title":"The MuSe 2024 Multimodal Sentiment Analysis Challenge: Social Perception\n  and Humor Recognition","summary":"  The Multimodal Sentiment Analysis Challenge (MuSe) 2024 addresses two\ncontemporary multimodal affect and sentiment analysis problems: In the Social\nPerception Sub-Challenge (MuSe-Perception), participants will predict 16\ndifferent social attributes of individuals such as assertiveness, dominance,\nlikability, and sincerity based on the provided audio-visual data. The\nCross-Cultural Humor Detection Sub-Challenge (MuSe-Humor) dataset expands upon\nthe Passau Spontaneous Football Coach Humor (Passau-SFCH) dataset, focusing on\nthe detection of spontaneous humor in a cross-lingual and cross-cultural\nsetting. The main objective of MuSe 2024 is to unite a broad audience from\nvarious research domains, including multimodal sentiment analysis, audio-visual\naffective computing, continuous signal processing, and natural language\nprocessing. By fostering collaboration and exchange among experts in these\nfields, the MuSe 2024 endeavors to advance the understanding and application of\nsentiment analysis and affective computing across multiple modalities. This\nbaseline paper provides details on each sub-challenge and its corresponding\ndataset, extracted features from each data modality, and discusses challenge\nbaselines. For our baseline system, we make use of a range of Transformers and\nexpert-designed features and train Gated Recurrent Unit (GRU)-Recurrent Neural\nNetwork (RNN) models on them, resulting in a competitive baseline system. On\nthe unseen test datasets of the respective sub-challenges, it achieves a mean\nPearson's Correlation Coefficient ($\\rho$) of 0.3573 for MuSe-Perception and an\nArea Under the Curve (AUC) value of 0.8682 for MuSe-Humor.\n","authors":["Shahin Amiriparian","Lukas Christ","Alexander Kathan","Maurice Gerczuk","Niklas Müller","Steffen Klug","Lukas Stappen","Andreas König","Erik Cambria","Björn Schuller","Simone Eulitz"],"pdf_url":"https://arxiv.org/pdf/2406.07753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07739v1","updated":"2024-06-11T21:53:46Z","published":"2024-06-11T21:53:46Z","title":"UICoder: Finetuning Large Language Models to Generate User Interface\n  Code through Automated Feedback","summary":"  Large language models (LLMs) struggle to consistently generate UI code that\ncompiles and produces visually relevant designs. Existing approaches to improve\ngeneration rely on expensive human feedback or distilling a proprietary model.\nIn this paper, we explore the use of automated feedback (compilers and\nmulti-modal models) to guide LLMs to generate high-quality UI code. Our method\nstarts with an existing LLM and iteratively produces improved models by\nself-generating a large synthetic dataset using an original model, applying\nautomated tools to aggressively filter, score, and de-duplicate the data into a\nrefined higher quality dataset. The original LLM is improved by finetuning on\nthis refined dataset. We applied our approach to several open-source LLMs and\ncompared the resulting performance to baseline models with both automated\nmetrics and human preferences. Our evaluation shows the resulting models\noutperform all other downloadable baselines and approach the performance of\nlarger proprietary models.\n","authors":["Jason Wu","Eldon Schoop","Alan Leung","Titus Barik","Jeffrey P. Bigham","Jeffrey Nichols"],"pdf_url":"https://arxiv.org/pdf/2406.07739v1.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2406.07736v1","updated":"2024-06-11T21:46:03Z","published":"2024-06-11T21:46:03Z","title":"MultiPragEval: Multilingual Pragmatic Evaluation of Large Language\n  Models","summary":"  As the capabilities of LLMs expand, it becomes increasingly important to\nevaluate them beyond basic knowledge assessment, focusing on higher-level\nlanguage understanding. This study introduces MultiPragEval, a robust test\nsuite designed for the multilingual pragmatic evaluation of LLMs across\nEnglish, German, Korean, and Chinese. Comprising 1200 question units\ncategorized according to Grice's Cooperative Principle and its four\nconversational maxims, MultiPragEval enables an in-depth assessment of LLMs'\ncontextual awareness and their ability to infer implied meanings. Our findings\ndemonstrate that Claude3-Opus significantly outperforms other models in all\ntested languages, establishing a state-of-the-art in the field. Among\nopen-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors.\nThis study not only leads the way in the multilingual evaluation of LLMs in\npragmatic inference but also provides valuable insights into the nuanced\ncapabilities necessary for advanced language comprehension in AI systems.\n","authors":["Dojun Park","Jiwoo Lee","Seohyun Park","Hyeyun Jeong","Youngeun Koo","Soonha Hwang","Seonwoo Park","Sungeun Lee"],"pdf_url":"https://arxiv.org/pdf/2406.07736v1.pdf","comment":"8 pages, under review"},{"id":"http://arxiv.org/abs/2406.07735v1","updated":"2024-06-11T21:44:49Z","published":"2024-06-11T21:44:49Z","title":"REAL Sampling: Boosting Factuality and Diversity of Open-Ended\n  Generation via Asymptotic Entropy","summary":"  Decoding methods for large language models (LLMs) usually struggle with the\ntradeoff between ensuring factuality and maintaining diversity. For example, a\nhigher p threshold in the nucleus (top-p) sampling increases the diversity but\ndecreases the factuality, and vice versa. In this paper, we propose REAL\n(Residual Entropy from Asymptotic Line) sampling, a decoding method that\nachieves improved factuality and diversity over nucleus sampling by predicting\nan adaptive threshold of $p$. Specifically, REAL sampling predicts the\nstep-wise likelihood of an LLM to hallucinate, and lowers the p threshold when\nan LLM is likely to hallucinate. Otherwise, REAL sampling increases the p\nthreshold to boost the diversity. To predict the step-wise hallucination\nlikelihood without supervision, we construct a Token-level Hallucination\nForecasting (THF) model to predict the asymptotic entropy (i.e., inherent\nuncertainty) of the next token by extrapolating the next-token entropies from a\nseries of LLMs with different sizes. If a LLM's entropy is higher than the\nasymptotic entropy (i.e., the LLM is more uncertain than it should be), the THF\nmodel predicts a high hallucination hazard, which leads to a lower p threshold\nin REAL sampling. In the FactualityPrompts benchmark, we demonstrate that REAL\nsampling based on a 70M THF model can substantially improve the factuality and\ndiversity of 7B LLMs simultaneously, judged by both retrieval-based metrics and\nhuman evaluation. After combined with contrastive decoding, REAL sampling\noutperforms 9 sampling methods, and generates texts that are more factual than\nthe greedy sampling and more diverse than the nucleus sampling with $p=0.5$.\nFurthermore, the predicted asymptotic entropy is also a useful unsupervised\nsignal for hallucination detection tasks.\n","authors":["Haw-Shiuan Chang","Nanyun Peng","Mohit Bansal","Anil Ramakrishna","Tagyoung Chung"],"pdf_url":"https://arxiv.org/pdf/2406.07735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12940v2","updated":"2024-06-11T20:28:35Z","published":"2024-02-20T11:52:29Z","title":"Normalized Orthography for Tunisian Arabic","summary":"  Tunisian Arabic (ISO 693-3: aeb) isa distinct variety native to Tunisia,\nderived from Arabic and enriched by various historical influences. This\nresearch introduces the \"Normalized Orthography for Tunisian Arabic\" (NOTA), an\nadaptation of CODA* guidelines for transcribing Tunisian Arabic using Arabic\nscript. The aim is to enhance language resource development by ensuring\nuser-friendliness and consistency. The updated standard addresses challenges in\naccurately representing Tunisian phonology and morphology, correcting issues\nfrom transcriptions based on Modern Standard Arabic.\n","authors":["Houcemeddine Turki","Kawthar Ellouze","Hager Ben Ammar","Mohamed Ali Hadj Taieb","Imed Adel","Mohamed Ben Aouicha","Pier Luigi Farri","Abderrezak Bennour"],"pdf_url":"https://arxiv.org/pdf/2402.12940v2.pdf","comment":"Final Report for the Derja Association. Camera-Ready for LPKM 2024"},{"id":"http://arxiv.org/abs/2406.07696v1","updated":"2024-06-11T20:21:36Z","published":"2024-06-11T20:21:36Z","title":"Sustainable self-supervised learning for speech representations","summary":"  Sustainable artificial intelligence focuses on data, hardware, and algorithms\nto make machine learning models more environmentally responsible. In\nparticular, machine learning models for speech representations are\ncomputationally expensive, generating environmental concerns because of their\nhigh energy consumption. Thus, we propose a sustainable self-supervised model\nto learn speech representation, combining optimizations in neural layers and\ntraining to reduce computing costs. The proposed model improves over a\nresource-efficient baseline, reducing both memory usage and computing cost\nestimations. It pretrains using a single GPU in less than a day. On top of\nthat, it improves the error rate performance of the baseline in downstream task\nevaluations. When comparing it to large speech representation approaches, there\nis an order of magnitude reduction in memory usage, while computing cost\nreductions represent almost three orders of magnitude improvement.\n","authors":["Luis Lugo","Valentin Vielzeuf"],"pdf_url":"https://arxiv.org/pdf/2406.07696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07693v1","updated":"2024-06-11T20:14:22Z","published":"2024-06-11T20:14:22Z","title":"A Labelled Dataset for Sentiment Analysis of Videos on YouTube, TikTok,\n  and Other Sources about the 2024 Outbreak of Measles","summary":"  The work of this paper presents a dataset that contains the data of 4011\nvideos about the ongoing outbreak of measles published on 264 websites on the\ninternet between January 1, 2024, and May 31, 2024. The dataset is available at\nhttps://dx.doi.org/10.21227/40s8-xf63. These websites primarily include YouTube\nand TikTok, which account for 48.6% and 15.2% of the videos, respectively. The\nremainder of the websites include Instagram and Facebook as well as the\nwebsites of various global and local news organizations. For each of these\nvideos, the URL of the video, title of the post, description of the post, and\nthe date of publication of the video are presented as separate attributes in\nthe dataset. After developing this dataset, sentiment analysis (using VADER),\nsubjectivity analysis (using TextBlob), and fine-grain sentiment analysis\n(using DistilRoBERTa-base) of the video titles and video descriptions were\nperformed. This included classifying each video title and video description\ninto (i) one of the sentiment classes i.e. positive, negative, or neutral, (ii)\none of the subjectivity classes i.e. highly opinionated, neutral opinionated,\nor least opinionated, and (iii) one of the fine-grain sentiment classes i.e.\nfear, surprise, joy, sadness, anger, disgust, or neutral. These results are\npresented as separate attributes in the dataset for the training and testing of\nmachine learning algorithms for performing sentiment analysis or subjectivity\nanalysis in this field as well as for other applications. Finally, this paper\nalso presents a list of open research questions that may be investigated using\nthis dataset.\n","authors":["Nirmalya Thakur","Vanessa Su","Mingchen Shao","Kesha A. Patel","Hongseok Jeong","Victoria Knieling","Andrew Brian"],"pdf_url":"https://arxiv.org/pdf/2406.07693v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2406.07692v1","updated":"2024-06-11T20:14:09Z","published":"2024-06-11T20:14:09Z","title":"Transformer Models in Education: Summarizing Science Textbooks with\n  AraBART, MT5, AraT5, and mBART","summary":"  Recently, with the rapid development in the fields of technology and the\nincreasing amount of text t available on the internet, it has become urgent to\ndevelop effective tools for processing and understanding texts in a way that\nsummaries the content without losing the fundamental essence of the\ninformation. Given this challenge, we have developed an advanced text\nsummarization system targeting Arabic textbooks. Relying on modern natu-ral\nlanguage processing models such as MT5, AraBART, AraT5, and mBART50, this\nsystem evaluates and extracts the most important sentences found in biology\ntextbooks for the 11th and 12th grades in the Palestinian curriculum, which\nenables students and teachers to obtain accurate and useful summaries that help\nthem easily understand the content. We utilized the Rouge metric to evaluate\nthe performance of the trained models. Moreover, experts in education Edu\ntextbook authoring assess the output of the trained models. This approach aims\nto identify the best solutions and clarify areas needing improvement. This\nresearch provides a solution for summarizing Arabic text. It enriches the field\nby offering results that can open new horizons for research and development in\nthe technologies for understanding and generating the Arabic language.\nAdditionally, it contributes to the field with Arabic texts through creating\nand compiling schoolbook texts and building a dataset.\n","authors":["Sari Masri","Yaqeen Raddad","Fidaa Khandaqji","Huthaifa I. Ashqar","Mohammed Elhenawy"],"pdf_url":"https://arxiv.org/pdf/2406.07692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07685v1","updated":"2024-06-11T20:05:15Z","published":"2024-06-11T20:05:15Z","title":"Out-Of-Context Prompting Boosts Fairness and Robustness in Large\n  Language Model Predictions","summary":"  Frontier Large Language Models (LLMs) are increasingly being deployed for\nhigh-stakes decision-making. On the other hand, these models are still\nconsistently making predictions that contradict users' or society's\nexpectations, e.g., hallucinating, or discriminating. Thus, it is important\nthat we develop test-time strategies to improve their trustworthiness. Inspired\nby prior work, we leverage causality as a tool to formally encode two aspects\nof trustworthiness in LLMs: fairness and robustness. Under this perspective,\nexisting test-time solutions explicitly instructing the model to be fair or\nrobust implicitly depend on the LLM's causal reasoning capabilities. In this\nwork, we explore the opposite approach. Instead of explicitly asking the LLM\nfor trustworthiness, we design prompts to encode the underlying causal\ninference algorithm that will, by construction, result in more trustworthy\npredictions. Concretely, we propose out-of-context prompting as a test-time\nsolution to encourage fairness and robustness in LLMs. Out-of-context prompting\nleverages the user's prior knowledge of the task's causal model to apply\n(random) counterfactual transformations and improve the model's\ntrustworthiness. Empirically, we show that out-of-context prompting\nconsistently improves the fairness and robustness of frontier LLMs across five\ndifferent benchmark datasets without requiring additional data, finetuning or\npre-training.\n","authors":["Leonardo Cotta","Chris J. Maddison"],"pdf_url":"https://arxiv.org/pdf/2406.07685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06406v2","updated":"2024-06-11T19:54:35Z","published":"2024-06-10T15:58:42Z","title":"Controlling Emotion in Text-to-Speech with Natural Language Prompts","summary":"  In recent years, prompting has quickly become one of the standard ways of\nsteering the outputs of generative machine learning models, due to its\nintuitive use of natural language. In this work, we propose a system\nconditioned on embeddings derived from an emotionally rich text that serves as\nprompt. Thereby, a joint representation of speaker and prompt embeddings is\nintegrated at several points within a transformer-based architecture. Our\napproach is trained on merged emotional speech and text datasets and varies\nprompts in each training iteration to increase the generalization capabilities\nof the model. Objective and subjective evaluation results demonstrate the\nability of the conditioned synthesis system to accurately transfer the emotions\npresent in a prompt to speech. At the same time, precise tractability of\nspeaker identities as well as overall high speech quality and intelligibility\nare maintained.\n","authors":["Thomas Bott","Florian Lux","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2406.06406v2.pdf","comment":"accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2308.13170v2","updated":"2024-06-11T19:38:28Z","published":"2023-08-25T04:19:58Z","title":"Measuring Spurious Correlation in Classification: 'Clever Hans' in\n  Translationese","summary":"  Recent work has shown evidence of 'Clever Hans' behavior in high-performance\nneural translationese classifiers, where BERT-based classifiers capitalize on\nspurious correlations, in particular topic information, between data and target\nclassification labels, rather than genuine translationese signals.\nTranslationese signals are subtle (especially for professional translation) and\ncompete with many other signals in the data such as genre, style, author, and,\nin particular, topic. This raises the general question of how much of the\nperformance of a classifier is really due to spurious correlations in the data\nversus the signals actually targeted for by the classifier, especially for\nsubtle target signals and in challenging (low resource) data settings. We focus\non topic-based spurious correlation and approach the question from two\ndirections: (i) where we have no knowledge about spurious topic information and\nits distribution in the data, (ii) where we have some indication about the\nnature of spurious topic correlations. For (i) we develop a measure from first\nprinciples capturing alignment of unsupervised topics with target\nclassification labels as an indication of spurious topic information in the\ndata. We show that our measure is the same as purity in clustering and propose\na 'topic floor' (as in a 'noise floor') for classification. For (ii) we\ninvestigate masking of known spurious topic carriers in classification. Both\n(i) and (ii) contribute to quantifying and (ii) to mitigating spurious\ncorrelations.\n","authors":["Angana Borah","Daria Pylypenko","Cristina Espana-Bonet","Josef van Genabith"],"pdf_url":"https://arxiv.org/pdf/2308.13170v2.pdf","comment":"Accepted to RANLP 2023 (oral)"},{"id":"http://arxiv.org/abs/2404.03022v2","updated":"2024-06-11T19:34:19Z","published":"2024-04-03T19:17:43Z","title":"BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and\n  Multilingual Exploration of Persuasion in Memes","summary":"  Memes, combining text and images, frequently use metaphors to convey\npersuasive messages, shaping public opinion. Motivated by this, our team\nengaged in SemEval-2024 Task 4, a hierarchical multi-label classification task\ndesigned to identify rhetorical and psychological persuasion techniques\nembedded within memes. To tackle this problem, we introduced a caption\ngeneration step to assess the modality gap and the impact of additional\nsemantic information from images, which improved our result. Our best model\nutilizes GPT-4 generated captions alongside meme text to fine-tune RoBERTa as\nthe text encoder and CLIP as the image encoder. It outperforms the baseline by\na large margin in all 12 subtasks. In particular, it ranked in top-3 across all\nlanguages in Subtask 2a, and top-4 in Subtask 2b, demonstrating quantitatively\nstrong performance. The improvement achieved by the introduced intermediate\nstep is likely attributable to the metaphorical essence of images that\nchallenges visual encoders. This highlights the potential for improving\nabstract visual semantics encoding.\n","authors":["Amirhossein Abaskohi","Amirhossein Dabiriaghdam","Lele Wang","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2404.03022v2.pdf","comment":"12 pages, 5 tables, 2 figures, Proceedings of the 18th International\n  Workshop on Semantic Evaluation (SemEval-2024) @ NAACL 2024"},{"id":"http://arxiv.org/abs/2310.02107v4","updated":"2024-06-11T19:00:03Z","published":"2023-10-03T14:51:34Z","title":"Instances Need More Care: Rewriting Prompts for Instances with LLMs in\n  the Loop Yields Better Zero-Shot Performance","summary":"  Large language models (LLMs) have revolutionized zero-shot task performance,\nmitigating the need for task-specific annotations while enhancing task\ngeneralizability. Despite its advancements, current methods using trigger\nphrases such as \"Let's think step by step\" remain limited. This study\nintroduces PRomPTed, an approach that optimizes the zero-shot prompts for\nindividual task instances following an innovative manner of \"LLMs in the loop\".\nOur comprehensive evaluation across 13 datasets and 10 task types based on\nGPT-4 reveals that PRomPTed significantly outperforms both the naive zero-shot\napproaches and a strong baseline (i.e., \"Output Refinement\") which refines the\ntask output instead of the input prompt. Our experimental results also\nconfirmed the generalization of this advantage to the relatively weaker\nGPT-3.5. Even more intriguingly, we found that leveraging GPT-3.5 to rewrite\nprompts for the stronger GPT-4 not only matches but occasionally exceeds the\nefficacy of using GPT-4 as the prompt rewriter. Our research thus presents a\nhuge value in not only enhancing zero-shot LLM performance but also potentially\nenabling supervising LLMs with their weaker counterparts, a capability\nattracting much interest recently. Finally, our additional experiments confirm\nthe generalization of the advantages to open-source LLMs such as Mistral 7B and\nMixtral 8x7B.\n","authors":["Saurabh Srivastava","Chengyue Huang","Weiguo Fan","Ziyu Yao"],"pdf_url":"https://arxiv.org/pdf/2310.02107v4.pdf","comment":"Accepted at ACL 2024 - Findings"},{"id":"http://arxiv.org/abs/2406.07657v1","updated":"2024-06-11T18:55:04Z","published":"2024-06-11T18:55:04Z","title":"OPTune: Efficient Online Preference Tuning","summary":"  Reinforcement learning with human feedback~(RLHF) is critical for aligning\nLarge Language Models (LLMs) with human preference. Compared to the widely\nstudied offline version of RLHF, \\emph{e.g.} direct preference optimization\n(DPO), recent works have shown that the online variants achieve even better\nalignment. However, online alignment requires on-the-fly generation of new\ntraining data, which is costly, hard to parallelize, and suffers from varying\nquality and utility. In this paper, we propose a more efficient data\nexploration strategy for online preference tuning (OPTune), which does not rely\non human-curated or pre-collected teacher responses but dynamically samples\ninformative responses for on-policy preference alignment. During data\ngeneration, OPTune only selects prompts whose (re)generated responses can\npotentially provide more informative and higher-quality training signals than\nthe existing responses. In the training objective, OPTune reweights each\ngenerated response (pair) by its utility in improving the alignment so that\nlearning can be focused on the most helpful samples. Throughout our\nevaluations, OPTune'd LLMs maintain the instruction-following benefits provided\nby standard preference tuning whilst enjoying 1.27-1.56x faster training speed\ndue to the efficient data exploration strategy.\n","authors":["Lichang Chen","Jiuhai Chen","Chenxi Liu","John Kirchenbauer","Davit Soselia","Chen Zhu","Tom Goldstein","Tianyi Zhou","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2406.07657v1.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.16617v2","updated":"2024-06-11T18:54:13Z","published":"2024-02-26T14:47:35Z","title":"Long-Context Language Modeling with Parallel Context Encoding","summary":"  Extending large language models (LLMs) to process longer inputs is crucial\nfor a wide range of applications. However, the substantial computational cost\nof transformers and limited generalization of positional encoding restrict the\nsize of their context window. We introduce Context Expansion with Parallel\nEncoding (CEPE), a framework that can be applied to any existing decoder-only\nLLMs to extend their context window. CEPE employs a small encoder to process\nlong inputs chunk by chunk, enabling the frozen decoder to utilize additional\ncontexts via cross-attention. CEPE is efficient, generalizable, and versatile:\ntrained with 8K-token documents, it extends the context window of LLAMA-2 to\n128K tokens, offering 10x the throughput with only 1/6 of the memory. CEPE\nyields strong performance on language modeling and in-context learning. CEPE\nalso excels in retrieval-augmented applications, while existing long-context\nmodels degenerate with retrieved contexts. We further introduce a CEPE variant\nthat can extend the context window of instruction-tuned models using only\nunlabeled data, and showcase its effectiveness on LLAMA-2-CHAT, leading to a\nstrong instruction-following model that can leverage very long contexts on\ndownstream tasks.\n","authors":["Howard Yen","Tianyu Gao","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2402.16617v2.pdf","comment":"ACL 2024. Code, models, and data are available at\n  https://github.com/princeton-nlp/CEPE. arXiv admin note: text overlap with\n  arXiv:1912.01214 by other authors"},{"id":"http://arxiv.org/abs/2312.15842v3","updated":"2024-06-11T18:01:09Z","published":"2023-12-26T01:24:25Z","title":"Knowledge Distillation of LLM for Automatic Scoring of Science Education\n  Assessments","summary":"  This study proposes a method for knowledge distillation (KD) of fine-tuned\nLarge Language Models (LLMs) into smaller, more efficient, and accurate neural\nnetworks. We specifically target the challenge of deploying these models on\nresource-constrained devices. Our methodology involves training the smaller\nstudent model (Neural Network) using the prediction probabilities (as soft\nlabels) of the LLM, which serves as a teacher model. This is achieved through a\nspecialized loss function tailored to learn from the LLM's output\nprobabilities, ensuring that the student model closely mimics the teacher's\nperformance. To validate the performance of the KD approach, we utilized a\nlarge dataset, 7T, containing 6,684 student-written responses to science\nquestions and three mathematical reasoning datasets with student-written\nresponses graded by human experts. We compared accuracy with state-of-the-art\n(SOTA) distilled models, TinyBERT, and artificial neural network (ANN) models.\nResults have shown that the KD approach has 3% and 2% higher scoring accuracy\nthan ANN and TinyBERT, respectively, and comparable accuracy to the teacher\nmodel. Furthermore, the student model size is 0.03M, 4,000 times smaller in\nparameters and x10 faster in inferencing than the teacher model and TinyBERT,\nrespectively. The significance of this research lies in its potential to make\nadvanced AI technologies accessible in typical educational settings,\nparticularly for automatic scoring.\n","authors":["Ehsan Latif","Luyang Fang","Ping Ma","Xiaoming Zhai"],"pdf_url":"https://arxiv.org/pdf/2312.15842v3.pdf","comment":"Accepted to AIED2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.07550v1","updated":"2024-06-11T17:59:56Z","published":"2024-06-11T17:59:56Z","title":"An Image is Worth 32 Tokens for Reconstruction and Generation","summary":"  Recent advancements in generative models have highlighted the crucial role of\nimage tokenization in the efficient synthesis of high-resolution images.\nTokenization, which transforms images into latent representations, reduces\ncomputational demands compared to directly processing pixels and enhances the\neffectiveness and efficiency of the generation process. Prior methods, such as\nVQGAN, typically utilize 2D latent grids with fixed downsampling factors.\nHowever, these 2D tokenizations face challenges in managing the inherent\nredundancies present in images, where adjacent regions frequently display\nsimilarities. To overcome this issue, we introduce Transformer-based\n1-Dimensional Tokenizer (TiTok), an innovative approach that tokenizes images\ninto 1D latent sequences. TiTok provides a more compact latent representation,\nyielding substantially more efficient and effective representations than\nconventional techniques. For example, a 256 x 256 x 3 image can be reduced to\njust 32 discrete tokens, a significant reduction from the 256 or 1024 tokens\nobtained by prior methods. Despite its compact nature, TiTok achieves\ncompetitive performance to state-of-the-art approaches. Specifically, using the\nsame generator framework, TiTok attains 1.97 gFID, outperforming MaskGIT\nbaseline significantly by 4.21 at ImageNet 256 x 256 benchmark. The advantages\nof TiTok become even more significant when it comes to higher resolution. At\nImageNet 512 x 512 benchmark, TiTok not only outperforms state-of-the-art\ndiffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image\ntokens by 64x, leading to 410x faster generation process. Our best-performing\nvariant can significantly surpasses DiT-XL/2 (gFID 2.13 vs. 3.04) while still\ngenerating high-quality samples 74x faster.\n","authors":["Qihang Yu","Mark Weber","Xueqing Deng","Xiaohui Shen","Daniel Cremers","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2406.07550v1.pdf","comment":"A compact 1D Image Tokenization method, leading to SOTA generation\n  performance while being substantially faster. Project page at\n  https://yucornetto.github.io/projects/titok.html"},{"id":"http://arxiv.org/abs/2406.07551v1","updated":"2024-06-11T17:59:56Z","published":"2024-06-11T17:59:56Z","title":"Blur-aware Spatio-temporal Sparse Transformer for Video Deblurring","summary":"  Video deblurring relies on leveraging information from other frames in the\nvideo sequence to restore the blurred regions in the current frame. Mainstream\napproaches employ bidirectional feature propagation, spatio-temporal\ntransformers, or a combination of both to extract information from the video\nsequence. However, limitations in memory and computational resources\nconstraints the temporal window length of the spatio-temporal transformer,\npreventing the extraction of longer temporal contextual information from the\nvideo sequence. Additionally, bidirectional feature propagation is highly\nsensitive to inaccurate optical flow in blurry frames, leading to error\naccumulation during the propagation process. To address these issues, we\npropose \\textbf{BSSTNet}, \\textbf{B}lur-aware \\textbf{S}patio-temporal\n\\textbf{S}parse \\textbf{T}ransformer Network. It introduces the blur map, which\nconverts the originally dense attention into a sparse form, enabling a more\nextensive utilization of information throughout the entire video sequence.\nSpecifically, BSSTNet (1) uses a longer temporal window in the transformer,\nleveraging information from more distant frames to restore the blurry pixels in\nthe current frame. (2) introduces bidirectional feature propagation guided by\nblur maps, which reduces error accumulation caused by the blur frame. The\nexperimental results demonstrate the proposed BSSTNet outperforms the\nstate-of-the-art methods on the GoPro and DVD datasets.\n","authors":["Huicong Zhang","Haozhe Xie","Hongxun Yao"],"pdf_url":"https://arxiv.org/pdf/2406.07551v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.07548v1","updated":"2024-06-11T17:59:53Z","published":"2024-06-11T17:59:53Z","title":"Image and Video Tokenization with Binary Spherical Quantization","summary":"  We propose a new transformer-based image and video tokenizer with Binary\nSpherical Quantization (BSQ). BSQ projects the high-dimensional visual\nembedding to a lower-dimensional hypersphere and then applies binary\nquantization. BSQ is (1) parameter-efficient without an explicit codebook, (2)\nscalable to arbitrary token dimensions, and (3) compact: compressing visual\ndata by up to 100$\\times$ with minimal distortion. Our tokenizer uses a\ntransformer encoder and decoder with simple block-wise causal masking to\nsupport variable-length videos as input. The resulting BSQ-ViT achieves\nstate-of-the-art visual reconstruction quality on image and video\nreconstruction benchmarks with 2.4$\\times$ throughput compared to the best\nprior methods. Furthermore, by learning an autoregressive prior for adaptive\narithmetic coding, BSQ-ViT achieves comparable results on video compression\nwith state-of-the-art video compression standards. BSQ-ViT also enables masked\nlanguage models to achieve competitive image synthesis quality to GAN- and\ndiffusion-based methods.\n","authors":["Yue Zhao","Yuanjun Xiong","Philipp Krähenbühl"],"pdf_url":"https://arxiv.org/pdf/2406.07548v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2406.07547v1","updated":"2024-06-11T17:59:51Z","published":"2024-06-11T17:59:51Z","title":"Zero-shot Image Editing with Reference Imitation","summary":"  Image editing serves as a practical yet challenging task considering the\ndiverse demands from users, where one of the hardest parts is to precisely\ndescribe how the edited image should look like. In this work, we present a new\nform of editing, termed imitative editing, to help users exercise their\ncreativity more conveniently. Concretely, to edit an image region of interest,\nusers are free to directly draw inspiration from some in-the-wild references\n(e.g., some relative pictures come across online), without having to cope with\nthe fit between the reference and the source. Such a design requires the system\nto automatically figure out what to expect from the reference to perform the\nediting. For this purpose, we propose a generative training framework, dubbed\nMimicBrush, which randomly selects two frames from a video clip, masks some\nregions of one frame, and learns to recover the masked regions using the\ninformation from the other frame. That way, our model, developed from a\ndiffusion prior, is able to capture the semantic correspondence between\nseparate images in a self-supervised manner. We experimentally show the\neffectiveness of our method under various test cases as well as its superiority\nover existing alternatives. We also construct a benchmark to facilitate further\nresearch.\n","authors":["Xi Chen","Yutong Feng","Mengting Chen","Yiyang Wang","Shilong Zhang","Yu Liu","Yujun Shen","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.07547v1.pdf","comment":"https://xavierchen34.github.io/MimicBrush-Page"},{"id":"http://arxiv.org/abs/2406.07546v1","updated":"2024-06-11T17:59:48Z","published":"2024-06-11T17:59:48Z","title":"Commonsense-T2I Challenge: Can Text-to-Image Generation Models\n  Understand Commonsense?","summary":"  We present a novel task and benchmark for evaluating the ability of\ntext-to-image(T2I) generation models to produce images that fit commonsense in\nreal life, which we call Commonsense-T2I. Given two adversarial text prompts\ncontaining an identical set of action words with minor differences, such as \"a\nlightbulb without electricity\" v.s. \"a lightbulb with electricity\", we evaluate\nwhether T2I models can conduct visual-commonsense reasoning, e.g. produce\nimages that fit \"the lightbulb is unlit\" vs. \"the lightbulb is lit\"\ncorrespondingly. Commonsense-T2I presents an adversarial challenge, providing\npairwise text prompts along with expected outputs. The dataset is carefully\nhand-curated by experts and annotated with fine-grained labels, such as\ncommonsense type and likelihood of the expected outputs, to assist analyzing\nmodel behavior. We benchmark a variety of state-of-the-art (sota) T2I models\nand surprisingly find that, there is still a large gap between image synthesis\nand real life photos--even the DALL-E 3 model could only achieve 48.92% on\nCommonsense-T2I, and the stable diffusion XL model only achieves 24.92%\naccuracy. Our experiments show that GPT-enriched prompts cannot solve this\nchallenge, and we include a detailed analysis about possible reasons for such\ndeficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation\nbenchmark for T2I commonsense checking, fostering advancements in real life\nimage generation.\n","authors":["Xingyu Fu","Muyu He","Yujie Lu","William Yang Wang","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.07546v1.pdf","comment":"Text-to-Image Generation, Commonsense, Project Url:\n  https://zeyofu.github.io/CommonsenseT2I/"},{"id":"http://arxiv.org/abs/2406.07544v1","updated":"2024-06-11T17:59:45Z","published":"2024-06-11T17:59:45Z","title":"Situational Awareness Matters in 3D Vision Language Reasoning","summary":"  Being able to carry out complicated vision language reasoning tasks in 3D\nspace represents a significant milestone in developing household robots and\nhuman-centered embodied AI. In this work, we demonstrate that a critical and\ndistinct challenge in 3D vision language reasoning is situational awareness,\nwhich incorporates two key components: (1) The autonomous agent grounds its\nself-location based on a language prompt. (2) The agent answers open-ended\nquestions from the perspective of its calculated position. To address this\nchallenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D\nvision language reasoning. We tokenize the 3D scene into sparse voxel\nrepresentation and propose a language-grounded situation estimator, followed by\na situated question answering module. Experiments on the SQA3D and ScanQA\ndatasets show that SIG3D outperforms state-of-the-art models in situation\nestimation and question answering by a large margin (e.g., an enhancement of\nover 30% on situation estimation accuracy). Subsequent analysis corroborates\nour architectural design choices, explores the distinct functions of visual and\ntextual tokens, and highlights the importance of situational awareness in the\ndomain of 3D question answering.\n","authors":["Yunze Man","Liang-Yan Gui","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07544v1.pdf","comment":"CVPR 2024. Project Page: https://yunzeman.github.io/situation3d"},{"id":"http://arxiv.org/abs/2406.07543v1","updated":"2024-06-11T17:59:35Z","published":"2024-06-11T17:59:35Z","title":"Vision Model Pre-training on Interleaved Image-Text Data via Latent\n  Compression Learning","summary":"  Recently, vision model pre-training has evolved from relying on manually\nannotated datasets to leveraging large-scale, web-crawled image-text data.\nDespite these advances, there is no pre-training method that effectively\nexploits the interleaved image-text data, which is very prevalent on the\nInternet. Inspired by the recent success of compression learning in natural\nlanguage processing, we propose a novel vision model pre-training method called\nLatent Compression Learning (LCL) for interleaved image-text data. This method\nperforms latent compression learning by maximizing the mutual information\nbetween the inputs and outputs of a causal attention model. The training\nobjective can be decomposed into two basic tasks: 1) contrastive learning\nbetween visual representation and preceding context, and 2) generating\nsubsequent text based on visual representation. Our experiments demonstrate\nthat our method not only matches the performance of CLIP on paired pre-training\ndatasets (e.g., LAION), but can also leverage interleaved pre-training data\n(e.g., MMC4) to learn robust visual representation from scratch, showcasing the\npotential of vision model pre-training with interleaved image-text data. Code\nis released at https://github.com/OpenGVLab/LCL.\n","authors":["Chenyu Yang","Xizhou Zhu","Jinguo Zhu","Weijie Su","Junjie Wang","Xuan Dong","Wenhai Wang","Lewei Lu","Bin Li","Jie Zhou","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2406.07543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07540v1","updated":"2024-06-11T17:59:01Z","published":"2024-06-11T17:59:01Z","title":"Ctrl-X: Controlling Structure and Appearance for Text-To-Image\n  Generation Without Guidance","summary":"  Recent controllable generation approaches such as FreeControl and Diffusion\nSelf-guidance bring fine-grained spatial and appearance control to\ntext-to-image (T2I) diffusion models without training auxiliary modules.\nHowever, these methods optimize the latent embedding for each type of score\nfunction with longer diffusion steps, making the generation process\ntime-consuming and limiting their flexibility and use. This work presents\nCtrl-X, a simple framework for T2I diffusion controlling structure and\nappearance without additional training or guidance. Ctrl-X designs feed-forward\nstructure control to enable the structure alignment with a structure image and\nsemantic-aware appearance transfer to facilitate the appearance transfer from a\nuser-input image. Extensive qualitative and quantitative experiments illustrate\nthe superior performance of Ctrl-X on various condition inputs and model\ncheckpoints. In particular, Ctrl-X supports novel structure and appearance\ncontrol with arbitrary condition images of any modality, exhibits superior\nimage quality and appearance transfer compared to existing works, and provides\ninstant plug-and-play functionality to any T2I and text-to-video (T2V)\ndiffusion model. See our project page for an overview of the results:\nhttps://genforce.github.io/ctrl-x\n","authors":["Kuan Heng Lin","Sicheng Mo","Ben Klingher","Fangzhou Mu","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.07540v1.pdf","comment":"18 pages, 11 figures, see project page at\n  https://genforce.github.io/ctrl-x"},{"id":"http://arxiv.org/abs/2406.07537v1","updated":"2024-06-11T17:58:34Z","published":"2024-06-11T17:58:34Z","title":"Autoregressive Pretraining with Mamba in Vision","summary":"  The vision community has started to build with the recently developed state\nspace model, Mamba, as the new backbone for a range of tasks. This paper shows\nthat Mamba's visual capability can be significantly enhanced through\nautoregressive pretraining, a direction not previously explored.\nEfficiency-wise, the autoregressive nature can well capitalize on the Mamba's\nunidirectional recurrent structure, enabling faster overall training speed\ncompared to other training strategies like mask modeling. Performance-wise,\nautoregressive pretraining equips the Mamba architecture with markedly higher\naccuracy over its supervised-trained counterparts and, more importantly,\nsuccessfully unlocks its scaling potential to large and even huge model sizes.\nFor example, with autoregressive pretraining, a base-size Mamba attains 83.2\\%\nImageNet accuracy, outperforming its supervised counterpart by 2.0\\%; our\nhuge-size Mamba, the largest Vision Mamba to date, attains 85.0\\% ImageNet\naccuracy (85.5\\% when finetuned with $384\\times384$ inputs), notably surpassing\nall other Mamba variants in vision. The code is available at\n\\url{https://github.com/OliverRensu/ARM}.\n","authors":["Sucheng Ren","Xianhang Li","Haoqin Tu","Feng Wang","Fangxun Shu","Lei Zhang","Jieru Mei","Linjie Yang","Peng Wang","Heng Wang","Alan Yuille","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2406.07537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07536v1","updated":"2024-06-11T17:57:49Z","published":"2024-06-11T17:57:49Z","title":"Towards Fundamentally Scalable Model Selection: Asymptotically Fast\n  Update and Selection","summary":"  The advancement of deep learning technologies is bringing new models every\nday, motivating the study of scalable model selection. An ideal model selection\nscheme should minimally support two operations efficiently over a large pool of\ncandidate models: update, which involves either adding a new candidate model or\nremoving an existing candidate model, and selection, which involves locating\nhighly performing models for a given task. However, previous solutions to model\nselection require high computational complexity for at least one of these two\noperations. In this work, we target fundamentally (more) scalable model\nselection that supports asymptotically fast update and asymptotically fast\nselection at the same time. Firstly, we define isolated model embedding, a\nfamily of model selection schemes supporting asymptotically fast update and\nselection: With respect to the number of candidate models $m$, the update\ncomplexity is O(1) and the selection consists of a single sweep over $m$\nvectors in addition to O(1) model operations. Isolated model embedding also\nimplies several desirable properties for applications. Secondly, we present\nStandardized Embedder, an empirical realization of isolated model embedding. We\nassess its effectiveness by using it to select representations from a pool of\n100 pre-trained vision models for classification tasks and measuring the\nperformance gaps between the selected models and the best candidates with a\nlinear probing protocol. Experiments suggest our realization is effective in\nselecting models with competitive performances and highlight isolated model\nembedding as a promising direction towards model selection that is\nfundamentally (more) scalable.\n","authors":["Wenxiao Wang","Weiming Zhuang","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2406.07536v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.07532v1","updated":"2024-06-11T17:56:14Z","published":"2024-06-11T17:56:14Z","title":"Hearing Anything Anywhere","summary":"  Recent years have seen immense progress in 3D computer vision and computer\ngraphics, with emerging tools that can virtualize real-world 3D environments\nfor numerous Mixed Reality (XR) applications. However, alongside immersive\nvisual experiences, immersive auditory experiences are equally vital to our\nholistic perception of an environment. In this paper, we aim to reconstruct the\nspatial acoustic characteristics of an arbitrary environment given only a\nsparse set of (roughly 12) room impulse response (RIR) recordings and a planar\nreconstruction of the scene, a setup that is easily achievable by ordinary\nusers. To this end, we introduce DiffRIR, a differentiable RIR rendering\nframework with interpretable parametric models of salient acoustic features of\nthe scene, including sound source directivity and surface reflectivity. This\nallows us to synthesize novel auditory experiences through the space with any\nsource audio. To evaluate our method, we collect a dataset of RIR recordings\nand music in four diverse, real environments. We show that our model\noutperforms state-ofthe-art baselines on rendering monaural and binaural RIRs\nand music at unseen locations, and learns physically interpretable parameters\ncharacterizing acoustic properties of the sound source and surfaces in the\nscene.\n","authors":["Mason Wang","Ryosuke Sawata","Samuel Clarke","Ruohan Gao","Shangzhe Wu","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2406.07532v1.pdf","comment":"CVPR 2024. The first two authors contributed equally. Project page:\n  https://masonlwang.com/hearinganythinganywhere/"},{"id":"http://arxiv.org/abs/2406.07520v1","updated":"2024-06-11T17:50:15Z","published":"2024-06-11T17:50:15Z","title":"Neural Gaffer: Relighting Any Object via Diffusion","summary":"  Single-image relighting is a challenging task that involves reasoning about\nthe complex interplay between geometry, materials, and lighting. Many prior\nmethods either support only specific categories of images, such as portraits,\nor require special capture conditions, like using a flashlight. Alternatively,\nsome methods explicitly decompose a scene into intrinsic components, such as\nnormals and BRDFs, which can be inaccurate or under-expressive. In this work,\nwe propose a novel end-to-end 2D relighting diffusion model, called Neural\nGaffer, that takes a single image of any object and can synthesize an accurate,\nhigh-quality relit image under any novel environmental lighting condition,\nsimply by conditioning an image generator on a target environment map, without\nan explicit scene decomposition. Our method builds on a pre-trained diffusion\nmodel, and fine-tunes it on a synthetic relighting dataset, revealing and\nharnessing the inherent understanding of lighting present in the diffusion\nmodel. We evaluate our model on both synthetic and in-the-wild Internet imagery\nand demonstrate its advantages in terms of generalization and accuracy.\nMoreover, by combining with other generative methods, our model enables many\ndownstream 2D tasks, such as text-based relighting and object insertion. Our\nmodel can also operate as a strong relighting prior for 3D tasks, such as\nrelighting a radiance field.\n","authors":["Haian Jin","Yuan Li","Fujun Luan","Yuanbo Xiangli","Sai Bi","Kai Zhang","Zexiang Xu","Jin Sun","Noah Snavely"],"pdf_url":"https://arxiv.org/pdf/2406.07520v1.pdf","comment":"Project Website: https://neural-gaffer.github.io"},{"id":"http://arxiv.org/abs/2406.07516v1","updated":"2024-06-11T17:47:27Z","published":"2024-06-11T17:47:27Z","title":"Instant 3D Human Avatar Generation using Image Diffusion Models","summary":"  We present AvatarPopUp, a method for fast, high quality 3D human avatar\ngeneration from different input modalities, such as images and text prompts and\nwith control over the generated pose and shape. The common theme is the use of\ndiffusion-based image generation networks that are specialized for each\nparticular task, followed by a 3D lifting network. We purposefully decouple the\ngeneration from the 3D modeling which allow us to leverage powerful image\nsynthesis priors, trained on billions of text-image pairs. We fine-tune latent\ndiffusion networks with additional image conditioning to solve tasks such as\nimage generation and back-view prediction, and to support qualitatively\ndifferent multiple 3D hypotheses. Our partial fine-tuning approach allows to\nadapt the networks for each task without inducing catastrophic forgetting. In\nour experiments, we demonstrate that our method produces accurate, high-quality\n3D avatars with diverse appearance that respect the multimodal text, image, and\nbody control signals. Our approach can produce a 3D model in as few as 2\nseconds, a four orders of magnitude speedup w.r.t. the vast majority of\nexisting methods, most of which solve only a subset of our tasks, and with\nfewer controls, thus enabling applications that require the controlled 3D\ngeneration of human avatars at scale. The project website can be found at\nhttps://www.nikoskolot.com/avatarpopup/.\n","authors":["Nikos Kolotouros","Thiemo Alldieck","Enric Corona","Eduard Gabriel Bazavan","Cristian Sminchisescu"],"pdf_url":"https://arxiv.org/pdf/2406.07516v1.pdf","comment":"Project page: https://www.nikoskolot.com/avatarpopup/"},{"id":"http://arxiv.org/abs/2401.04079v4","updated":"2024-06-11T17:46:38Z","published":"2024-01-08T18:31:38Z","title":"RudolfV: A Foundation Model by Pathologists for Pathologists","summary":"  Artificial intelligence has started to transform histopathology impacting\nclinical diagnostics and biomedical research. However, while many computational\npathology approaches have been proposed, most current AI models are limited\nwith respect to generalization, application variety, and handling rare\ndiseases. Recent efforts introduced self-supervised foundation models to\naddress these challenges, yet existing approaches do not leverage pathologist\nknowledge by design. In this study, we present a novel approach to designing\nfoundation models for computational pathology, incorporating pathologist\nexpertise, semi-automated data curation, and a diverse dataset from over 15\nlaboratories, including 58 tissue types, and encompassing 129 different\nhistochemical and immunohistochemical staining modalities. We demonstrate that\nour model \"RudolfV\" surpasses existing state-of-the-art foundation models\nacross different benchmarks focused on tumor microenvironment profiling,\nbiomarker evaluation, and reference case search while exhibiting favorable\nrobustness properties. Our study shows how domain-specific knowledge can\nincrease the efficiency and performance of pathology foundation models and\nenable novel application areas.\n","authors":["Jonas Dippel","Barbara Feulner","Tobias Winterhoff","Timo Milbich","Stephan Tietz","Simon Schallenberg","Gabriel Dernbach","Andreas Kunft","Simon Heinke","Marie-Lisa Eich","Julika Ribbat-Idel","Rosemarie Krupar","Philipp Anders","Niklas Prenißl","Philipp Jurmeister","David Horst","Lukas Ruff","Klaus-Robert Müller","Frederick Klauschen","Maximilian Alber"],"pdf_url":"https://arxiv.org/pdf/2401.04079v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07506v1","updated":"2024-06-11T17:40:31Z","published":"2024-06-11T17:40:31Z","title":"Understanding Visual Concepts Across Models","summary":"  Large multimodal models such as Stable Diffusion can generate, detect, and\nclassify new visual concepts after fine-tuning just a single word embedding. Do\nmodels learn similar words for the same concepts (i.e. <orange-cat> = orange +\ncat)? We conduct a large-scale analysis on three state-of-the-art models in\ntext-to-image generation, open-set object detection, and zero-shot\nclassification, and find that new word embeddings are model-specific and\nnon-transferable. Across 4,800 new embeddings trained for 40 diverse visual\nconcepts on four standard datasets, we find perturbations within an\n$\\epsilon$-ball to any prior embedding that generate, detect, and classify an\narbitrary concept. When these new embeddings are spliced into new models,\nfine-tuning that targets the original model is lost. We show popular soft\nprompt-tuning approaches find these perturbative solutions when applied to\nvisual concept learning tasks, and embeddings for visual concepts are not\ntransferable. Code for reproducing our work is available at:\nhttps://visual-words.github.io.\n","authors":["Brandon Trabucco","Max Gurinas","Kyle Doherty","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2406.07506v1.pdf","comment":"Official code at: https://github.com/visual-words/visual-words"},{"id":"http://arxiv.org/abs/2406.07502v1","updated":"2024-06-11T17:37:45Z","published":"2024-06-11T17:37:45Z","title":"Image Textualization: An Automatic Framework for Creating Accurate and\n  Detailed Image Descriptions","summary":"  Image description datasets play a crucial role in the advancement of various\napplications such as image understanding, text-to-image generation, and\ntext-image retrieval. Currently, image description datasets primarily originate\nfrom two sources. One source is the scraping of image-text pairs from the web.\nDespite their abundance, these descriptions are often of low quality and noisy.\nAnother is through human labeling. Datasets such as COCO are generally very\nshort and lack details. Although detailed image descriptions can be annotated\nby humans, the high annotation cost limits the feasibility. These limitations\nunderscore the need for more efficient and scalable methods to generate\naccurate and detailed image descriptions. In this paper, we propose an\ninnovative framework termed Image Textualization (IT), which automatically\nproduces high-quality image descriptions by leveraging existing multi-modal\nlarge language models (MLLMs) and multiple vision expert models in a\ncollaborative manner, which maximally convert the visual information into text.\nTo address the current lack of benchmarks for detailed descriptions, we propose\nseveral benchmarks for comprehensive evaluation, which verifies the quality of\nimage descriptions created by our framework. Furthermore, we show that\nLLaVA-7B, benefiting from training on IT-curated descriptions, acquire improved\ncapability to generate richer image descriptions, substantially increasing the\nlength and detail of their output with less hallucination.\n","authors":["Renjie Pi","Jianshu Zhang","Jipeng Zhang","Rui Pan","Zhekai Chen","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07500v1","updated":"2024-06-11T17:35:39Z","published":"2024-06-11T17:35:39Z","title":"SPIN: Spacecraft Imagery for Navigation","summary":"  Data acquired in space operational conditions is scarce due to the costs and\ncomplexity of space operations. This poses a challenge to learning-based\nvisual-based navigation algorithms employed in autonomous spacecraft\nnavigation. Existing datasets, which largely depend on computer-simulated data,\nhave partially filled this gap. However, the image generation tools they use\nare proprietary, which limits the evaluation of methods to unseen scenarios.\nFurthermore, these datasets provide limited ground-truth data, primarily\nfocusing on the spacecraft's translation and rotation relative to the camera.\nTo address these limitations, we present SPIN (SPacecraft Imagery for\nNavigation), an open-source realistic spacecraft image generation tool for\nrelative navigation between two spacecrafts. SPIN provides a wide variety of\nground-truth data and allows researchers to employ custom 3D models of\nsatellites, define specific camera-relative poses, and adjust various settings\nsuch as camera parameters and environmental illumination conditions. For the\ntask of spacecraft pose estimation, we compare the results of training with a\nSPIN-generated dataset against existing synthetic datasets. We show a %50\naverage error reduction in common testbed data (that simulates realistic space\nconditions). Both the SPIN tool (and source code) and our enhanced version of\nthe synthetic datasets will be publicly released upon paper acceptance on\nGitHub https://github.com/vpulab/SPIN.\n","authors":["Javier Montalvo","Juan Ignacio Bravo Pérez-Villar","Álvaro García-Martín","Pablo Carballeira","Jesús Besc'os"],"pdf_url":"https://arxiv.org/pdf/2406.07500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07499v1","updated":"2024-06-11T17:34:46Z","published":"2024-06-11T17:34:46Z","title":"Trim 3D Gaussian Splatting for Accurate Geometry Representation","summary":"  In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to\nreconstruct accurate 3D geometry from images. Previous arts for geometry\nreconstruction from 3D Gaussians mainly focus on exploring strong geometry\nregularization. Instead, from a fresh perspective, we propose to obtain\naccurate 3D geometry of a scene by Gaussian trimming, which selectively removes\nthe inaccurate geometry while preserving accurate structures. To achieve this,\nwe analyze the contributions of individual 3D Gaussians and propose a\ncontribution-based trimming strategy to remove the redundant or inaccurate\nGaussians. Furthermore, our experimental and theoretical analyses reveal that a\nrelatively small Gaussian scale is a non-negligible factor in representing and\noptimizing the intricate details. Therefore the proposed TrimGS maintains\nrelatively small Gaussian scales. In addition, TrimGS is also compatible with\nthe effective geometry regularization strategies in previous arts. When\ncombined with the original 3DGS and the state-of-the-art 2DGS, TrimGS\nconsistently yields more accurate geometry and higher perceptual quality. Our\nproject page is https://trimgs.github.io\n","authors":["Lue Fan","Yuxue Yang","Minxing Li","Hongsheng Li","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07499v1.pdf","comment":"Project page: https://trimgs.github.io/"},{"id":"http://arxiv.org/abs/2406.07488v1","updated":"2024-06-11T17:28:09Z","published":"2024-06-11T17:28:09Z","title":"ReduceFormer: Attention with Tensor Reduction by Summation","summary":"  Transformers have excelled in many tasks including vision. However, efficient\ndeployment of transformer models in low-latency or high-throughput applications\nis hindered by the computation in the attention mechanism which involves\nexpensive operations such as matrix multiplication and Softmax. To address\nthis, we introduce ReduceFormer, a family of models optimized for efficiency\nwith the spirit of attention. ReduceFormer leverages only simple operations\nsuch as reduction and element-wise multiplication, leading to greatly\nsimplified architecture and improved inference performance, with up to 37%\nreduction in latency and 44% improvement in throughput, while maintaining\ncompetitive accuracy comparable to other recent methods. The proposed model\nfamily is suitable for edge devices where compute resource and memory bandwidth\nare limited, as well as for cloud computing where high throughput is sought\nafter.\n","authors":["John Yang","Le An","Su Inn Park"],"pdf_url":"https://arxiv.org/pdf/2406.07488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07487v1","updated":"2024-06-11T17:27:23Z","published":"2024-06-11T17:27:23Z","title":"GLAD: Towards Better Reconstruction with Global and Local Adaptive\n  Diffusion Models for Unsupervised Anomaly Detection","summary":"  Diffusion models have shown superior performance on unsupervised anomaly\ndetection tasks. Since trained with normal data only, diffusion models tend to\nreconstruct normal counterparts of test images with certain noises added.\nHowever, these methods treat all potential anomalies equally, which may cause\ntwo main problems. From the global perspective, the difficulty of\nreconstructing images with different anomalies is uneven. Therefore, instead of\nutilizing the same setting for all samples, we propose to predict a particular\ndenoising step for each sample by evaluating the difference between image\ncontents and the priors extracted from diffusion models. From the local\nperspective, reconstructing abnormal regions differs from normal areas even in\nthe same image. Theoretically, the diffusion model predicts a noise for each\nstep, typically following a standard Gaussian distribution. However, due to the\ndifference between the anomaly and its potential normal counterpart, the\npredicted noise in abnormal regions will inevitably deviate from the standard\nGaussian distribution. To this end, we propose introducing synthetic abnormal\nsamples in training to encourage the diffusion models to break through the\nlimitation of standard Gaussian distribution, and a spatial-adaptive feature\nfusion scheme is utilized during inference. With the above modifications, we\npropose a global and local adaptive diffusion model (abbreviated to GLAD) for\nunsupervised anomaly detection, which introduces appealing flexibility and\nachieves anomaly-free reconstruction while retaining as much normal information\nas possible. Extensive experiments are conducted on three commonly used anomaly\ndetection datasets (MVTec-AD, MPDD, and VisA) and a printed circuit board\ndataset (PCB-Bank) we integrated, showing the effectiveness of the proposed\nmethod.\n","authors":["Hang Yao","Ming Liu","Haolin Wang","Zhicun Yin","Zifei Yan","Xiaopeng Hong","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2406.07487v1.pdf","comment":"Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file"},{"id":"http://arxiv.org/abs/2403.01444v4","updated":"2024-06-11T17:26:34Z","published":"2024-03-03T08:42:40Z","title":"3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming\n  of Photo-Realistic Free-Viewpoint Videos","summary":"  Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes\nfrom multi-view videos remains a challenging endeavor. Despite the remarkable\nadvancements achieved by current neural rendering techniques, these methods\ngenerally require complete video sequences for offline training and are not\ncapable of real-time rendering. To address these constraints, we introduce\n3DGStream, a method designed for efficient FVV streaming of real-world dynamic\nscenes. Our method achieves fast on-the-fly per-frame reconstruction within 12\nseconds and real-time rendering at 200 FPS. Specifically, we utilize 3D\nGaussians (3DGs) to represent the scene. Instead of the na\\\"ive approach of\ndirectly optimizing 3DGs per-frame, we employ a compact Neural Transformation\nCache (NTC) to model the translations and rotations of 3DGs, markedly reducing\nthe training time and storage required for each FVV frame. Furthermore, we\npropose an adaptive 3DG addition strategy to handle emerging objects in dynamic\nscenes. Experiments demonstrate that 3DGStream achieves competitive performance\nin terms of rendering speed, image quality, training time, and model storage\nwhen compared with state-of-the-art methods.\n","authors":["Jiakai Sun","Han Jiao","Guangyuan Li","Zhanjie Zhang","Lei Zhao","Wei Xing"],"pdf_url":"https://arxiv.org/pdf/2403.01444v4.pdf","comment":"CVPR 2024 Accepted (Highlight). Project Page:\n  https://sjojok.github.io/3dgstream"},{"id":"http://arxiv.org/abs/2406.07482v1","updated":"2024-06-11T17:25:46Z","published":"2024-06-11T17:25:46Z","title":"Comparing Deep Learning Models for Rice Mapping in Bhutan Using High\n  Resolution Satellite Imagery","summary":"  The Bhutanese government is increasing its utilization of technological\napproaches such as including Remote Sensing-based knowledge in their\ndecision-making process. This study focuses on crop type and crop extent in\nParo, one of the top rice-yielding districts in Bhutan, and employs publicly\navailable NICFI high-resolution satellite imagery from Planet. Two Deep\nLearning (DL) approaches, point-based (DNN) and patch-based (U-Net), models\nwere used in conjunction with cloud-computing platforms. Three different models\nper DL approaches (DNN and U-Net) were trained: 1) RGBN channels from Planet;\n2) RGBN and elevation data (RGBNE); 3) RGBN and Sentinel-1 (S1) data (RGBNS),\nand RGBN with E and S1 data (RGBNES). From this comprehensive analysis, the\nU-Net displayed higher performance metrics across both model training and model\nvalidation efforts. Among the U-Net model sets, the RGBN, RGBNE, RGBNS, and\nRGBNES models had an F1-score of 0.8546, 0.8563, 0.8467, and 0.8500\nrespectively. An independent model evaluation was performed and found a high\nlevel of performance variation across all the metrics. For this independent\nmodel evaluation, the U-Net RGBN, RGBNE, RGBNES, and RGBN models displayed the\nF1-scores of 0.5935, 0.6154, 0.5882, and 0.6582, suggesting U-Net RGBNES as the\nbest model. The study shows that the DL approaches can predict rice. Also, DL\nmethods can be used with the survey-based approaches currently utilized by the\nBhutan Department of Agriculture. Further, this study demonstrated the usage of\nregional land cover products such as SERVIR's RLCMS as a weak label approach to\ncapture different strata addressing the class imbalance problem and improving\nthe sampling design for DL application. Finally, through preliminary model\ntesting and comparisons outlined it was shown that using additional features\nsuch as NDVI, EVI, and NDWI did not drastically improve model performance.\n","authors":["Biplov Bhandari","Timothy Mayer"],"pdf_url":"https://arxiv.org/pdf/2406.07482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07480v1","updated":"2024-06-11T17:24:02Z","published":"2024-06-11T17:24:02Z","title":"Image Neural Field Diffusion Models","summary":"  Diffusion models have shown an impressive ability to model complex data\ndistributions, with several key advantages over GANs, such as stable training,\nbetter coverage of the training distribution's modes, and the ability to solve\ninverse problems without extra training. However, most diffusion models learn\nthe distribution of fixed-resolution images. We propose to learn the\ndistribution of continuous images by training diffusion models on image neural\nfields, which can be rendered at any resolution, and show its advantages over\nfixed-resolution models. To achieve this, a key challenge is to obtain a latent\nspace that represents photorealistic image neural fields. We propose a simple\nand effective method, inspired by several recent techniques but with key\nchanges to make the image neural fields photorealistic. Our method can be used\nto convert existing latent diffusion autoencoders into image neural field\nautoencoders. We show that image neural field diffusion models can be trained\nusing mixed-resolution image datasets, outperform fixed-resolution diffusion\nmodels followed by super-resolution models, and can solve inverse problems with\nconditions applied at different scales efficiently.\n","authors":["Yinbo Chen","Oliver Wang","Richard Zhang","Eli Shechtman","Xiaolong Wang","Michael Gharbi"],"pdf_url":"https://arxiv.org/pdf/2406.07480v1.pdf","comment":"Project page: https://yinboc.github.io/infd/"},{"id":"http://arxiv.org/abs/2406.07476v1","updated":"2024-06-11T17:22:23Z","published":"2024-06-11T17:22:23Z","title":"VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio\n  Understanding in Video-LLMs","summary":"  In this paper, we present the VideoLLaMA 2, a set of Video Large Language\nModels (Video-LLMs) designed to enhance spatial-temporal modeling and audio\nunderstanding in video and audio-oriented tasks. Building upon its predecessor,\nVideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC)\nconnector, which effectively captures the intricate spatial and temporal\ndynamics of video data. Additionally, we integrate an Audio Branch into the\nmodel through joint training, thereby enriching the multimodal understanding\ncapabilities of the model by seamlessly incorporating audio cues. Comprehensive\nevaluations on multiple-choice video question answering (MC-VQA), open-ended\nvideo question answering (OE-VQA), and video captioning (VC) tasks demonstrate\nthat VideoLLaMA 2 consistently achieves competitive results among open-source\nmodels and even gets close to some proprietary models on several benchmarks.\nFurthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and\naudio-video question-answering (AQA & OE-AVQA) benchmarks over existing models.\nThese advancements underline VideoLLaMA 2's superior performance in multimodal\ncomprehension, setting a new standard for intelligent video analysis systems.\nAll models are public to facilitate further research.\n","authors":["Zesen Cheng","Sicong Leng","Hang Zhang","Yifei Xin","Xin Li","Guanzheng Chen","Yongxin Zhu","Wenqi Zhang","Ziyang Luo","Deli Zhao","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2406.07476v1.pdf","comment":"ZC, SL, HZ, YX, and XL contributed equally to this project"},{"id":"http://arxiv.org/abs/2406.07472v1","updated":"2024-06-11T17:19:26Z","published":"2024-06-11T17:19:26Z","title":"4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion\n  Models","summary":"  Existing dynamic scene generation methods mostly rely on distilling knowledge\nfrom pre-trained 3D generative models, which are typically fine-tuned on\nsynthetic object datasets. As a result, the generated scenes are often\nobject-centric and lack photorealism. To address these limitations, we\nintroduce a novel pipeline designed for photorealistic text-to-4D scene\ngeneration, discarding the dependency on multi-view generative models and\ninstead fully utilizing video generative models trained on diverse real-world\ndatasets. Our method begins by generating a reference video using the video\ngeneration model. We then learn the canonical 3D representation of the video\nusing a freeze-time video, delicately generated from the reference video. To\nhandle inconsistencies in the freeze-time video, we jointly learn a per-frame\ndeformation to model these imperfections. We then learn the temporal\ndeformation based on the canonical representation to capture dynamic\ninteractions in the reference video. The pipeline facilitates the generation of\ndynamic scenes with enhanced photorealism and structural integrity, viewable\nfrom multiple perspectives, thereby setting a new standard in 4D scene\ngeneration.\n","authors":["Heng Yu","Chaoyang Wang","Peiye Zhuang","Willi Menapace","Aliaksandr Siarohin","Junli Cao","Laszlo A Jeni","Sergey Tulyakov","Hsin-Ying Lee"],"pdf_url":"https://arxiv.org/pdf/2406.07472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01915v2","updated":"2024-06-11T17:19:00Z","published":"2024-02-02T21:34:51Z","title":"Robust Inverse Graphics via Probabilistic Inference","summary":"  How do we infer a 3D scene from a single image in the presence of corruptions\nlike rain, snow or fog? Straightforward domain randomization relies on knowing\nthe family of corruptions ahead of time. Here, we propose a Bayesian\napproach-dubbed robust inverse graphics (RIG)-that relies on a strong scene\nprior and an uninformative uniform corruption prior, making it applicable to a\nwide range of corruptions. Given a single image, RIG performs posterior\ninference jointly over the scene and the corruption. We demonstrate this idea\nby training a neural radiance field (NeRF) scene prior and using a secondary\nNeRF to represent the corruptions over which we place an uninformative prior.\nRIG, trained only on clean data, outperforms depth estimators and alternative\nNeRF approaches that perform point estimation instead of full inference. The\nresults hold for a number of scene prior architectures based on normalizing\nflows and diffusion models. For the latter, we develop reconstruction-guidance\nwith auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is\napplicable in the presence of auxiliary latent variables such as the\ncorruption. RIG demonstrates how scene priors can be used beyond generation\ntasks.\n","authors":["Tuan Anh Le","Pavel Sountsov","Matthew D. Hoffman","Ben Lee","Brian Patton","Rif A. Saurous"],"pdf_url":"https://arxiv.org/pdf/2402.01915v2.pdf","comment":"ICML submission. Reworked main body, new appendix figures"},{"id":"http://arxiv.org/abs/2406.07471v1","updated":"2024-06-11T17:18:11Z","published":"2024-06-11T17:18:11Z","title":"OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow\n  Understanding","summary":"  Surgical scene perception via videos are critical for advancing robotic\nsurgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology.\nHowever, the scarcity of diverse and richly annotated video datasets has\nhindered the development of intelligent systems for surgical workflow analysis.\nExisting datasets for surgical workflow analysis, which typically face\nchallenges such as small scale, a lack of diversity in surgery and phase\ncategories, and the absence of time-localized annotations, limit the\nrequirements for action understanding and model generalization validation in\ncomplex and diverse real-world surgical scenarios. To address this gap, we\nintroduce OphNet, a large-scale, expert-annotated video benchmark for\nophthalmic surgical workflow understanding. OphNet features: 1) A diverse\ncollection of 2,278 surgical videos spanning 66 types of cataract, glaucoma,\nand corneal surgeries, with detailed annotations for 102 unique surgical phases\nand 150 granular operations; 2) It offers sequential and hierarchical\nannotations for each surgery, phase, and operation, enabling comprehensive\nunderstanding and improved interpretability; 3) Moreover, OphNet provides\ntime-localized annotations, facilitating temporal localization and prediction\ntasks within surgical workflows. With approximately 205 hours of surgical\nvideos, OphNet is about 20 times larger than the largest existing surgical\nworkflow analysis benchmark. Our dataset and code have been made available at:\n\\url{https://github.com/minghu0830/OphNet-benchmark}.\n","authors":["Ming Hu","Peng Xia","Lin Wang","Siyuan Yan","Feilong Tang","Zhongxing Xu","Yimin Luo","Kaimin Song","Jurgen Leitner","Xuelian Cheng","Jun Cheng","Chi Liu","Kaijing Zhou","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2406.07471v1.pdf","comment":"Version 1. arXiv admin note: text overlap with arXiv:2305.15701 by\n  other authors"},{"id":"http://arxiv.org/abs/2405.03650v2","updated":"2024-06-11T17:12:26Z","published":"2024-05-06T17:14:09Z","title":"Generated Contents Enrichment","summary":"  In this paper, we investigate a novel artificial intelligence generation\ntask, termed as generated contents enrichment (GCE). Different from\nconventional artificial intelligence contents generation task that enriches the\ngiven textual description implicitly with limited semantics for generating\nvisually real content, our proposed GCE strives to perform content enrichment\nexplicitly on both the visual and textual domain, from which the enriched\ncontents are visually real, structurally reasonable, and semantically abundant.\nTowards to solve GCE, we propose a deep end-to-end method that explicitly\nexplores the semantics and inter-semantic relationships during the enrichment.\nSpecifically, we first model the input description as a semantic graph, wherein\neach node represents an object and each edge corresponds to the inter-object\nrelationship. We then adopt Graph Convolutional Networks on top of the input\nscene description to predict the enriching objects and their relationships with\nthe input objects. Finally, the enriched description is fed into an image\nsynthesis model to carry out the visual contents generation. Our experiments\nconducted on the Visual Genome dataset exhibit promising and visually plausible\nresults.\n","authors":["Mahdi Naseri","Jiayan Qiu","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2405.03650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08960v2","updated":"2024-06-11T17:01:02Z","published":"2024-02-14T06:01:44Z","title":"Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision","summary":"  Current state-of-the-art open-vocabulary segmentation methods typically rely\non image-mask-text triplet annotations for supervision. However, acquiring such\ndetailed annotations is labour-intensive and poses scalability challenges in\ncomplex real-world scenarios. While existing weakly-supervised approaches\nleverage image-text pairs to reduce the expansive annotation cost, the lack of\nmask supervision makes it difficult for the model to locate multiple instances\nand accurately group pixels with similar semantics, significantly hampering\nversatility and performance. In this paper, we introduce Unpair-Seg, a novel\nweakly-supervised open-vocabulary segmentation framework that learns from\nunpaired image-mask and image-text pairs, which can be independently and\nefficiently collected. Unpair-Seg initially predicts a set of binary masks and\ngenerates pseudo labels by identifying confident pairs of masks and text\nentities. We then train a feature adapter to align region embeddings with text\nembeddings based on these pseudo labels, achieving open-vocabulary\nsegmentation. However, the inherent noise in the mask-entity correspondence\nposes a challenge to obtaining reliable pairs. To address this, we employ a\nvision-language large model to re-caption the input images and extract precise\nentities, and we design a multi-scale matching strategy to reduce noisy\nmask-entity pairs. Our Unpair-Seg framework demonstrates impressive\nperformance, achieving 14.6\\% and 19.5\\% mIoU on the ADE-847 and PASCAL\nContext-459 datasets, significantly narrowing the gap between fully-supervised\nand weakly-supervised methods.\n","authors":["Zhaoqing Wang","Xiaobo Xia","Ziye Chen","Xiao He","Yandong Guo","Mingming Gong","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.08960v2.pdf","comment":"27 pages, 18 figures, 10 tables"},{"id":"http://arxiv.org/abs/2406.07450v1","updated":"2024-06-11T16:55:38Z","published":"2024-06-11T16:55:38Z","title":"Benchmarking Vision-Language Contrastive Methods for Medical\n  Representation Learning","summary":"  We perform a comprehensive benchmarking of contrastive frameworks for\nlearning multimodal representations in the medical domain. Through this study,\nwe aim to answer the following research questions: (i) How transferable are\ngeneral-domain representations to the medical domain? (ii) Is multimodal\ncontrastive training sufficient, or does it benefit from unimodal training as\nwell? (iii) What is the impact of feature granularity on the effectiveness of\nmultimodal medical representation learning? To answer these questions, we\ninvestigate eight contrastive learning approaches under identical training\nsetups, and train them on 2.8 million image-text pairs from four datasets, and\nevaluate them on 25 downstream tasks, including classification (zero-shot and\nlinear probing), image-to-text and text-to-image retrieval, and visual\nquestion-answering. Our findings suggest a positive answer to the first\nquestion, a negative answer to the second question, and the benefit of learning\nfine-grained features. Finally, we make our code publicly available.\n","authors":["Shuvendu Roy","Yasaman Parhizkar","Franklin Ogidi","Vahid Reza Khazaie","Michael Colacci","Ali Etemad","Elham Dolatabadi","Arash Afkanpour"],"pdf_url":"https://arxiv.org/pdf/2406.07450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07435v1","updated":"2024-06-11T16:42:17Z","published":"2024-06-11T16:42:17Z","title":"Beware of Aliases -- Signal Preservation is Crucial for Robust Image\n  Restoration","summary":"  Image restoration networks are usually comprised of an encoder and a decoder,\nresponsible for aggregating image content from noisy, distorted data and to\nrestore clean, undistorted images, respectively. Data aggregation as well as\nhigh-resolution image generation both usually come at the risk of involving\naliases, i.e.~standard architectures put their ability to reconstruct the model\ninput in jeopardy to reach high PSNR values on validation data. The price to be\npaid is low model robustness. In this work, we show that simply providing\nalias-free paths in state-of-the-art reconstruction transformers supports\nimproved model robustness at low costs on the restoration performance. We do so\nby proposing BOA-Restormer, a transformer-based image restoration model that\nexecutes downsampling and upsampling operations partly in the frequency domain\nto ensure alias-free paths along the entire model while potentially preserving\nall relevant high-frequency information.\n","authors":["Shashank Agnihotri","Julia Grabinski","Janis Keuper","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2406.07435v1.pdf","comment":"Tags: Adversarial attack, image restoration, image deblurring,\n  frequency sampling"},{"id":"http://arxiv.org/abs/2406.07431v1","updated":"2024-06-11T16:34:16Z","published":"2024-06-11T16:34:16Z","title":"Active Scout: Multi-Target Tracking Using Neural Radiance Fields in\n  Dense Urban Environments","summary":"  We study pursuit-evasion games in highly occluded urban environments, e.g.\ntall buildings in a city, where a scout (quadrotor) tracks multiple dynamic\ntargets on the ground. We show that we can build a neural radiance field (NeRF)\nrepresentation of the city -- online -- using RGB and depth images from\ndifferent vantage points. This representation is used to calculate the\ninformation gain to both explore unknown parts of the city and track the\ntargets -- thereby giving a completely first-principles approach to actively\ntracking dynamic targets. We demonstrate, using a custom-built simulator using\nOpen Street Maps data of Philadelphia and New York City, that we can explore\nand locate 20 stationary targets within 300 steps. This is slower than a greedy\nbaseline which which does not use active perception. But for dynamic targets\nthat actively hide behind occlusions, we show that our approach maintains, at\nworst, a tracking error of 200m; the greedy baseline can have a tracking error\nas large as 600m. We observe a number of interesting properties in the scout's\npolicies, e.g., it switches its attention to track a different target\nperiodically, as the quality of the NeRF representation improves over time, the\nscout also becomes better in terms of target tracking.\n","authors":["Christopher D. Hsu","Pratik Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2406.07431v1.pdf","comment":"8 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2406.07426v1","updated":"2024-06-11T16:27:32Z","published":"2024-06-11T16:27:32Z","title":"DERM12345: A Large, Multisource Dermatoscopic Skin Lesion Dataset with\n  38 Subclasses","summary":"  Skin lesion datasets provide essential information for understanding various\nskin conditions and developing effective diagnostic tools. They aid the\nartificial intelligence-based early detection of skin cancer, facilitate\ntreatment planning, and contribute to medical education and research. Published\nlarge datasets have partially coverage the subclassifications of the skin\nlesions. This limitation highlights the need for more expansive and varied\ndatasets to reduce false predictions and help improve the failure analysis for\nskin lesions. This study presents a diverse dataset comprising 12,345\ndermatoscopic images with 38 subclasses of skin lesions collected in Turkiye\nwhich comprises different skin types in the transition zone between Europe and\nAsia. Each subgroup contains high-resolution photos and expert annotations,\nproviding a strong and reliable basis for future research. The detailed\nanalysis of each subgroup provided in this study facilitates targeted research\nendeavors and enhances the depth of understanding regarding the skin lesions.\nThis dataset distinguishes itself through a diverse structure with 5 super\nclasses, 15 main classes, 38 subclasses and its 12,345 high-resolution\ndermatoscopic images.\n","authors":["Abdurrahim Yilmaz","Sirin Pekcan Yasar","Gulsum Gencoglan","Burak Temelkuran"],"pdf_url":"https://arxiv.org/pdf/2406.07426v1.pdf","comment":"12 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2406.03877v2","updated":"2024-06-11T16:06:32Z","published":"2024-06-06T09:12:30Z","title":"Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop\n  End-To-End Autonomous Driving","summary":"  In an era marked by the rapid scaling of foundation models, autonomous\ndriving technologies are approaching a transformative threshold where\nend-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling\nup in the data-driven manner. However, existing E2E-AD methods are mostly\nevaluated under the open-loop log-replay manner with L2 errors and collision\nrate as metrics (e.g., in nuScenes), which could not fully reflect the driving\nperformance of algorithms as recently acknowledged in the community. For those\nE2E-AD methods evaluated under the closed-loop protocol, they are tested in\nfixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as\nmetrics, which is known for high variance due to the unsmoothed metric function\nand large randomness in the long route. Besides, these methods usually collect\ntheir own data for training, which makes algorithm-level fair comparison\ninfeasible.\n  To fulfill the paramount need of comprehensive, realistic, and fair testing\nenvironments for Full Self-Driving (FSD), we present Bench2Drive, the first\nbenchmark for evaluating E2E-AD systems' multiple abilities in a closed-loop\nmanner. Bench2Drive's official training data consists of 2 million fully\nannotated frames, collected from 10000 short clips uniformly distributed under\n44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny,\nfoggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2.\nIts evaluation protocol requires E2E-AD models to pass 44 interactive scenarios\nunder different locations and weathers which sums up to 220 routes and thus\nprovides a comprehensive and disentangled assessment about their driving\ncapability under different situations. We implement state-of-the-art E2E-AD\nmodels and evaluate them in Bench2Drive, providing insights regarding current\nstatus and future directions.\n","authors":["Xiaosong Jia","Zhenjie Yang","Qifeng Li","Zhiyuan Zhang","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2406.03877v2.pdf","comment":"Fix typos in text and Table 4. More reference"},{"id":"http://arxiv.org/abs/2406.07398v1","updated":"2024-06-11T16:05:15Z","published":"2024-06-11T16:05:15Z","title":"Visual Representation Learning with Stochastic Frame Prediction","summary":"  Self-supervised learning of image representations by predicting future frames\nis a promising direction but still remains a challenge. This is because of the\nunder-determined nature of frame prediction; multiple potential futures can\narise from a single current frame. To tackle this challenge, in this paper, we\nrevisit the idea of stochastic video generation that learns to capture\nuncertainty in frame prediction and explore its effectiveness for\nrepresentation learning. Specifically, we design a framework that trains a\nstochastic frame prediction model to learn temporal information between frames.\nMoreover, to learn dense information within each frame, we introduce an\nauxiliary masked image modeling objective along with a shared decoder\narchitecture. We find this architecture allows for combining both objectives in\na synergistic and compute-efficient manner. We demonstrate the effectiveness of\nour framework on a variety of tasks from video label propagation and\nvision-based robot learning domains, such as video segmentation, pose tracking,\nvision-based robotic locomotion, and manipulation tasks. Code is available on\nthe project webpage: https://sites.google.com/view/2024rsp.\n","authors":["Huiwon Jang","Dongyoung Kim","Junsu Kim","Jinwoo Shin","Pieter Abbeel","Younggyo Seo"],"pdf_url":"https://arxiv.org/pdf/2406.07398v1.pdf","comment":"International Conference on Machine Learning (ICML) 2024"},{"id":"http://arxiv.org/abs/2309.11267v2","updated":"2024-06-11T15:55:48Z","published":"2023-09-20T12:50:52Z","title":"From Classification to Segmentation with Explainable AI: A Study on\n  Crack Detection and Growth Monitoring","summary":"  Monitoring surface cracks in infrastructure is crucial for structural health\nmonitoring. Automatic visual inspection offers an effective solution,\nespecially in hard-to-reach areas. Machine learning approaches have proven\ntheir effectiveness but typically require large annotated datasets for\nsupervised training. Once a crack is detected, monitoring its severity often\ndemands precise segmentation of the damage. However, pixel-level annotation of\nimages for segmentation is labor-intensive. To mitigate this cost, one can\nleverage explainable artificial intelligence (XAI) to derive segmentations from\nthe explanations of a classifier, requiring only weak image-level supervision.\nThis paper proposes applying this methodology to segment and monitor surface\ncracks. We evaluate the performance of various XAI methods and examine how this\napproach facilitates severity quantification and growth monitoring. Results\nreveal that while the resulting segmentation masks may exhibit lower quality\nthan those produced by supervised methods, they remain meaningful and enable\nseverity monitoring, thus reducing substantial labeling costs.\n","authors":["Florent Forest","Hugo Porta","Devis Tuia","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2309.11267v2.pdf","comment":"49 pages. Accepted for publication in Automation in Construction"},{"id":"http://arxiv.org/abs/2405.15734v2","updated":"2024-06-11T15:42:29Z","published":"2024-05-24T17:25:00Z","title":"LM4LV: A Frozen Large Language Model for Low-level Vision Tasks","summary":"  The success of large language models (LLMs) has fostered a new research trend\nof multi-modality large language models (MLLMs), which changes the paradigm of\nvarious fields in computer vision. Though MLLMs have shown promising results in\nnumerous high-level vision and vision-language tasks such as VQA and\ntext-to-image, no works have demonstrated how low-level vision tasks can\nbenefit from MLLMs. We find that most current MLLMs are blind to low-level\nfeatures due to their design of vision modules, thus are inherently incapable\nfor solving low-level vision tasks. In this work, we purpose $\\textbf{LM4LV}$,\na framework that enables a FROZEN LLM to solve a range of low-level vision\ntasks without any multi-modal data or prior. This showcases the LLM's strong\npotential in low-level vision and bridges the gap between MLLMs and low-level\nvision tasks. We hope this work can inspire new perspectives on LLMs and deeper\nunderstanding of their mechanisms. Code is available at\nhttps://github.com/bytetriper/LM4LV.\n","authors":["Boyang Zheng","Jinjin Gu","Shijun Li","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2405.15734v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11961v2","updated":"2024-06-11T15:39:52Z","published":"2023-11-20T17:38:35Z","title":"NNG-Mix: Improving Semi-supervised Anomaly Detection with Pseudo-anomaly\n  Generation","summary":"  Anomaly detection (AD) is essential in identifying rare and often critical\nevents in complex systems, finding applications in fields such as network\nintrusion detection, financial fraud detection, and fault detection in\ninfrastructure and industrial systems. While AD is typically treated as an\nunsupervised learning task due to the high cost of label annotation, it is more\npractical to assume access to a small set of labeled anomaly samples from\ndomain experts, as is the case for semi-supervised anomaly detection.\nSemi-supervised and supervised approaches can leverage such labeled data,\nresulting in improved performance. In this paper, rather than proposing a new\nsemi-supervised or supervised approach for AD, we introduce a novel algorithm\nfor generating additional pseudo-anomalies on the basis of the limited labeled\nanomalies and a large volume of unlabeled data. This serves as an augmentation\nto facilitate the detection of new anomalies. Our proposed algorithm, named\nNearest Neighbor Gaussian Mixup (NNG-Mix), efficiently integrates information\nfrom both labeled and unlabeled data to generate pseudo-anomalies. We compare\nthe performance of this novel algorithm with commonly applied augmentation\ntechniques, such as Mixup and Cutout. We evaluate NNG-Mix by training various\nexisting semi-supervised and supervised anomaly detection algorithms on the\noriginal training data along with the generated pseudo-anomalies. Through\nextensive experiments on 57 benchmark datasets in ADBench, reflecting different\ndata types, we demonstrate that NNG-Mix outperforms other data augmentation\nmethods. It yields significant performance improvements compared to the\nbaselines trained exclusively on the original training data. Notably, NNG-Mix\nyields up to 16.4%, 8.8%, and 8.0% improvements on Classical, CV, and NLP\ndatasets in ADBench. Our source code is available at\nhttps://github.com/donghao51/NNG-Mix.\n","authors":["Hao Dong","Gaëtan Frusque","Yue Zhao","Eleni Chatzi","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2311.11961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07361v1","updated":"2024-06-11T15:28:48Z","published":"2024-06-11T15:28:48Z","title":"Deep Implicit Optimization for Robust and Flexible Image Registration","summary":"  Deep Learning in Image Registration (DLIR) methods have been tremendously\nsuccessful in image registration due to their speed and ability to incorporate\nweak label supervision at training time. However, DLIR methods forego many of\nthe benefits of classical optimization-based methods. The functional nature of\ndeep networks do not guarantee that the predicted transformation is a local\nminima of the registration objective, the representation of the transformation\n(displacement/velocity field/affine) is fixed, and the networks are not robust\nto domain shift. Our method aims to bridge this gap between classical and\nlearning methods by incorporating optimization as a layer in a deep network. A\ndeep network is trained to predict multi-scale dense feature images that are\nregistered using a black box iterative optimization solver. This optimal warp\nis then used to minimize image and label alignment errors. By implicitly\ndifferentiating end-to-end through an iterative optimization solver, our\nlearned features are registration and label-aware, and the warp functions are\nguaranteed to be local minima of the registration objective in the feature\nspace. Our framework shows excellent performance on in-domain datasets, and is\nagnostic to domain shift such as anisotropy and varying intensity profiles. For\nthe first time, our method allows switching between arbitrary transformation\nrepresentations (free-form to diffeomorphic) at test time with zero retraining.\nEnd-to-end feature learning also facilitates interpretability of features, and\nout-of-the-box promptability using additional label-fidelity terms at\ninference.\n","authors":["Rohit Jena","Pratik Chaudhari","James C. Gee"],"pdf_url":"https://arxiv.org/pdf/2406.07361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07353v1","updated":"2024-06-11T15:22:48Z","published":"2024-06-11T15:22:48Z","title":"Toxic Memes: A Survey of Computational Perspectives on the Detection and\n  Explanation of Meme Toxicities","summary":"  Internet memes, channels for humor, social commentary, and cultural\nexpression, are increasingly used to spread toxic messages. Studies on the\ncomputational analyses of toxic memes have significantly grown over the past\nfive years, and the only three surveys on computational toxic meme analysis\ncover only work published until 2022, leading to inconsistent terminology and\nunexplored trends. Our work fills this gap by surveying content-based\ncomputational perspectives on toxic memes, and reviewing key developments until\nearly 2024. Employing the PRISMA methodology, we systematically extend the\npreviously considered papers, achieving a threefold result. First, we survey\n119 new papers, analyzing 158 computational works focused on content-based\ntoxic meme analysis. We identify over 30 datasets used in toxic meme analysis\nand examine their labeling systems. Second, after observing the existence of\nunclear definitions of meme toxicity in computational works, we introduce a new\ntaxonomy for categorizing meme toxicity types. We also note an expansion in\ncomputational tasks beyond the simple binary classification of memes as toxic\nor non-toxic, indicating a shift towards achieving a nuanced comprehension of\ntoxicity. Third, we identify three content-based dimensions of meme toxicity\nunder automatic study: target, intent, and conveyance tactics. We develop a\nframework illustrating the relationships between these dimensions and meme\ntoxicities. The survey analyzes key challenges and recent trends, such as\nenhanced cross-modal reasoning, integrating expert and cultural knowledge, the\ndemand for automatic toxicity explanations, and handling meme toxicity in\nlow-resource languages. Also, it notes the rising use of Large Language Models\n(LLMs) and generative AI for detecting and generating toxic memes. Finally, it\nproposes pathways for advancing toxic meme detection and interpretation.\n","authors":["Delfina Sol Martinez Pandiani","Erik Tjong Kim Sang","Davide Ceolin"],"pdf_url":"https://arxiv.org/pdf/2406.07353v1.pdf","comment":"39 pages, 12 figures, 9 tables"},{"id":"http://arxiv.org/abs/2402.07207v2","updated":"2024-06-11T15:16:37Z","published":"2024-02-11T13:40:08Z","title":"GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided\n  Generative Gaussian Splatting","summary":"  We present GALA3D, generative 3D GAussians with LAyout-guided control, for\neffective compositional text-to-3D generation. We first utilize large language\nmodels (LLMs) to generate the initial layout and introduce a layout-guided 3D\nGaussian representation for 3D content generation with adaptive geometric\nconstraints. We then propose an instance-scene compositional optimization\nmechanism with conditioned diffusion to collaboratively generate realistic 3D\nscenes with consistent geometry, texture, scale, and accurate interactions\namong multiple objects while simultaneously adjusting the coarse layout priors\nextracted from the LLMs to align with the generated scene. Experiments show\nthat GALA3D is a user-friendly, end-to-end framework for state-of-the-art\nscene-level 3D content generation and controllable editing while ensuring the\nhigh fidelity of object-level entities within the scene. The source codes and\nmodels will be available at gala3d.github.io.\n","authors":["Xiaoyu Zhou","Xingjian Ran","Yajiao Xiong","Jinlin He","Zhiwei Lin","Yongtao Wang","Deqing Sun","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2402.07207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07333v1","updated":"2024-06-11T15:02:16Z","published":"2024-06-11T15:02:16Z","title":"Global-Regularized Neighborhood Regression for Efficient Zero-Shot\n  Texture Anomaly Detection","summary":"  Texture surface anomaly detection finds widespread applications in industrial\nsettings. However, existing methods often necessitate gathering numerous\nsamples for model training. Moreover, they predominantly operate within a\nclose-set detection framework, limiting their ability to identify anomalies\nbeyond the training dataset. To tackle these challenges, this paper introduces\na novel zero-shot texture anomaly detection method named Global-Regularized\nNeighborhood Regression (GRNR). Unlike conventional approaches, GRNR can detect\nanomalies on arbitrary textured surfaces without any training data or cost.\nDrawing from human visual cognition, GRNR derives two intrinsic prior supports\ndirectly from the test texture image: local neighborhood priors characterized\nby coherent similarities and global normality priors featuring typical normal\npatterns. The fundamental principle of GRNR involves utilizing the two\nextracted intrinsic support priors for self-reconstructive regression of the\nquery sample. This process employs the transformation facilitated by local\nneighbor support while being regularized by global normality support, aiming to\nnot only achieve visually consistent reconstruction results but also preserve\nnormality properties. We validate the effectiveness of GRNR across various\nindustrial scenarios using eight benchmark datasets, demonstrating its superior\ndetection performance without the need for training data. Remarkably, our\nmethod is applicable for open-set texture defect detection and can even surpass\nexisting vanilla approaches that require extensive training.\n","authors":["Haiming Yao","Wei Luo","Yunkang Cao","Yiheng Zhang","Wenyong Yu","Weiming Shen"],"pdf_url":"https://arxiv.org/pdf/2406.07333v1.pdf","comment":"SUBMISSION TO IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS:\n  SYSTEMS"},{"id":"http://arxiv.org/abs/2406.07332v1","updated":"2024-06-11T15:01:20Z","published":"2024-06-11T15:01:20Z","title":"Minimizing Energy Costs in Deep Learning Model Training: The Gaussian\n  Sampling Approach","summary":"  Computing the loss gradient via backpropagation consumes considerable energy\nduring deep learning (DL) model training. In this paper, we propose a novel\napproach to efficiently compute DL models' gradients to mitigate the\nsubstantial energy overhead associated with backpropagation. Exploiting the\nover-parameterized nature of DL models and the smoothness of their loss\nlandscapes, we propose a method called {\\em GradSamp} for sampling gradient\nupdates from a Gaussian distribution. Specifically, we update model parameters\nat a given epoch (chosen periodically or randomly) by perturbing the parameters\n(element-wise) from the previous epoch with Gaussian ``noise''. The parameters\nof the Gaussian distribution are estimated using the error between the model\nparameter values from the two previous epochs. {\\em GradSamp} not only\nstreamlines gradient computation but also enables skipping entire epochs,\nthereby enhancing overall efficiency. We rigorously validate our hypothesis\nacross a diverse set of standard and non-standard CNN and transformer-based\nmodels, spanning various computer vision tasks such as image classification,\nobject detection, and image segmentation. Additionally, we explore its efficacy\nin out-of-distribution scenarios such as Domain Adaptation (DA), Domain\nGeneralization (DG), and decentralized settings like Federated Learning (FL).\nOur experimental results affirm the effectiveness of {\\em GradSamp} in\nachieving notable energy savings without compromising performance, underscoring\nits versatility and potential impact in practical DL applications.\n","authors":["Challapalli Phanindra Revanth","Sumohana S. Channappayya","C Krishna Mohan"],"pdf_url":"https://arxiv.org/pdf/2406.07332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07329v1","updated":"2024-06-11T15:00:24Z","published":"2024-06-11T15:00:24Z","title":"Cinematic Gaussians: Real-Time HDR Radiance Fields with Depth of Field","summary":"  Radiance field methods represent the state of the art in reconstructing\ncomplex scenes from multi-view photos. However, these reconstructions often\nsuffer from one or both of the following limitations: First, they typically\nrepresent scenes in low dynamic range (LDR), which restricts their use to\nevenly lit environments and hinders immersive viewing experiences. Secondly,\ntheir reliance on a pinhole camera model, assuming all scene elements are in\nfocus in the input images, presents practical challenges and complicates\nrefocusing during novel-view synthesis. Addressing these limitations, we\npresent a lightweight method based on 3D Gaussian Splatting that utilizes\nmulti-view LDR images of a scene with varying exposure times, apertures, and\nfocus distances as input to reconstruct a high-dynamic-range (HDR) radiance\nfield. By incorporating analytical convolutions of Gaussians based on a\nthin-lens camera model as well as a tonemapping module, our reconstructions\nenable the rendering of HDR content with flexible refocusing capabilities. We\ndemonstrate that our combined treatment of HDR and depth of field facilitates\nreal-time cinematic rendering, outperforming the state of the art.\n","authors":["Chao Wang","Krzysztof Wolski","Bernhard Kerbl","Ana Serrano","Mojtaba Bemana","Hans-Peter Seidel","Karol Myszkowski","Thomas Leimkühler"],"pdf_url":"https://arxiv.org/pdf/2406.07329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07320v1","updated":"2024-06-11T14:49:04Z","published":"2024-06-11T14:49:04Z","title":"A Framework for Efficient Model Evaluation through Stratification,\n  Sampling, and Estimation","summary":"  Model performance evaluation is a critical and expensive task in machine\nlearning and computer vision. Without clear guidelines, practitioners often\nestimate model accuracy using a one-time random selection of the data. However,\nby employing tailored sampling and estimation strategies, one can obtain more\nprecise estimates and reduce annotation costs. In this paper, we propose a\nstatistical framework for model evaluation that includes stratification,\nsampling, and estimation components. We examine the statistical properties of\neach component and evaluate their efficiency (precision). One key result of our\nwork is that stratification via k-means clustering based on accurate\npredictions of model performance yields efficient estimators. Our experiments\non computer vision datasets show that this method consistently provides more\nprecise accuracy estimates than the traditional simple random sampling, even\nwith substantial efficiency gains of 10x. We also find that model-assisted\nestimators, which leverage predictions of model accuracy on the unlabeled\nportion of the dataset, are generally more efficient than the traditional\nestimates based solely on the labeled data.\n","authors":["Riccardo Fogliato","Pratik Patil","Mathew Monfort","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2406.07320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07318v1","updated":"2024-06-11T14:47:36Z","published":"2024-06-11T14:47:36Z","title":"Embedded Graph Convolutional Networks for Real-Time Event Data\n  Processing on SoC FPGAs","summary":"  The utilisation of event cameras represents an important and swiftly evolving\ntrend aimed at addressing the constraints of traditional video systems.\nParticularly within the automotive domain, these cameras find significant\nrelevance for their integration into embedded real-time systems due to lower\nlatency and energy consumption. One effective approach to ensure the necessary\nthroughput and latency for event processing systems is through the utilisation\nof graph convolutional networks (GCNs). In this study, we introduce a series of\nhardware-aware optimisations tailored for PointNet++, a GCN architecture\ndesigned for point cloud processing. The proposed techniques result in more\nthan a 100-fold reduction in model size compared to Asynchronous Event-based\nGNN (AEGNN), one of the most recent works in the field, with a relatively small\ndecrease in accuracy (2.3% for N-Caltech101 classification, 1.7% for N-Cars\nclassification), thus following the TinyML trend. Based on software research,\nwe designed a custom EFGCN (Event-Based FPGA-accelerated Graph Convolutional\nNetwork) and we implemented it on ZCU104 SoC FPGA platform, achieving a\nthroughput of 13.3 million events per second (MEPS) and real-time partially\nasynchronous processing with a latency of 4.47 ms. We also address the\nscalability of the proposed hardware model to improve the obtained accuracy\nscore. To the best of our knowledge, this study marks the first endeavour in\naccelerating PointNet++ networks on SoC FPGAs, as well as the first hardware\narchitecture exploration of graph convolutional networks implementation for\nreal-time continuous event data processing. We publish both software and\nhardware source code in an open repository: https://github.com/vision-agh/***\n(will be published upon acceptance).\n","authors":["Kamil Jeziorek","Piotr Wzorek","Krzysztof Blachut","Andrea Pinna","Tomasz Kryjak"],"pdf_url":"https://arxiv.org/pdf/2406.07318v1.pdf","comment":"Submitted to the IEEE Transactions on Circuits and System for Video\n  Technology. This manuscript was first submitted for publication on March 31,\n  2024. It has since been revised twice: on May 22, 2024 and June 10, 2024"},{"id":"http://arxiv.org/abs/2406.07315v1","updated":"2024-06-11T14:45:00Z","published":"2024-06-11T14:45:00Z","title":"Fetch-A-Set: A Large-Scale OCR-Free Benchmark for Historical Document\n  Retrieval","summary":"  This paper introduces Fetch-A-Set (FAS), a comprehensive benchmark tailored\nfor legislative historical document analysis systems, addressing the challenges\nof large-scale document retrieval in historical contexts. The benchmark\ncomprises a vast repository of documents dating back to the XVII century,\nserving both as a training resource and an evaluation benchmark for retrieval\nsystems. It fills a critical gap in the literature by focusing on complex\nextractive tasks within the domain of cultural heritage. The proposed benchmark\ntackles the multifaceted problem of historical document analysis, including\ntext-to-image retrieval for queries and image-to-text topic extraction from\ndocument fragments, all while accommodating varying levels of document\nlegibility. This benchmark aims to spur advancements in the field by providing\nbaselines and data for the development and evaluation of robust historical\ndocument retrieval systems, particularly in scenarios characterized by wide\nhistorical spectrum.\n","authors":["Adrià Molina","Oriol Ramos Terrades","Josep Lladós"],"pdf_url":"https://arxiv.org/pdf/2406.07315v1.pdf","comment":"Preprint for the manuscript accepted for publication in the DAS2024\n  LNCS proceedings"},{"id":"http://arxiv.org/abs/2406.07294v1","updated":"2024-06-11T14:23:48Z","published":"2024-06-11T14:23:48Z","title":"OTO Planner: An Efficient Only Travelling Once Exploration Planner for\n  Complex and Unknown Environments","summary":"  Autonomous exploration in complex and cluttered environments is essential for\nvarious applications. However, there are many challenges due to the lack of\nglobal heuristic information. Existing exploration methods suffer from the\nrepeated paths and considerable computational resource requirement in\nlarge-scale environments. To address the above issues, this letter proposes an\nefficient exploration planner that reduces repeated paths in complex\nenvironments, hence it is called \"Only Travelling Once Planner\". OTO Planner\nincludes fast frontier updating, viewpoint evaluation and viewpoint refinement.\nA selective frontier updating mechanism is designed, saving a large amount of\ncomputational resources. In addition, a novel viewpoint evaluation system is\ndevised to reduce the repeated paths utilizing the enclosed sub-region\ndetection. Besides, a viewpoint refinement approach is raised to concentrate\nthe redundant viewpoints, leading to smoother paths. We conduct extensive\nsimulation and real-world experiments to validate the proposed method. Compared\nto the state-of-the-art approach, the proposed method reduces the exploration\ntime and movement distance by 10%-20% and improves the speed of frontier\ndetection by 6-9 times.\n","authors":["Bo Zhou","Chuanzhao Lu","Yan Pan","Fu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.07294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07284v1","updated":"2024-06-11T14:12:31Z","published":"2024-06-11T14:12:31Z","title":"Unsupervised Object Detection with Theoretical Guarantees","summary":"  Unsupervised object detection using deep neural networks is typically a\ndifficult problem with few to no guarantees about the learned representation.\nIn this work we present the first unsupervised object detection method that is\ntheoretically guaranteed to recover the true object positions up to\nquantifiable small shifts. We develop an unsupervised object detection\narchitecture and prove that the learned variables correspond to the true object\npositions up to small shifts related to the encoder and decoder receptive field\nsizes, the object sizes, and the widths of the Gaussians used in the rendering\nprocess. We perform detailed analysis of how the error depends on each of these\nvariables and perform synthetic experiments validating our theoretical\npredictions up to a precision of individual pixels. We also perform experiments\non CLEVR-based data and show that, unlike current SOTA object detection methods\n(SAM, CutLER), our method's prediction errors always lie within our theoretical\nbounds. We hope that this work helps open up an avenue of research into object\ndetection methods with theoretical guarantees.\n","authors":["Marian Longa","João F. Henriques"],"pdf_url":"https://arxiv.org/pdf/2406.07284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08098v2","updated":"2024-06-11T14:07:59Z","published":"2023-07-16T16:49:59Z","title":"CalibNet: Dual-branch Cross-modal Calibration for RGB-D Salient Instance\n  Segmentation","summary":"  We propose a novel approach for RGB-D salient instance segmentation using a\ndual-branch cross-modal feature calibration architecture called CalibNet. Our\nmethod simultaneously calibrates depth and RGB features in the kernel and mask\nbranches to generate instance-aware kernels and mask features. CalibNet\nconsists of three simple modules, a dynamic interactive kernel (DIK) and a\nweight-sharing fusion (WSF), which work together to generate effective\ninstance-aware kernels and integrate cross-modal features. To improve the\nquality of depth features, we incorporate a depth similarity assessment (DSA)\nmodule prior to DIK and WSF. In addition, we further contribute a new DSIS\ndataset, which contains 1,940 images with elaborate instance-level annotations.\nExtensive experiments on three challenging benchmarks show that CalibNet yields\na promising result, i.e., 58.0% AP with 320*480 input size on the COME15K-N\ntest set, which significantly surpasses the alternative frameworks. Our code\nand dataset are available at: https://github.com/PJLallen/CalibNet.\n","authors":["Jialun Pei","Tao Jiang","He Tang","Nian Liu","Yueming Jin","Deng-Ping Fan","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2307.08098v2.pdf","comment":"This work has been accepted by TIP 2024"},{"id":"http://arxiv.org/abs/2401.04071v3","updated":"2024-06-11T14:01:18Z","published":"2024-01-08T18:18:02Z","title":"Fun with Flags: Robust Principal Directions via Flag Manifolds","summary":"  Principal component analysis (PCA), along with its extensions to manifolds\nand outlier contaminated data, have been indispensable in computer vision and\nmachine learning. In this work, we present a unifying formalism for PCA and its\nvariants, and introduce a framework based on the flags of linear subspaces, ie\na hierarchy of nested linear subspaces of increasing dimension, which not only\nallows for a common implementation but also yields novel variants, not explored\npreviously. We begin by generalizing traditional PCA methods that either\nmaximize variance or minimize reconstruction error. We expand these\ninterpretations to develop a wide array of new dimensionality reduction\nalgorithms by accounting for outliers and the data manifold. To devise a common\ncomputational approach, we recast robust and dual forms of PCA as optimization\nproblems on flag manifolds. We then integrate tangent space approximations of\nprincipal geodesic analysis (tangent-PCA) into this flag-based framework,\ncreating novel robust and dual geodesic PCA variations. The remarkable\nflexibility offered by the 'flagification' introduced here enables even more\nalgorithmic variants identified by specific flag types. Last but not least, we\npropose an effective convergent solver for these flag-formulations employing\nthe Stiefel manifold. Our empirical results on both real-world and synthetic\nscenarios, demonstrate the superiority of our novel algorithms, especially in\nterms of robustness to outliers on manifolds.\n","authors":["Nathan Mankovich","Gustau Camps-Valls","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2401.04071v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06701v2","updated":"2024-06-11T13:54:55Z","published":"2023-07-13T11:58:27Z","title":"S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized\n  Variational Autoencoder for Video Prediction","summary":"  We address the video prediction task by putting forth a novel model that\ncombines (i) our recently proposed hierarchical residual vector quantized\nvariational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN\n(ST-PixelCNN). We refer to this approach as a sequential hierarchical residual\nlearning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging\nthe intrinsic capabilities of HR-VQVAE at modeling still images with a\nparsimonious representation, combined with the ST-PixelCNN's ability at\nhandling spatiotemporal information, S-HR-VQVAE can better deal with chief\nchallenges in video prediction. These include learning spatiotemporal\ninformation, handling high dimensional data, combating blurry prediction, and\nimplicit modeling of physical characteristics. Extensive experimental results\non the KTH Human Action and Moving-MNIST tasks demonstrate that our model\ncompares favorably against top video prediction techniques both in quantitative\nand qualitative evaluations despite a much smaller model size. Finally, we\nboost S-HR-VQVAE by proposing a novel training method to jointly estimate the\nHR-VQVAE and ST-PixelCNN parameters.\n","authors":["Mohammad Adiban","Kalin Stefanov","Sabato Marco Siniscalchi","Giampiero Salvi"],"pdf_url":"https://arxiv.org/pdf/2307.06701v2.pdf","comment":"14 pages, 7 figures, 3 tables. Submitted to IEEE Transactions on\n  Pattern Analysis and Machine Intelligence on 2023-07-12"},{"id":"http://arxiv.org/abs/2406.07268v1","updated":"2024-06-11T13:52:29Z","published":"2024-06-11T13:52:29Z","title":"Advancing Grounded Multimodal Named Entity Recognition via LLM-Based\n  Reformulation and Box-Based Segmentation","summary":"  Grounded Multimodal Named Entity Recognition (GMNER) task aims to identify\nnamed entities, entity types and their corresponding visual regions. GMNER task\nexhibits two challenging attributes: 1) The tenuous correlation between images\nand text on social media contributes to a notable proportion of named entities\nbeing ungroundable. 2) There exists a distinction between coarse-grained noun\nphrases used in similar tasks (e.g., phrase localization) and fine-grained\nnamed entities. In this paper, we propose RiVEG, a unified framework that\nreformulates GMNER into a joint MNER-VE-VG task by leveraging large language\nmodels (LLMs) as connecting bridges. This reformulation brings two benefits: 1)\nIt enables us to optimize the MNER module for optimal MNER performance and\neliminates the need to pre-extract region features using object detection\nmethods, thus naturally addressing the two major limitations of existing GMNER\nmethods. 2) The introduction of Entity Expansion Expression module and Visual\nEntailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG).\nThis endows the proposed framework with unlimited data and model scalability.\nFurthermore, to address the potential ambiguity stemming from the\ncoarse-grained bounding box output in GMNER, we further construct the new\nSegmented Multimodal Named Entity Recognition (SMNER) task and corresponding\nTwitter-SMNER dataset aimed at generating fine-grained segmentation masks, and\nexperimentally demonstrate the feasibility and effectiveness of using box\nprompt-based Segment Anything Model (SAM) to empower any GMNER model with the\nability to accomplish the SMNER task. Extensive experiments demonstrate that\nRiVEG significantly outperforms SoTA methods on four datasets across the MNER,\nGMNER, and SMNER tasks.\n","authors":["Jinyuan Li","Ziyan Li","Han Li","Jianfei Yu","Rui Xia","Di Sun","Gang Pan"],"pdf_url":"https://arxiv.org/pdf/2406.07268v1.pdf","comment":"Extension of our Findings of EMNLP 2023 & ACL 2024 paper"},{"id":"http://arxiv.org/abs/2406.07255v1","updated":"2024-06-11T13:34:57Z","published":"2024-06-11T13:34:57Z","title":"Towards Realistic Data Generation for Real-World Super-Resolution","summary":"  Existing image super-resolution (SR) techniques often fail to generalize\neffectively in complex real-world settings due to the significant divergence\nbetween training data and practical scenarios. To address this challenge,\nprevious efforts have either manually simulated intricate physical-based\ndegradations or utilized learning-based techniques, yet these approaches remain\ninadequate for producing large-scale, realistic, and diverse data\nsimultaneously. In this paper, we introduce a novel Realistic Decoupled Data\nGenerator (RealDGen), an unsupervised learning data generation framework\ndesigned for real-world super-resolution. We meticulously develop content and\ndegradation extraction strategies, which are integrated into a novel\ncontent-degradation decoupled diffusion model to create realistic\nlow-resolution images from unpaired real LR and HR images. Extensive\nexperiments demonstrate that RealDGen excels in generating large-scale,\nhigh-quality paired data that mirrors real-world degradations, significantly\nadvancing the performance of popular SR models on various real-world\nbenchmarks.\n","authors":["Long Peng","Wenbo Li","Renjing Pei","Jingjing Ren","Yang Wang","Yang Cao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2406.07255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07251v1","updated":"2024-06-11T13:33:33Z","published":"2024-06-11T13:33:33Z","title":"Is One GPU Enough? Pushing Image Generation at Higher-Resolutions with\n  Foundation Models","summary":"  In this work, we introduce Pixelsmith, a zero-shot text-to-image generative\nframework to sample images at higher resolutions with a single GPU. We are the\nfirst to show that it is possible to scale the output of a pre-trained\ndiffusion model by a factor of 1000, opening the road for gigapixel image\ngeneration at no additional cost. Our cascading method uses the image generated\nat the lowest resolution as a baseline to sample at higher resolutions. For the\nguidance, we introduce the Slider, a tunable mechanism that fuses the overall\nstructure contained in the first-generated image with enhanced fine details. At\neach inference step, we denoise patches rather than the entire latent space,\nminimizing memory demands such that a single GPU can handle the process,\nregardless of the image's resolution. Our experimental results show that\nPixelsmith not only achieves higher quality and diversity compared to existing\ntechniques, but also reduces sampling time and artifacts. The code for our work\nis available at https://github.com/Thanos-DB/Pixelsmith.\n","authors":["Athanasios Tragakis","Marco Aversa","Chaitanya Kaul","Roderick Murray-Smith","Daniele Faccio"],"pdf_url":"https://arxiv.org/pdf/2406.07251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03229v2","updated":"2024-06-11T13:22:47Z","published":"2024-06-05T13:06:17Z","title":"Global Clipper: Enhancing Safety and Reliability of Transformer-based\n  Object Detection Models","summary":"  As transformer-based object detection models progress, their impact in\ncritical sectors like autonomous vehicles and aviation is expected to grow.\nSoft errors causing bit flips during inference have significantly impacted DNN\nperformance, altering predictions. Traditional range restriction solutions for\nCNNs fall short for transformers. This study introduces the Global Clipper and\nGlobal Hybrid Clipper, effective mitigation strategies specifically designed\nfor transformer-based models. It significantly enhances their resilience to\nsoft errors and reduces faulty inferences to ~ 0\\%. We also detail extensive\ntesting across over 64 scenarios involving two transformer models (DINO-DETR\nand Lite-DETR) and two CNN models (YOLOv3 and SSD) using three datasets,\ntotalling approximately 3.3 million inferences, to assess model robustness\ncomprehensively. Moreover, the paper explores unique aspects of attention\nblocks in transformers and their operational differences from CNNs.\n","authors":["Qutub Syed Sha","Michael Paulitsch","Karthik Pattabiraman","Korbinian Hagn","Fabian Oboril","Cornelius Buerkle","Kay-Ulrich Scholl","Gereon Hinz","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2406.03229v2.pdf","comment":"Accepted at IJCAI-AISafety'24 Workshop"},{"id":"http://arxiv.org/abs/2312.13729v4","updated":"2024-06-11T13:09:36Z","published":"2023-12-21T10:52:59Z","title":"Gaussian Splatting with NeRF-based Color and Opacity","summary":"  Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of\nneural networks to capture the intricacies of 3D objects. By encoding the shape\nand color information within neural network weights, NeRFs excel at producing\nstrikingly sharp novel views of 3D objects. Recently, numerous generalizations\nof NeRFs utilizing generative models have emerged, expanding its versatility.\nIn contrast, Gaussian Splatting (GS) offers a similar render quality with\nfaster training and inference as it does not need neural networks to work. It\nencodes information about the 3D objects in the set of Gaussian distributions\nthat can be rendered in 3D similarly to classical meshes. Unfortunately, GS are\ndifficult to condition since they usually require circa hundred thousand\nGaussian components. To mitigate the caveats of both models, we propose a\nhybrid model Viewing Direction Gaussian Splatting (VDGS) that uses GS\nrepresentation of the 3D object's shape and NeRF-based encoding of color and\nopacity. Our model uses Gaussian distributions with trainable positions (i.e.\nmeans of Gaussian), shape (i.e. covariance of Gaussian), color and opacity, and\na neural network that takes Gaussian parameters and viewing direction to\nproduce changes in the said color and opacity. As a result, our model better\ndescribes shadows, light reflections, and the transparency of 3D objects\nwithout adding additional texture and light components.\n","authors":["Dawid Malarz","Weronika Smolak","Jacek Tabor","Sławomir Tadeja","Przemysław Spurek"],"pdf_url":"https://arxiv.org/pdf/2312.13729v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07230v1","updated":"2024-06-11T13:09:16Z","published":"2024-06-11T13:09:16Z","title":"Needle In A Multimodal Haystack","summary":"  With the rapid advancement of multimodal large language models (MLLMs), their\nevaluation has become increasingly comprehensive. However, understanding long\nmultimodal content, as a foundational ability for real-world applications,\nremains underexplored. In this work, we present Needle In A Multimodal Haystack\n(MM-NIAH), the first benchmark specifically designed to systematically evaluate\nthe capability of existing MLLMs to comprehend long multimodal documents. Our\nbenchmark includes three types of evaluation tasks: multimodal retrieval,\ncounting, and reasoning. In each task, the model is required to answer the\nquestions according to different key information scattered throughout the given\nmultimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that\nexisting models still have significant room for improvement on these tasks,\nespecially on vision-centric evaluation. We hope this work can provide a\nplatform for further research on long multimodal document comprehension and\ncontribute to the advancement of MLLMs. Code and benchmark are released at\nhttps://github.com/OpenGVLab/MM-NIAH.\n","authors":["Weiyun Wang","Shuibo Zhang","Yiming Ren","Yuchen Duan","Tiantong Li","Shuo Liu","Mengkang Hu","Zhe Chen","Kaipeng Zhang","Lewei Lu","Xizhou Zhu","Ping Luo","Yu Qiao","Jifeng Dai","Wenqi Shao","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07227v1","updated":"2024-06-11T13:06:09Z","published":"2024-06-11T13:06:09Z","title":"Which Country Is This? Automatic Country Ranking of Street View Photos","summary":"  In this demonstration, we present Country Guesser, a live system that guesses\nthe country that a photo is taken in. In particular, given a Google Street View\nimage, our federated ranking model uses a combination of computer vision,\nmachine learning and text retrieval methods to compute a ranking of likely\ncountries of the location shown in a given image from Street View.\nInterestingly, using text-based features to probe large pre-trained language\nmodels can assist to provide cross-modal supervision. We are not aware of\nprevious country guessing systems informed by visual and textual features.\n","authors":["Tim Menzner","Jochen L. Leidner","Florian Mittag"],"pdf_url":"https://arxiv.org/pdf/2406.07227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07221v1","updated":"2024-06-11T13:01:45Z","published":"2024-06-11T13:01:45Z","title":"Open-World Human-Object Interaction Detection via Multi-modal Prompts","summary":"  In this paper, we develop \\textbf{MP-HOI}, a powerful Multi-modal\nPrompt-based HOI detector designed to leverage both textual descriptions for\nopen-set generalization and visual exemplars for handling high ambiguity in\ndescriptions, realizing HOI detection in the open world. Specifically, it\nintegrates visual prompts into existing language-guided-only HOI detectors to\nhandle situations where textual descriptions face difficulties in\ngeneralization and to address complex scenarios with high interaction\nambiguity. To facilitate MP-HOI training, we build a large-scale HOI dataset\nnamed Magic-HOI, which gathers six existing datasets into a unified label\nspace, forming over 186K images with 2.4K objects, 1.2K actions, and 20K HOI\ninteractions. Furthermore, to tackle the long-tail issue within the Magic-HOI\ndataset, we introduce an automated pipeline for generating realistically\nannotated HOI images and present SynHOI, a high-quality synthetic HOI dataset\ncontaining 100K images. Leveraging these two datasets, MP-HOI optimizes the HOI\ntask as a similarity learning process between multi-modal prompts and\nobjects/interactions via a unified contrastive loss, to learn generalizable and\ntransferable objects/interactions representations from large-scale data. MP-HOI\ncould serve as a generalist HOI detector, surpassing the HOI vocabulary of\nexisting expert models by more than 30 times. Concurrently, our results\ndemonstrate that MP-HOI exhibits remarkable zero-shot capability in real-world\nscenarios and consistently achieves a new state-of-the-art performance across\nvarious benchmarks.\n","authors":["Jie Yang","Bingliang Li","Ailing Zeng","Lei Zhang","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07221v1.pdf","comment":"CVPR24. arXiv admin note: text overlap with arXiv:2305.12252"},{"id":"http://arxiv.org/abs/2406.05753v2","updated":"2024-06-11T12:45:08Z","published":"2024-06-09T12:16:30Z","title":"Grounding Continuous Representations in Geometry: Equivariant Neural\n  Fields","summary":"  Recently, Neural Fields have emerged as a powerful modelling paradigm to\nrepresent continuous signals. In a conditional neural field, a field is\nrepresented by a latent variable that conditions the NeF, whose parametrisation\nis otherwise shared over an entire dataset. We propose Equivariant Neural\nFields based on cross attention transformers, in which NeFs are conditioned on\na geometric conditioning variable, a latent point cloud, that enables an\nequivariant decoding from latent to field. Our equivariant approach induces a\nsteerability property by which both field and latent are grounded in geometry\nand amenable to transformation laws if the field transforms, the latent\nrepresents transforms accordingly and vice versa. Crucially, the equivariance\nrelation ensures that the latent is capable of (1) representing geometric\npatterns faitfhully, allowing for geometric reasoning in latent space, (2)\nweightsharing over spatially similar patterns, allowing for efficient learning\nof datasets of fields. These main properties are validated using classification\nexperiments and a verification of the capability of fitting entire datasets, in\ncomparison to other non-equivariant NeF approaches. We further validate the\npotential of ENFs by demonstrate unique local field editing properties.\n","authors":["David R Wessels","David M Knigge","Samuele Papa","Riccardo Valperga","Sharvaree Vadgama","Efstratios Gavves","Erik J Bekkers"],"pdf_url":"https://arxiv.org/pdf/2406.05753v2.pdf","comment":"Preprint for Neurips submission"},{"id":"http://arxiv.org/abs/2308.05474v3","updated":"2024-06-11T12:42:18Z","published":"2023-08-10T10:01:56Z","title":"Spatio-Temporal Encoding of Brain Dynamics with Surface Masked\n  Autoencoders","summary":"  The development of robust and generalisable models for encoding the\nspatio-temporal dynamics of human brain activity is crucial for advancing\nneuroscientific discoveries. However, significant individual variation in the\norganisation of the human cerebral cortex makes it difficult to identify\npopulation-level trends in these signals. Recently, Surface Vision Transformers\n(SiTs) have emerged as a promising approach for modelling cortical signals, yet\nthey face some limitations in low-data scenarios due to the lack of inductive\nbiases in their architecture. To address these challenges, this paper proposes\nthe surface Masked AutoEncoder (sMAE) and video surface Masked AutoEncoder\n(vsMAE) - for multivariate and spatio-temporal pre-training of cortical signals\nover regular icosahedral grids. These models are trained to reconstruct\ncortical feature maps from masked versions of the input by learning strong\nlatent representations of cortical structure and function. Such representations\ntranslate into better modelling of individual phenotypes and enhanced\nperformance in downstream tasks. The proposed approach was evaluated on\ncortical phenotype regression using data from the young adult Human Connectome\nProject (HCP) and developing HCP (dHCP). Results show that (v)sMAE pre-trained\nmodels improve phenotyping prediction performance on multiple tasks by $\\ge\n26\\%$, and offer faster convergence relative to models trained from scratch.\nFinally, we show that pre-training vision transformers on large datasets, such\nas the UK Biobank (UKB), supports transfer learning to low-data regimes. Our\ncode and pre-trained models are publicly available at\nhttps://github.com/metrics-lab/surface-masked-autoencoders .\n","authors":["Simon Dahan","Logan Z. J. Williams","Yourong Guo","Daniel Rueckert","Emma C. Robinson"],"pdf_url":"https://arxiv.org/pdf/2308.05474v3.pdf","comment":"Accepted for publications for MIDL 2024; 20 figures; 7 figures"},{"id":"http://arxiv.org/abs/2406.00934v3","updated":"2024-06-11T12:40:50Z","published":"2024-06-03T02:12:27Z","title":"LanEvil: Benchmarking the Robustness of Lane Detection to Environmental\n  Illusions","summary":"  Lane detection (LD) is an essential component of autonomous driving systems,\nproviding fundamental functionalities like adaptive cruise control and\nautomated lane centering. Existing LD benchmarks primarily focus on evaluating\ncommon cases, neglecting the robustness of LD models against environmental\nillusions such as shadows and tire marks on the road. This research gap poses\nsignificant safety challenges since these illusions exist naturally in\nreal-world traffic situations. For the first time, this paper studies the\npotential threats caused by these environmental illusions to LD and establishes\nthe first comprehensive benchmark LanEvil for evaluating the robustness of LD\nagainst this natural corruption. We systematically design 14 prevalent yet\ncritical types of environmental illusions (e.g., shadow, reflection) that cover\na wide spectrum of real-world influencing factors in LD tasks. Based on\nreal-world environments, we create 94 realistic and customizable 3D cases using\nthe widely used CARLA simulator, resulting in a dataset comprising 90,292\nsampled images. Through extensive experiments, we benchmark the robustness of\npopular LD methods using LanEvil, revealing substantial performance degradation\n(-5.37% Accuracy and -10.70% F1-Score on average), with shadow effects posing\nthe greatest risk (-7.39% Accuracy). Additionally, we assess the performance of\ncommercial auto-driving systems OpenPilot and Apollo through collaborative\nsimulations, demonstrating that proposed environmental illusions can lead to\nincorrect decisions and potential traffic accidents. To defend against\nenvironmental illusions, we propose the Attention Area Mixing (AAM) approach\nusing hard examples, which witness significant robustness improvement (+3.76%)\nunder illumination effects. We hope our paper can contribute to advancing more\nrobust auto-driving systems in the future. Website: https://lanevil.github.io/.\n","authors":["Tianyuan Zhang","Lu Wang","Hainan Li","Yisong Xiao","Siyuan Liang","Aishan Liu","Xianglong Liu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2406.00934v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06211v2","updated":"2024-06-11T12:37:23Z","published":"2024-06-10T12:22:06Z","title":"iMotion-LLM: Motion Prediction Instruction Tuning","summary":"  We introduce iMotion-LLM: a Multimodal Large Language Models (LLMs) with\ntrajectory prediction, tailored to guide interactive multi-agent scenarios.\nDifferent from conventional motion prediction approaches, iMotion-LLM\ncapitalizes on textual instructions as key inputs for generating contextually\nrelevant trajectories. By enriching the real-world driving scenarios in the\nWaymo Open Dataset with textual motion instructions, we created InstructWaymo.\nLeveraging this dataset, iMotion-LLM integrates a pretrained LLM, fine-tuned\nwith LoRA, to translate scene features into the LLM input space. iMotion-LLM\noffers significant advantages over conventional motion prediction models.\nFirst, it can generate trajectories that align with the provided instructions\nif it is a feasible direction. Second, when given an infeasible direction, it\ncan reject the instruction, thereby enhancing safety. These findings act as\nmilestones in empowering autonomous navigation systems to interpret and predict\nthe dynamics of multi-agent environments, laying the groundwork for future\nadvancements in this field.\n","authors":["Abdulwahab Felemban","Eslam Mohamed Bakr","Xiaoqian Shen","Jian Ding","Abduallah Mohamed","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2406.06211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05862v2","updated":"2024-06-11T12:33:42Z","published":"2024-06-09T17:25:47Z","title":"II-Bench: An Image Implication Understanding Benchmark for Multimodal\n  Large Language Models","summary":"  The rapid advancements in the development of multimodal large language models\n(MLLMs) have consistently led to new breakthroughs on various benchmarks. In\nresponse, numerous challenging and comprehensive benchmarks have been proposed\nto more accurately assess the capabilities of MLLMs. However, there is a dearth\nof exploration of the higher-order perceptual capabilities of MLLMs. To fill\nthis gap, we propose the Image Implication understanding Benchmark, II-Bench,\nwhich aims to evaluate the model's higher-order perception of images. Through\nextensive experiments on II-Bench across multiple MLLMs, we have made\nsignificant findings. Initially, a substantial gap is observed between the\nperformance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs\nattains 74.8%, whereas human accuracy averages 90%, peaking at an impressive\n98%. Subsequently, MLLMs perform worse on abstract and complex images,\nsuggesting limitations in their ability to understand high-level semantics and\ncapture image details. Finally, it is observed that most models exhibit\nenhanced accuracy when image sentiment polarity hints are incorporated into the\nprompts. This observation underscores a notable deficiency in their inherent\nunderstanding of image sentiment. We believe that II-Bench will inspire the\ncommunity to develop the next generation of MLLMs, advancing the journey\ntowards expert artificial general intelligence (AGI). II-Bench is publicly\navailable at https://huggingface.co/datasets/m-a-p/II-Bench.\n","authors":["Ziqiang Liu","Feiteng Fang","Xi Feng","Xinrun Du","Chenhao Zhang","Zekun Wang","Yuelin Bai","Qixuan Zhao","Liyang Fan","Chengguang Gan","Hongquan Lin","Jiaming Li","Yuansheng Ni","Haihong Wu","Yaswanth Narsupalli","Zhigang Zheng","Chengming Li","Xiping Hu","Ruifeng Xu","Xiaojun Chen","Min Yang","Jiaheng Liu","Ruibo Liu","Wenhao Huang","Ge Zhang","Shiwen Ni"],"pdf_url":"https://arxiv.org/pdf/2406.05862v2.pdf","comment":"100 pages, 82 figures, add citations"},{"id":"http://arxiv.org/abs/2406.07209v1","updated":"2024-06-11T12:32:53Z","published":"2024-06-11T12:32:53Z","title":"MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout\n  Guidance","summary":"  Recent advancements in text-to-image generation models have dramatically\nenhanced the generation of photorealistic images from textual prompts, leading\nto an increased interest in personalized text-to-image applications,\nparticularly in multi-subject scenarios. However, these advances are hindered\nby two main challenges: firstly, the need to accurately maintain the details of\neach referenced subject in accordance with the textual descriptions; and\nsecondly, the difficulty in achieving a cohesive representation of multiple\nsubjects in a single image without introducing inconsistencies. To address\nthese concerns, our research introduces the MS-Diffusion framework for\nlayout-guided zero-shot image personalization with multi-subjects. This\ninnovative approach integrates grounding tokens with the feature resampler to\nmaintain detail fidelity among subjects. With the layout guidance, MS-Diffusion\nfurther improves the cross-attention to adapt to the multi-subject inputs,\nensuring that each subject condition acts on specific areas. The proposed\nmulti-subject cross-attention orchestrates harmonious inter-subject\ncompositions while preserving the control of texts. Comprehensive quantitative\nand qualitative experiments affirm that this method surpasses existing models\nin both image and text fidelity, promoting the development of personalized\ntext-to-image generation.\n","authors":["X. Wang","Siming Fu","Qihan Huang","Wanggui He","Hao Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.07209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11909v3","updated":"2024-06-11T12:30:59Z","published":"2023-03-21T15:00:17Z","title":"The Multiscale Surface Vision Transformer","summary":"  Surface meshes are a favoured domain for representing structural and\nfunctional information on the human cortex, but their complex topology and\ngeometry pose significant challenges for deep learning analysis. While\nTransformers have excelled as domain-agnostic architectures for\nsequence-to-sequence learning, the quadratic cost of the self-attention\noperation remains an obstacle for many dense prediction tasks. Inspired by some\nof the latest advances in hierarchical modelling with vision transformers, we\nintroduce the Multiscale Surface Vision Transformer (MS-SiT) as a backbone\narchitecture for surface deep learning. The self-attention mechanism is applied\nwithin local-mesh-windows to allow for high-resolution sampling of the\nunderlying data, while a shifted-window strategy improves the sharing of\ninformation between windows. Neighbouring patches are successively merged,\nallowing the MS-SiT to learn hierarchical representations suitable for any\nprediction task. Results demonstrate that the MS-SiT outperforms existing\nsurface deep learning methods for neonatal phenotyping prediction tasks using\nthe Developing Human Connectome Project (dHCP) dataset. Furthermore, building\nthe MS-SiT backbone into a U-shaped architecture for surface segmentation\ndemonstrates competitive results on cortical parcellation using the UK Biobank\n(UKB) and manually-annotated MindBoggle datasets. Code and trained models are\npublicly available at\nhttps://github.com/metrics-lab/surface-vision-transformers.\n","authors":["Simon Dahan","Logan Z. J. Williams","Daniel Rueckert","Emma C. Robinson"],"pdf_url":"https://arxiv.org/pdf/2303.11909v3.pdf","comment":"Accepted for publication at MIDL 2024, 17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.07202v1","updated":"2024-06-11T12:22:49Z","published":"2024-06-11T12:22:49Z","title":"Can Foundation Models Reliably Identify Spatial Hazards? A Case Study on\n  Curb Segmentation","summary":"  Curbs serve as vital borders that delineate safe pedestrian zones from\npotential vehicular traffic hazards. Curbs also represent a primary spatial\nhazard during dynamic navigation with significant stumbling potential. Such\nvulnerabilities are particularly exacerbated for persons with blindness and low\nvision (PBLV). Accurate visual-based discrimination of curbs is paramount for\nassistive technologies that aid PBLV with safe navigation in urban\nenvironments. Herein, we investigate the efficacy of curb segmentation for\nfoundation models. We introduce the largest curb segmentation dataset to-date\nto benchmark leading foundation models. Our results show that state-of-the-art\nfoundation models face significant challenges in curb segmentation. This is due\nto their high false-positive rates (up to 95%) with poor performance\ndistinguishing curbs from curb-like objects or non-curb areas, such as\nsidewalks. In addition, the best-performing model averaged a 3.70-second\ninference time, underscoring problems in providing real-time assistance. In\nresponse, we propose solutions including filtered bounding box selections to\nachieve more accurate curb segmentation. Overall, despite the immediate\nflexibility of foundation models, their application for practical assistive\ntechnology applications still requires refinement. This research highlights the\ncritical need for specialized datasets and tailored model training to address\nnavigation challenges for PBLV and underscores implicit weaknesses in\nfoundation models.\n","authors":["Diwei Sheng","Giles Hamilton-Fletcher","Mahya Beheshti","Chen Feng","John-Ross Rizzo"],"pdf_url":"https://arxiv.org/pdf/2406.07202v1.pdf","comment":"21 pages, 8 figures, submitted to Assistive Technology"},{"id":"http://arxiv.org/abs/2405.05007v3","updated":"2024-06-11T12:07:02Z","published":"2024-05-08T12:24:50Z","title":"HC-Mamba: Vision MAMBA with Hybrid Convolutional Techniques for Medical\n  Image Segmentation","summary":"  Automatic medical image segmentation technology has the potential to expedite\npathological diagnoses, thereby enhancing the efficiency of patient care.\nHowever, medical images often have complex textures and structures, and the\nmodels often face the problem of reduced image resolution and information loss\ndue to downsampling. To address this issue, we propose HC-Mamba, a new medical\nimage segmentation model based on the modern state space model Mamba.\nSpecifically, we introduce the technique of dilated convolution in the HC-Mamba\nmodel to capture a more extensive range of contextual information without\nincreasing the computational cost by extending the perceptual field of the\nconvolution kernel. In addition, the HC-Mamba model employs depthwise separable\nconvolutions, significantly reducing the number of parameters and the\ncomputational power of the model. By combining dilated convolution and\ndepthwise separable convolutions, HC-Mamba is able to process large-scale\nmedical image data at a much lower computational cost while maintaining a high\nlevel of performance. We conduct comprehensive experiments on segmentation\ntasks including organ segmentation and skin lesion, and conduct extensive\nexperiments on Synapse, ISIC17 and ISIC18 to demonstrate the potential of the\nHC-Mamba model in medical image segmentation. The experimental results show\nthat HC-Mamba exhibits competitive performance on all these datasets, thereby\nproving its effectiveness and usefulness in medical image segmentation.\n","authors":["Jiashu Xu"],"pdf_url":"https://arxiv.org/pdf/2405.05007v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07191v1","updated":"2024-06-11T12:03:57Z","published":"2024-06-11T12:03:57Z","title":"MeMSVD: Long-Range Temporal Structure Capturing Using Incremental SVD","summary":"  This paper is on long-term video understanding where the goal is to recognise\nhuman actions over long temporal windows (up to minutes long). In prior work,\nlong temporal context is captured by constructing a long-term memory bank\nconsisting of past and future video features which are then integrated into\nstandard (short-term) video recognition backbones through the use of attention\nmechanisms. Two well-known problems related to this approach are the quadratic\ncomplexity of the attention operation and the fact that the whole feature bank\nmust be stored in memory for inference. To address both issues, we propose an\nalternative to attention-based schemes which is based on a low-rank\napproximation of the memory obtained using Singular Value Decomposition. Our\nscheme has two advantages: (a) it reduces complexity by more than an order of\nmagnitude, and (b) it is amenable to an efficient implementation for the\ncalculation of the memory bases in an incremental fashion which does not\nrequire the storage of the whole feature bank in memory. The proposed scheme\nmatches or surpasses the accuracy achieved by attention-based mechanisms while\nbeing memory-efficient. Through extensive experiments, we demonstrate that our\nframework generalises to different architectures and tasks, outperforming the\nstate-of-the-art in three datasets.\n","authors":["Ioanna Ntinou","Enrique Sanchez","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2406.07191v1.pdf","comment":"Accepted to ICIP 2024"},{"id":"http://arxiv.org/abs/2406.07189v1","updated":"2024-06-11T12:01:11Z","published":"2024-06-11T12:01:11Z","title":"RGB-Sonar Tracking Benchmark and Spatial Cross-Attention Transformer\n  Tracker","summary":"  Vision camera and sonar are naturally complementary in the underwater\nenvironment. Combining the information from two modalities will promote better\nobservation of underwater targets. However, this problem has not received\nsufficient attention in previous research. Therefore, this paper introduces a\nnew challenging RGB-Sonar (RGB-S) tracking task and investigates how to achieve\nefficient tracking of an underwater target through the interaction of RGB and\nsonar modalities. Specifically, we first propose an RGBS50 benchmark dataset\ncontaining 50 sequences and more than 87000 high-quality annotated bounding\nboxes. Experimental results show that the RGBS50 benchmark poses a challenge to\ncurrently popular SOT trackers. Second, we propose an RGB-S tracker called\nSCANet, which includes a spatial cross-attention module (SCAM) consisting of a\nnovel spatial cross-attention layer and two independent global integration\nmodules. The spatial cross-attention is used to overcome the problem of spatial\nmisalignment of between RGB and sonar images. Third, we propose a SOT\ndata-based RGB-S simulation training method (SRST) to overcome the lack of\nRGB-S training datasets. It converts RGB images into sonar-like saliency images\nto construct pseudo-data pairs, enabling the model to learn the semantic\nstructure of RGB-S-like data. Comprehensive experiments show that the proposed\nspatial cross-attention effectively achieves the interaction between RGB and\nsonar modalities and SCANet achieves state-of-the-art performance on the\nproposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/RGBS50.\n","authors":["Yunfeng Li","Bo Wang","Jiuran Sun","Xueyi Wu","Ye Li"],"pdf_url":"https://arxiv.org/pdf/2406.07189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07176v1","updated":"2024-06-11T11:39:44Z","published":"2024-06-11T11:39:44Z","title":"RAD: A Comprehensive Dataset for Benchmarking the Robustness of Image\n  Anomaly Detection","summary":"  Robustness against noisy imaging is crucial for practical image anomaly\ndetection systems. This study introduces a Robust Anomaly Detection (RAD)\ndataset with free views, uneven illuminations, and blurry collections to\nsystematically evaluate the robustness of current anomaly detection methods.\nSpecifically, RAD aims to identify foreign objects on working platforms as\nanomalies. The collection process incorporates various sources of imaging\nnoise, such as viewpoint changes, uneven illuminations, and blurry collections,\nto replicate real-world inspection scenarios. Subsequently, we assess and\nanalyze 11 state-of-the-art unsupervised and zero-shot methods on RAD. Our\nfindings indicate that: 1) Variations in viewpoint, illumination, and blurring\naffect anomaly detection methods to varying degrees; 2) Methods relying on\nmemory banks and assisted by synthetic anomalies demonstrate stronger\nrobustness; 3) Effectively leveraging the general knowledge of foundational\nmodels is a promising avenue for enhancing the robustness of anomaly detection\nmethods.\n","authors":["Yuqi Cheng","Yunkang Cao","Rui Chen","Weiming Shen"],"pdf_url":"https://arxiv.org/pdf/2406.07176v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.07170v1","updated":"2024-06-11T11:26:27Z","published":"2024-06-11T11:26:27Z","title":"VoxNeuS: Enhancing Voxel-Based Neural Surface Reconstruction via\n  Gradient Interpolation","summary":"  Neural Surface Reconstruction learns a Signed Distance Field~(SDF) to\nreconstruct the 3D model from multi-view images. Previous works adopt\nvoxel-based explicit representation to improve efficiency. However, they\nignored the gradient instability of interpolation in the voxel grid, leading to\ndegradation on convergence and smoothness. Besides, previous works entangled\nthe optimization of geometry and radiance, which leads to the deformation of\ngeometry to explain radiance, causing artifacts when reconstructing textured\nplanes.\n  In this work, we reveal that the instability of gradient comes from its\ndiscontinuity during trilinear interpolation, and propose to use the\ninterpolated gradient instead of the original analytical gradient to eliminate\nthe discontinuity. Based on gradient interpolation, we propose VoxNeuS, a\nlightweight surface reconstruction method for computational and memory\nefficient neural surface reconstruction. Thanks to the explicit representation,\nthe gradient of regularization terms, i.e. Eikonal and curvature loss, are\ndirectly solved, avoiding computation and memory-access overhead.\n  Further, VoxNeuS adopts a geometry-radiance disentangled architecture to\nhandle the geometry deformation from radiance optimization.\n  The experimental results show that VoxNeuS achieves better reconstruction\nquality than previous works. The entire training process takes 15 minutes and\nless than 3 GB of memory on a single 2080ti GPU.\n","authors":["Sidun Liu","Peng Qiao","Zongxin Ye","Wenyu Li","Yong Dou"],"pdf_url":"https://arxiv.org/pdf/2406.07170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07169v1","updated":"2024-06-11T11:25:37Z","published":"2024-06-11T11:25:37Z","title":"RecMoDiffuse: Recurrent Flow Diffusion for Human Motion Generation","summary":"  Human motion generation has paramount importance in computer animation. It is\na challenging generative temporal modelling task due to the vast possibilities\nof human motion, high human sensitivity to motion coherence and the difficulty\nof accurately generating fine-grained motions. Recently, diffusion methods have\nbeen proposed for human motion generation due to their high sample quality and\nexpressiveness. However, generated sequences still suffer from motion\nincoherence, and are limited to short duration, and simpler motion and take\nconsiderable time during inference. To address these limitations, we propose\n\\textit{RecMoDiffuse: Recurrent Flow Diffusion}, a new recurrent diffusion\nformulation for temporal modelling. Unlike previous work, which applies\ndiffusion to the whole sequence without any temporal dependency, an approach\nthat inherently makes temporal consistency hard to achieve. Our method\nexplicitly enforces temporal constraints with the means of normalizing flow\nmodels in the diffusion process and thereby extends diffusion to the temporal\ndimension. We demonstrate the effectiveness of RecMoDiffuse in the temporal\nmodelling of human motion. Our experiments show that RecMoDiffuse achieves\ncomparable results with state-of-the-art methods while generating coherent\nmotion sequences and reducing the computational overhead in the inference\nstage.\n","authors":["Mirgahney Mohamed","Harry Jake Cunningham","Marc P. Deisenroth","Lourdes Agapito"],"pdf_url":"https://arxiv.org/pdf/2406.07169v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.07163v1","updated":"2024-06-11T11:13:29Z","published":"2024-06-11T11:13:29Z","title":"FaceGPT: Self-supervised Learning to Chat about 3D Human Faces","summary":"  We introduce FaceGPT, a self-supervised learning framework for Large\nVision-Language Models (VLMs) to reason about 3D human faces from images and\ntext. Typical 3D face reconstruction methods are specialized algorithms that\nlack semantic reasoning capabilities. FaceGPT overcomes this limitation by\nembedding the parameters of a 3D morphable face model (3DMM) into the token\nspace of a VLM, enabling the generation of 3D faces from both textual and\nvisual inputs. FaceGPT is trained in a self-supervised manner as a model-based\nautoencoder from in-the-wild images. In particular, the hidden state of LLM is\nprojected into 3DMM parameters and subsequently rendered as 2D face image to\nguide the self-supervised learning process via image-based reconstruction.\nWithout relying on expensive 3D annotations of human faces, FaceGPT obtains a\ndetailed understanding about 3D human faces, while preserving the capacity to\nunderstand general user instructions. Our experiments demonstrate that FaceGPT\nnot only achieves high-quality 3D face reconstructions but also retains the\nability for general-purpose visual instruction following. Furthermore, FaceGPT\nlearns fully self-supervised to generate 3D faces based on complex textual\ninputs, which opens a new direction in human face analysis.\n","authors":["Haoran Wang","Mohit Mendiratta","Christian Theobalt","Adam Kortylewski"],"pdf_url":"https://arxiv.org/pdf/2406.07163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.10959v2","updated":"2024-06-11T11:04:06Z","published":"2022-04-22T23:49:44Z","title":"A benchmark dataset for deep learning-based airplane detection: HRPlanes","summary":"  Airplane detection from satellite imagery is a challenging task due to the\ncomplex backgrounds in the images and differences in data acquisition\nconditions caused by the sensor geometry and atmospheric effects. Deep learning\nmethods provide reliable and accurate solutions for automatic detection of\nairplanes; however, huge amount of training data is required to obtain\npromising results. In this study, we create a novel airplane detection dataset\ncalled High Resolution Planes (HRPlanes) by using images from Google Earth (GE)\nand labeling the bounding box of each plane on the images. HRPlanes include GE\nimages of several different airports across the world to represent a variety of\nlandscape, seasonal and satellite geometry conditions obtained from different\nsatellites. We evaluated our dataset with two widely used object detection\nmethods namely YOLOv4 and Faster R-CNN. Our preliminary results show that the\nproposed dataset can be a valuable data source and benchmark data set for\nfuture applications. Moreover, proposed architectures and results of this study\ncould be used for transfer learning of different datasets and models for\nairplane detection.\n","authors":["Tolga Bakirman","Elif Sertel"],"pdf_url":"https://arxiv.org/pdf/2204.10959v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2405.05953v2","updated":"2024-06-11T10:55:20Z","published":"2024-05-09T17:46:22Z","title":"Frame Interpolation with Consecutive Brownian Bridge Diffusion","summary":"  Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a\ndiffusion-based conditional image generation problem, synthesizing the\nintermediate frame given a random noise and neighboring frames. Due to the\nrelatively high resolution of videos, Latent Diffusion Models (LDMs) are\nemployed as the conditional generation model, where the autoencoder compresses\nimages into latent representations for diffusion and then reconstructs images\nfrom these latent representations. Such a formulation poses a crucial\nchallenge: VFI expects that the output is deterministically equal to the ground\ntruth intermediate frame, but LDMs randomly generate a diverse set of different\nimages when the model runs multiple times. The reason for the diverse\ngeneration is that the cumulative variance (variance accumulated at each step\nof generation) of generated latent representations in LDMs is large. This makes\nthe sampling trajectory random, resulting in diverse rather than deterministic\ngenerations. To address this problem, we propose our unique solution: Frame\nInterpolation with Consecutive Brownian Bridge Diffusion. Specifically, we\npropose consecutive Brownian Bridge diffusion that takes a deterministic\ninitial value as input, resulting in a much smaller cumulative variance of\ngenerated latent representations. Our experiments suggest that our method can\nimprove together with the improvement of the autoencoder and achieve\nstate-of-the-art performance in VFI, leaving strong potential for further\nenhancement.\n","authors":["Zonglin Lyu","Ming Li","Jianbo Jiao","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2405.05953v2.pdf","comment":"corrected typo"},{"id":"http://arxiv.org/abs/2312.01464v2","updated":"2024-06-11T10:51:21Z","published":"2023-12-03T17:20:11Z","title":"CT Reconstruction using Diffusion Posterior Sampling conditioned on a\n  Nonlinear Measurement Model","summary":"  Diffusion models have been demonstrated as powerful deep learning tools for\nimage generation in CT reconstruction and restoration. Recently, diffusion\nposterior sampling, where a score-based diffusion prior is combined with a\nlikelihood model, has been used to produce high quality CT images given\nlow-quality measurements. This technique is attractive since it permits a\none-time, unsupervised training of a CT prior; which can then be incorporated\nwith an arbitrary data model. However, current methods rely on a linear model\nof x-ray CT physics to reconstruct or restore images. While it is common to\nlinearize the transmission tomography reconstruction problem, this is an\napproximation to the true and inherently nonlinear forward model. We propose a\nnew method that solves the inverse problem of nonlinear CT image reconstruction\nvia diffusion posterior sampling. We implement a traditional unconditional\ndiffusion model by training a prior score function estimator, and apply Bayes\nrule to combine this prior with a measurement likelihood score function derived\nfrom the nonlinear physical model to arrive at a posterior score function that\ncan be used to sample the reverse-time diffusion process. This plug-and-play\nmethod allows incorporation of a diffusion-based prior with generalized\nnonlinear CT image reconstruction into multiple CT system designs with\ndifferent forward models, without the need for any additional training. We\ndevelop the algorithm that performs this reconstruction, including an\nordered-subsets variant for accelerated processing and demonstrate the\ntechnique in both fully sampled low dose data and sparse-view geometries using\na single unsupervised training of the prior.\n","authors":["Shudong Li","Xiao Jiang","Matthew Tivnan","Grace J. Gang","Yuan Shen","J. Webster Stayman"],"pdf_url":"https://arxiv.org/pdf/2312.01464v2.pdf","comment":"24 pages, 12 figures, 1 table, submitted to SPIE Journal of Medical\n  Imaging. Updated with more realistic phantom data, Poisson likelihood, and\n  additional evaluations including hallucination evaluation, performance under\n  multiple noise levels, inference time evaluation, and etc. Changes in\n  authorship is based on unanimous agreement to acknowledge the adding authors'\n  contributions in this work"},{"id":"http://arxiv.org/abs/2406.07146v1","updated":"2024-06-11T10:45:59Z","published":"2024-06-11T10:45:59Z","title":"Benchmarking and Boosting Radiology Report Generation for 3D\n  High-Resolution Medical Images","summary":"  Automatic radiology report generation can significantly benefit the\nlabor-intensive process of report writing by radiologists, especially for 3D\nradiographs like CT scans, which are crucial for broad clinical diagnostics yet\nunderexplored compared to 2D radiographs. Existing methods often handle 3D\nvolumes either slice-wise or with aggressive downsampling due to current GPU\nmemory limitations, which results in a loss of the inherent 3D nature and\ncritical details. To overcome these issues, we introduce a novel framework that\nefficiently and effectively generates radiology reports for high-resolution\n(HR) 3D volumes, based on large language models (LLMs). Specifically, our\nframework utilizes low-resolution (LR) visual tokens as queries to mine\ninformation from HR tokens, preserving detailed HR information while reducing\ncomputational costs by only processing HR informed LR visual queries. Further\nbenefiting the field, we curate and release BIMCV-RG, a new dataset with 5,328\nHR 3D volumes and paired reports, establishing the first benchmarks for report\ngeneration from 3D HR medical images. Our method consistently surpasses\nexisting methods on this benchmark across three different settings:\nnormal-resolution, high-resolution inputs, and zero-shot domain transfer, all\nat an acceptable computational cost, trainable on a single A100-80G.\n","authors":["Che Liu","Zhongwei Wan","Yuqi Wang","Hui Shen","Haozhe Wang","Kangyu Zheng","Mi Zhang","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2406.07146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13478v2","updated":"2024-06-11T10:18:08Z","published":"2024-01-24T14:23:12Z","title":"SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval","summary":"  Multi-modal information retrieval (MMIR) is a rapidly evolving field, where\nsignificant progress, particularly in image-text pairing, has been made through\nadvanced representation learning and cross-modality alignment research.\nHowever, current benchmarks for evaluating MMIR performance in image-text\npairing within the scientific domain show a notable gap, where chart and table\nimages described in scholarly language usually do not play a significant role.\nTo bridge this gap, we develop a specialised scientific MMIR (SciMMIR)\nbenchmark by leveraging open-access paper collections to extract data relevant\nto the scientific domain. This benchmark comprises 530K meticulously curated\nimage-text pairs, extracted from figures and tables with detailed captions in\nscientific documents. We further annotate the image-text pairs with two-level\nsubset-subcategory hierarchy annotations to facilitate a more comprehensive\nevaluation of the baselines. We conducted zero-shot and fine-tuning evaluations\non prominent multi-modal image-captioning and visual language models, such as\nCLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific\ndomain, including the impact of pre-training and fine-tuning settings and the\ninfluence of the visual and textual encoders. All our data and checkpoints are\npublicly available at https://github.com/Wusiwei0410/SciMMIR.\n","authors":["Siwei Wu","Yizhi Li","Kang Zhu","Ge Zhang","Yiming Liang","Kaijing Ma","Chenghao Xiao","Haoran Zhang","Bohao Yang","Wenhu Chen","Wenhao Huang","Noura Al Moubayed","Jie Fu","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2401.13478v2.pdf","comment":"camera-ready version for ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.07119v1","updated":"2024-06-11T10:06:53Z","published":"2024-06-11T10:06:53Z","title":"T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language\n  Production from Text","summary":"  In this work, we propose a two-stage sign language production (SLP) paradigm\nthat first encodes sign language sequences into discrete codes and then\nautoregressively generates sign language from text based on the learned\ncodebook. However, existing vector quantization (VQ) methods are fixed-length\nencodings, overlooking the uneven information density in sign language, which\nleads to under-encoding of important regions and over-encoding of unimportant\nregions. To address this issue, we propose a novel dynamic vector quantization\n(DVA-VAE) model that can dynamically adjust the encoding length based on the\ninformation density in sign language to achieve accurate and compact encoding.\nThen, a GPT-like model learns to generate code sequences and their\ncorresponding durations from spoken language text. Extensive experiments\nconducted on the PHOENIX14T dataset demonstrate the effectiveness of our\nproposed method. To promote sign language research, we propose a new large\nGerman sign language dataset, PHOENIX-News, which contains 486 hours of sign\nlanguage videos, audio, and transcription texts.Experimental analysis on\nPHOENIX-News shows that the performance of our model can be further improved by\nincreasing the size of the training data. Our project homepage is\nhttps://t2sgpt-demo.yinaoxiong.cn.\n","authors":["Aoxiong Yin","Haoyuan Li","Kai Shen","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2406.07119v1.pdf","comment":"Accepted by ACL 2024"},{"id":"http://arxiv.org/abs/2406.07113v1","updated":"2024-06-11T09:57:04Z","published":"2024-06-11T09:57:04Z","title":"Beyond Bare Queries: Open-Vocabulary Object Retrieval with 3D Scene\n  Graph","summary":"  Locating objects referred to in natural language poses a significant\nchallenge for autonomous agents. Existing CLIP-based open-vocabulary methods\nsuccessfully perform 3D object retrieval with simple (bare) queries but cannot\ncope with ambiguous descriptions that demand an understanding of object\nrelations. To tackle this problem, we propose a modular approach called BBQ\n(Beyond Bare Queries), which constructs 3D scene spatial graph representation\nwith metric edges and utilizes a large language model as a human-to-agent\ninterface through our deductive scene reasoning algorithm. BBQ employs robust\nDINO-powered associations to form 3D objects, an advanced raycasting algorithm\nto project them to 2D, and a vision-language model to describe them as graph\nnodes. On Replica and ScanNet datasets, we show that the designed method\naccurately constructs 3D object-centric maps. We have demonstrated that their\nquality takes a leading place for open-vocabulary 3D semantic segmentation\nagainst other zero-shot methods. Also, we show that leveraging spatial\nrelations is especially effective for scenes containing multiple entities of\nthe same semantic class. On Sr3D and Nr3D benchmarks, our deductive approach\ndemonstrates a significant improvement, enabling retrieving objects by complex\nqueries compared to other state-of-the-art methods. Considering our design\nsolutions, we achieved a processing speed approximately x3 times faster than\nthe closest analog. This promising performance enables our approach for usage\nin applied intelligent robotics projects. We make the code publicly available\nat linukc.github.io/bbq/.\n","authors":["Sergey Linok","Tatiana Zemskova","Svetlana Ladanova","Roman Titkov","Dmitry Yudin"],"pdf_url":"https://arxiv.org/pdf/2406.07113v1.pdf","comment":"9 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.03461v2","updated":"2024-06-11T09:56:15Z","published":"2024-06-05T17:09:51Z","title":"Polarization Wavefront Lidar: Learning Large Scene Reconstruction from\n  Polarized Wavefronts","summary":"  Lidar has become a cornerstone sensing modality for 3D vision, especially for\nlarge outdoor scenarios and autonomous driving. Conventional lidar sensors are\ncapable of providing centimeter-accurate distance information by emitting laser\npulses into a scene and measuring the time-of-flight (ToF) of the reflection.\nHowever, the polarization of the received light that depends on the surface\norientation and material properties is usually not considered. As such, the\npolarization modality has the potential to improve scene reconstruction beyond\ndistance measurements. In this work, we introduce a novel long-range\npolarization wavefront lidar sensor (PolLidar) that modulates the polarization\nof the emitted and received light. Departing from conventional lidar sensors,\nPolLidar allows access to the raw time-resolved polarimetric wavefronts. We\nleverage polarimetric wavefronts to estimate normals, distance, and material\nproperties in outdoor scenarios with a novel learned reconstruction method. To\ntrain and evaluate the method, we introduce a simulated and real-world\nlong-range dataset with paired raw lidar data, ground truth distance, and\nnormal maps. We find that the proposed method improves normal and distance\nreconstruction by 53\\% mean angular error and 41\\% mean absolute error compared\nto existing shape-from-polarization (SfP) and ToF methods. Code and data are\nopen-sourced at https://light.princeton.edu/pollidar.\n","authors":["Dominik Scheuble","Chenyang Lei","Seung-Hwan Baek","Mario Bijelic","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2406.03461v2.pdf","comment":"Accepted at CVPR 2024; Project Website:\n  https://light.princeton.edu/publication/pollidar"},{"id":"http://arxiv.org/abs/2311.14772v2","updated":"2024-06-11T09:53:51Z","published":"2023-11-24T13:37:19Z","title":"Trainwreck: A damaging adversarial attack on image classifiers","summary":"  Adversarial attacks are an important security concern for computer vision\n(CV). As CV models are becoming increasingly valuable assets in applied\npractice, disrupting them is emerging as a form of economic sabotage. This\npaper opens up the exploration of damaging adversarial attacks (DAAs) that seek\nto damage target CV models. DAAs are formalized by defining the threat model,\nthe cost function DAAs maximize, and setting three requirements for success:\npotency, stealth, and customizability. As a pioneer DAA, this paper proposes\nTrainwreck, a train-time attack that conflates the data of similar classes in\nthe training data using stealthy ($\\epsilon \\leq 8/255$) class-pair universal\nperturbations obtained from a surrogate model. Trainwreck is a black-box,\ntransferable attack: it requires no knowledge of the target architecture, and a\nsingle poisoned dataset degrades the performance of any model trained on it.\nThe experimental evaluation on CIFAR-10 and CIFAR-100 and various model\narchitectures (EfficientNetV2, ResNeXt-101, and a finetuned ViT-L-16)\ndemonstrates Trainwreck's efficiency. Trainwreck achieves similar or better\npotency compared to the data poisoning state of the art and is fully\ncustomizable by the poison rate parameter. Finally, data redundancy with\nhashing is identified as a reliable defense against Trainwreck or similar DAAs.\nThe code is available at https://github.com/JanZahalka/trainwreck.\n","authors":["Jan Zahálka"],"pdf_url":"https://arxiv.org/pdf/2311.14772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07111v1","updated":"2024-06-11T09:53:18Z","published":"2024-06-11T09:53:18Z","title":"NeRSP: Neural 3D Reconstruction for Reflective Objects with Sparse\n  Polarized Images","summary":"  We present NeRSP, a Neural 3D reconstruction technique for Reflective\nsurfaces with Sparse Polarized images. Reflective surface reconstruction is\nextremely challenging as specular reflections are view-dependent and thus\nviolate the multiview consistency for multiview stereo. On the other hand,\nsparse image inputs, as a practical capture setting, commonly cause incomplete\nor distorted results due to the lack of correspondence matching. This paper\njointly handles the challenges from sparse inputs and reflective surfaces by\nleveraging polarized images. We derive photometric and geometric cues from the\npolarimetric image formation model and multiview azimuth consistency, which\njointly optimize the surface geometry modeled via implicit neural\nrepresentation. Based on the experiments on our synthetic and real datasets, we\nachieve the state-of-the-art surface reconstruction results with only 6 views\nas input.\n","authors":["Yufei Han","Heng Guo","Koki Fukai","Hiroaki Santo","Boxin Shi","Fumio Okura","Zhanyu Ma","Yunpeng Jia"],"pdf_url":"https://arxiv.org/pdf/2406.07111v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2405.12728v3","updated":"2024-06-11T09:42:29Z","published":"2024-05-21T12:34:03Z","title":"Leveraging Neural Radiance Fields for Pose Estimation of an Unknown\n  Space Object during Proximity Operations","summary":"  We address the estimation of the 6D pose of an unknown target spacecraft\nrelative to a monocular camera, a key step towards the autonomous rendezvous\nand proximity operations required by future Active Debris Removal missions. We\npresent a novel method that enables an \"off-the-shelf\" spacecraft pose\nestimator, which is supposed to known the target CAD model, to be applied on an\nunknown target. Our method relies on an in-the wild NeRF, i.e., a Neural\nRadiance Field that employs learnable appearance embeddings to represent\nvarying illumination conditions found in natural scenes. We train the NeRF\nmodel using a sparse collection of images that depict the target, and in turn\ngenerate a large dataset that is diverse both in terms of viewpoint and\nillumination. This dataset is then used to train the pose estimation network.\nWe validate our method on the Hardware-In-the-Loop images of SPEED+ that\nemulate lighting conditions close to those encountered on orbit. We demonstrate\nthat our method successfully enables the training of an off-the-shelf\nspacecraft pose estimation network from a sparse set of images. Furthermore, we\nshow that a network trained using our method performs similarly to a model\ntrained on synthetic images generated using the CAD model of the target.\n","authors":["Antoine Legrand","Renaud Detry","Christophe De Vleeschouwer"],"pdf_url":"https://arxiv.org/pdf/2405.12728v3.pdf","comment":"Accepted at IEEE International Conference on Space Robotics 2024\n  (ISpaRo 2024), Workshop on Advances in Orbital Robotics: In Orbit\n  Manipulation, Servicing, and Assembly"},{"id":"http://arxiv.org/abs/2405.11985v2","updated":"2024-06-11T09:32:56Z","published":"2024-05-20T12:35:01Z","title":"MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering","summary":"  Text-Centric Visual Question Answering (TEC-VQA) in its proper format not\nonly facilitates human-machine interaction in text-centric visual environments\nbut also serves as a de facto gold proxy to evaluate AI models in the domain of\ntext-centric scene understanding. Nonetheless, most existing TEC-VQA benchmarks\nhave focused on high-resource languages like English and Chinese. Despite\npioneering works to expand multilingual QA pairs in non-text-centric VQA\ndatasets through translation engines, the translation-based protocol encounters\na substantial \"visual-textual misalignment\" problem when applied to TEC-VQA.\nSpecifically, it prioritizes the text in question-answer pairs while\ndisregarding the visual text present in images. Moreover, it fails to address\ncomplexities related to nuanced meaning, contextual distortion, language bias,\nand question-type diversity. In this work, we tackle multilingual TEC-VQA by\nintroducing MTVQA, the first benchmark featuring high-quality human expert\nannotations across 9 diverse languages, consisting of 6,778 question-answer\npairs across 2,116 images. Further, by comprehensively evaluating numerous\nstate-of-the-art Multimodal Large Language Models (MLLMs), including GPT-4o,\nGPT-4V, Claude3, and Gemini, on the MTVQA dataset, it is evident that there is\nstill a large room for performance improvement, underscoring the value of\nMTVQA. Additionally, we supply multilingual training data within the MTVQA\ndataset, demonstrating that straightforward fine-tuning with this data can\nsubstantially enhance multilingual TEC-VQA performance. We aspire that MTVQA\nwill offer the research community fresh insights and stimulate further\nexploration in multilingual visual text comprehension. The project homepage is\navailable at https://bytedance.github.io/MTVQA/.\n","authors":["Jingqun Tang","Qi Liu","Yongjie Ye","Jinghui Lu","Shu Wei","Chunhui Lin","Wanqing Li","Mohamad Fitri Faiz Bin Mahmood","Hao Feng","Zhen Zhao","Yanjie Wang","Yuliang Liu","Hao Liu","Xiang Bai","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2405.11985v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07091v1","updated":"2024-06-11T09:31:37Z","published":"2024-06-11T09:31:37Z","title":"AutoTVG: A New Vision-language Pre-training Paradigm for Temporal Video\n  Grounding","summary":"  Temporal Video Grounding (TVG) aims to localize a moment from an untrimmed\nvideo given the language description. Since the annotation of TVG is\nlabor-intensive, TVG under limited supervision has accepted attention in recent\nyears. The great success of vision-language pre-training guides TVG to follow\nthe traditional \"pre-training + fine-tuning\" paradigm, however, the\npre-training process would suffer from a lack of temporal modeling and\nfine-grained alignment due to the difference of data nature between pre-train\nand test. Besides, the large gap between pretext and downstream tasks makes\nzero-shot testing impossible for the pre-trained model. To avoid the drawbacks\nof the traditional paradigm, we propose AutoTVG, a new vision-language\npre-training paradigm for TVG that enables the model to learn semantic\nalignment and boundary regression from automatically annotated untrimmed\nvideos. To be specific, AutoTVG consists of a novel Captioned Moment Generation\n(CMG) module to generate captioned moments from untrimmed videos, and TVGNet\nwith a regression head to predict localization results. Experimental results on\nCharades-STA and ActivityNet Captions show that, regarding zero-shot temporal\nvideo grounding, AutoTVG achieves highly competitive performance with\nin-distribution methods under out-of-distribution testing, and is superior to\nexisting pre-training frameworks with much less training data.\n","authors":["Xing Zhang","Jiaxi Gu","Haoyu Zhao","Shicong Wang","Hang Xu","Renjing Pei","Songcen Xu","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.07091v1.pdf","comment":"Technique Report"},{"id":"http://arxiv.org/abs/2406.07089v1","updated":"2024-06-11T09:30:02Z","published":"2024-06-11T09:30:02Z","title":"RS-Agent: Automating Remote Sensing Tasks through Intelligent Agents","summary":"  An increasing number of models have achieved great performance in remote\nsensing tasks with the recent development of Large Language Models (LLMs) and\nVisual Language Models (VLMs). However, these models are constrained to basic\nvision and language instruction-tuning tasks, facing challenges in complex\nremote sensing applications. Additionally, these models lack specialized\nexpertise in professional domains. To address these limitations, we propose a\nLLM-driven remote sensing intelligent agent named RS-Agent. Firstly, RS-Agent\nis powered by a large language model (LLM) that acts as its \"Central\nController,\" enabling it to understand and respond to various problems\nintelligently. Secondly, our RS-Agent integrates many high-performance remote\nsensing image processing tools, facilitating multi-tool and multi-turn\nconversations. Thirdly, our RS-Agent can answer professional questions by\nleveraging robust knowledge documents. We conducted experiments using several\ndatasets, e.g., RSSDIVCS, RSVQA, and DOTAv1. The experimental results\ndemonstrate that our RS-Agent delivers outstanding performance in many tasks,\ni.e., scene classification, visual question answering, and object counting\ntasks.\n","authors":["Wenjia Xu","Zijian Yu","Yixu Wang","Jiuniu Wang","Mugen Peng"],"pdf_url":"https://arxiv.org/pdf/2406.07089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07085v1","updated":"2024-06-11T09:22:39Z","published":"2024-06-11T09:22:39Z","title":"CAT: Coordinating Anatomical-Textual Prompts for Multi-Organ and Tumor\n  Segmentation","summary":"  Existing promptable segmentation methods in the medical imaging field\nprimarily consider either textual or visual prompts to segment relevant\nobjects, yet they often fall short when addressing anomalies in medical images,\nlike tumors, which may vary greatly in shape, size, and appearance. Recognizing\nthe complexity of medical scenarios and the limitations of textual or visual\nprompts, we propose a novel dual-prompt schema that leverages the complementary\nstrengths of visual and textual prompts for segmenting various organs and\ntumors. Specifically, we introduce CAT, an innovative model that Coordinates\nAnatomical prompts derived from 3D cropped images with Textual prompts enriched\nby medical domain knowledge. The model architecture adopts a general\nquery-based design, where prompt queries facilitate segmentation queries for\nmask prediction. To synergize two types of prompts within a unified framework,\nwe implement a ShareRefiner, which refines both segmentation and prompt queries\nwhile disentangling the two types of prompts. Trained on a consortium of 10\npublic CT datasets, CAT demonstrates superior performance in multiple\nsegmentation tasks. Further validation on a specialized in-house dataset\nreveals the remarkable capacity of segmenting tumors across multiple cancer\nstages. This approach confirms that coordinating multimodal prompts is a\npromising avenue for addressing complex scenarios in the medical domain.\n","authors":["Zhongzhen Huang","Yankai Jiang","Rongzhao Zhang","Shaoting Zhang","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15323v2","updated":"2024-06-11T09:19:24Z","published":"2023-08-29T14:20:13Z","title":"Occlusion-Aware Deep Convolutional Neural Network via Homogeneous\n  Tanh-transforms for Face Parsing","summary":"  Face parsing infers a pixel-wise label map for each semantic facial\ncomponent. Previous methods generally work well for uncovered faces, however,\nthey overlook facial occlusion and ignore some contextual areas outside a\nsingle face, especially when facial occlusion has become a common situation\nduring the COVID-19 epidemic. Inspired by the lighting phenomena in everyday\nlife, where illumination from four distinct lamps provides a more uniform\ndistribution than a single central light source, we propose a novel homogeneous\ntanh-transform for image preprocessing, which is made up of four\ntanh-transforms. These transforms fuse the central vision and the peripheral\nvision together. Our proposed method addresses the dilemma of face parsing\nunder occlusion and compresses more information from the surrounding context.\nBased on homogeneous tanh-transforms, we propose an occlusion-aware\nconvolutional neural network for occluded face parsing. It combines information\nin both Tanh-polar space and Tanh-Cartesian space, capable of enhancing\nreceptive fields. Furthermore, we introduce an occlusion-aware loss to focus on\nthe boundaries of occluded regions. The network is simple, flexible, and can be\ntrained end-to-end. To facilitate future research of occluded face parsing, we\nalso contribute a new cleaned face parsing dataset. This dataset is manually\npurified from several academic or industrial datasets, including CelebAMask-HQ,\nShort-video Face Parsing, and the Helen dataset, and will be made public.\nExperiments demonstrate that our method surpasses state-of-the-art methods in\nface parsing under occlusion.\n","authors":["Jianhua Qiua","Weihua Liu","Chaochao Lin","Jiaojiao Li","Haoping Yu","Said Boumaraf"],"pdf_url":"https://arxiv.org/pdf/2308.15323v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00335v2","updated":"2024-06-11T09:17:59Z","published":"2023-12-01T04:07:12Z","title":"Learning Anatomically Consistent Embedding for Chest Radiography","summary":"  Self-supervised learning (SSL) approaches have recently shown substantial\nsuccess in learning visual representations from unannotated images. Compared\nwith photographic images, medical images acquired with the same imaging\nprotocol exhibit high consistency in anatomy. To exploit this anatomical\nconsistency, this paper introduces a novel SSL approach, called PEAC (patch\nembedding of anatomical consistency), for medical image analysis. Specifically,\nin this paper, we propose to learn global and local consistencies via stable\ngrid-based matching, transfer pre-trained PEAC models to diverse downstream\ntasks, and extensively demonstrate that (1) PEAC achieves significantly better\nperformance than the existing state-of-the-art fully/self-supervised methods,\nand (2) PEAC captures the anatomical structure consistency across views of the\nsame patient and across patients of different genders, weights, and healthy\nstatuses, which enhances the interpretability of our method for medical image\nanalysis.\n","authors":["Ziyu Zhou","Haozhe Luo","Jiaxuan Pang","Xiaowei Ding","Michael Gotway","Jianming Liang"],"pdf_url":"https://arxiv.org/pdf/2312.00335v2.pdf","comment":"BMVC 2023, oral"},{"id":"http://arxiv.org/abs/2406.07078v1","updated":"2024-06-11T09:06:41Z","published":"2024-06-11T09:06:41Z","title":"Unified Modeling Enhanced Multimodal Learning for Precision\n  Neuro-Oncology","summary":"  Multimodal learning, integrating histology images and genomics, promises to\nenhance precision oncology with comprehensive views at microscopic and\nmolecular levels. However, existing methods may not sufficiently model the\nshared or complementary information for more effective integration. In this\nstudy, we introduce a Unified Modeling Enhanced Multimodal Learning (UMEML)\nframework that employs a hierarchical attention structure to effectively\nleverage shared and complementary features of both modalities of histology and\ngenomics. Specifically, to mitigate unimodal bias from modality imbalance, we\nutilize a query-based cross-attention mechanism for prototype clustering in the\npathology encoder. Our prototype assignment and modularity strategy are\ndesigned to align shared features and minimizes modality gaps. An additional\nregistration mechanism with learnable tokens is introduced to enhance\ncross-modal feature integration and robustness in multimodal unified modeling.\nOur experiments demonstrate that our method surpasses previous state-of-the-art\napproaches in glioma diagnosis and prognosis tasks, underscoring its\nsuperiority in precision neuro-Oncology.\n","authors":["Huahui Yi","Xiaofei Wang","Kang Li","Chao Li"],"pdf_url":"https://arxiv.org/pdf/2406.07078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.01928v2","updated":"2024-06-11T08:49:25Z","published":"2023-05-03T07:02:57Z","title":"Visual Transformation Telling","summary":"  Humans can naturally reason from superficial state differences (e.g. ground\nwetness) to transformations descriptions (e.g. raining) according to their life\nexperience. In this paper, we propose a new visual reasoning task to test this\ntransformation reasoning ability in real-world scenarios, called\n\\textbf{V}isual \\textbf{T}ransformation \\textbf{T}elling (VTT). Given a series\nof states (i.e. images), VTT requires to describe the transformation occurring\nbetween every two adjacent states. Different from existing visual reasoning\ntasks that focus on surface state reasoning, the advantage of VTT is that it\ncaptures the underlying causes, e.g. actions or events, behind the differences\namong states. We collect a novel dataset to support the study of transformation\nreasoning from two existing instructional video datasets, CrossTask and COIN,\ncomprising 13,547 samples. Each sample involves the key state images along with\ntheir transformation descriptions. Our dataset covers diverse real-world\nactivities, providing a rich resource for training and evaluation. To construct\nan initial benchmark for VTT, we test several models, including traditional\nvisual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal\nlarge language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o,\nand GPT-4). Experimental results reveal that even state-of-the-art models still\nface challenges in VTT, highlighting substantial areas for improvement.\n","authors":["Wanqing Cui","Xin Hong","Yanyan Lan","Liang Pang","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2305.01928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07061v1","updated":"2024-06-11T08:42:07Z","published":"2024-06-11T08:42:07Z","title":"Triage of 3D pathology data via 2.5D multiple-instance learning to guide\n  pathologist assessments","summary":"  Accurate patient diagnoses based on human tissue biopsies are hindered by\ncurrent clinical practice, where pathologists assess only a limited number of\nthin 2D tissue slices sectioned from 3D volumetric tissue. Recent advances in\nnon-destructive 3D pathology, such as open-top light-sheet microscopy, enable\ncomprehensive imaging of spatially heterogeneous tissue morphologies, offering\nthe feasibility to improve diagnostic determinations. A potential early route\ntowards clinical adoption for 3D pathology is to rely on pathologists for final\ndiagnosis based on viewing familiar 2D H&E-like image sections from the 3D\ndatasets. However, manual examination of the massive 3D pathology datasets is\ninfeasible. To address this, we present CARP3D, a deep learning triage approach\nthat automatically identifies the highest-risk 2D slices within 3D volumetric\nbiopsy, enabling time-efficient review by pathologists. For a given slice in\nthe biopsy, we estimate its risk by performing attention-based aggregation of\n2D patches within each slice, followed by pooling of the neighboring slices to\ncompute a context-aware 2.5D risk score. For prostate cancer risk\nstratification, CARP3D achieves an area under the curve (AUC) of 90.4% for\ntriaging slices, outperforming methods relying on independent analysis of 2D\nsections (AUC=81.3%). These results suggest that integrating additional depth\ncontext enhances the model's discriminative capabilities. In conclusion, CARP3D\nhas the potential to improve pathologist diagnosis via accurate triage of\nhigh-risk slices within large-volume 3D pathology datasets.\n","authors":["Gan Gao","Andrew H. Song","Fiona Wang","David Brenes","Rui Wang","Sarah S. L. Chow","Kevin W. Bishop","Lawrence D. True","Faisal Mahmood","Jonathan T. C. Liu"],"pdf_url":"https://arxiv.org/pdf/2406.07061v1.pdf","comment":"CVPR CVMI 2024"},{"id":"http://arxiv.org/abs/2406.07057v1","updated":"2024-06-11T08:38:13Z","published":"2024-06-11T08:38:13Z","title":"Benchmarking Trustworthiness of Multimodal Large Language Models: A\n  Comprehensive Study","summary":"  Despite the superior capabilities of Multimodal Large Language Models (MLLMs)\nacross diverse tasks, they still face significant trustworthiness challenges.\nYet, current literature on the assessment of trustworthy MLLMs remains limited,\nlacking a holistic evaluation to offer thorough insights into future\nimprovements. In this work, we establish MultiTrust, the first comprehensive\nand unified benchmark on the trustworthiness of MLLMs across five primary\naspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark\nemploys a rigorous evaluation strategy that addresses both multimodal risks and\ncross-modal impacts, encompassing 32 diverse tasks with self-curated datasets.\nExtensive experiments with 21 modern MLLMs reveal some previously unexplored\ntrustworthiness issues and risks, highlighting the complexities introduced by\nthe multimodality and underscoring the necessity for advanced methodologies to\nenhance their reliability. For instance, typical proprietary models still\nstruggle with the perception of visually confusing images and are vulnerable to\nmultimodal jailbreaking and adversarial attacks; MLLMs are more inclined to\ndisclose privacy in text and reveal ideological and cultural biases even when\npaired with irrelevant images in inference, indicating that the multimodality\namplifies the internal risks from base LLMs. Additionally, we release a\nscalable toolbox for standardized trustworthiness research, aiming to\nfacilitate future advancements in this important field. Code and resources are\npublicly available at: https://multi-trust.github.io/.\n","authors":["Yichi Zhang","Yao Huang","Yitong Sun","Chang Liu","Zhe Zhao","Zhengwei Fang","Yifan Wang","Huanran Chen","Xiao Yang","Xingxing Wei","Hang Su","Yinpeng Dong","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.07057v1.pdf","comment":"100 pages, 84 figures, 33 tables"},{"id":"http://arxiv.org/abs/2406.07050v1","updated":"2024-06-11T08:26:42Z","published":"2024-06-11T08:26:42Z","title":"DualMamba: A Lightweight Spectral-Spatial Mamba-Convolution Network for\n  Hyperspectral Image Classification","summary":"  The effectiveness and efficiency of modeling complex spectral-spatial\nrelations are both crucial for Hyperspectral image (HSI) classification. Most\nexisting methods based on CNNs and transformers still suffer from heavy\ncomputational burdens and have room for improvement in capturing the\nglobal-local spectral-spatial feature representation. To this end, we propose a\nnovel lightweight parallel design called lightweight dual-stream\nMamba-convolution network (DualMamba) for HSI classification. Specifically, a\nparallel lightweight Mamba and CNN block are first developed to extract global\nand local spectral-spatial features. First, the cross-attention\nspectral-spatial Mamba module is proposed to leverage the global modeling of\nMamba at linear complexity. Within this module, dynamic positional embedding is\ndesigned to enhance the spatial location information of visual sequences. The\nlightweight spectral/spatial Mamba blocks comprise an efficient scanning\nstrategy and a lightweight Mamba design to efficiently extract global\nspectral-spatial features. And the cross-attention spectral-spatial fusion is\ndesigned to learn cross-correlation and fuse spectral-spatial features. Second,\nthe lightweight spectral-spatial residual convolution module is proposed with\nlightweight spectral and spatial branches to extract local spectral-spatial\nfeatures through residual learning. Finally, the adaptive global-local fusion\nis proposed to dynamically combine global Mamba features and local convolution\nfeatures for a global-local spectral-spatial representation. Compared with\nstate-of-the-art HSI classification methods, experimental results demonstrate\nthat DualMamba achieves significant classification accuracy on three public HSI\ndatasets and a superior reduction in model parameters and floating point\noperations (FLOPs).\n","authors":["Jiamu Sheng","Jingyi Zhou","Jiong Wang","Peng Ye","Jiayuan Fan"],"pdf_url":"https://arxiv.org/pdf/2406.07050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07043v1","updated":"2024-06-11T08:05:26Z","published":"2024-06-11T08:05:26Z","title":"1st Place Solution for MeViS Track in CVPR 2024 PVUW Workshop: Motion\n  Expression guided Video Segmentation","summary":"  Motion Expression guided Video Segmentation (MeViS), as an emerging task,\nposes many new challenges to the field of referring video object segmentation\n(RVOS). In this technical report, we investigated and validated the\neffectiveness of static-dominant data and frame sampling on this challenging\nsetting. Our solution achieves a J&F score of 0.5447 in the competition phase\nand ranks 1st in the MeViS track of the PVUW Challenge. The code is available\nat: https://github.com/Tapall-AI/MeViS_Track_Solution_2024.\n","authors":["Mingqi Gao","Jingnan Luo","Jinyu Yang","Jungong Han","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.07043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07042v1","updated":"2024-06-11T08:01:02Z","published":"2024-06-11T08:01:02Z","title":"EFFOcc: A Minimal Baseline for EFficient Fusion-based 3D Occupancy\n  Network","summary":"  3D occupancy prediction (Occ) is a rapidly rising challenging perception task\nin the field of autonomous driving which represents the driving scene as\nuniformly partitioned 3D voxel grids with semantics. Compared to 3D object\ndetection, grid perception has great advantage of better recognizing\nirregularly shaped, unknown category, or partially occluded general objects.\nHowever, existing 3D occupancy networks (occnets) are both computationally\nheavy and label-hungry. In terms of model complexity, occnets are commonly\ncomposed of heavy Conv3D modules or transformers on the voxel level. In terms\nof label annotations requirements, occnets are supervised with large-scale\nexpensive dense voxel labels. Model and data inefficiency, caused by excessive\nnetwork parameters and label annotations requirement, severely hinder the\nonboard deployment of occnets. This paper proposes an efficient 3d occupancy\nnetwork (EFFOcc), that targets the minimal network complexity and label\nrequirement while achieving state-of-the-art accuracy. EFFOcc only uses simple\n2D operators, and improves Occ accuracy to the state-of-the-art on multiple\nlarge-scale benchmarks: Occ3D-nuScenes, Occ3D-Waymo, and\nOpenOccupancy-nuScenes. On Occ3D-nuScenes benchmark, EFFOcc has only 18.4M\nparameters, and achieves 50.46 in terms of mean IoU (mIoU), to our knowledge,\nit is the occnet with minimal parameters compared with related occnets.\nMoreover, we propose a two-stage active learning strategy to reduce the\nrequirements of labelled data. Active EFFOcc trained with 6\\% labelled voxels\nachieves 47.19 mIoU, which is 95.7% fully supervised performance. The proposed\nEFFOcc also supports improved vision-only occupancy prediction with the aid of\nregion-decomposed distillation. Code and demo videos will be available at\nhttps://github.com/synsin0/EFFOcc.\n","authors":["Yining Shi","Kun Jiang","Ke Wang","Kangan Qian","Yunlong Wang","Jiusi Li","Tuopu Wen","Mengmeng Yang","Yiliang Xu","Diange Yang"],"pdf_url":"https://arxiv.org/pdf/2406.07042v1.pdf","comment":"preprint under review"},{"id":"http://arxiv.org/abs/2406.07037v1","updated":"2024-06-11T07:51:26Z","published":"2024-06-11T07:51:26Z","title":"PanoSSC: Exploring Monocular Panoptic 3D Scene Reconstruction for\n  Autonomous Driving","summary":"  Vision-centric occupancy networks, which represent the surrounding\nenvironment with uniform voxels with semantics, have become a new trend for\nsafe driving of camera-only autonomous driving perception systems, as they are\nable to detect obstacles regardless of their shape and occlusion. Modern\noccupancy networks mainly focus on reconstructing visible voxels from object\nsurfaces with voxel-wise semantic prediction. Usually, they suffer from\ninconsistent predictions of one object and mixed predictions for adjacent\nobjects. These confusions may harm the safety of downstream planning modules.\nTo this end, we investigate panoptic segmentation on 3D voxel scenarios and\npropose an instance-aware occupancy network, PanoSSC. We predict foreground\nobjects and backgrounds separately and merge both in post-processing. For\nforeground instance grouping, we propose a novel 3D instance mask decoder that\ncan efficiently extract individual objects. we unify geometric reconstruction,\n3D semantic segmentation, and 3D instance segmentation into PanoSSC framework\nand propose new metrics for evaluating panoptic voxels. Extensive experiments\nshow that our method achieves competitive results on SemanticKITTI semantic\nscene completion benchmark.\n","authors":["Yining Shi","Jiusi Li","Kun Jiang","Ke Wang","Yunlong Wang","Mengmeng Yang","Diange Yang"],"pdf_url":"https://arxiv.org/pdf/2406.07037v1.pdf","comment":"3dv2024"},{"id":"http://arxiv.org/abs/2406.05755v2","updated":"2024-06-11T07:50:33Z","published":"2024-06-09T12:18:15Z","title":"A DeNoising FPN With Transformer R-CNN for Tiny Object Detection","summary":"  Despite notable advancements in the field of computer vision, the precise\ndetection of tiny objects continues to pose a significant challenge, largely\nowing to the minuscule pixel representation allocated to these objects in\nimagery data. This challenge resonates profoundly in the domain of geoscience\nand remote sensing, where high-fidelity detection of tiny objects can\nfacilitate a myriad of applications ranging from urban planning to\nenvironmental monitoring. In this paper, we propose a new framework, namely,\nDeNoising FPN with Trans R-CNN (DNTR), to improve the performance of tiny\nobject detection. DNTR consists of an easy plug-in design, DeNoising FPN\n(DN-FPN), and an effective Transformer-based detector, Trans R-CNN.\nSpecifically, feature fusion in the feature pyramid network is important for\ndetecting multiscale objects. However, noisy features may be produced during\nthe fusion process since there is no regularization between the features of\ndifferent scales. Therefore, we introduce a DN-FPN module that utilizes\ncontrastive learning to suppress noise in each level's features in the top-down\npath of FPN. Second, based on the two-stage framework, we replace the obsolete\nR-CNN detector with a novel Trans R-CNN detector to focus on the representation\nof tiny objects with self-attention. Experimental results manifest that our\nDNTR outperforms the baselines by at least 17.4% in terms of APvt on the AI-TOD\ndataset and 9.6% in terms of AP on the VisDrone dataset, respectively. Our code\nwill be available at https://github.com/hoiliu-0801/DNTR.\n","authors":["Hou-I Liu","Yu-Wen Tseng","Kai-Cheng Chang","Pin-Jyun Wang","Hong-Han Shuai","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.05755v2.pdf","comment":"The article is accepted by IEEE Transactions on Geoscience and Remote\n  Sensing. Our code will be available at https://github.com/hoiliu-0801/DNTR"},{"id":"http://arxiv.org/abs/2406.07032v1","updated":"2024-06-11T07:46:47Z","published":"2024-06-11T07:46:47Z","title":"RS-DFM: A Remote Sensing Distributed Foundation Model for Diverse\n  Downstream Tasks","summary":"  Remote sensing lightweight foundation models have achieved notable success in\nonline perception within remote sensing. However, their capabilities are\nrestricted to performing online inference solely based on their own\nobservations and models, thus lacking a comprehensive understanding of\nlarge-scale remote sensing scenarios. To overcome this limitation, we propose a\nRemote Sensing Distributed Foundation Model (RS-DFM) based on generalized\ninformation mapping and interaction. This model can realize online\ncollaborative perception across multiple platforms and various downstream tasks\nby mapping observations into a unified space and implementing a task-agnostic\ninformation interaction strategy. Specifically, we leverage the ground-based\ngeometric prior of remote sensing oblique observations to transform the feature\nmapping from absolute depth estimation to relative depth estimation, thereby\nenhancing the model's ability to extract generalized features across diverse\nheights and perspectives. Additionally, we present a dual-branch information\ncompression module to decouple high-frequency and low-frequency feature\ninformation, achieving feature-level compression while preserving essential\ntask-agnostic details. In support of our research, we create a multi-task\nsimulation dataset named AirCo-MultiTasks for multi-UAV collaborative\nobservation. We also conduct extensive experiments, including 3D object\ndetection, instance segmentation, and trajectory prediction. The numerous\nresults demonstrate that our RS-DFM achieves state-of-the-art performance\nacross various downstream tasks.\n","authors":["Zhechao Wang","Peirui Cheng","Pengju Tian","Yuchao Wang","Mingxin Chen","Shujing Duan","Zhirui Wang","Xinming Li","Xian Sun"],"pdf_url":"https://arxiv.org/pdf/2406.07032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02651v2","updated":"2024-06-11T07:43:22Z","published":"2024-01-05T05:58:22Z","title":"Benchmarking PathCLIP for Pathology Image Analysis","summary":"  Accurate image classification and retrieval are of importance for clinical\ndiagnosis and treatment decision-making. The recent contrastive language-image\npretraining (CLIP) model has shown remarkable proficiency in understanding\nnatural images. Drawing inspiration from CLIP, PathCLIP is specifically\ndesigned for pathology image analysis, utilizing over 200,000 image and text\npairs in training. While the performance the PathCLIP is impressive, its\nrobustness under a wide range of image corruptions remains unknown. Therefore,\nwe conduct an extensive evaluation to analyze the performance of PathCLIP on\nvarious corrupted images from the datasets of Osteosarcoma and WSSS4LUAD. In\nour experiments, we introduce seven corruption types including brightness,\ncontrast, Gaussian blur, resolution, saturation, hue, and markup at four\nseverity levels. Through experiments, we find that PathCLIP is relatively\nrobustness to image corruptions and surpasses OpenAI-CLIP and PLIP in zero-shot\nclassification. Among the seven corruptions, blur and resolution can cause\nserver performance degradation of the PathCLIP. This indicates that ensuring\nthe quality of images is crucial before conducting a clinical test.\nAdditionally, we assess the robustness of PathCLIP in the task of image-image\nretrieval, revealing that PathCLIP performs less effectively than PLIP on\nOsteosarcoma but performs better on WSSS4LUAD under diverse corruptions.\nOverall, PathCLIP presents impressive zero-shot classification and retrieval\nperformance for pathology images, but appropriate care needs to be taken when\nusing it. We hope this study provides a qualitative impression of PathCLIP and\nhelps understand its differences from other CLIP models.\n","authors":["Sunyi Zheng","Xiaonan Cui","Yuxuan Sun","Jingxiong Li","Honglin Li","Yunlong Zhang","Pingyi Chen","Xueping Jing","Zhaoxiang Ye","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.02651v2.pdf","comment":"Needing to make substantial changes that go beyond minor corrections\n  or clarifications, essentially rewriting significant portions of the paper"},{"id":"http://arxiv.org/abs/2403.04306v4","updated":"2024-06-11T07:42:51Z","published":"2024-03-07T08:25:27Z","title":"Effectiveness Assessment of Recent Large Vision-Language Models","summary":"  The advent of large vision-language models (LVLMs) represents a remarkable\nadvance in the quest for artificial general intelligence. However, the model's\neffectiveness in both specialized and general tasks warrants further\ninvestigation. This paper endeavors to evaluate the competency of popular LVLMs\nin specialized and general tasks, respectively, aiming to offer a comprehensive\nunderstanding of these novel models. To gauge their effectiveness in\nspecialized tasks, we employ six challenging tasks in three different\napplication scenarios: natural, healthcare, and industrial. These six tasks\ninclude salient/camouflaged/transparent object detection, as well as polyp\ndetection, skin lesion detection, and industrial anomaly detection. We examine\nthe performance of three recent open-source LVLMs, including MiniGPT-v2,\nLLaVA-1.5, and Shikra, on both visual recognition and localization in these\ntasks. Moreover, we conduct empirical investigations utilizing the\naforementioned LVLMs together with GPT-4V, assessing their multi-modal\nunderstanding capabilities in general tasks including object counting, absurd\nquestion answering, affordance reasoning, attribute recognition, and spatial\nrelation reasoning. Our investigations reveal that these LVLMs demonstrate\nlimited proficiency not only in specialized tasks but also in general tasks. We\ndelve deep into this inadequacy and uncover several potential factors,\nincluding limited cognition in specialized tasks, object hallucination,\ntext-to-image interference, and decreased robustness in complex problems. We\nhope that this study can provide useful insights for the future development of\nLVLMs, helping researchers improve LVLMs for both general and specialized\napplications.\n","authors":["Yao Jiang","Xinyu Yan","Ge-Peng Ji","Keren Fu","Meijun Sun","Huan Xiong","Deng-Ping Fan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.04306v4.pdf","comment":"Accepted by Visual Intelligence"},{"id":"http://arxiv.org/abs/2302.09585v2","updated":"2024-06-11T07:42:12Z","published":"2023-02-19T14:38:01Z","title":"StreamingFlow: Streaming Occupancy Forecasting with Asynchronous\n  Multi-modal Data Streams via Neural Ordinary Differential Equation","summary":"  Predicting the future occupancy states of the surrounding environment is a\nvital task for autonomous driving. However, current best-performing\nsingle-modality methods or multi-modality fusion perception methods are only\nable to predict uniform snapshots of future occupancy states and require\nstrictly synchronized sensory data for sensor fusion. We propose a novel\nframework, StreamingFlow, to lift these strong limitations. StreamingFlow is a\nnovel BEV occupancy predictor that ingests asynchronous multi-sensor data\nstreams for fusion and performs streaming forecasting of the future occupancy\nmap at any future timestamps. By integrating neural ordinary differential\nequations (N-ODE) into recurrent neural networks, StreamingFlow learns\nderivatives of BEV features over temporal horizons, updates the implicit\nsensor's BEV features as part of the fusion process, and propagates BEV states\nto the desired future time point. It shows good zero-shot generalization\nability of prediction, reflected in the interpolation of the observed\nprediction time horizon and the reasonable inference of the unseen farther\nfuture period. Extensive experiments on two large-scale datasets, nuScenes and\nLyft L5, demonstrate that StreamingFlow significantly outperforms previous\nvision-based, LiDAR-based methods, and shows superior performance compared to\nstate-of-the-art fusion-based methods.\n","authors":["Yining Shi","Kun Jiang","Ke Wang","Jiusi Li","Yunlong Wang","Mengmeng Yang","Diange Yang"],"pdf_url":"https://arxiv.org/pdf/2302.09585v2.pdf","comment":"cvpr2024 poster (highlight), code at\n  https://github.com/synsin0/StreamingFlow"},{"id":"http://arxiv.org/abs/2405.17158v3","updated":"2024-06-11T07:29:43Z","published":"2024-05-27T13:31:46Z","title":"PatchScaler: An Efficient Patch-Independent Diffusion Model for\n  Super-Resolution","summary":"  Diffusion models significantly improve the quality of super-resolved images\nwith their impressive content generation capabilities. However, the huge\ncomputational costs limit the applications of these methods.Recent efforts have\nexplored reasonable inference acceleration to reduce the number of sampling\nsteps, but the computational cost remains high as each step is performed on the\nentire image.This paper introduces PatchScaler, a patch-independent\ndiffusion-based single image super-resolution (SR) method, designed to enhance\nthe efficiency of the inference process.The proposed method is motivated by the\nobservation that not all the image patches within an image need the same\nsampling steps for reconstructing high-resolution images.Based on this\nobservation, we thus develop a Patch-adaptive Group Sampling (PGS) to divide\nfeature patches into different groups according to the patch-level\nreconstruction difficulty and dynamically assign an appropriate sampling\nconfiguration for each group so that the inference speed can be better\naccelerated.In addition, to improve the denoising ability at each step of the\nsampling, we develop a texture prompt to guide the estimations of the diffusion\nmodel by retrieving high-quality texture priors from a patch-independent\nreference texture memory.Experiments show that our PatchScaler achieves\nfavorable performance in both quantitative and qualitative evaluations with\nfast inference speed.Our code and model are available at\n\\url{https://github.com/yongliuy/PatchScaler}.\n","authors":["Yong Liu","Hang Dong","Jinshan Pan","Qingji Dong","Kai Chen","Rongxiang Zhang","Lean Fu","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2405.17158v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07023v1","updated":"2024-06-11T07:26:54Z","published":"2024-06-11T07:26:54Z","title":"LiSD: An Efficient Multi-Task Learning Framework for LiDAR Segmentation\n  and Detection","summary":"  With the rapid proliferation of autonomous driving, there has been a\nheightened focus on the research of lidar-based 3D semantic segmentation and\nobject detection methodologies, aiming to ensure the safety of traffic\nparticipants. In recent decades, learning-based approaches have emerged,\ndemonstrating remarkable performance gains in comparison to conventional\nalgorithms. However, the segmentation and detection tasks have traditionally\nbeen examined in isolation to achieve the best precision. To this end, we\npropose an efficient multi-task learning framework named LiSD which can address\nboth segmentation and detection tasks, aiming to optimize the overall\nperformance. Our proposed LiSD is a voxel-based encoder-decoder framework that\ncontains a hierarchical feature collaboration module and a holistic information\naggregation module. Different integration methods are adopted to keep sparsity\nin segmentation while densifying features for query initialization in\ndetection. Besides, cross-task information is utilized in an instance-aware\nrefinement module to obtain more accurate predictions. Experimental results on\nthe nuScenes dataset and Waymo Open Dataset demonstrate the effectiveness of\nour proposed model. It is worth noting that LiSD achieves the state-of-the-art\nperformance of 83.3% mIoU on the nuScenes segmentation benchmark for lidar-only\nmethods.\n","authors":["Jiahua Xu","Si Zuo","Chenfeng Wei","Wei Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.07023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01306v3","updated":"2024-06-11T07:18:44Z","published":"2024-03-02T20:36:10Z","title":"ICC: Quantifying Image Caption Concreteness for Multimodal Dataset\n  Curation","summary":"  Web-scale training on paired text-image data is becoming increasingly central\nto multimodal learning, but is challenged by the highly noisy nature of\ndatasets in the wild. Standard data filtering approaches succeed in removing\nmismatched text-image pairs, but permit semantically related but highly\nabstract or subjective text. These approaches lack the fine-grained ability to\nisolate the most concrete samples that provide the strongest signal for\nlearning in a noisy dataset. In this work, we propose a new metric, image\ncaption concreteness, that evaluates caption text without an image reference to\nmeasure its concreteness and relevancy for use in multimodal learning. Our\napproach leverages strong foundation models for measuring visual-semantic\ninformation loss in multimodal representations. We demonstrate that this\nstrongly correlates with human evaluation of concreteness in both single-word\nand sentence-level texts. Moreover, we show that curation using ICC complements\nexisting approaches: It succeeds in selecting the highest quality samples from\nmultimodal web-scale datasets to allow for efficient training in\nresource-constrained settings.\n","authors":["Moran Yanuka","Morris Alper","Hadar Averbuch-Elor","Raja Giryes"],"pdf_url":"https://arxiv.org/pdf/2403.01306v3.pdf","comment":"Accepted to ACL 2024 (Finding). For Project webpage, see\n  https://moranyanuka.github.io/icc/"},{"id":"http://arxiv.org/abs/2306.16979v3","updated":"2024-06-11T07:14:18Z","published":"2023-06-29T14:33:20Z","title":"Post-train Black-box Defense via Bayesian Boundary Correction","summary":"  Classifiers based on deep neural networks are susceptible to adversarial\nattack, where the widely existing vulnerability has invoked the research in\ndefending them from potential threats. Given a vulnerable classifier, existing\ndefense methods are mostly white-box and often require re-training the victim\nunder modified loss functions/training regimes. While the model/data/training\nspecifics of the victim are usually unavailable to the user, re-training is\nunappealing, if not impossible for reasons such as limited computational\nresources. To this end, we propose a new post-train black-box defense\nframework. It can turn any pre-trained classifier into a resilient one with\nlittle knowledge of the model specifics. This is achieved by new joint Bayesian\ntreatments on the clean data, the adversarial examples and the classifier, for\nmaximizing their joint probability. It is further equipped with a new\npost-train strategy which keeps the victim intact, avoiding re-training. We\nname our framework Bayesian Boundary Correction (BBC). BBC is a general and\nflexible framework that can easily adapt to different data types. We\ninstantiate BBC for image classification and skeleton-based human activity\nrecognition, for both static and dynamic data. Exhaustive evaluation shows that\nBBC has superior robustness and can enhance robustness without severely hurting\nthe clean accuracy, compared with existing defense methods.\n","authors":["He Wang","Yunfeng Diao"],"pdf_url":"https://arxiv.org/pdf/2306.16979v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2203.04713"},{"id":"http://arxiv.org/abs/2406.07008v1","updated":"2024-06-11T07:08:48Z","published":"2024-06-11T07:08:48Z","title":"Eye-for-an-eye: Appearance Transfer with Semantic Correspondence in\n  Diffusion Models","summary":"  As pretrained text-to-image diffusion models have become a useful tool for\nimage synthesis, people want to specify the results in various ways. In this\npaper, we introduce a method to produce results with the same structure of a\ntarget image but painted with colors from a reference image, i.e., appearance\ntransfer, especially following the semantic correspondence between the result\nand the reference. E.g., the result wing takes color from the reference wing,\nnot the reference head. Existing methods rely on the query-key similarity\nwithin self-attention layer, usually producing defective results. To this end,\nwe propose to find semantic correspondences and explicitly rearrange the\nfeatures according to the semantic correspondences. Extensive experiments show\nthe superiority of our method in various aspects: preserving the structure of\nthe target and reflecting the color from the reference according to the\nsemantic correspondences, even when the two images are not aligned.\n","authors":["Sooyeon Go","Kyungmook Choi","Minjung Shin","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2406.07008v1.pdf","comment":"project page : https://sooyeon-go.github.io/eye_for_an_eye/"},{"id":"http://arxiv.org/abs/2406.07006v1","updated":"2024-06-11T06:59:55Z","published":"2024-06-11T06:59:55Z","title":"MIPI 2024 Challenge on Few-shot RAW Image Denoising: Methods and Results","summary":"  The increasing demand for computational photography and imaging on mobile\nplatforms has led to the widespread development and integration of advanced\nimage sensors with novel algorithms in camera systems. However, the scarcity of\nhigh-quality data for research and the rare opportunity for in-depth exchange\nof views from industry and academia constrain the development of mobile\nintelligent photography and imaging (MIPI). Building on the achievements of the\nprevious MIPI Workshops held at ECCV 2022 and CVPR 2023, we introduce our third\nMIPI challenge including three tracks focusing on novel image sensors and\nimaging algorithms. In this paper, we summarize and review the Few-shot RAW\nImage Denoising track on MIPI 2024. In total, 165 participants were\nsuccessfully registered, and 7 teams submitted results in the final testing\nphase. The developed solutions in this challenge achieved state-of-the-art\nerformance on Few-shot RAW Image Denoising. More details of this challenge and\nthe link to the dataset can be found at https://mipichallenge.org/MIPI2024.\n","authors":["Xin Jin","Chunle Guo","Xiaoming Li","Zongsheng Yue","Chongyi Li","Shangchen Zhou","Ruicheng Feng","Yuekun Dai","Peiqing Yang","Chen Change Loy","Ruoqi Li","Chang Liu","Ziyi Wang","Yao Du","Jingjing Yang","Long Bao","Heng Sun","Xiangyu Kong","Xiaoxia Xing","Jinlong Wu","Yuanyang Xue","Hyunhee Park","Sejun Song","Changho Kim","Jingfan Tan","Wenhan Luo","Zikun Liu","Mingde Qiao","Junjun Jiang","Kui Jiang","Yao Xiao","Chuyang Sun","Jinhui Hu","Weijian Ruan","Yubo Dong","Kai Chen","Hyejeong Jo","Jiahao Qin","Bingjie Han","Pinle Qin","Rui Chai","Pengyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07006v1.pdf","comment":"CVPR 2024 Mobile Intelligent Photography and Imaging (MIPI)\n  Workshop--Few-shot RAWImage Denoising Challenge Report. Website:\n  https://mipi-challenge.org/MIPI2024/"},{"id":"http://arxiv.org/abs/2312.06722v3","updated":"2024-06-11T06:53:44Z","published":"2023-12-11T03:35:58Z","title":"EgoPlan-Bench: Benchmarking Multimodal Large Language Models for\n  Human-Level Planning","summary":"  The pursuit of artificial general intelligence (AGI) has been accelerated by\nMultimodal Large Language Models (MLLMs), which exhibit superior reasoning,\ngeneralization capabilities, and proficiency in processing multimodal inputs. A\ncrucial milestone in the evolution of AGI is the attainment of human-level\nplanning, a fundamental ability for making informed decisions in complex\nenvironments, and solving a wide range of real-world problems. Despite the\nimpressive advancements in MLLMs, a question remains: How far are current MLLMs\nfrom achieving human-level planning? To shed light on this question, we\nintroduce EgoPlan-Bench, a comprehensive benchmark to evaluate the planning\nabilities of MLLMs in real-world scenarios from an egocentric perspective,\nmirroring human perception. EgoPlan-Bench emphasizes the evaluation of planning\ncapabilities of MLLMs, featuring realistic tasks, diverse action plans, and\nintricate visual observations. Our rigorous evaluation of a wide range of MLLMs\nreveals that EgoPlan-Bench poses significant challenges, highlighting a\nsubstantial scope for improvement in MLLMs to achieve human-level task\nplanning. To facilitate this advancement, we further present EgoPlan-IT, a\nspecialized instruction-tuning dataset that effectively enhances model\nperformance on EgoPlan-Bench. We have made all codes, data, and a maintained\nbenchmark leaderboard available to advance future research.\n","authors":["Yi Chen","Yuying Ge","Yixiao Ge","Mingyu Ding","Bohao Li","Rui Wang","Ruifeng Xu","Ying Shan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2312.06722v3.pdf","comment":"Project released at: https://github.com/ChenYi99/EgoPlan"},{"id":"http://arxiv.org/abs/2406.06999v1","updated":"2024-06-11T06:51:02Z","published":"2024-06-11T06:51:02Z","title":"Teaching with Uncertainty: Unleashing the Potential of Knowledge\n  Distillation in Object Detection","summary":"  Knowledge distillation (KD) is a widely adopted and effective method for\ncompressing models in object detection tasks. Particularly, feature-based\ndistillation methods have shown remarkable performance. Existing approaches\noften ignore the uncertainty in the teacher model's knowledge, which stems from\ndata noise and imperfect training. This limits the student model's ability to\nlearn latent knowledge, as it may overly rely on the teacher's imperfect\nguidance. In this paper, we propose a novel feature-based distillation paradigm\nwith knowledge uncertainty for object detection, termed \"Uncertainty\nEstimation-Discriminative Knowledge Extraction-Knowledge Transfer (UET)\", which\ncan seamlessly integrate with existing distillation methods. By leveraging the\nMonte Carlo dropout technique, we introduce knowledge uncertainty into the\ntraining process of the student model, facilitating deeper exploration of\nlatent knowledge. Our method performs effectively during the KD process without\nrequiring intricate structures or extensive computational resources. Extensive\nexperiments validate the effectiveness of our proposed approach across various\ndistillation strategies, detectors, and backbone architectures. Specifically,\nfollowing our proposed paradigm, the existing FGD method achieves\nstate-of-the-art (SoTA) performance, with ResNet50-based GFL achieving 44.1%\nmAP on the COCO dataset, surpassing the baselines by 3.9%.\n","authors":["Junfei Yi","Jianxu Mao","Tengfei Liu","Mingjie Li","Hanyu Gu","Hui Zhang","Xiaojun Chang","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11614v3","updated":"2024-06-11T06:34:01Z","published":"2024-03-18T09:44:44Z","title":"CRS-Diff: Controllable Generative Remote Sensing Foundation Model","summary":"  The emergence of generative models has revolutionized the field of remote\nsensing (RS) image generation. Despite generating high-quality images, existing\nmethods are limited in relying mainly on text control conditions and thus don't\nalways generate images accurately and stablely. In this paper, we propose\nCRS-Diff, a new RS generative foundation framework specifically tailored for RS\nimage generation, leveraging the inherent advantages of diffusion models while\nintegrating more advanced control mechanisms. Specifically, CRS-Diff can\nsimultaneously support text-condition, metadata-condition, and image-condition\ncontrol inputs, thus enabling more precise control to refine the generation\nprocess. To effectively integrate multiple condition control information, we\nintroduce a new conditional control mechanism to achieve multi-scale feature\nfusion, thus enhancing the guiding effect of control conditions. To our\nknowledge, CRS-Diff is the first multiple-condition controllable generative RS\nfoundation model. Experimental results in single-condition and\nmultiple-condition cases have demonstrated the superior ability of our CRS-Diff\nto generate RS images both quantitatively and qualitatively compared with\nprevious methods. Additionally, our CRS-Diff can serve as a data engine that\ngenerates high-quality training data for downstream tasks, e.g., road\nextraction. The code is available at https://github.com/Sonettoo/CRS-Diff.\n","authors":["Datao Tang","Xiangyong Cao","Xingsong Hou","Zhongyuan Jiang","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2403.11614v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18159v2","updated":"2024-06-11T06:22:57Z","published":"2023-11-30T00:29:13Z","title":"Compact3D: Smaller and Faster Gaussian Splatting with Vector\n  Quantization","summary":"  3D Gaussian Splatting (3DGS) is a new method for modeling and rendering 3D\nradiance fields that achieves much faster learning and rendering time compared\nto SOTA NeRF methods. However, it comes with the drawback of a much larger\nstorage demand compared to NeRF methods since it needs to store the parameters\nfor millions of 3D Gaussians. We notice that large groups of Gaussians share\nsimilar parameters and introduce a simple vector quantization method based on\nK-means algorithm to quantize the Gaussian parameters. Then, we store the small\ncodebook along with the index of the code for each Gaussian. We compress the\nindices further by sorting them and using a method similar to run-length\nencoding. Moreover, we use a simple regularizer that encourages zero opacity\n(invisible Gaussians) to reduce the number of Gaussians, thereby compressing\nthe model and speeding up the rendering. We do extensive experiments on\nstandard benchmarks as well as an existing 3D dataset that is an order of\nmagnitude larger than the standard benchmarks used in this field. We show that\nour simple yet effective method can reduce the storage costs for 3DGS by 40 to\n50x and rendering time by 2 to 3x with a very small drop in the quality of\nrendered images.\n","authors":["KL Navaneet","Kossar Pourahmadi Meibodi","Soroush Abbasi Koohpayegani","Hamed Pirsiavash"],"pdf_url":"https://arxiv.org/pdf/2311.18159v2.pdf","comment":"Code is available at https://github.com/UCDvision/compact3d"},{"id":"http://arxiv.org/abs/2406.05768v2","updated":"2024-06-11T06:22:53Z","published":"2024-06-09T12:55:50Z","title":"MLCM: Multistep Consistency Distillation of Latent Diffusion Model","summary":"  Distilling large latent diffusion models (LDMs) into ones that are fast to\nsample from is attracting growing research interest. However, the majority of\nexisting methods face a dilemma where they either (i) depend on multiple\nindividual distilled models for different sampling budgets, or (ii) sacrifice\ngeneration quality with limited (e.g., 2-4) and/or moderate (e.g., 5-8)\nsampling steps. To address these, we extend the recent multistep consistency\ndistillation (MCD) strategy to representative LDMs, establishing the Multistep\nLatent Consistency Models (MLCMs) approach for low-cost high-quality image\nsynthesis. MLCM serves as a unified model for various sampling steps due to the\npromise of MCD. We further augment MCD with a progressive training strategy to\nstrengthen inter-segment consistency to boost the quality of few-step\ngenerations. We take the states from the sampling trajectories of the teacher\nmodel as training data for MLCMs to lift the requirements for high-quality\ntraining datasets and to bridge the gap between the training and inference of\nthe distilled model. MLCM is compatible with preference learning strategies for\nfurther improvement of visual quality and aesthetic appeal. Empirically, MLCM\ncan generate high-quality, delightful images with only 2-8 sampling steps. On\nthe MSCOCO-2017 5K benchmark, MLCM distilled from SDXL gets a CLIP Score of\n33.30, Aesthetic Score of 6.19, and Image Reward of 1.20 with only 4 steps,\nsubstantially surpassing 4-step LCM [23], 8-step SDXL-Lightning [17], and\n8-step HyperSD [33]. We also demonstrate the versatility of MLCMs in\napplications including controllable generation, image style transfer, and\nChinese-to-image generation.\n","authors":["Qingsong Xie","Zhenyi Liao","Zhijie Deng","Chen chen","Shixiang Tang","Haonan Lu"],"pdf_url":"https://arxiv.org/pdf/2406.05768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04788v3","updated":"2024-06-11T06:21:46Z","published":"2024-02-07T12:28:32Z","title":"MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with\n  Vision-Language Benchmark","summary":"  Multimodal Large Language Models (MLLMs) have gained significant attention\nrecently, showing remarkable potential in artificial general intelligence.\nHowever, assessing the utility of MLLMs presents considerable challenges,\nprimarily due to the absence of multimodal benchmarks that align with human\npreferences. Drawing inspiration from the concept of LLM-as-a-Judge within\nLLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to\nassess the ability of MLLMs in assisting judges across diverse modalities,\nencompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and\nBatch Ranking. Our study reveals that, while MLLMs demonstrate remarkable\nhuman-like discernment in Pair Comparison, there is a significant divergence\nfrom human preferences in Scoring Evaluation and Batch Ranking. Furthermore, a\ncloser examination reveals persistent challenges in the judgment capacities of\nLLMs, including diverse biases, hallucinatory responses, and inconsistencies in\njudgment, even in advanced models such as GPT-4V. These findings emphasize the\npressing need for enhancements and further research efforts to be undertaken\nbefore regarding MLLMs as fully reliable evaluators. In light of this, we\nadvocate for additional efforts dedicated to supporting the continuous\ndevelopment within the domain of MLLM functioning as judges. The code and\ndataset are publicly available at our project homepage:\n\\url{https://mllm-judge.github.io/}.\n","authors":["Dongping Chen","Ruoxi Chen","Shilin Zhang","Yinuo Liu","Yaochen Wang","Huichi Zhou","Qihui Zhang","Yao Wan","Pan Zhou","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2402.04788v3.pdf","comment":"ICML 2024 (Oral)"},{"id":"http://arxiv.org/abs/2406.06978v1","updated":"2024-06-11T06:18:26Z","published":"2024-06-11T06:18:26Z","title":"Hydra-MDP: End-to-end Multimodal Planning with Multi-target\n  Hydra-Distillation","summary":"  We propose Hydra-MDP, a novel paradigm employing multiple teachers in a\nteacher-student model. This approach uses knowledge distillation from both\nhuman and rule-based teachers to train the student model, which features a\nmulti-head decoder to learn diverse trajectory candidates tailored to various\nevaluation metrics. With the knowledge of rule-based teachers, Hydra-MDP learns\nhow the environment influences the planning in an end-to-end manner instead of\nresorting to non-differentiable post-processing. This method achieves the\n$1^{st}$ place in the Navsim challenge, demonstrating significant improvements\nin generalization across diverse driving environments and conditions. Code will\nbe available at \\url{https://github.com/woxihuanjiangguo/Hydra-MDP}\n","authors":["Zhenxin Li","Kailin Li","Shihao Wang","Shiyi Lan","Zhiding Yu","Yishen Ji","Zhiqi Li","Ziyue Zhu","Jan Kautz","Zuxuan Wu","Yu-Gang Jiang","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2406.06978v1.pdf","comment":"The 1st place solution of End-to-end Driving at Scale at the CVPR\n  2024 Autonomous Grand Challenge"},{"id":"http://arxiv.org/abs/2406.06973v1","updated":"2024-06-11T06:10:46Z","published":"2024-06-11T06:10:46Z","title":"RWKV-CLIP: A Robust Vision-Language Representation Learner","summary":"  Contrastive Language-Image Pre-training (CLIP) has significantly improved\nperformance in various vision-language tasks by expanding the dataset with\nimage-text pairs obtained from websites. This paper further explores CLIP from\nthe perspectives of data and model architecture. To address the prevalence of\nnoisy data and enhance the quality of large-scale image-text data crawled from\nthe internet, we introduce a diverse description generation framework that can\nleverage Large Language Models (LLMs) to synthesize and refine content from\nweb-based texts, synthetic captions, and detection tags. Furthermore, we\npropose RWKV-CLIP, the first RWKV-driven vision-language representation\nlearning model that combines the effective parallel training of transformers\nwith the efficient inference of RNNs. Comprehensive experiments across various\nmodel scales and pre-training datasets demonstrate that RWKV-CLIP is a robust\nand efficient vision-language representation learner, it achieves\nstate-of-the-art performance in several downstream tasks, including linear\nprobe, zero-shot classification, and zero-shot image-text retrieval. To\nfacilitate future research, the code and pre-trained models are released at\nhttps://github.com/deepglint/RWKV-CLIP\n","authors":["Tiancheng Gu","Kaicheng Yang","Xiang An","Ziyong Feng","Dongnan Liu","Weidong Cai","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2406.06973v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.06972v1","updated":"2024-06-11T06:09:41Z","published":"2024-06-11T06:09:41Z","title":"Generative Lifting of Multiview to 3D from Unknown Pose: Wrapping NeRF\n  inside Diffusion","summary":"  We cast multiview reconstruction from unknown pose as a generative modeling\nproblem. From a collection of unannotated 2D images of a scene, our approach\nsimultaneously learns both a network to predict camera pose from 2D image\ninput, as well as the parameters of a Neural Radiance Field (NeRF) for the 3D\nscene. To drive learning, we wrap both the pose prediction network and NeRF\ninside a Denoising Diffusion Probabilistic Model (DDPM) and train the system\nvia the standard denoising objective. Our framework requires the system\naccomplish the task of denoising an input 2D image by predicting its pose and\nrendering the NeRF from that pose. Learning to denoise thus forces the system\nto concurrently learn the underlying 3D NeRF representation and a mapping from\nimages to camera extrinsic parameters. To facilitate the latter, we design a\ncustom network architecture to represent pose as a distribution, granting\nimplicit capacity for discovering view correspondences when trained end-to-end\nfor denoising alone. This technique allows our system to successfully build\nNeRFs, without pose knowledge, for challenging scenes where competing methods\nfail. At the conclusion of training, our learned NeRF can be extracted and used\nas a 3D scene model; our full system can be used to sample novel camera poses\nand generate novel-view images.\n","authors":["Xin Yuan","Rana Hanocka","Michael Maire"],"pdf_url":"https://arxiv.org/pdf/2406.06972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06967v1","updated":"2024-06-11T05:50:34Z","published":"2024-06-11T05:50:34Z","title":"Dual Thinking and Perceptual Analysis of Deep Learning Models using\n  Human Adversarial Examples","summary":"  The dual thinking framework considers fast, intuitive processing and slower,\nlogical processing. The perception of dual thinking in vision requires images\nwhere inferences from intuitive and logical processing differ. We introduce an\nadversarial dataset to provide evidence for the dual thinking framework in\nhuman vision, which also aids in studying the qualitative behavior of deep\nlearning models. Our study also addresses a major criticism of using\nclassification models as computational models of human vision by using instance\nsegmentation models that localize objects. The evidence underscores the\nimportance of shape in identifying instances in human vision and shows that\ndeep learning models lack an understanding of sub-structures, as indicated by\nerrors related to the position and number of sub-components. Additionally, the\nsimilarity in errors made by models and intuitive human processing indicates\nthat models only address intuitive thinking in human vision.\n","authors":["Kailas Dayanandan","Anand Sinha","Brejesh Lall"],"pdf_url":"https://arxiv.org/pdf/2406.06967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06965v1","updated":"2024-06-11T05:48:04Z","published":"2024-06-11T05:48:04Z","title":"Evolving from Single-modal to Multi-modal Facial Deepfake Detection: A\n  Survey","summary":"  This survey addresses the critical challenge of deepfake detection amidst the\nrapid advancements in artificial intelligence. As AI-generated media, including\nvideo, audio and text, become more realistic, the risk of misuse to spread\nmisinformation and commit identity fraud increases. Focused on face-centric\ndeepfakes, this work traces the evolution from traditional single-modality\nmethods to sophisticated multi-modal approaches that handle audio-visual and\ntext-visual scenarios. We provide comprehensive taxonomies of detection\ntechniques, discuss the evolution of generative methods from auto-encoders and\nGANs to diffusion models, and categorize these technologies by their unique\nattributes. To our knowledge, this is the first survey of its kind. We also\nexplore the challenges of adapting detection methods to new generative models\nand enhancing the reliability and robustness of deepfake detectors, proposing\ndirections for future research. This survey offers a detailed roadmap for\nresearchers, supporting the development of technologies to counter the\ndeceptive use of AI in media creation, particularly facial forgery. A curated\nlist of all related papers can be found at\n\\href{https://github.com/qiqitao77/Comprehensive-Advances-in-Deepfake-Detection-Spanning-Diverse-Modalities}{https://github.com/qiqitao77/Awesome-Comprehensive-Deepfake-Detection}.\n","authors":["Ping Liu","Qiqi Tao","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.06965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06953v1","updated":"2024-06-11T05:25:25Z","published":"2024-06-11T05:25:25Z","title":"Stepwise Regression and Pre-trained Edge for Robust Stereo Matching","summary":"  Due to the difficulty in obtaining real samples and ground truth, the\ngeneralization performance and the fine-tuned performance are critical for the\nfeasibility of stereo matching methods in real-world applications. However, the\npresence of substantial disparity distributions and density variations across\ndifferent datasets presents significant challenges for the generalization and\nfine-tuning of the model. In this paper, we propose a novel stereo matching\nmethod, called SR-Stereo, which mitigates the distributional differences across\ndifferent datasets by predicting the disparity clips and uses a loss weight\nrelated to the regression target scale to improve the accuracy of the disparity\nclips. Moreover, this stepwise regression architecture can be easily extended\nto existing iteration-based methods to improve the performance without changing\nthe structure. In addition, to mitigate the edge blurring of the fine-tuned\nmodel on sparse ground truth, we propose Domain Adaptation Based on Pre-trained\nEdges (DAPE). Specifically, we use the predicted disparity and RGB image to\nestimate the edge map of the target domain image. The edge map is filtered to\ngenerate edge map background pseudo-labels, which together with the sparse\nground truth disparity on the target domain are used as a supervision to\njointly fine-tune the pre-trained stereo matching model. These proposed methods\nare extensively evaluated on SceneFlow, KITTI, Middbury 2014 and ETH3D. The\nSR-Stereo achieves competitive disparity estimation performance and\nstate-of-the-art cross-domain generalisation performance. Meanwhile, the\nproposed DAPE significantly improves the disparity estimation performance of\nfine-tuned models, especially in the textureless and detail regions.\n","authors":["Weiqing Xiao","Wei Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.06953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06949v1","updated":"2024-06-11T05:21:30Z","published":"2024-06-11T05:21:30Z","title":"Triple-domain Feature Learning with Frequency-aware Memory Enhancement\n  for Moving Infrared Small Target Detection","summary":"  Moving infrared small target detection presents significant challenges due to\ntiny target sizes and low contrast against backgrounds. Currently-existing\nmethods primarily focus on extracting target features only from the\nspatial-temporal domain. For further enhancing feature representation, more\ninformation domains such as frequency are believed to be potentially valuable.\nTo extend target feature learning, we propose a new Triple-domain Strategy\n(Tridos) with the frequency-aware memory enhancement on the spatial-temporal\ndomain. In our scheme, it effectively detaches and enhances frequency features\nby a local-global frequency-aware module with Fourier transform. Inspired by\nthe human visual system, our memory enhancement aims to capture the target\nspatial relations between video frames. Furthermore, it encodes temporal\ndynamics motion features via differential learning and residual enhancing.\nAdditionally, we further design a residual compensation unit to reconcile\npossible cross-domain feature mismatches. To our best knowledge, our Tridos is\nthe first work to explore target feature learning comprehensively in\nspatial-temporal-frequency domains. The extensive experiments on three datasets\n(DAUB, ITSDT-15K, and IRDST) validate that our triple-domain learning scheme\ncould be obviously superior to state-of-the-art ones. Source codes are\navailable at https://github.com/UESTC-nnLab/Tridos.\n","authors":["Weiwei Duan","Luping Ji","Shengjia Chen","Sicheng Zhu","Mao Ye"],"pdf_url":"https://arxiv.org/pdf/2406.06949v1.pdf","comment":"This paper has submitted to IEEE TGRS,under review"},{"id":"http://arxiv.org/abs/2406.06948v1","updated":"2024-06-11T05:21:21Z","published":"2024-06-11T05:21:21Z","title":"Neural Visibility Field for Uncertainty-Driven Active Mapping","summary":"  This paper presents Neural Visibility Field (NVF), a novel uncertainty\nquantification method for Neural Radiance Fields (NeRF) applied to active\nmapping. Our key insight is that regions not visible in the training views lead\nto inherently unreliable color predictions by NeRF at this region, resulting in\nincreased uncertainty in the synthesized views. To address this, we propose to\nuse Bayesian Networks to composite position-based field uncertainty into\nray-based uncertainty in camera observations. Consequently, NVF naturally\nassigns higher uncertainty to unobserved regions, aiding robots to select the\nmost informative next viewpoints. Extensive evaluations show that NVF excels\nnot only in uncertainty quantification but also in scene reconstruction for\nactive mapping, outperforming existing methods.\n","authors":["Shangjie Xue","Jesse Dill","Pranay Mathur","Frank Dellaert","Panagiotis Tsiotra","Danfei Xu"],"pdf_url":"https://arxiv.org/pdf/2406.06948v1.pdf","comment":"Accepted to CVPR 2024. More details can be found at\n  https://sites.google.com/view/nvf-cvpr24/"},{"id":"http://arxiv.org/abs/2406.06946v1","updated":"2024-06-11T05:12:00Z","published":"2024-06-11T05:12:00Z","title":"Sparse Bayesian Networks: Efficient Uncertainty Quantification in\n  Medical Image Analysis","summary":"  Efficiently quantifying predictive uncertainty in medical images remains a\nchallenge. While Bayesian neural networks (BNN) offer predictive uncertainty,\nthey require substantial computational resources to train. Although Bayesian\napproximations such as ensembles have shown promise, they still suffer from\nhigh training and inference costs. Existing approaches mainly address the costs\nof BNN inference post-training, with little focus on improving training\nefficiency and reducing parameter complexity. This study introduces a training\nprocedure for a sparse (partial) Bayesian network. Our method selectively\nassigns a subset of parameters as Bayesian by assessing their deterministic\nsaliency through gradient sensitivity analysis. The resulting network combines\ndeterministic and Bayesian parameters, exploiting the advantages of both\nrepresentations to achieve high task-specific performance and minimize\npredictive uncertainty. Demonstrated on multi-label ChestMNIST for\nclassification and ISIC, LIDC-IDRI for segmentation, our approach achieves\ncompetitive performance and predictive uncertainty estimation by reducing\nBayesian parameters by over 95\\%, significantly reducing computational expenses\ncompared to fully Bayesian and ensemble methods.\n","authors":["Zeinab Abboud","Herve Lombaert","Samuel Kadoury"],"pdf_url":"https://arxiv.org/pdf/2406.06946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06942v1","updated":"2024-06-11T04:52:23Z","published":"2024-06-11T04:52:23Z","title":"Optimal Matrix-Mimetic Tensor Algebras via Variable Projection","summary":"  Recent advances in {matrix-mimetic} tensor frameworks have made it possible\nto preserve linear algebraic properties for multilinear data analysis and, as a\nresult, to obtain optimal representations of multiway data. Matrix mimeticity\narises from interpreting tensors as operators that can be multiplied,\nfactorized, and analyzed analogous to matrices. Underlying the tensor operation\nis an algebraic framework parameterized by an invertible linear transformation.\nThe choice of linear mapping is crucial to representation quality and, in\npractice, is made heuristically based on expected correlations in the data.\nHowever, in many cases, these correlations are unknown and common heuristics\nlead to suboptimal performance. In this work, we simultaneously learn optimal\nlinear mappings and corresponding tensor representations without relying on\nprior knowledge of the data. Our new framework explicitly captures the coupling\nbetween the transformation and representation using variable projection. We\npreserve the invertibility of the linear mapping by learning orthogonal\ntransformations with Riemannian optimization. We provide original theory of\nuniqueness of the transformation and convergence analysis of our\nvariable-projection-based algorithm. We demonstrate the generality of our\nframework through numerical experiments on a wide range of applications,\nincluding financial index tracking, image compression, and reduced order\nmodeling. We have published all the code related to this work at\nhttps://github.com/elizabethnewman/star-M-opt.\n","authors":["Elizabeth Newman","Katherine Keegan"],"pdf_url":"https://arxiv.org/pdf/2406.06942v1.pdf","comment":"46 pages, 15 figures"},{"id":"http://arxiv.org/abs/2312.00343v6","updated":"2024-06-11T04:41:28Z","published":"2023-12-01T04:35:47Z","title":"OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong\n  Baseline","summary":"  Stereo matching aims to estimate the disparity between matching pixels in a\nstereo image pair, which is important to robotics, autonomous driving, and\nother computer vision tasks. Despite the development of numerous impressive\nmethods in recent years, determining the most suitable architecture for\npractical application remains challenging. Addressing this gap, our paper\nintroduces a comprehensive benchmark focusing on practical applicability rather\nthan solely on individual models for optimized performance. Specifically, we\ndevelop a flexible and efficient stereo matching codebase, called OpenStereo.\nOpenStereo includes training and inference codes of more than 10 network\nmodels, making it, to our knowledge, the most complete stereo matching toolbox\navailable. Based on OpenStereo, we conducted experiments and have achieved or\nsurpassed the performance metrics reported in the original paper. Additionally,\nwe conduct an exhaustive analysis and deconstruction of recent developments in\nstereo matching through comprehensive ablative experiments. These\ninvestigations inspired the creation of StereoBase, a strong baseline model.\nOur StereoBase ranks 1st on SceneFlow, KITTI 2015, 2012 (Reflective) among\npublished methods and achieves the best performance across all metrics. In\naddition, StereoBase has strong cross-dataset generalization. Code is available\nat \\url{https://github.com/XiandaGuo/OpenStereo}.\n","authors":["Xianda Guo","Chenming Zhang","Juntao Lu","Yiqi Wang","Yiqun Duan","Tian Yang","Zheng Zhu","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2312.00343v6.pdf","comment":"Code is available at: https://github.com/XiandaGuo/OpenStereo"},{"id":"http://arxiv.org/abs/2406.06930v1","updated":"2024-06-11T04:08:37Z","published":"2024-06-11T04:08:37Z","title":"Explaining Representation Learning with Perceptual Components","summary":"  Self-supervised models create representation spaces that lack clear semantic\nmeaning. This interpretability problem of representations makes traditional\nexplainability methods ineffective in this context. In this paper, we introduce\na novel method to analyze representation spaces using three key perceptual\ncomponents: color, shape, and texture. We employ selective masking of these\ncomponents to observe changes in representations, resulting in distinct\nimportance maps for each. In scenarios, where labels are absent, these\nimportance maps provide more intuitive explanations as they are integral to the\nhuman visual system. Our approach enhances the interpretability of the\nrepresentation space, offering explanations that resonate with human visual\nperception. We analyze how different training objectives create distinct\nrepresentation spaces using perceptual components. Additionally, we examine the\nrepresentation of images across diverse image domains, providing insights into\nthe role of these components in different contexts.\n","authors":["Yavuz Yarici","Kiran Kokilepersaud","Mohit Prabhushankar","Ghassan AlRegib"],"pdf_url":"https://arxiv.org/pdf/2406.06930v1.pdf","comment":"8 Pages, 3 Figures, Accepted to 2024 IEEE International Conference on\n  Image Processing (ICIP), Abu Dhabi, United Arab Emirates (UAE). Date of\n  Acceptance: June 6th, 2024"},{"id":"http://arxiv.org/abs/2406.04338v3","updated":"2024-06-11T03:36:09Z","published":"2024-06-06T17:59:47Z","title":"Physics3D: Learning Physical Properties of 3D Gaussians via Video\n  Diffusion","summary":"  In recent years, there has been rapid development in 3D generation models,\nopening up new possibilities for applications such as simulating the dynamic\nmovements of 3D objects and customizing their behaviors. However, current 3D\ngenerative models tend to focus only on surface features such as color and\nshape, neglecting the inherent physical properties that govern the behavior of\nobjects in the real world. To accurately simulate physics-aligned dynamics, it\nis essential to predict the physical properties of materials and incorporate\nthem into the behavior prediction process. Nonetheless, predicting the diverse\nmaterials of real-world objects is still challenging due to the complex nature\nof their physical attributes. In this paper, we propose \\textbf{Physics3D}, a\nnovel method for learning various physical properties of 3D objects through a\nvideo diffusion model. Our approach involves designing a highly generalizable\nphysical simulation system based on a viscoelastic material model, which\nenables us to simulate a wide range of materials with high-fidelity\ncapabilities. Moreover, we distill the physical priors from a video diffusion\nmodel that contains more understanding of realistic object materials. Extensive\nexperiments demonstrate the effectiveness of our method with both elastic and\nplastic materials. Physics3D shows great potential for bridging the gap between\nthe physical world and virtual neural space, providing a better integration and\napplication of realistic physical principles in virtual environments. Project\npage: https://liuff19.github.io/Physics3D.\n","authors":["Fangfu Liu","Hanyang Wang","Shunyu Yao","Shengjun Zhang","Jie Zhou","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2406.04338v3.pdf","comment":"Project page: https://liuff19.github.io/Physics3D"},{"id":"http://arxiv.org/abs/2405.12806v2","updated":"2024-06-11T03:33:23Z","published":"2024-05-21T13:57:53Z","title":"MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video","summary":"  Single-view clothed human reconstruction holds a central position in virtual\nreality applications, especially in contexts involving intricate human motions.\nIt presents notable challenges in achieving realistic clothing deformation.\nCurrent methodologies often overlook the influence of motion on surface\ndeformation, resulting in surfaces lacking the constraints imposed by global\nmotion. To overcome these limitations, we introduce an innovative framework,\nMotion-Based 3D Clo}thed Humans Synthesis (MOSS), which employs kinematic\ninformation to achieve motion-aware Gaussian split on the human surface. Our\nframework consists of two modules: Kinematic Gaussian Locating Splatting (KGAS)\nand Surface Deformation Detector (UID). KGAS incorporates matrix-Fisher\ndistribution to propagate global motion across the body surface. The density\nand rotation factors of this distribution explicitly control the Gaussians,\nthereby enhancing the realism of the reconstructed surface. Additionally, to\naddress local occlusions in single-view, based on KGAS, UID identifies\nsignificant surfaces, and geometric reconstruction is performed to compensate\nfor these deformations. Experimental results demonstrate that MOSS achieves\nstate-of-the-art visual quality in 3D clothed human synthesis from monocular\nvideos. Notably, we improve the Human NeRF and the Gaussian Splatting by 33.94%\nand 16.75% in LPIPS* respectively. Codes are available at\nhttps://wanghongsheng01.github.io/MOSS/.\n","authors":["Hongsheng Wang","Xiang Cai","Xi Sun","Jinhong Yue","Zhanyun Tang","Shengyu Zhang","Feng Lin","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2405.12806v2.pdf","comment":"arXiv admin note: text overlap with arXiv:1710.03746 by other authors"},{"id":"http://arxiv.org/abs/2406.06911v1","updated":"2024-06-11T03:09:37Z","published":"2024-06-11T03:09:37Z","title":"AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising","summary":"  Diffusion models have garnered significant interest from the community for\ntheir great generative ability across various applications. However, their\ntypical multi-step sequential-denoising nature gives rise to high cumulative\nlatency, thereby precluding the possibilities of parallel computation. To\naddress this, we introduce AsyncDiff, a universal and plug-and-play\nacceleration scheme that enables model parallelism across multiple devices. Our\napproach divides the cumbersome noise prediction model into multiple\ncomponents, assigning each to a different device. To break the dependency chain\nbetween these components, it transforms the conventional sequential denoising\ninto an asynchronous process by exploiting the high similarity between hidden\nstates in consecutive diffusion steps. Consequently, each component is\nfacilitated to compute in parallel on separate devices. The proposed strategy\nsignificantly reduces inference latency while minimally impacting the\ngenerative quality. Specifically, for the Stable Diffusion v2.1, AsyncDiff\nachieves a 2.7x speedup with negligible degradation and a 4.0x speedup with\nonly a slight reduction of 0.38 in CLIP Score, on four NVIDIA A5000 GPUs. Our\nexperiments also demonstrate that AsyncDiff can be readily applied to video\ndiffusion models with encouraging performances. The code is available at\nhttps://github.com/czg1225/AsyncDiff.\n","authors":["Zigeng Chen","Xinyin Ma","Gongfan Fang","Zhenxiong Tan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06911v1.pdf","comment":"Work in progress. Project Page:\n  https://czg1225.github.io/asyncdiff_page/"},{"id":"http://arxiv.org/abs/2406.06258v2","updated":"2024-06-11T03:06:14Z","published":"2024-06-10T13:41:10Z","title":"Tuning-Free Visual Customization via View Iterative Self-Attention\n  Control","summary":"  Fine-Tuning Diffusion Models enable a wide range of personalized generation\nand editing applications on diverse visual modalities. While Low-Rank\nAdaptation (LoRA) accelerates the fine-tuning process, it still requires\nmultiple reference images and time-consuming training, which constrains its\nscalability for large-scale and real-time applications. In this paper, we\npropose \\textit{View Iterative Self-Attention Control (VisCtrl)} to tackle this\nchallenge. Specifically, VisCtrl is a training-free method that injects the\nappearance and structure of a user-specified subject into another subject in\nthe target image, unlike previous approaches that require fine-tuning the\nmodel. Initially, we obtain the initial noise for both the reference and target\nimages through DDIM inversion. Then, during the denoising phase, features from\nthe reference image are injected into the target image via the self-attention\nmechanism. Notably, by iteratively performing this feature injection process,\nwe ensure that the reference image features are gradually integrated into the\ntarget image. This approach results in consistent and harmonious editing with\nonly one reference image in a few denoising steps. Moreover, benefiting from\nour plug-and-play architecture design and the proposed Feature Gradual Sampling\nstrategy for multi-view editing, our method can be easily extended to edit in\ncomplex visual domains. Extensive experiments show the efficacy of VisCtrl\nacross a spectrum of tasks, including personalized editing of images, videos,\nand 3D scenes.\n","authors":["Xiaojie Li","Chenghao Gu","Shuzhao Xie","Yunpeng Bai","Weixiang Zhang","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06258v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2406.06908v1","updated":"2024-06-11T03:05:50Z","published":"2024-06-11T03:05:50Z","title":"UVIS: Unsupervised Video Instance Segmentation","summary":"  Video instance segmentation requires classifying, segmenting, and tracking\nevery object across video frames. Unlike existing approaches that rely on\nmasks, boxes, or category labels, we propose UVIS, a novel Unsupervised Video\nInstance Segmentation (UVIS) framework that can perform video instance\nsegmentation without any video annotations or dense label-based pretraining.\nOur key insight comes from leveraging the dense shape prior from the\nself-supervised vision foundation model DINO and the openset recognition\nability from the image-caption supervised vision-language model CLIP. Our UVIS\nframework consists of three essential steps: frame-level pseudo-label\ngeneration, transformer-based VIS model training, and query-based tracking. To\nimprove the quality of VIS predictions in the unsupervised setup, we introduce\na dual-memory design. This design includes a semantic memory bank for\ngenerating accurate pseudo-labels and a tracking memory bank for maintaining\ntemporal consistency in object tracks. We evaluate our approach on three\nstandard VIS benchmarks, namely YoutubeVIS-2019, YoutubeVIS-2021, and Occluded\nVIS. Our UVIS achieves 21.1 AP on YoutubeVIS-2019 without any video annotations\nor dense pretraining, demonstrating the potential of our unsupervised VIS\nframework.\n","authors":["Shuaiyi Huang","Saksham Suri","Kamal Gupta","Sai Saketh Rambhatla","Ser-nam Lim","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2406.06908v1.pdf","comment":"CVPR2024 Workshop"},{"id":"http://arxiv.org/abs/2406.06907v1","updated":"2024-06-11T03:00:41Z","published":"2024-06-11T03:00:41Z","title":"SignMusketeers: An Efficient Multi-Stream Approach for Sign Language\n  Translation at Scale","summary":"  A persistent challenge in sign language video processing, including the task\nof sign language to written language translation, is how we learn\nrepresentations of sign language in an effective and efficient way that can\npreserve the important attributes of these languages, while remaining invariant\nto irrelevant visual differences. Informed by the nature and linguistics of\nsigned languages, our proposed method focuses on just the most relevant parts\nin a signing video: the face, hands and body posture of the signer. However,\ninstead of using pose estimation coordinates from off-the-shelf pose tracking\nmodels, which have inconsistent performance for hands and faces, we propose to\nlearn the complex handshapes and rich facial expressions of sign languages in a\nself-supervised fashion. Our approach is based on learning from individual\nframes (rather than video sequences) and is therefore much more efficient than\nprior work on sign language pre-training. Compared to a recent model that\nestablished a new state of the art in sign language translation on the How2Sign\ndataset, our approach yields similar translation performance, using less than\n3\\% of the compute.\n","authors":["Shester Gueuwou","Xiaodan Du","Greg Shakhnarovich","Karen Livescu"],"pdf_url":"https://arxiv.org/pdf/2406.06907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02099v3","updated":"2024-06-11T02:33:56Z","published":"2024-01-04T07:11:16Z","title":"Oceanship: A Large-Scale Dataset for Underwater Audio Target Recognition","summary":"  The recognition of underwater audio plays a significant role in identifying a\nvessel while it is in motion. Underwater target recognition tasks have a wide\nrange of applications in areas such as marine environmental protection,\ndetection of ship radiated noise, underwater noise control, and coastal vessel\ndispatch. The traditional UATR task involves training a network to extract\nfeatures from audio data and predict the vessel type. The current UATR dataset\nexhibits shortcomings in both duration and sample quantity. In this paper, we\npropose Oceanship, a large-scale and diverse underwater audio dataset. This\ndataset comprises 15 categories, spans a total duration of 121 hours, and\nincludes comprehensive annotation information such as coordinates, velocity,\nvessel types, and timestamps. We compiled the dataset by crawling and\norganizing original communication data from the Ocean Communication Network\n(ONC) database between 2021 and 2022. While audio retrieval tasks are\nwell-established in general audio classification, they have not been explored\nin the context of underwater audio recognition. Leveraging the Oceanship\ndataset, we introduce a baseline model named Oceannet for underwater audio\nretrieval. This model achieves a recall at 1 (R@1) accuracy of 67.11% and a\nrecall at 5 (R@5) accuracy of 99.13% on the Deepship dataset.\n","authors":["Zeyu Li","Suncheng Xiang","Tong Yu","Jingsheng Gao","Jiacheng Ruan","Yanping Hu","Ting Liu","Yuzhuo Fu"],"pdf_url":"https://arxiv.org/pdf/2401.02099v3.pdf","comment":"Accepted by ICIC 2024"},{"id":"http://arxiv.org/abs/2406.01388v2","updated":"2024-06-11T02:29:53Z","published":"2024-06-03T14:51:24Z","title":"AutoStudio: Crafting Consistent Subjects in Multi-turn Interactive Image\n  Generation","summary":"  As cutting-edge Text-to-Image (T2I) generation models already excel at\nproducing remarkable single images, an even more challenging task, i.e.,\nmulti-turn interactive image generation begins to attract the attention of\nrelated research communities. This task requires models to interact with users\nover multiple turns to generate a coherent sequence of images. However, since\nusers may switch subjects frequently, current efforts struggle to maintain\nsubject consistency while generating diverse images. To address this issue, we\nintroduce a training-free multi-agent framework called AutoStudio. AutoStudio\nemploys three agents based on large language models (LLMs) to handle\ninteractions, along with a stable diffusion (SD) based agent for generating\nhigh-quality images. Specifically, AutoStudio consists of (i) a subject manager\nto interpret interaction dialogues and manage the context of each subject, (ii)\na layout generator to generate fine-grained bounding boxes to control subject\nlocations, (iii) a supervisor to provide suggestions for layout refinements,\nand (iv) a drawer to complete image generation. Furthermore, we introduce a\nParallel-UNet to replace the original UNet in the drawer, which employs two\nparallel cross-attention modules for exploiting subject-aware features. We also\nintroduce a subject-initialized generation method to better preserve small\nsubjects. Our AutoStudio hereby can generate a sequence of multi-subject images\ninteractively and consistently. Extensive experiments on the public CMIGBench\nbenchmark and human evaluations show that AutoStudio maintains multi-subject\nconsistency across multiple turns well, and it also raises the state-of-the-art\nperformance by 13.65% in average Frechet Inception Distance and 2.83% in\naverage character-character similarity.\n","authors":["Junhao Cheng","Xi Lu","Hanhui Li","Khun Loun Zai","Baiqiao Yin","Yuhao Cheng","Yiqiang Yan","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2406.01388v2.pdf","comment":"Multi-turn interactive image generation"},{"id":"http://arxiv.org/abs/2312.06169v2","updated":"2024-06-11T02:13:38Z","published":"2023-12-11T07:16:49Z","title":"Two-Stage Adaptive Network for Semi-Supervised Cross-Domain Crater\n  Detection under Varying Scenario Distributions","summary":"  Crater detection can provide valuable information for humans to explore the\ntopography and understand the history of extraterrestrial planets. Due to the\nsignificantly varying scenario distributions, existing detection models trained\non known labelled crater datasets are hardly effective when applied to new\nunlabelled planets. To address this issue, we propose a two-stage adaptive\nnetwork (TAN) for semi-supervised cross-domain crater detection. Our network is\nbuilt on the YOLOv5 detector, where a series of strategies are employed to\nenhance its cross-domain generalisation ability. In the first stage, we propose\nan attention-based scale-adaptive fusion (ASAF) strategy to handle objects with\nsignificant scale variances. Furthermore, we propose a smoothing hard example\nmining (SHEM) loss function to address the issue of overfitting on hard\nexamples. In the second stage, we propose a sort-based pseudo-labelling\nfine-tuning (SPF) strategy for semi-supervised learning to mitigate the\ndistributional differences between source and target domains. For both stages,\nwe employ weak or strong image augmentation to suit different cross-domain\ntasks. Experimental results on benchmark datasets demonstrate that the proposed\nnetwork can enhance domain adaptation ability for crater detection under\nvarying scenario distributions.\n","authors":["Yifan Liu","Tiecheng Song","Chengye Xian","Ruiyuan Chen","Yi Zhao","Rui Li","Tan Guo"],"pdf_url":"https://arxiv.org/pdf/2312.06169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06890v1","updated":"2024-06-11T02:09:46Z","published":"2024-06-11T02:09:46Z","title":"Motion Consistency Model: Accelerating Video Diffusion with Disentangled\n  Motion-Appearance Distillation","summary":"  Image diffusion distillation achieves high-fidelity generation with very few\nsampling steps. However, applying these techniques directly to video diffusion\noften results in unsatisfactory frame quality due to the limited visual quality\nin public video datasets. This affects the performance of both teacher and\nstudent video diffusion models. Our study aims to improve video diffusion\ndistillation while improving frame appearance using abundant high-quality image\ndata. We propose motion consistency model (MCM), a single-stage video diffusion\ndistillation method that disentangles motion and appearance learning.\nSpecifically, MCM includes a video consistency model that distills motion from\nthe video teacher model, and an image discriminator that enhances frame\nappearance to match high-quality image data. This combination presents two\nchallenges: (1) conflicting frame learning objectives, as video distillation\nlearns from low-quality video frames while the image discriminator targets\nhigh-quality images; and (2) training-inference discrepancies due to the\ndiffering quality of video samples used during training and inference. To\naddress these challenges, we introduce disentangled motion distillation and\nmixed trajectory distillation. The former applies the distillation objective\nsolely to the motion representation, while the latter mitigates\ntraining-inference discrepancies by mixing distillation trajectories from both\nthe low- and high-quality video domains. Extensive experiments show that our\nMCM achieves the state-of-the-art video diffusion distillation performance.\nAdditionally, our method can enhance frame quality in video diffusion models,\nproducing frames with high aesthetic scores or specific styles without\ncorresponding video data.\n","authors":["Yuanhao Zhai","Kevin Lin","Zhengyuan Yang","Linjie Li","Jianfeng Wang","Chung-Ching Lin","David Doermann","Junsong Yuan","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06890v1.pdf","comment":"Project page: https://yhzhai.github.io/mcm/"},{"id":"http://arxiv.org/abs/2307.12900v5","updated":"2024-06-11T01:57:25Z","published":"2023-07-24T15:47:21Z","title":"Automotive Object Detection via Learning Sparse Events by Spiking\n  Neurons","summary":"  Event-based sensors, distinguished by their high temporal resolution of 1\n$\\mathrm{\\mu}\\text{s}$ and a dynamic range of 120 $\\text{dB}$, stand out as\nideal tools for deployment in fast-paced settings like vehicles and drones.\nTraditional object detection techniques that utilize Artificial Neural Networks\n(ANNs) face challenges due to the sparse and asynchronous nature of the events\nthese sensors capture. In contrast, Spiking Neural Networks (SNNs) offer a\npromising alternative, providing a temporal representation that is inherently\naligned with event-based data. This paper explores the unique membrane\npotential dynamics of SNNs and their ability to modulate sparse events. We\nintroduce an innovative spike-triggered adaptive threshold mechanism designed\nfor stable training. Building on these insights, we present a specialized\nspiking feature pyramid network (SpikeFPN) optimized for automotive event-based\nobject detection. Comprehensive evaluations demonstrate that SpikeFPN surpasses\nboth traditional SNNs and advanced ANNs enhanced with attention mechanisms.\nEvidently, SpikeFPN achieves a mean Average Precision (mAP) of 0.477 on the\nGEN1 Automotive Detection (GAD) benchmark dataset, marking significant\nincreases over the selected SNN baselines. Moreover, the efficient design of\nSpikeFPN ensures robust performance while optimizing computational resources,\nattributed to its innate sparse computation capabilities. Source codes are\npublicly accessible at https://github.com/EMI-Group/spikefpn.\n","authors":["Hu Zhang","Yanchen Li","Luziwei Leng","Kaiwei Che","Qian Liu","Qinghai Guo","Jianxing Liao","Ran Cheng"],"pdf_url":"https://arxiv.org/pdf/2307.12900v5.pdf","comment":"IEEE Transactions on Cognitive and Developmental Systems"},{"id":"http://arxiv.org/abs/2406.01062v2","updated":"2024-06-11T01:17:02Z","published":"2024-06-03T07:20:34Z","title":"SceneTextGen: Layout-Agnostic Scene Text Image Synthesis with Diffusion\n  Models","summary":"  While diffusion models have significantly advanced the quality of image\ngeneration, their capability to accurately and coherently render text within\nthese images remains a substantial challenge. Conventional diffusion-based\nmethods for scene text generation are typically limited by their reliance on an\nintermediate layout output. This dependency often results in a constrained\ndiversity of text styles and fonts, an inherent limitation stemming from the\ndeterministic nature of the layout generation phase. To address these\nchallenges, this paper introduces SceneTextGen, a novel diffusion-based model\nspecifically designed to circumvent the need for a predefined layout stage. By\ndoing so, SceneTextGen facilitates a more natural and varied representation of\ntext. The novelty of SceneTextGen lies in its integration of three key\ncomponents: a character-level encoder for capturing detailed typographic\nproperties, coupled with a character-level instance segmentation model and a\nword-level spotting model to address the issues of unwanted text generation and\nminor character inaccuracies. We validate the performance of our method by\ndemonstrating improved character recognition rates on generated images across\ndifferent public visual text datasets in comparison to both standard diffusion\nbased methods and text specific methods.\n","authors":["Qilong Zhangli","Jindong Jiang","Di Liu","Licheng Yu","Xiaoliang Dai","Ankit Ramchandani","Guan Pang","Dimitris N. Metaxas","Praveen Krishnan"],"pdf_url":"https://arxiv.org/pdf/2406.01062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05658v2","updated":"2024-06-11T01:15:17Z","published":"2024-06-09T05:57:40Z","title":"Visual Prompt Tuning in Null Space for Continual Learning","summary":"  Existing prompt-tuning methods have demonstrated impressive performances in\ncontinual learning (CL), by selecting and updating relevant prompts in the\nvision-transformer models. On the contrary, this paper aims to learn each task\nby tuning the prompts in the direction orthogonal to the subspace spanned by\nprevious tasks' features, so as to ensure no interference on tasks that have\nbeen learned to overcome catastrophic forgetting in CL. However, different from\nthe orthogonal projection in the traditional CNN architecture, the prompt\ngradient orthogonal projection in the ViT architecture shows completely\ndifferent and greater challenges, i.e., 1) the high-order and non-linear\nself-attention operation; 2) the drift of prompt distribution brought by the\nLayerNorm in the transformer block. Theoretically, we have finally deduced two\nconsistency conditions to achieve the prompt gradient orthogonal projection,\nwhich provide a theoretical guarantee of eliminating interference on previously\nlearned knowledge via the self-attention mechanism in visual prompt tuning. In\npractice, an effective null-space-based approximation solution has been\nproposed to implement the prompt gradient orthogonal projection. Extensive\nexperimental results demonstrate the effectiveness of anti-forgetting on four\nclass-incremental benchmarks with diverse pre-trained baseline models, and our\napproach achieves superior performances to state-of-the-art methods. Our code\nis available at https://github.com/zugexiaodui/VPTinNSforCL.\n","authors":["Yue Lu","Shizhou Zhang","De Cheng","Yinghui Xing","Nannan Wang","Peng Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.05658v2.pdf","comment":"20 pages, 10 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.07432v1","updated":"2024-06-11T16:34:21Z","published":"2024-06-11T16:34:21Z","title":"Matryoshka Representation Learning for Recommendation","summary":"  Representation learning is essential for deep-neural-network-based\nrecommender systems to capture user preferences and item features within\nfixed-dimensional user and item vectors. Unlike existing representation\nlearning methods that either treat each user preference and item feature\nuniformly or categorize them into discrete clusters, we argue that in the real\nworld, user preferences and item features are naturally expressed and organized\nin a hierarchical manner, leading to a new direction for representation\nlearning. In this paper, we introduce a novel matryoshka representation\nlearning method for recommendation (MRL4Rec), by which we restructure user and\nitem vectors into matryoshka representations with incrementally dimensional and\noverlapping vector spaces to explicitly represent user preferences and item\nfeatures at different hierarchical levels. We theoretically establish that\nconstructing training triplets specific to each level is pivotal in\nguaranteeing accurate matryoshka representation learning. Subsequently, we\npropose the matryoshka negative sampling mechanism to construct training\ntriplets, which further ensures the effectiveness of the matryoshka\nrepresentation learning in capturing hierarchical user preferences and item\nfeatures. The experiments demonstrate that MRL4Rec can consistently and\nsubstantially outperform a number of state-of-the-art competitors on several\nreal-life datasets. Our code is publicly available at\nhttps://github.com/Riwei-HEU/MRL.\n","authors":["Riwei Lai","Li Chen","Weixin Chen","Rui Chen"],"pdf_url":"https://arxiv.org/pdf/2406.07432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07420v1","updated":"2024-06-11T16:21:57Z","published":"2024-06-11T16:21:57Z","title":"Graph Reasoning for Explainable Cold Start Recommendation","summary":"  The cold start problem, where new users or items have no interaction history,\nremains a critical challenge in recommender systems (RS). A common solution\ninvolves using Knowledge Graphs (KG) to train entity embeddings or Graph Neural\nNetworks (GNNs). Since KGs incorporate auxiliary data and not just user/item\ninteractions, these methods can make relevant recommendations for cold users or\nitems. Graph Reasoning (GR) methods, however, find paths from users to items to\nrecommend using relations in the KG and, in the context of RS, have been used\nfor interpretability. In this study, we propose GRECS: a framework for adapting\nGR to cold start recommendations. By utilizing explicit paths starting for\nusers rather than relying only on entity embeddings, GRECS can find items\ncorresponding to users' preferences by navigating the graph, even when limited\ninformation about users is available. Our experiments show that GRECS mitigates\nthe cold start problem and outperforms competitive baselines across 5 standard\ndatasets while being explainable. This study highlights the potential of GR for\ndeveloping explainable recommender systems better suited for managing cold\nusers and items.\n","authors":["Jibril Frej","Marta Knezevic","Tanja Kaser"],"pdf_url":"https://arxiv.org/pdf/2406.07420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07331v1","updated":"2024-06-11T15:01:03Z","published":"2024-06-11T15:01:03Z","title":"Text Information Retrieval in Tetun: A Preliminary Study","summary":"  Tetun is one of Timor-Leste's official languages alongside Portuguese. It is\na low-resource language with over 932,400 speakers that started developing when\nTimor-Leste restored its independence in 2002. The media mainly uses Tetun, and\nmore than ten national online newspapers actively broadcast news in Tetun every\nday. However, since information retrieval-based solutions for Tetun do not\nexist, finding Tetun information on the internet is challenging. This work aims\nto investigate and develop solutions that can enable the application of\ninformation retrieval techniques to develop search solutions for Tetun. We\npresent a preliminary result of an experiment conducted on the task of ad-hoc\nretrieval in Tetun.\n","authors":["Gabriel de Jesus"],"pdf_url":"https://arxiv.org/pdf/2406.07331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05967v2","updated":"2024-06-11T14:58:42Z","published":"2024-01-11T15:13:00Z","title":"Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph\n  Embedding","summary":"  The primary aim of Knowledge Graph embeddings (KGE) is to learn\nlow-dimensional representations of entities and relations for predicting\nmissing facts. While rotation-based methods like RotatE and QuatE perform well\nin KGE, they face two challenges: limited model flexibility requiring\nproportional increases in relation size with entity dimension, and difficulties\nin generalizing the model for higher-dimensional rotations. To address these\nissues, we introduce OrthogonalE, a novel KGE model employing matrices for\nentities and block-diagonal orthogonal matrices with Riemannian optimization\nfor relations. This approach enhances the generality and flexibility of KGE\nmodels. The experimental results indicate that our new KGE model, OrthogonalE,\nis both general and flexible, significantly outperforming state-of-the-art KGE\nmodels while substantially reducing the number of relation parameters.\n","authors":["Yihua Zhu","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2401.05967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07315v1","updated":"2024-06-11T14:45:00Z","published":"2024-06-11T14:45:00Z","title":"Fetch-A-Set: A Large-Scale OCR-Free Benchmark for Historical Document\n  Retrieval","summary":"  This paper introduces Fetch-A-Set (FAS), a comprehensive benchmark tailored\nfor legislative historical document analysis systems, addressing the challenges\nof large-scale document retrieval in historical contexts. The benchmark\ncomprises a vast repository of documents dating back to the XVII century,\nserving both as a training resource and an evaluation benchmark for retrieval\nsystems. It fills a critical gap in the literature by focusing on complex\nextractive tasks within the domain of cultural heritage. The proposed benchmark\ntackles the multifaceted problem of historical document analysis, including\ntext-to-image retrieval for queries and image-to-text topic extraction from\ndocument fragments, all while accommodating varying levels of document\nlegibility. This benchmark aims to spur advancements in the field by providing\nbaselines and data for the development and evaluation of robust historical\ndocument retrieval systems, particularly in scenarios characterized by wide\nhistorical spectrum.\n","authors":["Adrià Molina","Oriol Ramos Terrades","Josep Lladós"],"pdf_url":"https://arxiv.org/pdf/2406.07315v1.pdf","comment":"Preprint for the manuscript accepted for publication in the DAS2024\n  LNCS proceedings"},{"id":"http://arxiv.org/abs/2406.07299v1","updated":"2024-06-11T14:28:24Z","published":"2024-06-11T14:28:24Z","title":"Exploring Large Language Models for Relevance Judgments in Tetun","summary":"  The Cranfield paradigm has served as a foundational approach for developing\ntest collections, with relevance judgments typically conducted by human\nassessors. However, the emergence of large language models (LLMs) has\nintroduced new possibilities for automating these tasks. This paper explores\nthe feasibility of using LLMs to automate relevance assessments, particularly\nwithin the context of low-resource languages. In our study, LLMs are employed\nto automate relevance judgment tasks, by providing a series of query-document\npairs in Tetun as the input text. The models are tasked with assigning\nrelevance scores to each pair, where these scores are then compared to those\nfrom human annotators to evaluate the inter-annotator agreement levels. Our\ninvestigation reveals results that align closely with those reported in studies\nof high-resource languages.\n","authors":["Gabriel de Jesus","Sérgio Nunes"],"pdf_url":"https://arxiv.org/pdf/2406.07299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07227v1","updated":"2024-06-11T13:06:09Z","published":"2024-06-11T13:06:09Z","title":"Which Country Is This? Automatic Country Ranking of Street View Photos","summary":"  In this demonstration, we present Country Guesser, a live system that guesses\nthe country that a photo is taken in. In particular, given a Google Street View\nimage, our federated ranking model uses a combination of computer vision,\nmachine learning and text retrieval methods to compute a ranking of likely\ncountries of the location shown in a given image from Street View.\nInterestingly, using text-based features to probe large pre-trained language\nmodels can assist to provide cross-modal supervision. We are not aware of\nprevious country guessing systems informed by visual and textual features.\n","authors":["Tim Menzner","Jochen L. Leidner","Florian Mittag"],"pdf_url":"https://arxiv.org/pdf/2406.07227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04761v4","updated":"2024-06-11T11:38:57Z","published":"2023-09-09T11:20:40Z","title":"A Comprehensive Survey on Deep Learning Techniques in Educational Data\n  Mining","summary":"  Educational Data Mining (EDM) has emerged as a vital field of research, which\nharnesses the power of computational techniques to analyze educational data.\nWith the increasing complexity and diversity of educational data, Deep Learning\ntechniques have shown significant advantages in addressing the challenges\nassociated with analyzing and modeling this data. This survey aims to\nsystematically review the state-of-the-art in EDM with Deep Learning. We begin\nby providing a brief introduction to EDM and Deep Learning, highlighting their\nrelevance in the context of modern education. Next, we present a detailed\nreview of Deep Learning techniques applied in four typical educational\nscenarios, including knowledge tracing, student behavior detection, performance\nprediction, and personalized recommendation. Furthermore, a comprehensive\noverview of public datasets and processing tools for EDM is provided. We then\nanalyze the practical challenges in EDM and propose targeted solutions.\nFinally, we point out emerging trends and future directions in this research\narea.\n","authors":["Yuanguo Lin","Hong Chen","Wei Xia","Fan Lin","Zongyue Wang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2309.04761v4.pdf","comment":"21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.07151v1","updated":"2024-06-11T10:52:17Z","published":"2024-06-11T10:52:17Z","title":"EEG-ImageNet: An Electroencephalogram Dataset and Benchmarks with Image\n  Visual Stimuli of Multi-Granularity Labels","summary":"  Identifying and reconstructing what we see from brain activity gives us a\nspecial insight into investigating how the biological visual system represents\nthe world. While recent efforts have achieved high-performance image\nclassification and high-quality image reconstruction from brain signals\ncollected by Functional Magnetic Resonance Imaging (fMRI) or\nmagnetoencephalogram (MEG), the expensiveness and bulkiness of these devices\nmake relevant applications difficult to generalize to practical applications.\nOn the other hand, Electroencephalography (EEG), despite its advantages of ease\nof use, cost-efficiency, high temporal resolution, and non-invasive nature, has\nnot been fully explored in relevant studies due to the lack of comprehensive\ndatasets. To address this gap, we introduce EEG-ImageNet, a novel EEG dataset\ncomprising recordings from 16 subjects exposed to 4000 images selected from the\nImageNet dataset. EEG-ImageNet consists of 5 times EEG-image pairs larger than\nexisting similar EEG benchmarks. EEG-ImageNet is collected with image stimuli\nof multi-granularity labels, i.e., 40 images with coarse-grained labels and 40\nwith fine-grained labels. Based on it, we establish benchmarks for object\nclassification and image reconstruction. Experiments with several commonly used\nmodels show that the best models can achieve object classification with\naccuracy around 60% and image reconstruction with two-way identification around\n64%. These results demonstrate the dataset's potential to advance EEG-based\nvisual brain-computer interfaces, understand the visual perception of\nbiological systems, and provide potential applications in improving machine\nvisual models.\n","authors":["Shuqi Zhu","Ziyi Ye","Qingyao Ai","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2406.07151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07136v1","updated":"2024-06-11T10:30:19Z","published":"2024-06-11T10:30:19Z","title":"Progressive Query Expansion for Retrieval Over Cost-constrained Data\n  Sources","summary":"  Query expansion has been employed for a long time to improve the accuracy of\nquery retrievers. Earlier works relied on pseudo-relevance feedback (PRF)\ntechniques, which augment a query with terms extracted from documents retrieved\nin a first stage. However, the documents may be noisy hindering the\neffectiveness of the ranking. To avoid this, recent studies have instead used\nLarge Language Models (LLMs) to generate additional content to expand a query.\nThese techniques are prone to hallucination and also focus on the LLM usage\ncost. However, the cost may be dominated by the retrieval in several important\npractical scenarios, where the corpus is only available via APIs which charge a\nfee per retrieved document. We propose combining classic PRF techniques with\nLLMs and create a progressive query expansion algorithm ProQE that iteratively\nexpands the query as it retrieves more documents. ProQE is compatible with both\nsparse and dense retrieval systems. Our experimental results on four retrieval\ndatasets show that ProQE outperforms state-of-the-art baselines by 37% and is\nthe most cost-effective.\n","authors":["Muhammad Shihab Rashid","Jannat Ara Meem","Yue Dong","Vagelis Hristidis"],"pdf_url":"https://arxiv.org/pdf/2406.07136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13478v2","updated":"2024-06-11T10:18:08Z","published":"2024-01-24T14:23:12Z","title":"SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval","summary":"  Multi-modal information retrieval (MMIR) is a rapidly evolving field, where\nsignificant progress, particularly in image-text pairing, has been made through\nadvanced representation learning and cross-modality alignment research.\nHowever, current benchmarks for evaluating MMIR performance in image-text\npairing within the scientific domain show a notable gap, where chart and table\nimages described in scholarly language usually do not play a significant role.\nTo bridge this gap, we develop a specialised scientific MMIR (SciMMIR)\nbenchmark by leveraging open-access paper collections to extract data relevant\nto the scientific domain. This benchmark comprises 530K meticulously curated\nimage-text pairs, extracted from figures and tables with detailed captions in\nscientific documents. We further annotate the image-text pairs with two-level\nsubset-subcategory hierarchy annotations to facilitate a more comprehensive\nevaluation of the baselines. We conducted zero-shot and fine-tuning evaluations\non prominent multi-modal image-captioning and visual language models, such as\nCLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific\ndomain, including the impact of pre-training and fine-tuning settings and the\ninfluence of the visual and textual encoders. All our data and checkpoints are\npublicly available at https://github.com/Wusiwei0410/SciMMIR.\n","authors":["Siwei Wu","Yizhi Li","Kang Zhu","Ge Zhang","Yiming Liang","Kaijing Ma","Chenghao Xiao","Haoran Zhang","Bohao Yang","Wenhu Chen","Wenhao Huang","Noura Al Moubayed","Jie Fu","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2401.13478v2.pdf","comment":"camera-ready version for ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.07121v1","updated":"2024-06-11T10:10:22Z","published":"2024-06-11T10:10:22Z","title":"The Treatment of Ties in Rank-Biased Overlap","summary":"  Rank-Biased Overlap (RBO) is a similarity measure for indefinite rankings: it\nis top-weighted, and can be computed when only a prefix of the rankings is\nknown or when they have only some items in common. It is widely used for\ninstance to analyze differences between search engines by comparing the\nrankings of documents they retrieve for the same queries. In these situations,\nthough, it is very frequent to find tied documents that have the same score.\nUnfortunately, the treatment of ties in RBO remains superficial and incomplete,\nin the sense that it is not clear how to calculate it from the ranking prefixes\nonly. In addition, the existing way of dealing with ties is very different from\nthe one traditionally followed in the field of Statistics, most notably found\nin rank correlation coefficients such as Kendall's and Spearman's. In this\npaper we propose a generalized formulation for RBO to handle ties, thanks to\nwhich we complete the original definitions by showing how to perform prefix\nevaluation. We also use it to fully develop two variants that align with the\nones found in the Statistics literature: one when there is a reference ranking\nto compare to, and one when there is not. Overall, these three variants provide\nresearchers with flexibility when comparing rankings with RBO, by clearly\ndetermining what ties mean, and how they should be treated. Finally, using both\nsynthetic and TREC data, we demonstrate the use of these new tie-aware RBO\nmeasures. We show that the scores may differ substantially from the original\ntie-unaware RBO measure, where ties had to be broken at random or by arbitrary\ncriteria such as by document ID. Overall, these results evidence the need for a\nproper account of ties in rank similarity measures such as RBO.\n","authors":["Matteo Corsi","Julián Urbano"],"pdf_url":"https://arxiv.org/pdf/2406.07121v1.pdf","comment":"10 pages, 5 figures, 4 tables, SIGIR 2024"},{"id":"http://arxiv.org/abs/2406.07114v1","updated":"2024-06-11T09:58:27Z","published":"2024-06-11T09:58:27Z","title":"Unlocking the Potential of the Metaverse for Innovative and Immersive\n  Digital Care","summary":"  The Metaverse, a persistent, immersive virtual environment, has the immense\npotential to revolutionize healthcare by transforming patient care, medical\neducation, and research. This paper explores the applications, benefits, and\nchallenges associated with this transformative technology, highlighting its\nability to improve patient engagement, communication, access to information,\nand health outcomes. The paper also examines how the analysis of Metaverse data\nusing machine learning techniques can unlock insights to further enhance\nhealthcare applications. The discussion summarizes key findings, analyzes the\nsignificance and practical implications of Metaverse integration, and\nidentifies areas for future research. It underscores the role of major tech\ncompanies in developing Metaverse-based solutions and the importance of\naddressing emerging opportunities and challenges to unlock the transformative\npotential of this technology in healthcare. The paper concludes by emphasizing\nthe need for collaboration between stakeholders to ensure the ethical and\neffective implementation of these technologies, ultimately leading to a more\naccessible, personalized, and efficient healthcare system.\n","authors":["Fatemeh Ebrahimzadeh","Ramin Safa"],"pdf_url":"https://arxiv.org/pdf/2406.07114v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.07098v1","updated":"2024-06-11T09:38:46Z","published":"2024-06-11T09:38:46Z","title":"Guiding Catalogue Enrichment with User Queries","summary":"  Techniques for knowledge graph (KGs) enrichment have been increasingly\ncrucial for commercial applications that rely on evolving product catalogues.\nHowever, because of the huge search space of potential enrichment, predictions\nfrom KG completion (KGC) methods suffer from low precision, making them\nunreliable for real-world catalogues. Moreover, candidate facts for enrichment\nhave varied relevance to users. While making correct predictions for incomplete\ntriplets in KGs has been the main focus of KGC method, the relevance of when to\napply such predictions has been neglected. Motivated by the product search use\ncase, we address the angle of generating relevant completion for a catalogue\nusing user search behaviour and the users property association with a product.\nIn this paper, we present our intuition for identifying enrichable data points\nand use general-purpose KGs to show-case the performance benefits. In\nparticular, we extract entity-predicate pairs from user queries, which are more\nlikely to be correct and relevant, and use these pairs to guide the prediction\nof KGC methods. We assess our method on two popular encyclopedia KGs, DBPedia\nand YAGO 4. Our results from both automatic and human evaluations show that\nquery guidance can significantly improve the correctness and relevance of\nprediction.\n","authors":["Yupei Du","Jacek Golebiowski","Philipp Schmidt","Ziawasch Abedjan"],"pdf_url":"https://arxiv.org/pdf/2406.07098v1.pdf","comment":"ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2406.07094v1","updated":"2024-06-11T09:33:15Z","published":"2024-06-11T09:33:15Z","title":"Grapevine Disease Prediction Using Climate Variables from Multi-Sensor\n  Remote Sensing Imagery via a Transformer Model","summary":"  Early detection and management of grapevine diseases are important in\npursuing sustainable viticulture. This paper introduces a novel framework\nleveraging the TabPFN model to forecast blockwise grapevine diseases using\nclimate variables from multi-sensor remote sensing imagery. By integrating\nadvanced machine learning techniques with detailed environmental data, our\napproach significantly enhances the accuracy and efficiency of disease\nprediction in vineyards. The TabPFN model's experimental evaluations showcase\ncomparable performance to traditional gradient-boosted decision trees, such as\nXGBoost, CatBoost, and LightGBM. The model's capability to process complex data\nand provide per-pixel disease-affecting probabilities enables precise, targeted\ninterventions, contributing to more sustainable disease management practices.\nOur findings underscore the transformative potential of combining Transformer\nmodels with remote sensing data in precision agriculture, offering a scalable\nsolution for improving crop health and productivity while reducing\nenvironmental impact.\n","authors":["Weiying Zhao","Natalia Efremova"],"pdf_url":"https://arxiv.org/pdf/2406.07094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20718v2","updated":"2024-06-11T09:29:46Z","published":"2024-05-31T09:14:48Z","title":"Popularity-Aware Alignment and Contrast for Mitigating Popularity Bias","summary":"  Collaborative Filtering (CF) typically suffers from the significant challenge\nof popularity bias due to the uneven distribution of items in real-world\ndatasets. This bias leads to a significant accuracy gap between popular and\nunpopular items. It not only hinders accurate user preference understanding but\nalso exacerbates the Matthew effect in recommendation systems. To alleviate\npopularity bias, existing efforts focus on emphasizing unpopular items or\nseparating the correlation between item representations and their popularity.\nDespite the effectiveness, existing works still face two persistent challenges:\n(1) how to extract common supervision signals from popular items to improve the\nunpopular item representations, and (2) how to alleviate the representation\nseparation caused by popularity bias. In this work, we conduct an empirical\nanalysis of popularity bias and propose Popularity-Aware Alignment and Contrast\n(PAAC) to address two challenges. Specifically, we use the common supervisory\nsignals modeled in popular item representations and propose a novel\npopularity-aware supervised alignment module to learn unpopular item\nrepresentations. Additionally, we suggest re-weighting the contrastive learning\nloss to mitigate the representation separation from a popularity-centric\nperspective. Finally, we validate the effectiveness and rationale of PAAC in\nmitigating popularity bias through extensive experiments on three real-world\ndatasets. Our code is available at\nhttps://github.com/miaomiao-cai2/KDD2024-PAAC.\n","authors":["Miaomiao Cai","Lei Chen","Yifan Wang","Haoyue Bai","Peijie Sun","Le Wu","Min Zhang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2405.20718v2.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2406.07067v1","updated":"2024-06-11T08:53:15Z","published":"2024-06-11T08:53:15Z","title":"TIM: Temporal Interaction Model in Notification System","summary":"  Modern mobile applications heavily rely on the notification system to acquire\ndaily active users and enhance user engagement. Being able to proactively reach\nusers, the system has to decide when to send notifications to users. Although\nmany researchers have studied optimizing the timing of sending notifications,\nthey only utilized users' contextual features, without modeling users' behavior\npatterns. Additionally, these efforts only focus on individual notifications,\nand there is a lack of studies on optimizing the holistic timing of multiple\nnotifications within a period. To bridge these gaps, we propose the Temporal\nInteraction Model (TIM), which models users' behavior patterns by estimating\nCTR in every time slot over a day in our short video application Kuaishou. TIM\nleverages long-term user historical interaction sequence features such as\nnotification receipts, clicks, watch time and effective views, and employs a\ntemporal attention unit (TAU) to extract user behavior patterns. Moreover, we\nprovide an elegant strategy of holistic notifications send time control to\nimprove user engagement while minimizing disruption. We evaluate the\neffectiveness of TIM through offline experiments and online A/B tests. The\nresults indicate that TIM is a reliable tool for forecasting user behavior,\nleading to a remarkable enhancement in user engagement without causing undue\ndisturbance.\n","authors":["Huxiao Ji","Haitao Yang","Linchuan Li","Shunyu Zhang","Cunyi Zhang","Xuanping Li","Wenwu Ou"],"pdf_url":"https://arxiv.org/pdf/2406.07067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18146v2","updated":"2024-06-11T06:47:50Z","published":"2024-05-28T13:06:32Z","title":"Unified Low-rank Compression Framework for Click-through Rate Prediction","summary":"  Deep Click-Through Rate (CTR) prediction models play an important role in\nmodern industrial recommendation scenarios. However, high memory overhead and\ncomputational costs limit their deployment in resource-constrained\nenvironments. Low-rank approximation is an effective method for computer vision\nand natural language processing models, but its application in compressing CTR\nprediction models has been less explored. Due to the limited memory and\ncomputing resources, compression of CTR prediction models often confronts three\nfundamental challenges, i.e., (1). How to reduce the model sizes to adapt to\nedge devices? (2). How to speed up CTR prediction model inference? (3). How to\nretain the capabilities of original models after compression? Previous low-rank\ncompression research mostly uses tensor decomposition, which can achieve a high\nparameter compression ratio, but brings in AUC degradation and additional\ncomputing overhead. To address these challenges, we propose a unified low-rank\ndecomposition framework for compressing CTR prediction models. We find that\neven with the most classic matrix decomposition SVD method, our framework can\nachieve better performance than the original model. To further improve the\neffectiveness of our framework, we locally compress the output features instead\nof compressing the model weights. Our unified low-rank compression framework\ncan be applied to embedding tables and MLP layers in various CTR prediction\nmodels. Extensive experiments on two academic datasets and one real industrial\nbenchmark demonstrate that, with 3-5x model size reduction, our compressed\nmodels can achieve both faster inference and higher AUC than the uncompressed\noriginal models. Our code is at\nhttps://github.com/yuhao318/Atomic_Feature_Mimicking.\n","authors":["Hao Yu","Minghao Fu","Jiandong Ding","Yusheng Zhou","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2405.18146v2.pdf","comment":"Accepted by KDD2024 Applied Data Science (ADS) Track"},{"id":"http://arxiv.org/abs/2406.06955v1","updated":"2024-06-11T05:25:48Z","published":"2024-06-11T05:25:48Z","title":"ElasticRec: A Microservice-based Model Serving Architecture Enabling\n  Elastic Resource Scaling for Recommendation Models","summary":"  With the increasing popularity of recommendation systems (RecSys), the demand\nfor compute resources in datacenters has surged. However, the model-wise\nresource allocation employed in current RecSys model serving architectures\nfalls short in effectively utilizing resources, leading to sub-optimal total\ncost of ownership. We propose ElasticRec, a model serving architecture for\nRecSys providing resource elasticity and high memory efficiency. ElasticRec is\nbased on a microservice-based software architecture for fine-grained resource\nallocation, tailored to the heterogeneous resource demands of RecSys.\nAdditionally, ElasticRec achieves high memory efficiency via our utility-based\nresource allocation. Overall, ElasticRec achieves an average 3.3x reduction in\nmemory allocation size and 8.1x increase in memory utility, resulting in an\naverage 1.6x reduction in deployment cost compared to state-of-the-art RecSys\ninference serving system.\n","authors":["Yujeong Choi","Jiin Kim","Minsoo Rhu"],"pdf_url":"https://arxiv.org/pdf/2406.06955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03578v2","updated":"2024-06-11T04:49:42Z","published":"2023-08-07T13:35:02Z","title":"TeraHAC: Hierarchical Agglomerative Clustering of Trillion-Edge Graphs","summary":"  We introduce TeraHAC, a $(1+\\epsilon)$-approximate hierarchical agglomerative\nclustering (HAC) algorithm which scales to trillion-edge graphs. Our algorithm\nis based on a new approach to computing $(1+\\epsilon)$-approximate HAC, which\nis a novel combination of the nearest-neighbor chain algorithm and the notion\nof $(1+\\epsilon)$-approximate HAC. Our approach allows us to partition the\ngraph among multiple machines and make significant progress in computing the\nclustering within each partition before any communication with other partitions\nis needed.\n  We evaluate TeraHAC on a number of real-world and synthetic graphs of up to 8\ntrillion edges. We show that TeraHAC requires over 100x fewer rounds compared\nto previously known approaches for computing HAC. It is up to 8.3x faster than\nSCC, the state-of-the-art distributed algorithm for hierarchical clustering,\nwhile achieving 1.16x higher quality. In fact, TeraHAC essentially retains the\nquality of the celebrated HAC algorithm while significantly improving the\nrunning time.\n","authors":["Laxman Dhulipala","Jason Lee","Jakub Łącki","Vahab Mirrokni"],"pdf_url":"https://arxiv.org/pdf/2308.03578v2.pdf","comment":"SIGMOD 2024"},{"id":"http://arxiv.org/abs/2406.06925v1","updated":"2024-06-11T03:44:17Z","published":"2024-06-11T03:44:17Z","title":"Non-autoregressive Personalized Bundle Generation","summary":"  The personalized bundle generation problem, which aims to create a preferred\nbundle for user from numerous candidate items, receives increasing attention in\nrecommendation. However, existing works ignore the order-invariant nature of\nthe bundle and adopt sequential modeling methods as the solution, which might\nintroduce inductive bias and cause a large latency in prediction. To address\nthis problem, we propose to perform the bundle generation via\nnon-autoregressive mechanism and design a novel encoder-decoder framework named\nBundleNAT, which can effectively output the targeted bundle in one-shot without\nrelying on any inherent order. In detail, instead of learning sequential\ndependency, we propose to adopt pre-training techniques and graph neural\nnetwork to fully embed user-based preference and item-based compatibility\ninformation, and use a self-attention based encoder to further extract global\ndependency pattern. We then design a permutation-equivariant decoding\narchitecture that is able to directly output the desired bundle in a one-shot\nmanner. Experiments on three real-world datasets from Youshu and Netease show\nthe proposed BundleNAT significantly outperforms the current state-of-the-art\nmethods in average by up to 35.92%, 10.97% and 23.67% absolute improvements in\nPrecision, Precision+, and Recall, respectively.\n","authors":["Wenchuan Yang","Cheng Yang","Jichao Li","Yuejin Tan","Xin Lu","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2406.06925v1.pdf","comment":"Submitted to Information Processing & Management"},{"id":"http://arxiv.org/abs/2308.04913v2","updated":"2024-06-11T02:14:06Z","published":"2023-08-09T12:26:37Z","title":"LLaMA-E: Empowering E-commerce Authoring with Object-Interleaved\n  Instruction Following","summary":"  E-commerce authoring entails creating engaging, diverse, and targeted content\nto enhance preference elicitation and retrieval experience. While Large\nLanguage Models (LLMs) have revolutionized content generation, they often fall\nshort in e-commerce applications due to their limited memorization of\ndomain-specific features. This paper proposes LLaMA-E, the unified e-commerce\nauthoring models that address the contextual preferences of customers, sellers,\nand platforms, the essential objects in e-commerce operation. We design the\ninstruction set derived from tasks of ads generation, query-enhanced product\ntitle rewriting, product classification, purchase intent speculation, and\ngeneral e-commerce Q&A. The instruction formulation ensures the interleaved\ncover of the presented and required object features, allowing the alignment of\nbase models to parameterise e-commerce knowledge comprehensively. The proposed\nLLaMA-E models achieve state-of-the-art evaluation performance and exhibit the\nadvantage in zero-shot practical applications. To our knowledge, this is the\nfirst LLM tailored to empower authoring applications with comprehensive\nscenario understanding by integrating features focused on participated objects.\n","authors":["Kaize Shi","Xueyao Sun","Dingxian Wang","Yinlin Fu","Guandong Xu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2308.04913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07730v1","updated":"2024-06-11T21:25:14Z","published":"2024-06-11T21:25:14Z","title":"\"It answers questions that I didn't know I had\": Ph.D. Students'\n  Evaluation of an Information Sharing Knowledge Graph","summary":"  Interdisciplinary PhD programs can be challenging as the vital information\nneeded by students may not be readily available, it is scattered across\nuniversity's websites, while tacit knowledge can be obtained only by\ninteracting with people. Hence, there is a need to develop a knowledge\nmanagement model to create, query, and maintain a knowledge repository for\ninterdisciplinary students. We propose a knowledge graph containing information\non critical categories and their relationships, extracted from multiple\nsources, essential for interdisciplinary PhD students. This study evaluates the\nusability of a participatory designed knowledge graph intended to facilitate\ninformation exchange and decision-making. The usability findings demonstrate\nthat interaction with this knowledge graph benefits PhD students by notably\nreducing uncertainty and academic stress, particularly among newcomers.\nKnowledge graph supported them in decision making, especially when choosing\ncollaborators in an interdisciplinary setting. Key helpful features are related\nto exploring student faculty networks, milestones tracking, rapid access to\naggregated data, and insights into crowdsourced fellow students' activities.\nThe knowledge graph provides a solution to meet the personalized needs of\ndoctoral researchers and has the potential to improve the information discovery\nand decision-making process substantially. It also includes the tacit knowledge\nexchange support missing from most current approaches, which is critical for\nthis population and establishing interdisciplinary collaborations. This\napproach can be applied to other interdisciplinary programs and domains\nglobally.\n","authors":["Stanislava Gardasevic","Manika Lamba"],"pdf_url":"https://arxiv.org/pdf/2406.07730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.08433v2","updated":"2024-06-11T19:44:07Z","published":"2021-04-17T03:29:22Z","title":"Are Word Embedding Methods Stable and Should We Care About It?","summary":"  A representation learning method is considered stable if it consistently\ngenerates similar representation of the given data across multiple runs. Word\nEmbedding Methods (WEMs) are a class of representation learning methods that\ngenerate dense vector representation for each word in the given text data. The\ncentral idea of this paper is to explore the stability measurement of WEMs\nusing intrinsic evaluation based on word similarity. We experiment with three\npopular WEMs: Word2Vec, GloVe, and fastText. For stability measurement, we\ninvestigate the effect of five parameters involved in training these models. We\nperform experiments using four real-world datasets from different domains:\nWikipedia, News, Song lyrics, and European parliament proceedings. We also\nobserve the effect of WEM stability on three downstream tasks: Clustering, POS\ntagging, and Fairness evaluation. Our experiments indicate that amongst the\nthree WEMs, fastText is the most stable, followed by GloVe and Word2Vec.\n","authors":["Angana Borah","Manash Pratim Barman","Amit Awekar"],"pdf_url":"https://arxiv.org/pdf/2104.08433v2.pdf","comment":"Accepted to ACM Hypertext 2021"},{"id":"http://arxiv.org/abs/2406.09438v1","updated":"2024-06-11T20:07:39Z","published":"2024-06-11T20:07:39Z","title":"Exploring Traffic Crash Narratives in Jordan Using Text Mining Analytics","summary":"  This study explores traffic crash narratives in an attempt to inform and\nenhance effective traffic safety policies using text-mining analytics. Text\nmining techniques are employed to unravel key themes and trends within the\nnarratives, aiming to provide a deeper understanding of the factors\ncontributing to traffic crashes. This study collected crash data from five\nmajor freeways in Jordan that cover narratives of 7,587 records from 2018-2022.\nAn unsupervised learning method was adopted to learn the pattern from crash\ndata. Various text mining techniques, such as topic modeling, keyword\nextraction, and Word Co-Occurrence Network, were also used to reveal the\nco-occurrence of crash patterns. Results show that text mining analytics is a\npromising method and underscore the multifactorial nature of traffic crashes,\nincluding intertwining human decisions and vehicular conditions. The recurrent\nthemes across all analyses highlight the need for a balanced approach to road\nsafety, merging both proactive and reactive measures. Emphasis on driver\neducation and awareness around animal-related incidents is paramount.\n","authors":["Shadi Jaradat","Taqwa I. Alhadidi","Huthaifa I. Ashqar","Ahmed Hossain","Mohammed Elhenawy"],"pdf_url":"https://arxiv.org/pdf/2406.09438v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2406.07548v1","updated":"2024-06-11T17:59:53Z","published":"2024-06-11T17:59:53Z","title":"Image and Video Tokenization with Binary Spherical Quantization","summary":"  We propose a new transformer-based image and video tokenizer with Binary\nSpherical Quantization (BSQ). BSQ projects the high-dimensional visual\nembedding to a lower-dimensional hypersphere and then applies binary\nquantization. BSQ is (1) parameter-efficient without an explicit codebook, (2)\nscalable to arbitrary token dimensions, and (3) compact: compressing visual\ndata by up to 100$\\times$ with minimal distortion. Our tokenizer uses a\ntransformer encoder and decoder with simple block-wise causal masking to\nsupport variable-length videos as input. The resulting BSQ-ViT achieves\nstate-of-the-art visual reconstruction quality on image and video\nreconstruction benchmarks with 2.4$\\times$ throughput compared to the best\nprior methods. Furthermore, by learning an autoregressive prior for adaptive\narithmetic coding, BSQ-ViT achieves comparable results on video compression\nwith state-of-the-art video compression standards. BSQ-ViT also enables masked\nlanguage models to achieve competitive image synthesis quality to GAN- and\ndiffusion-based methods.\n","authors":["Yue Zhao","Yuanjun Xiong","Philipp Krähenbühl"],"pdf_url":"https://arxiv.org/pdf/2406.07548v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2406.07544v1","updated":"2024-06-11T17:59:45Z","published":"2024-06-11T17:59:45Z","title":"Situational Awareness Matters in 3D Vision Language Reasoning","summary":"  Being able to carry out complicated vision language reasoning tasks in 3D\nspace represents a significant milestone in developing household robots and\nhuman-centered embodied AI. In this work, we demonstrate that a critical and\ndistinct challenge in 3D vision language reasoning is situational awareness,\nwhich incorporates two key components: (1) The autonomous agent grounds its\nself-location based on a language prompt. (2) The agent answers open-ended\nquestions from the perspective of its calculated position. To address this\nchallenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D\nvision language reasoning. We tokenize the 3D scene into sparse voxel\nrepresentation and propose a language-grounded situation estimator, followed by\na situated question answering module. Experiments on the SQA3D and ScanQA\ndatasets show that SIG3D outperforms state-of-the-art models in situation\nestimation and question answering by a large margin (e.g., an enhancement of\nover 30% on situation estimation accuracy). Subsequent analysis corroborates\nour architectural design choices, explores the distinct functions of visual and\ntextual tokens, and highlights the importance of situational awareness in the\ndomain of 3D question answering.\n","authors":["Yunze Man","Liang-Yan Gui","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07544v1.pdf","comment":"CVPR 2024. Project Page: https://yunzeman.github.io/situation3d"},{"id":"http://arxiv.org/abs/2406.07542v1","updated":"2024-06-11T17:59:31Z","published":"2024-06-11T17:59:31Z","title":"Cognitive Insights Across Languages: Enhancing Multimodal Interview\n  Analysis","summary":"  Cognitive decline is a natural process that occurs as individuals age. Early\ndiagnosis of anomalous decline is crucial for initiating professional treatment\nthat can enhance the quality of life of those affected. To address this issue,\nwe propose a multimodal model capable of predicting Mild Cognitive Impairment\nand cognitive scores. The TAUKADIAL dataset is used to conduct the evaluation,\nwhich comprises audio recordings of clinical interviews. The proposed model\ndemonstrates the ability to transcribe and differentiate between languages used\nin the interviews. Subsequently, the model extracts audio and text features,\ncombining them into a multimodal architecture to achieve robust and generalized\nresults. Our approach involves in-depth research to implement various features\nobtained from the proposed modalities.\n","authors":["David Ortiz-Perez","Jose Garcia-Rodriguez","David Tomás"],"pdf_url":"https://arxiv.org/pdf/2406.07542v1.pdf","comment":"GitHub repository: https://github.com/davidorp/taukadial"},{"id":"http://arxiv.org/abs/2406.07541v1","updated":"2024-06-11T17:59:29Z","published":"2024-06-11T17:59:29Z","title":"CDSA: Conservative Denoising Score-based Algorithm for Offline\n  Reinforcement Learning","summary":"  Distribution shift is a major obstacle in offline reinforcement learning,\nwhich necessitates minimizing the discrepancy between the learned policy and\nthe behavior policy to avoid overestimating rare or unseen actions. Previous\nconservative offline RL algorithms struggle to generalize to unseen actions,\ndespite their success in learning good in-distribution policy. In contrast, we\npropose to use the gradient fields of the dataset density generated from a\npre-trained offline RL algorithm to adjust the original actions. We decouple\nthe conservatism constraints from the policy, thus can benefit wide offline RL\nalgorithms. As a consequence, we propose the Conservative Denoising Score-based\nAlgorithm (CDSA) which utilizes the denoising score-based model to model the\ngradient of the dataset density, rather than the dataset density itself, and\nfacilitates a more accurate and efficient method to adjust the action generated\nby the pre-trained policy in a deterministic and continuous MDP environment. In\nexperiments, we show that our approach significantly improves the performance\nof baseline algorithms in D4RL datasets, and demonstrate the generalizability\nand plug-and-play capability of our model across different pre-trained offline\nRL policy in different tasks. We also validate that the agent exhibits greater\nrisk aversion after employing our method while showcasing its ability to\ngeneralize effectively across diverse tasks.\n","authors":["Zeyuan Liu","Kai Yang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2406.07541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07540v1","updated":"2024-06-11T17:59:01Z","published":"2024-06-11T17:59:01Z","title":"Ctrl-X: Controlling Structure and Appearance for Text-To-Image\n  Generation Without Guidance","summary":"  Recent controllable generation approaches such as FreeControl and Diffusion\nSelf-guidance bring fine-grained spatial and appearance control to\ntext-to-image (T2I) diffusion models without training auxiliary modules.\nHowever, these methods optimize the latent embedding for each type of score\nfunction with longer diffusion steps, making the generation process\ntime-consuming and limiting their flexibility and use. This work presents\nCtrl-X, a simple framework for T2I diffusion controlling structure and\nappearance without additional training or guidance. Ctrl-X designs feed-forward\nstructure control to enable the structure alignment with a structure image and\nsemantic-aware appearance transfer to facilitate the appearance transfer from a\nuser-input image. Extensive qualitative and quantitative experiments illustrate\nthe superior performance of Ctrl-X on various condition inputs and model\ncheckpoints. In particular, Ctrl-X supports novel structure and appearance\ncontrol with arbitrary condition images of any modality, exhibits superior\nimage quality and appearance transfer compared to existing works, and provides\ninstant plug-and-play functionality to any T2I and text-to-video (T2V)\ndiffusion model. See our project page for an overview of the results:\nhttps://genforce.github.io/ctrl-x\n","authors":["Kuan Heng Lin","Sicheng Mo","Ben Klingher","Fangzhou Mu","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.07540v1.pdf","comment":"18 pages, 11 figures, see project page at\n  https://genforce.github.io/ctrl-x"},{"id":"http://arxiv.org/abs/2211.06829v2","updated":"2024-06-11T17:58:07Z","published":"2022-11-13T06:11:38Z","title":"Methods for Recovering Conditional Independence Graphs: A Survey","summary":"  Conditional Independence (CI) graphs are a type of probabilistic graphical\nmodels that are primarily used to gain insights about feature relationships.\nEach edge represents the partial correlation between the connected features\nwhich gives information about their direct dependence. In this survey, we list\nout different methods and study the advances in techniques developed to recover\nCI graphs. We cover traditional optimization methods as well as recently\ndeveloped deep learning architectures along with their recommended\nimplementations. To facilitate wider adoption, we include preliminaries that\nconsolidate associated operations, for example techniques to obtain covariance\nmatrix for mixed datatypes.\n","authors":["Harsh Shrivastava","Urszula Chajewska"],"pdf_url":"https://arxiv.org/pdf/2211.06829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07536v1","updated":"2024-06-11T17:57:49Z","published":"2024-06-11T17:57:49Z","title":"Towards Fundamentally Scalable Model Selection: Asymptotically Fast\n  Update and Selection","summary":"  The advancement of deep learning technologies is bringing new models every\nday, motivating the study of scalable model selection. An ideal model selection\nscheme should minimally support two operations efficiently over a large pool of\ncandidate models: update, which involves either adding a new candidate model or\nremoving an existing candidate model, and selection, which involves locating\nhighly performing models for a given task. However, previous solutions to model\nselection require high computational complexity for at least one of these two\noperations. In this work, we target fundamentally (more) scalable model\nselection that supports asymptotically fast update and asymptotically fast\nselection at the same time. Firstly, we define isolated model embedding, a\nfamily of model selection schemes supporting asymptotically fast update and\nselection: With respect to the number of candidate models $m$, the update\ncomplexity is O(1) and the selection consists of a single sweep over $m$\nvectors in addition to O(1) model operations. Isolated model embedding also\nimplies several desirable properties for applications. Secondly, we present\nStandardized Embedder, an empirical realization of isolated model embedding. We\nassess its effectiveness by using it to select representations from a pool of\n100 pre-trained vision models for classification tasks and measuring the\nperformance gaps between the selected models and the best candidates with a\nlinear probing protocol. Experiments suggest our realization is effective in\nselecting models with competitive performances and highlight isolated model\nembedding as a promising direction towards model selection that is\nfundamentally (more) scalable.\n","authors":["Wenxiao Wang","Weiming Zhuang","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2406.07536v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.07540v3","updated":"2024-06-11T17:57:15Z","published":"2023-12-12T18:59:30Z","title":"diff History for Neural Language Agents","summary":"  Neural Language Models (LMs) offer an exciting solution for general-purpose\nembodied control. However, a key technical issue arises when using an LM-based\ncontroller: environment observations must be converted to text, which coupled\nwith history, results in long and verbose textual prompts. As a result, prior\nwork in LM agents is limited to restricted domains with small observation size\nas well as minimal needs for interaction history or instruction tuning. In this\npaper, we introduce diff history, a simple and highly effective solution to\nthese issues. By applying the Unix diff command on consecutive text\nobservations in the interaction histories used to prompt LM policies, we can\nboth abstract away redundant information and focus the content of textual\ninputs on the salient changes in the environment. On NetHack, an unsolved video\ngame that requires long-horizon reasoning for decision-making, LMs tuned with\ndiff history match state-of-the-art performance for neural agents while needing\n1800x fewer training examples compared to prior work. Even on the simpler\nBabyAI-Text environment with concise text observations, we find that although\ndiff history increases the length of prompts, the representation it provides\noffers a 25% improvement in the efficiency of low-sample instruction tuning.\nFurther, we show that diff history scales favorably across different tuning\ndataset sizes. We open-source our code and data to\nhttps://diffhistory.github.io.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2312.07540v3.pdf","comment":"ICML 2024 version"},{"id":"http://arxiv.org/abs/2406.07532v1","updated":"2024-06-11T17:56:14Z","published":"2024-06-11T17:56:14Z","title":"Hearing Anything Anywhere","summary":"  Recent years have seen immense progress in 3D computer vision and computer\ngraphics, with emerging tools that can virtualize real-world 3D environments\nfor numerous Mixed Reality (XR) applications. However, alongside immersive\nvisual experiences, immersive auditory experiences are equally vital to our\nholistic perception of an environment. In this paper, we aim to reconstruct the\nspatial acoustic characteristics of an arbitrary environment given only a\nsparse set of (roughly 12) room impulse response (RIR) recordings and a planar\nreconstruction of the scene, a setup that is easily achievable by ordinary\nusers. To this end, we introduce DiffRIR, a differentiable RIR rendering\nframework with interpretable parametric models of salient acoustic features of\nthe scene, including sound source directivity and surface reflectivity. This\nallows us to synthesize novel auditory experiences through the space with any\nsource audio. To evaluate our method, we collect a dataset of RIR recordings\nand music in four diverse, real environments. We show that our model\noutperforms state-ofthe-art baselines on rendering monaural and binaural RIRs\nand music at unseen locations, and learns physically interpretable parameters\ncharacterizing acoustic properties of the sound source and surfaces in the\nscene.\n","authors":["Mason Wang","Ryosuke Sawata","Samuel Clarke","Ruohan Gao","Shangzhe Wu","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2406.07532v1.pdf","comment":"CVPR 2024. The first two authors contributed equally. Project page:\n  https://masonlwang.com/hearinganythinganywhere/"},{"id":"http://arxiv.org/abs/2406.07529v1","updated":"2024-06-11T17:55:25Z","published":"2024-06-11T17:55:25Z","title":"MAP: Low-compute Model Merging with Amortized Pareto Fronts via\n  Quadratic Approximation","summary":"  Model merging has emerged as an effective approach to combine multiple\nsingle-task models, fine-tuned from the same pre-trained model, into a\nmultitask model. This process typically involves computing a weighted average\nof the model parameters without any additional training. Existing model-merging\nmethods focus on enhancing average task accuracy. However, interference and\nconflicts between the objectives of different tasks can lead to trade-offs\nduring model merging. In real-world applications, a set of solutions with\nvarious trade-offs can be more informative, helping practitioners make\ndecisions based on diverse preferences. In this paper, we introduce a novel\nlow-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP\nidentifies a Pareto set of scaling coefficients for merging multiple models to\nreflect the trade-offs. The core component of MAP is approximating the\nevaluation metrics of the various tasks using a quadratic approximation\nsurrogate model derived from a pre-selected set of scaling coefficients,\nenabling amortized inference. Experimental results on vision and natural\nlanguage processing tasks show that MAP can accurately identify the Pareto\nfront. To further reduce the required computation of MAP, we propose (1) a\nBayesian adaptive sampling algorithm and (2) a nested merging scheme with\nmultiple stages.\n","authors":["Lu Li","Tianyu Zhang","Zhiqi Bu","Suyuchen Wang","Huan He","Jie Fu","Yonghui Wu","Jiang Bian","Yong Chen","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2406.07529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07528v1","updated":"2024-06-11T17:55:03Z","published":"2024-06-11T17:55:03Z","title":"QuickLLaMA: Query-aware Inference Acceleration for Large Language Models","summary":"  The capacity of Large Language Models (LLMs) to comprehend and reason over\nlong contexts is pivotal for advancements in diverse fields. Yet, they still\nstuggle with capturing long-distance dependencies within sequences to deeply\nunderstand semantics. To address this issue, we introduce Query-aware Inference\nfor LLMs (Q-LLM), a system designed to process extensive sequences akin to\nhuman cognition. By focusing on memory data relevant to a given query, Q-LLM\ncan accurately capture pertinent information within a fixed window size and\nprovide precise answers to queries. It doesn't require extra training and can\nbe seamlessly integrated with any LLMs. Q-LLM using LLaMA3 (QuickLLaMA) can\nread Harry Potter within 30s and accurately answer the questions. Q-LLM\nimproved by 7.17% compared to the current state-of-the-art on LLaMA3, and by\n3.26% on Mistral on the $\\infty$-bench. In the Needle-in-a-Haystack task, On\nwidely recognized benchmarks, Q-LLM improved upon the current SOTA by 7.0% on\nMistral and achieves 100% on LLaMA3. Our code can be found in\nhttps://github.com/dvlab-research/Q-LLM.\n","authors":["Jingyao Li","Han Shi","Xin Jiang","Zhenguo Li","Hong Xu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2406.07528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02191v2","updated":"2024-06-11T17:53:39Z","published":"2024-06-04T10:35:16Z","title":"On the Recoverability of Causal Relations from Temporally Aggregated\n  I.I.D. Data","summary":"  We consider the effect of temporal aggregation on instantaneous\n(non-temporal) causal discovery in general setting. This is motivated by the\nobservation that the true causal time lag is often considerably shorter than\nthe observational interval. This discrepancy leads to high aggregation, causing\ntime-delay causality to vanish and instantaneous dependence to manifest.\nAlthough we expect such instantaneous dependence has consistency with the true\ncausal relation in certain sense to make the discovery results meaningful, it\nremains unclear what type of consistency we need and when will such consistency\nbe satisfied. We proposed functional consistency and conditional independence\nconsistency in formal way correspond functional causal model-based methods and\nconditional independence-based methods respectively and provide the conditions\nunder which these consistencies will hold. We show theoretically and\nexperimentally that causal discovery results may be seriously distorted by\naggregation especially in complete nonlinear case and we also find causal\nrelationship still recoverable from aggregated data if we have partial\nlinearity or appropriate prior. Our findings suggest community should take a\ncautious and meticulous approach when interpreting causal discovery results\nfrom such data and show why and when aggregation will distort the performance\nof causal discovery methods.\n","authors":["Shunxing Fan","Mingming Gong","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.02191v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2406.07524v1","updated":"2024-06-11T17:51:40Z","published":"2024-06-11T17:51:40Z","title":"Simple and Effective Masked Diffusion Language Models","summary":"  While diffusion models excel at generating high-quality images, prior work\nreports a significant performance gap between diffusion and autoregressive (AR)\nmethods in language modeling. In this work, we show that simple masked discrete\ndiffusion is more performant than previously thought. We apply an effective\ntraining recipe that improves the performance of masked diffusion models and\nderive a simplified, Rao-Blackwellized objective that results in additional\nimprovements. Our objective has a simple form -- it is a mixture of classical\nmasked language modeling losses -- and can be used to train encoder-only\nlanguage models that admit efficient samplers, including ones that can generate\narbitrary lengths of text semi-autoregressively like a traditional language\nmodel. On language modeling benchmarks, a range of masked diffusion models\ntrained with modern engineering practices achieves a new state-of-the-art among\ndiffusion models, and approaches AR perplexity. We release our code at:\nhttps://github.com/kuleshov-group/mdlm\n","authors":["Subham Sekhar Sahoo","Marianne Arriola","Yair Schiff","Aaron Gokaslan","Edgar Marroquin","Justin T Chiu","Alexander Rush","Volodymyr Kuleshov"],"pdf_url":"https://arxiv.org/pdf/2406.07524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07522v1","updated":"2024-06-11T17:50:51Z","published":"2024-06-11T17:50:51Z","title":"Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling","summary":"  Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.\n","authors":["Liliang Ren","Yang Liu","Yadong Lu","Yelong Shen","Chen Liang","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.07522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07521v1","updated":"2024-06-11T17:50:20Z","published":"2024-06-11T17:50:20Z","title":"Faster Spectral Density Estimation and Sparsification in the Nuclear\n  Norm","summary":"  We consider the problem of estimating the spectral density of the normalized\nadjacency matrix of an $n$-node undirected graph. We provide a randomized\nalgorithm that, with $O(n\\epsilon^{-2})$ queries to a degree and neighbor\noracle and in $O(n\\epsilon^{-3})$ time, estimates the spectrum up to $\\epsilon$\naccuracy in the Wasserstein-1 metric. This improves on previous\nstate-of-the-art methods, including an $O(n\\epsilon^{-7})$ time algorithm from\n[Braverman et al., STOC 2022] and, for sufficiently small $\\epsilon$, a\n$2^{O(\\epsilon^{-1})}$ time method from [Cohen-Steiner et al., KDD 2018]. To\nachieve this result, we introduce a new notion of graph sparsification, which\nwe call nuclear sparsification. We provide an $O(n\\epsilon^{-2})$-query and\n$O(n\\epsilon^{-2})$-time algorithm for computing $O(n\\epsilon^{-2})$-sparse\nnuclear sparsifiers. We show that this bound is optimal in both its sparsity\nand query complexity, and we separate our results from the related notion of\nadditive spectral sparsification. Of independent interest, we show that our\nsparsification method also yields the first deterministic algorithm for\nspectral density estimation that scales linearly with $n$ (sublinear in the\nrepresentation size of the graph).\n","authors":["Yujia Jin","Ishani Karmarkar","Christopher Musco","Aaron Sidford","Apoorv Vikram Singh"],"pdf_url":"https://arxiv.org/pdf/2406.07521v1.pdf","comment":"Accepted for presentation at the Conference on Learning Theory (COLT)\n  2024"},{"id":"http://arxiv.org/abs/2406.07519v1","updated":"2024-06-11T17:50:04Z","published":"2024-06-11T17:50:04Z","title":"Physics-guided weak-form discovery of reduced-order models for trapped\n  ultracold hydrodynamics","summary":"  We study the relaxation of a highly collisional, ultracold but nondegenerate\ngas of polar molecules. Confined within a harmonic trap, the gas is subject to\nfluid-gaseous coupled dynamics that lead to a breakdown of first-order\nhydrodynamics. An attempt to treat these higher-order hydrodynamic effects was\npreviously made with a Gaussian ansatz and coarse-graining model parameter [R.\nR. W. Wang & J. L. Bohn, Phys. Rev. A 108, 013322 (2023)], leading to an\napproximate set of equations for a few collective observables accessible to\nexperiments. Here we present substantially improved reduced-order models for\nthese same observables, admissible beyond previous parameter regimes,\ndiscovered directly from particle simulations using the WSINDy algorithm\n(Weak-form Sparse Identification of Nonlinear Dynamics). The interpretable\nnature of the learning algorithm enables estimation of previously unknown\nphysical quantities and discovery of model terms with candidate physical\nmechanisms, revealing new physics in mixed collisional regimes. Our approach\nconstitutes a general framework for data-driven model identification leveraging\nknown physics.\n","authors":["Reuben R. W. Wang","Daniel Messenger"],"pdf_url":"https://arxiv.org/pdf/2406.07519v1.pdf","comment":"20 pages, 4 figures, 10 tables"},{"id":"http://arxiv.org/abs/2401.04079v4","updated":"2024-06-11T17:46:38Z","published":"2024-01-08T18:31:38Z","title":"RudolfV: A Foundation Model by Pathologists for Pathologists","summary":"  Artificial intelligence has started to transform histopathology impacting\nclinical diagnostics and biomedical research. However, while many computational\npathology approaches have been proposed, most current AI models are limited\nwith respect to generalization, application variety, and handling rare\ndiseases. Recent efforts introduced self-supervised foundation models to\naddress these challenges, yet existing approaches do not leverage pathologist\nknowledge by design. In this study, we present a novel approach to designing\nfoundation models for computational pathology, incorporating pathologist\nexpertise, semi-automated data curation, and a diverse dataset from over 15\nlaboratories, including 58 tissue types, and encompassing 129 different\nhistochemical and immunohistochemical staining modalities. We demonstrate that\nour model \"RudolfV\" surpasses existing state-of-the-art foundation models\nacross different benchmarks focused on tumor microenvironment profiling,\nbiomarker evaluation, and reference case search while exhibiting favorable\nrobustness properties. Our study shows how domain-specific knowledge can\nincrease the efficiency and performance of pathology foundation models and\nenable novel application areas.\n","authors":["Jonas Dippel","Barbara Feulner","Tobias Winterhoff","Timo Milbich","Stephan Tietz","Simon Schallenberg","Gabriel Dernbach","Andreas Kunft","Simon Heinke","Marie-Lisa Eich","Julika Ribbat-Idel","Rosemarie Krupar","Philipp Anders","Niklas Prenißl","Philipp Jurmeister","David Horst","Lukas Ruff","Klaus-Robert Müller","Frederick Klauschen","Maximilian Alber"],"pdf_url":"https://arxiv.org/pdf/2401.04079v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07515v1","updated":"2024-06-11T17:46:16Z","published":"2024-06-11T17:46:16Z","title":"Beyond Model Collapse: Scaling Up with Synthesized Data Requires\n  Reinforcement","summary":"  Synthesized data from generative models is increasingly considered as an\nalternative to human-annotated data for fine-tuning Large Language Models. This\nraises concerns about model collapse: a drop in performance of models\nfine-tuned on generated data. Considering that it is easier for both humans and\nmachines to tell between good and bad examples than to generate high-quality\nsamples, we investigate the use of feedback on synthesized data to prevent\nmodel collapse. We derive theoretical conditions under which a Gaussian mixture\nclassification model can achieve asymptotically optimal performance when\ntrained on feedback-augmented synthesized data, and provide supporting\nsimulations for finite regimes. We illustrate our theoretical predictions on\ntwo practical problems: computing matrix eigenvalues with transformers and news\nsummarization with large language models, which both undergo model collapse\nwhen trained on model-generated data. We show that training from\nfeedback-augmented synthesized data, either by pruning incorrect predictions or\nby selecting the best of several guesses, can prevent model collapse,\nvalidating popular approaches like RLHF.\n","authors":["Yunzhen Feng","Elvis Dohmatob","Pu Yang","Francois Charton","Julia Kempe"],"pdf_url":"https://arxiv.org/pdf/2406.07515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10367v2","updated":"2024-06-11T17:44:28Z","published":"2023-09-19T07:04:50Z","title":"Toward efficient resource utilization at edge nodes in federated\n  learning","summary":"  Federated learning (FL) enables edge nodes to collaboratively contribute to\nconstructing a global model without sharing their data. This is accomplished by\ndevices computing local, private model updates that are then aggregated by a\nserver. However, computational resource constraints and network communication\ncan become a severe bottleneck for larger model sizes typical for deep learning\napplications. Edge nodes tend to have limited hardware resources (RAM, CPU),\nand the network bandwidth and reliability at the edge is a concern for scaling\nfederated fleet applications. In this paper, we propose and evaluate a FL\nstrategy inspired by transfer learning in order to reduce resource utilization\non devices, as well as the load on the server and network in each global\ntraining round. For each local model update, we randomly select layers to\ntrain, freezing the remaining part of the model. In doing so, we can reduce\nboth server load and communication costs per round by excluding all untrained\nlayer weights from being transferred to the server. The goal of this study is\nto empirically explore the potential trade-off between resource utilization on\ndevices and global model convergence under the proposed strategy. We implement\nthe approach using the federated learning framework FEDn. A number of\nexperiments were carried out over different datasets (CIFAR-10, CASA, and\nIMDB), performing different tasks using different deep-learning model\narchitectures. Our results show that training the model partially can\naccelerate the training process, efficiently utilizes resources on-device, and\nreduce the data transmission by around 75% and 53% when we train 25%, and 50%\nof the model layers, respectively, without harming the resulting global model\naccuracy.\n","authors":["Sadi Alawadi","Addi Ait-Mlouk","Salman Toor","Andreas Hellander"],"pdf_url":"https://arxiv.org/pdf/2309.10367v2.pdf","comment":"16 pages, 5 tables, 8 figures"},{"id":"http://arxiv.org/abs/2406.07507v1","updated":"2024-06-11T17:41:26Z","published":"2024-06-11T17:41:26Z","title":"Flow Map Matching","summary":"  Generative models based on dynamical transport of measure, such as diffusion\nmodels, flow matching models, and stochastic interpolants, learn an ordinary or\nstochastic differential equation whose trajectories push initial conditions\nfrom a known base distribution onto the target. While training is cheap,\nsamples are generated via simulation, which is more expensive than one-step\nmodels like GANs. To close this gap, we introduce flow map matching -- an\nalgorithm that learns the two-time flow map of an underlying ordinary\ndifferential equation. The approach leads to an efficient few-step generative\nmodel whose step count can be chosen a-posteriori to smoothly trade off\naccuracy for computational expense. Leveraging the stochastic interpolant\nframework, we introduce losses for both direct training of flow maps and\ndistillation from pre-trained (or otherwise known) velocity fields.\nTheoretically, we show that our approach unifies many existing few-step\ngenerative models, including consistency models, consistency trajectory models,\nprogressive distillation, and neural operator approaches, which can be obtained\nas particular cases of our formalism. With experiments on CIFAR-10 and ImageNet\n32x32, we show that flow map matching leads to high-quality samples with\nsignificantly reduced sampling cost compared to diffusion or stochastic\ninterpolant methods.\n","authors":["Nicholas M. Boffi","Michael S. Albergo","Eric Vanden-Eijnden"],"pdf_url":"https://arxiv.org/pdf/2406.07507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07506v1","updated":"2024-06-11T17:40:31Z","published":"2024-06-11T17:40:31Z","title":"Understanding Visual Concepts Across Models","summary":"  Large multimodal models such as Stable Diffusion can generate, detect, and\nclassify new visual concepts after fine-tuning just a single word embedding. Do\nmodels learn similar words for the same concepts (i.e. <orange-cat> = orange +\ncat)? We conduct a large-scale analysis on three state-of-the-art models in\ntext-to-image generation, open-set object detection, and zero-shot\nclassification, and find that new word embeddings are model-specific and\nnon-transferable. Across 4,800 new embeddings trained for 40 diverse visual\nconcepts on four standard datasets, we find perturbations within an\n$\\epsilon$-ball to any prior embedding that generate, detect, and classify an\narbitrary concept. When these new embeddings are spliced into new models,\nfine-tuning that targets the original model is lost. We show popular soft\nprompt-tuning approaches find these perturbative solutions when applied to\nvisual concept learning tasks, and embeddings for visual concepts are not\ntransferable. Code for reproducing our work is available at:\nhttps://visual-words.github.io.\n","authors":["Brandon Trabucco","Max Gurinas","Kyle Doherty","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2406.07506v1.pdf","comment":"Official code at: https://github.com/visual-words/visual-words"},{"id":"http://arxiv.org/abs/2406.07496v1","updated":"2024-06-11T17:32:21Z","published":"2024-06-11T17:32:21Z","title":"TextGrad: Automatic \"Differentiation\" via Text","summary":"  AI is undergoing a paradigm shift, with breakthroughs achieved by systems\norchestrating multiple large language models (LLMs) and other complex\ncomponents. As a result, developing principled and automated optimization\nmethods for compound AI systems is one of the most important new challenges.\nNeural networks faced a similar challenge in its early days until\nbackpropagation and automatic differentiation transformed the field by making\noptimization turn-key. Inspired by this, we introduce TextGrad, a powerful\nframework performing automatic ``differentiation'' via text. TextGrad\nbackpropagates textual feedback provided by LLMs to improve individual\ncomponents of a compound AI system. In our framework, LLMs provide rich,\ngeneral, natural language suggestions to optimize variables in computation\ngraphs, ranging from code snippets to molecular structures. TextGrad follows\nPyTorch's syntax and abstraction and is flexible and easy-to-use. It works\nout-of-the-box for a variety of tasks, where the users only provide the\nobjective function without tuning components or prompts of the framework. We\nshowcase TextGrad's effectiveness and generality across a diverse range of\napplications, from question answering and molecule optimization to radiotherapy\ntreatment planning. Without modifying the framework, TextGrad improves the\nzero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\\%$ to\n$55\\%$, yields $20\\%$ relative performance gain in optimizing LeetCode-Hard\ncoding problem solutions, improves prompts for reasoning, designs new druglike\nsmall molecules with desirable in silico binding, and designs radiation\noncology treatment plans with high specificity. TextGrad lays a foundation to\naccelerate the development of the next-generation of AI systems.\n","authors":["Mert Yuksekgonul","Federico Bianchi","Joseph Boen","Sheng Liu","Zhi Huang","Carlos Guestrin","James Zou"],"pdf_url":"https://arxiv.org/pdf/2406.07496v1.pdf","comment":"41 pages, 6 figures"},{"id":"http://arxiv.org/abs/2309.08398v2","updated":"2024-06-11T17:29:51Z","published":"2023-09-15T13:50:16Z","title":"Exploring Meta Information for Audio-based Zero-shot Bird Classification","summary":"  Advances in passive acoustic monitoring and machine learning have led to the\nprocurement of vast datasets for computational bioacoustic research.\nNevertheless, data scarcity is still an issue for rare and underrepresented\nspecies. This study investigates how meta-information can improve zero-shot\naudio classification, utilising bird species as an example case study due to\nthe availability of rich and diverse meta-data. We investigate three different\nsources of metadata: textual bird sound descriptions encoded via (S)BERT,\nfunctional traits (AVONET), and bird life-history (BLH) characteristics. As\naudio features, we extract audio spectrogram transformer (AST) embeddings and\nproject them to the dimension of the auxiliary information by adopting a single\nlinear layer. Then, we employ the dot product as compatibility function and a\nstandard zero-shot learning ranking hinge loss to determine the correct class.\nThe best results are achieved by concatenating the AVONET and BLH features\nattaining a mean unweighted F1-score of .233 over five different test sets with\n8 to 10 classes.\n","authors":["Alexander Gebhard","Andreas Triantafyllopoulos","Teresa Bez","Lukas Christ","Alexander Kathan","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2309.08398v2.pdf","comment":"Accepted at ICASSP 2024"},{"id":"http://arxiv.org/abs/2406.07484v1","updated":"2024-06-11T17:26:14Z","published":"2024-06-11T17:26:14Z","title":"Towards Generalized Hydrological Forecasting using Transformer Models\n  for 120-Hour Streamflow Prediction","summary":"  This study explores the efficacy of a Transformer model for 120-hour\nstreamflow prediction across 125 diverse locations in Iowa, US. Utilizing data\nfrom the preceding 72 hours, including precipitation, evapotranspiration, and\ndischarge values, we developed a generalized model to predict future\nstreamflow. Our approach contrasts with traditional methods that typically rely\non location-specific models. We benchmarked the Transformer model's performance\nagainst three deep learning models (LSTM, GRU, and Seq2Seq) and the Persistence\napproach, employing Nash-Sutcliffe Efficiency (NSE), Kling-Gupta Efficiency\n(KGE), Pearson's r, and Normalized Root Mean Square Error (NRMSE) as metrics.\nThe study reveals the Transformer model's superior performance, maintaining\nhigher median NSE and KGE scores and exhibiting the lowest NRMSE values. This\nindicates its capability to accurately simulate and predict streamflow,\nadapting effectively to varying hydrological conditions and geographical\nvariances. Our findings underscore the Transformer model's potential as an\nadvanced tool in hydrological modeling, offering significant improvements over\ntraditional and contemporary approaches.\n","authors":["Bekir Z. Demiray","Ibrahim Demir"],"pdf_url":"https://arxiv.org/pdf/2406.07484v1.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.07482v1","updated":"2024-06-11T17:25:46Z","published":"2024-06-11T17:25:46Z","title":"Comparing Deep Learning Models for Rice Mapping in Bhutan Using High\n  Resolution Satellite Imagery","summary":"  The Bhutanese government is increasing its utilization of technological\napproaches such as including Remote Sensing-based knowledge in their\ndecision-making process. This study focuses on crop type and crop extent in\nParo, one of the top rice-yielding districts in Bhutan, and employs publicly\navailable NICFI high-resolution satellite imagery from Planet. Two Deep\nLearning (DL) approaches, point-based (DNN) and patch-based (U-Net), models\nwere used in conjunction with cloud-computing platforms. Three different models\nper DL approaches (DNN and U-Net) were trained: 1) RGBN channels from Planet;\n2) RGBN and elevation data (RGBNE); 3) RGBN and Sentinel-1 (S1) data (RGBNS),\nand RGBN with E and S1 data (RGBNES). From this comprehensive analysis, the\nU-Net displayed higher performance metrics across both model training and model\nvalidation efforts. Among the U-Net model sets, the RGBN, RGBNE, RGBNS, and\nRGBNES models had an F1-score of 0.8546, 0.8563, 0.8467, and 0.8500\nrespectively. An independent model evaluation was performed and found a high\nlevel of performance variation across all the metrics. For this independent\nmodel evaluation, the U-Net RGBN, RGBNE, RGBNES, and RGBN models displayed the\nF1-scores of 0.5935, 0.6154, 0.5882, and 0.6582, suggesting U-Net RGBNES as the\nbest model. The study shows that the DL approaches can predict rice. Also, DL\nmethods can be used with the survey-based approaches currently utilized by the\nBhutan Department of Agriculture. Further, this study demonstrated the usage of\nregional land cover products such as SERVIR's RLCMS as a weak label approach to\ncapture different strata addressing the class imbalance problem and improving\nthe sampling design for DL application. Finally, through preliminary model\ntesting and comparisons outlined it was shown that using additional features\nsuch as NDVI, EVI, and NDWI did not drastically improve model performance.\n","authors":["Biplov Bhandari","Timothy Mayer"],"pdf_url":"https://arxiv.org/pdf/2406.07482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17387v2","updated":"2024-06-11T17:22:28Z","published":"2023-05-27T06:46:08Z","title":"Learning from Integral Losses in Physics Informed Neural Networks","summary":"  This work proposes a solution for the problem of training physics-informed\nnetworks under partial integro-differential equations. These equations require\nan infinite or a large number of neural evaluations to construct a single\nresidual for training. As a result, accurate evaluation may be impractical, and\nwe show that naive approximations at replacing these integrals with unbiased\nestimates lead to biased loss functions and solutions. To overcome this bias,\nwe investigate three types of potential solutions: the deterministic sampling\napproaches, the double-sampling trick, and the delayed target method. We\nconsider three classes of PDEs for benchmarking; one defining Poisson problems\nwith singular charges and weak solutions of up to 10 dimensions, another\ninvolving weak solutions on electro-magnetic fields and a Maxwell equation, and\na third one defining a Smoluchowski coagulation problem. Our numerical results\nconfirm the existence of the aforementioned bias in practice and also show that\nour proposed delayed target approach can lead to accurate solutions with\ncomparable quality to ones estimated with a large sample size integral. Our\nimplementation is open-source and available at\nhttps://github.com/ehsansaleh/btspinn.\n","authors":["Ehsan Saleh","Saba Ghaffari","Timothy Bretl","Luke Olson","Matthew West"],"pdf_url":"https://arxiv.org/pdf/2305.17387v2.pdf","comment":"Accepted in the main track of ICML 2024"},{"id":"http://arxiv.org/abs/2406.07475v1","updated":"2024-06-11T17:21:15Z","published":"2024-06-11T17:21:15Z","title":"Partially Observed Trajectory Inference using Optimal Transport and a\n  Dynamics Prior","summary":"  Trajectory inference seeks to recover the temporal dynamics of a population\nfrom snapshots of its (uncoupled) temporal marginals, i.e. where observed\nparticles are not tracked over time. Lavenant et al. arXiv:2102.09204 addressed\nthis challenging problem under a stochastic differential equation (SDE) model\nwith a gradient-driven drift in the observed space, introducing a minimum\nentropy estimator relative to the Wiener measure. Chizat et al.\narXiv:2205.07146 then provided a practical grid-free mean-field Langevin (MFL)\nalgorithm using Schr\\\"odinger bridges. Motivated by the overwhelming success of\nobservable state space models in the traditional paired trajectory inference\nproblem (e.g. target tracking), we extend the above framework to a class of\nlatent SDEs in the form of observable state space models. In this setting, we\nuse partial observations to infer trajectories in the latent space under a\nspecified dynamics model (e.g. the constant velocity/acceleration models from\ntarget tracking). We introduce PO-MFL to solve this latent trajectory inference\nproblem and provide theoretical guarantees by extending the results of\narXiv:2102.09204 to the partially observed setting. We leverage the MFL\nframework of arXiv:2205.07146, yielding an algorithm based on entropic OT\nbetween dynamics-adjusted adjacent time marginals. Experiments validate the\nrobustness of our method and the exponential convergence of the MFL dynamics,\nand demonstrate significant outperformance over the latent-free method of\narXiv:2205.07146 in key scenarios.\n","authors":["Anming Gu","Edward Chien","Kristjan Greenewald"],"pdf_url":"https://arxiv.org/pdf/2406.07475v1.pdf","comment":"32 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.07474v1","updated":"2024-06-11T17:20:28Z","published":"2024-06-11T17:20:28Z","title":"Quantifying Local Model Validity using Active Learning","summary":"  Real-world applications of machine learning models are often subject to legal\nor policy-based regulations. Some of these regulations require ensuring the\nvalidity of the model, i.e., the approximation error being smaller than a\nthreshold. A global metric is generally too insensitive to determine the\nvalidity of a specific prediction, whereas evaluating local validity is costly\nsince it requires gathering additional data.We propose learning the model error\nto acquire a local validity estimate while reducing the amount of required data\nthrough active learning. Using model validation benchmarks, we provide\nempirical evidence that the proposed method can lead to an error model with\nsufficient discriminative properties using a relatively small amount of data.\nFurthermore, an increased sensitivity to local changes of the validity bounds\ncompared to alternative approaches is demonstrated.\n","authors":["Sven Lämmle","Can Bogoclu","Robert Voßhall","Anselm Haselhoff","Dirk Roos"],"pdf_url":"https://arxiv.org/pdf/2406.07474v1.pdf","comment":"40th Conference on Uncertainty in Artificial Intelligence"},{"id":"http://arxiv.org/abs/2008.08718v6","updated":"2024-06-11T17:15:26Z","published":"2020-08-20T00:13:19Z","title":"Minimum discrepancy principle strategy for choosing $k$ in $k$-NN\n  regression","summary":"  We present a novel data-driven strategy to choose the hyperparameter $k$ in\nthe $k$-NN regression estimator without using any hold-out data. We treat the\nproblem of choosing the hyperparameter as an iterative procedure (over $k$) and\npropose using an easily implemented in practice strategy based on the idea of\nearly stopping and the minimum discrepancy principle. This model selection\nstrategy is proven to be minimax-optimal, under the fixed-design assumption on\ncovariates, over some smoothness function classes, for instance, the Lipschitz\nfunctions class on a bounded domain. The novel method often improves\nstatistical performance on artificial and real-world data sets in comparison to\nother model selection strategies, such as the Hold-out method, 5-fold\ncross-validation, and AIC criterion. The novelty of the strategy comes from\nreducing the computational time of the model selection procedure while\npreserving the statistical (minimax) optimality of the resulting estimator.\nMore precisely, given a sample of size $n$, if one should choose $k$ among\n$\\left\\{ 1, \\ldots, n \\right\\}$, and $\\left\\{ f^1, \\ldots, f^n \\right\\}$ are\nthe estimators of the regression function, the minimum discrepancy principle\nrequires calculation of a fraction of the estimators, while this is not the\ncase for the generalized cross-validation, Akaike's AIC criteria or Lepskii\nprinciple.\n","authors":["Yaroslav Averyanov","Alain Celisse"],"pdf_url":"https://arxiv.org/pdf/2008.08718v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07466v1","updated":"2024-06-11T17:12:41Z","published":"2024-06-11T17:12:41Z","title":"Multimodal Belief Prediction","summary":"  Recognizing a speaker's level of commitment to a belief is a difficult task;\nhumans do not only interpret the meaning of the words in context, but also\nunderstand cues from intonation and other aspects of the audio signal. Many\npapers and corpora in the NLP community have approached the belief prediction\ntask using text-only approaches. We are the first to frame and present results\non the multimodal belief prediction task. We use the CB-Prosody corpus (CBP),\ncontaining aligned text and audio with speaker belief annotations. We first\nreport baselines and significant features using acoustic-prosodic features and\ntraditional machine learning methods. We then present text and audio baselines\nfor the CBP corpus fine-tuning on BERT and Whisper respectively. Finally, we\npresent our multimodal architecture which fine-tunes on BERT and Whisper and\nuses multiple fusion methods, improving on both modalities alone.\n","authors":["John Murzaku","Adil Soubki","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2406.07466v1.pdf","comment":"John Murzaku and Adil Soubki contributed equally to this work"},{"id":"http://arxiv.org/abs/2405.03650v2","updated":"2024-06-11T17:12:26Z","published":"2024-05-06T17:14:09Z","title":"Generated Contents Enrichment","summary":"  In this paper, we investigate a novel artificial intelligence generation\ntask, termed as generated contents enrichment (GCE). Different from\nconventional artificial intelligence contents generation task that enriches the\ngiven textual description implicitly with limited semantics for generating\nvisually real content, our proposed GCE strives to perform content enrichment\nexplicitly on both the visual and textual domain, from which the enriched\ncontents are visually real, structurally reasonable, and semantically abundant.\nTowards to solve GCE, we propose a deep end-to-end method that explicitly\nexplores the semantics and inter-semantic relationships during the enrichment.\nSpecifically, we first model the input description as a semantic graph, wherein\neach node represents an object and each edge corresponds to the inter-object\nrelationship. We then adopt Graph Convolutional Networks on top of the input\nscene description to predict the enriching objects and their relationships with\nthe input objects. Finally, the enriched description is fed into an image\nsynthesis model to carry out the visual contents generation. Our experiments\nconducted on the Visual Genome dataset exhibit promising and visually plausible\nresults.\n","authors":["Mahdi Naseri","Jiayan Qiu","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2405.03650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05369v3","updated":"2024-06-11T17:12:22Z","published":"2023-03-09T16:17:45Z","title":"Data-dependent Generalization Bounds via Variable-Size Compressibility","summary":"  In this paper, we establish novel data-dependent upper bounds on the\ngeneralization error through the lens of a \"variable-size compressibility\"\nframework that we introduce newly here. In this framework, the generalization\nerror of an algorithm is linked to a variable-size 'compression rate' of its\ninput data. This is shown to yield bounds that depend on the empirical measure\nof the given input data at hand, rather than its unknown distribution. Our new\ngeneralization bounds that we establish are tail bounds, tail bounds on the\nexpectation, and in-expectations bounds. Moreover, it is shown that our\nframework also allows to derive general bounds on any function of the input\ndata and output hypothesis random variables. In particular, these general\nbounds are shown to subsume and possibly improve over several existing\nPAC-Bayes and data-dependent intrinsic dimension-based bounds that are\nrecovered as special cases, thus unveiling a unifying character of our\napproach. For instance, a new data-dependent intrinsic dimension-based bound is\nestablished, which connects the generalization error to the optimization\ntrajectories and reveals various interesting connections with the\nrate-distortion dimension of a process, the R\\'enyi information dimension of a\nprocess, and the metric mean dimension.\n","authors":["Milad Sefidgaran","Abdellatif Zaidi"],"pdf_url":"https://arxiv.org/pdf/2303.05369v3.pdf","comment":"Accepted for publication in IEEE Transactions on Information Theory"},{"id":"http://arxiv.org/abs/2406.07457v1","updated":"2024-06-11T17:01:52Z","published":"2024-06-11T17:01:52Z","title":"Estimating the Hallucination Rate of Generative AI","summary":"  This work is about estimating the hallucination rate for in-context learning\n(ICL) with Generative AI. In ICL, a conditional generative model (CGM) is\nprompted with a dataset and asked to make a prediction based on that dataset.\nThe Bayesian interpretation of ICL assumes that the CGM is calculating a\nposterior predictive distribution over an unknown Bayesian model of a latent\nparameter and data. With this perspective, we define a \\textit{hallucination}\nas a generated prediction that has low-probability under the true latent\nparameter. We develop a new method that takes an ICL problem -- that is, a CGM,\na dataset, and a prediction question -- and estimates the probability that a\nCGM will generate a hallucination. Our method only requires generating queries\nand responses from the model and evaluating its response log probability. We\nempirically evaluate our method on synthetic regression and natural language\nICL tasks using large language models.\n","authors":["Andrew Jesson","Nicolas Beltran-Velez","Quentin Chu","Sweta Karlekar","Jannik Kossen","Yarin Gal","John P. Cunningham","David Blei"],"pdf_url":"https://arxiv.org/pdf/2406.07457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07456v1","updated":"2024-06-11T17:01:45Z","published":"2024-06-11T17:01:45Z","title":"fKAN: Fractional Kolmogorov-Arnold Networks with trainable Jacobi basis\n  functions","summary":"  Recent advancements in neural network design have given rise to the\ndevelopment of Kolmogorov-Arnold Networks (KANs), which enhance speed,\ninterpretability, and precision. This paper presents the Fractional\nKolmogorov-Arnold Network (fKAN), a novel neural network architecture that\nincorporates the distinctive attributes of KANs with a trainable adaptive\nfractional-orthogonal Jacobi function as its basis function. By leveraging the\nunique mathematical properties of fractional Jacobi functions, including simple\nderivative formulas, non-polynomial behavior, and activity for both positive\nand negative input values, this approach ensures efficient learning and\nenhanced accuracy. The proposed architecture is evaluated across a range of\ntasks in deep learning and physics-informed deep learning. Precision is tested\non synthetic regression data, image classification, image denoising, and\nsentiment analysis. Additionally, the performance is measured on various\ndifferential equations, including ordinary, partial, and fractional delay\ndifferential equations. The results demonstrate that integrating fractional\nJacobi functions into KANs significantly improves training speed and\nperformance across diverse fields and applications.\n","authors":["Alireza Afzal Aghaei"],"pdf_url":"https://arxiv.org/pdf/2406.07456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07455v1","updated":"2024-06-11T17:01:41Z","published":"2024-06-11T17:01:41Z","title":"Reinforcement Learning from Human Feedback without Reward Inference:\n  Model-Free Algorithm and Instance-Dependent Analysis","summary":"  In this paper, we study reinforcement learning from human feedback (RLHF)\nunder an episodic Markov decision process with a general trajectory-wise reward\nmodel. We developed a model-free RLHF best policy identification algorithm,\ncalled $\\mathsf{BSAD}$, without explicit reward model inference, which is a\ncritical intermediate step in the contemporary RLHF paradigms for training\nlarge language models (LLM). The algorithm identifies the optimal policy\ndirectly from human preference information in a backward manner, employing a\ndueling bandit sub-routine that constantly duels actions to identify the\nsuperior one. $\\mathsf{BSAD}$ adopts a reward-free exploration and\nbest-arm-identification-like adaptive stopping criteria to equalize the\nvisitation among all states in the same decision step while moving to the\nprevious step as soon as the optimal action is identifiable, leading to a\nprovable, instance-dependent sample complexity\n$\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which\nresembles the result in classic RL, where $c_{\\mathcal{M}}$ is the\ninstance-dependent constant and $M$ is the batch size. Moreover,\n$\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with\nlogarithmic regret and generalized to discounted MDPs using a frame-based\napproach. Our results show: (i) sample-complexity-wise, RLHF is not\nsignificantly harder than classic RL and (ii) end-to-end RLHF may deliver\nimproved performance by avoiding pitfalls in reward inferring such as overfit\nand distribution shift.\n","authors":["Qining Zhang","Honghao Wei","Lei Ying"],"pdf_url":"https://arxiv.org/pdf/2406.07455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07451v1","updated":"2024-06-11T16:57:48Z","published":"2024-06-11T16:57:48Z","title":"An Optimism-based Approach to Online Evaluation of Generative Models","summary":"  Existing frameworks for evaluating and comparing generative models typically\ntarget an offline setting, where the evaluator has access to full batches of\ndata produced by the models. However, in many practical scenarios, the goal is\nto identify the best model using the fewest generated samples to minimize the\ncosts of querying data from the models. Such an online comparison is\nchallenging with current offline assessment methods. In this work, we propose\nan online evaluation framework to find the generative model that maximizes a\nstandard assessment score among a group of available models. Our method uses an\noptimism-based multi-armed bandit framework to identify the model producing\ndata with the highest evaluation score, quantifying the quality and diversity\nof generated data. Specifically, we study the online assessment of generative\nmodels based on the Fr\\'echet Inception Distance (FID) and Inception Score (IS)\nmetrics and propose the FID-UCB and IS-UCB algorithms leveraging the upper\nconfidence bound approach in online learning. We prove sub-linear regret bounds\nfor these algorithms and present numerical results on standard image datasets,\ndemonstrating their effectiveness in identifying the score-maximizing\ngenerative model.\n","authors":["Xiaoyan Hu","Ho-fung Leung","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2406.07451v1.pdf","comment":"arXiv version"},{"id":"http://arxiv.org/abs/2406.07450v1","updated":"2024-06-11T16:55:38Z","published":"2024-06-11T16:55:38Z","title":"Benchmarking Vision-Language Contrastive Methods for Medical\n  Representation Learning","summary":"  We perform a comprehensive benchmarking of contrastive frameworks for\nlearning multimodal representations in the medical domain. Through this study,\nwe aim to answer the following research questions: (i) How transferable are\ngeneral-domain representations to the medical domain? (ii) Is multimodal\ncontrastive training sufficient, or does it benefit from unimodal training as\nwell? (iii) What is the impact of feature granularity on the effectiveness of\nmultimodal medical representation learning? To answer these questions, we\ninvestigate eight contrastive learning approaches under identical training\nsetups, and train them on 2.8 million image-text pairs from four datasets, and\nevaluate them on 25 downstream tasks, including classification (zero-shot and\nlinear probing), image-to-text and text-to-image retrieval, and visual\nquestion-answering. Our findings suggest a positive answer to the first\nquestion, a negative answer to the second question, and the benefit of learning\nfine-grained features. Finally, we make our code publicly available.\n","authors":["Shuvendu Roy","Yasaman Parhizkar","Franklin Ogidi","Vahid Reza Khazaie","Michael Colacci","Ali Etemad","Elham Dolatabadi","Arash Afkanpour"],"pdf_url":"https://arxiv.org/pdf/2406.07450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07438v1","updated":"2024-06-11T16:45:48Z","published":"2024-06-11T16:45:48Z","title":"DeformTime: Capturing Variable Dependencies with Deformable Attention\n  for Time Series Forecasting","summary":"  In multivariate time series (MTS) forecasting, existing state-of-the-art deep\nlearning approaches tend to focus on autoregressive formulations and overlook\nthe information within exogenous indicators. To address this limitation, we\npresent DeformTime, a neural network architecture that attempts to capture\ncorrelated temporal patterns from the input space, and hence, improve\nforecasting accuracy. It deploys two core operations performed by deformable\nattention blocks (DABs): learning dependencies across variables from different\ntime steps (variable DAB), and preserving temporal dependencies in data from\nprevious time steps (temporal DAB). Input data transformation is explicitly\ndesigned to enhance learning from the deformed series of information while\npassing through a DAB. We conduct extensive experiments on 6 MTS data sets,\nusing previously established benchmarks as well as challenging infectious\ndisease modelling tasks with more exogenous variables. The results demonstrate\nthat DeformTime improves accuracy against previous competitive methods across\nthe vast majority of MTS forecasting tasks, reducing the mean absolute error by\n10% on average. Notably, performance gains remain consistent across longer\nforecasting horizons.\n","authors":["Yuxuan Shu","Vasileios Lampos"],"pdf_url":"https://arxiv.org/pdf/2406.07438v1.pdf","comment":"The code is available at https://github.com/ClaudiaShu/DeformTime"},{"id":"http://arxiv.org/abs/2406.07435v1","updated":"2024-06-11T16:42:17Z","published":"2024-06-11T16:42:17Z","title":"Beware of Aliases -- Signal Preservation is Crucial for Robust Image\n  Restoration","summary":"  Image restoration networks are usually comprised of an encoder and a decoder,\nresponsible for aggregating image content from noisy, distorted data and to\nrestore clean, undistorted images, respectively. Data aggregation as well as\nhigh-resolution image generation both usually come at the risk of involving\naliases, i.e.~standard architectures put their ability to reconstruct the model\ninput in jeopardy to reach high PSNR values on validation data. The price to be\npaid is low model robustness. In this work, we show that simply providing\nalias-free paths in state-of-the-art reconstruction transformers supports\nimproved model robustness at low costs on the restoration performance. We do so\nby proposing BOA-Restormer, a transformer-based image restoration model that\nexecutes downsampling and upsampling operations partly in the frequency domain\nto ensure alias-free paths along the entire model while potentially preserving\nall relevant high-frequency information.\n","authors":["Shashank Agnihotri","Julia Grabinski","Janis Keuper","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2406.07435v1.pdf","comment":"Tags: Adversarial attack, image restoration, image deblurring,\n  frequency sampling"},{"id":"http://arxiv.org/abs/2406.05232v2","updated":"2024-06-11T16:41:52Z","published":"2024-06-07T19:38:05Z","title":"Improving Logits-based Detector without Logits from Black-box LLMs","summary":"  The advent of Large Language Models (LLMs) has revolutionized text\ngeneration, producing outputs that closely mimic human writing. This blurring\nof lines between machine- and human-written text presents new challenges in\ndistinguishing one from the other a task further complicated by the frequent\nupdates and closed nature of leading proprietary LLMs. Traditional logits-based\ndetection methods leverage surrogate models for identifying LLM-generated\ncontent when the exact logits are unavailable from black-box LLMs. However,\nthese methods grapple with the misalignment between the distributions of the\nsurrogate and the often undisclosed target models, leading to performance\ndegradation, particularly with the introduction of new, closed-source models.\nFurthermore, while current methodologies are generally effective when the\nsource model is identified, they falter in scenarios where the model version\nremains unknown, or the test set comprises outputs from various source models.\nTo address these limitations, we present Distribution-Aligned LLMs Detection\n(DALD), an innovative framework that redefines the state-of-the-art performance\nin black-box text detection even without logits from source LLMs. DALD is\ndesigned to align the surrogate model's distribution with that of unknown\ntarget LLMs, ensuring enhanced detection capability and resilience against\nrapid model iterations with minimal training investment. By leveraging corpus\nsamples from publicly accessible outputs of advanced models such as ChatGPT,\nGPT-4 and Claude-3, DALD fine-tunes surrogate models to synchronize with\nunknown source model distributions effectively.\n","authors":["Cong Zeng","Shengkun Tang","Xianjun Yang","Yuanzhou Chen","Yiyou Sun","zhiqiang xu","Yao Li","Haifeng Chen","Wei Cheng","Dongkuan Xu"],"pdf_url":"https://arxiv.org/pdf/2406.05232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14960v4","updated":"2024-06-11T16:39:05Z","published":"2022-11-27T22:54:48Z","title":"Label Alignment Regularization for Distribution Shift","summary":"  Recent work has highlighted the label alignment property (LAP) in supervised\nlearning, where the vector of all labels in the dataset is mostly in the span\nof the top few singular vectors of the data matrix. Drawing inspiration from\nthis observation, we propose a regularization method for unsupervised domain\nadaptation that encourages alignment between the predictions in the target\ndomain and its top singular vectors. Unlike conventional domain adaptation\napproaches that focus on regularizing representations, we instead regularize\nthe classifier to align with the unsupervised target data, guided by the LAP in\nboth the source and target domains. Theoretical analysis demonstrates that,\nunder certain assumptions, our solution resides within the span of the top\nright singular vectors of the target domain data and aligns with the optimal\nsolution. By removing the reliance on the commonly used optimal joint risk\nassumption found in classic domain adaptation theory, we showcase the\neffectiveness of our method on addressing problems where traditional domain\nadaptation methods often fall short due to high joint error. Additionally, we\nreport improved performance over domain adaptation baselines in well-known\ntasks such as MNIST-USPS domain adaptation and cross-lingual sentiment\nanalysis.\n","authors":["Ehsan Imani","Guojun Zhang","Runjia Li","Jun Luo","Pascal Poupart","Philip H. S. Torr","Yangchen Pan"],"pdf_url":"https://arxiv.org/pdf/2211.14960v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18477v2","updated":"2024-06-11T16:37:51Z","published":"2024-02-28T16:58:31Z","title":"Signature Kernel Conditional Independence Tests in Causal Discovery for\n  Stochastic Processes","summary":"  Inferring the causal structure underlying stochastic dynamical systems from\nobservational data holds great promise in domains ranging from science and\nhealth to finance. Such processes can often be accurately modeled via\nstochastic differential equations (SDEs), which naturally imply causal\nrelationships via \"which variables enter the differential of which other\nvariables\". In this paper, we develop a kernel-based test of conditional\nindependence (CI) on \"path-space\" -- e.g., solutions to SDEs, but applicable\nbeyond that -- by leveraging recent advances in signature kernels. We\ndemonstrate strictly superior performance of our proposed CI test compared to\nexisting approaches on path-space and provide theoretical consistency results.\nThen, we develop constraint-based causal discovery algorithms for acyclic\nstochastic dynamical systems (allowing for self-loops) that leverage temporal\ninformation to recover the entire directed acyclic graph. Assuming faithfulness\nand a CI oracle, we show that our algorithms are sound and complete. We\nempirically verify that our developed CI test in conjunction with the causal\ndiscovery algorithms outperform baselines across a range of settings.\n","authors":["Georg Manten","Cecilia Casolo","Emilio Ferrucci","Søren Wengel Mogensen","Cristopher Salvi","Niki Kilbertus"],"pdf_url":"https://arxiv.org/pdf/2402.18477v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07428v1","updated":"2024-06-11T16:30:30Z","published":"2024-06-11T16:30:30Z","title":"GemNet: Menu-Based, Strategy-Proof Multi-Bidder Auctions Through Deep\n  Learning","summary":"  Differentiable economics uses deep learning for automated mechanism design.\nDespite strong progress, it has remained an open problem to learn multi-bidder,\ngeneral, and fully strategy-proof (SP) auctions. We introduce GEneral\nMenu-based NETwork (GemNet), which significantly extends the menu-based\napproach of RochetNet [D\\\"utting et al., 2023] to the multi-bidder setting. The\nchallenge in achieving SP is to learn bidder-independent menus that are\nfeasible, so that the optimal menu choices for each bidder do not over-allocate\nitems when taken together (we call this menu compatibility). GemNet penalizes\nthe failure of menu compatibility during training, and transforms learned menus\nafter training through price changes, by considering a set of discretized\nbidder values and reasoning about Lipschitz smoothness to guarantee menu\ncompatibility on the entire value space. This approach is general, leaving\nundisturbed trained menus that already satisfy menu compatibility and reducing\nto RochetNet for a single bidder. Mixed-integer linear programs are used for\nmenu transforms and through a number of optimizations, including adaptive grids\nand methods to skip menu elements, we scale to large auction design problems.\nGemNet learns auctions with better revenue than affine maximization methods,\nachieves exact SP whereas previous general multi-bidder methods are\napproximately SP, and offers greatly enhanced interpretability.\n","authors":["Tonghan Wang","Yanchen Jiang","David C. Parkes"],"pdf_url":"https://arxiv.org/pdf/2406.07428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00886v4","updated":"2024-06-11T16:25:52Z","published":"2023-12-01T19:26:23Z","title":"Nash Learning from Human Feedback","summary":"  Reinforcement learning from human feedback (RLHF) has emerged as the main\nparadigm for aligning large language models (LLMs) with human preferences.\nTypically, RLHF involves the initial step of learning a reward model from human\nfeedback, often expressed as preferences between pairs of text generations\nproduced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by\noptimizing it to maximize the reward model through a reinforcement learning\nalgorithm. However, an inherent limitation of current reward models is their\ninability to fully represent the richness of human preferences and their\ndependency on the sampling distribution.\n  In this study, we introduce an alternative pipeline for the fine-tuning of\nLLMs using pairwise human feedback. Our approach entails the initial learning\nof a preference model, which is conditioned on two inputs given a prompt,\nfollowed by the pursuit of a policy that consistently generates responses\npreferred over those generated by any competing policy, thus defining the Nash\nequilibrium of this preference model. We term this approach Nash learning from\nhuman feedback (NLHF).\n  In the context of a tabular policy representation, we present a novel\nalgorithmic solution, Nash-MD, founded on the principles of mirror descent.\nThis algorithm produces a sequence of policies, with the last iteration\nconverging to the regularized Nash equilibrium. Additionally, we explore\nparametric representations of policies and introduce gradient descent\nalgorithms for deep-learning architectures. To demonstrate the effectiveness of\nour approach, we present experimental results involving the fine-tuning of a\nLLM for a text summarization task. We believe NLHF offers a compelling avenue\nfor preference learning and policy optimization with the potential of advancing\nthe field of aligning LLMs with human preferences.\n","authors":["Rémi Munos","Michal Valko","Daniele Calandriello","Mohammad Gheshlaghi Azar","Mark Rowland","Zhaohan Daniel Guo","Yunhao Tang","Matthieu Geist","Thomas Mesnard","Andrea Michi","Marco Selvi","Sertan Girgin","Nikola Momchev","Olivier Bachem","Daniel J. Mankowitz","Doina Precup","Bilal Piot"],"pdf_url":"https://arxiv.org/pdf/2312.00886v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07423v1","updated":"2024-06-11T16:23:33Z","published":"2024-06-11T16:23:33Z","title":"Beyond ELBOs: A Large-Scale Evaluation of Variational Methods for\n  Sampling","summary":"  Monte Carlo methods, Variational Inference, and their combinations play a\npivotal role in sampling from intractable probability distributions. However,\ncurrent studies lack a unified evaluation framework, relying on disparate\nperformance measures and limited method comparisons across diverse tasks,\ncomplicating the assessment of progress and hindering the decision-making of\npractitioners. In response to these challenges, our work introduces a benchmark\nthat evaluates sampling methods using a standardized task suite and a broad\nrange of performance criteria. Moreover, we study existing metrics for\nquantifying mode collapse and introduce novel metrics for this purpose. Our\nfindings provide insights into strengths and weaknesses of existing sampling\nmethods, serving as a valuable reference for future developments. The code is\npublicly available here.\n","authors":["Denis Blessing","Xiaogang Jia","Johannes Esslinger","Francisco Vargas","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2406.07423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07418v1","updated":"2024-06-11T16:21:33Z","published":"2024-06-11T16:21:33Z","title":"Enhanced Gene Selection in Single-Cell Genomics: Pre-Filtering Synergy\n  and Reinforced Optimization","summary":"  Recent advancements in single-cell genomics necessitate precision in gene\npanel selection to interpret complex biological data effectively. Those methods\naim to streamline the analysis of scRNA-seq data by focusing on the most\ninformative genes that contribute significantly to the specific analysis task.\nTraditional selection methods, which often rely on expert domain knowledge,\nembedded machine learning models, or heuristic-based iterative optimization,\nare prone to biases and inefficiencies that may obscure critical genomic\nsignals. Recognizing the limitations of traditional methods, we aim to\ntranscend these constraints with a refined strategy. In this study, we\nintroduce an iterative gene panel selection strategy that is applicable to\nclustering tasks in single-cell genomics. Our method uniquely integrates\nresults from other gene selection algorithms, providing valuable preliminary\nboundaries or prior knowledge as initial guides in the search space to enhance\nthe efficiency of our framework. Furthermore, we incorporate the stochastic\nnature of the exploration process in reinforcement learning (RL) and its\ncapability for continuous optimization through reward-based feedback. This\ncombination mitigates the biases inherent in the initial boundaries and\nharnesses RL's adaptability to refine and target gene panel selection\ndynamically. To illustrate the effectiveness of our method, we conducted\ndetailed comparative experiments, case studies, and visualization analysis.\n","authors":["Weiliang Zhang","Zhen Meng","Dongjie Wang","Min Wu","Kunpeng Liu","Yuanchun Zhou","Meng Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.07418v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2406.07413v1","updated":"2024-06-11T16:18:15Z","published":"2024-06-11T16:18:15Z","title":"Holistic Memory Diversification for Incremental Learning in Growing\n  Graphs","summary":"  This paper addresses the challenge of incremental learning in growing graphs\nwith increasingly complex tasks. The goal is to continually train a graph model\nto handle new tasks while retaining its inference ability on previous tasks.\nExisting methods usually neglect the importance of memory diversity, limiting\nin effectively selecting high-quality memory from previous tasks and\nremembering broad previous knowledge within the scarce memory on graphs. To\naddress that, we introduce a novel holistic Diversified Memory Selection and\nGeneration (DMSG) framework for incremental learning in graphs, which first\nintroduces a buffer selection strategy that considers both intra-class and\ninter-class diversities, employing an efficient greedy algorithm for sampling\nrepresentative training nodes from graphs into memory buffers after learning\neach new task. Then, to adequately rememorize the knowledge preserved in the\nmemory buffer when learning new tasks, we propose a diversified memory\ngeneration replay method. This method first utilizes a variational layer to\ngenerate the distribution of buffer node embeddings and sample synthesized ones\nfor replaying. Furthermore, an adversarial variational embedding learning\nmethod and a reconstruction-based decoder are proposed to maintain the\nintegrity and consolidate the generalization of the synthesized node\nembeddings, respectively. Finally, we evaluate our model on node classification\ntasks involving increasing class numbers. Extensive experimental results on\npublicly accessible datasets demonstrate the superiority of DMSG over\nstate-of-the-art methods.\n","authors":["Ziyue Qiao","Junren Xiao","Qingqiang Sun","Meng Xiao","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.07413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13927v3","updated":"2024-06-11T16:17:57Z","published":"2023-12-21T15:22:07Z","title":"On the Convergence of Loss and Uncertainty-based Active Learning\n  Algorithms","summary":"  We investigate the convergence rates and data sample sizes required for\ntraining a machine learning model using a stochastic gradient descent (SGD)\nalgorithm, where data points are sampled based on either their loss value or\nuncertainty value. These training methods are particularly relevant for active\nlearning and data subset selection problems. For SGD with a constant step size\nupdate, we present convergence results for linear classifiers and linearly\nseparable datasets using squared hinge loss and similar training loss\nfunctions. Additionally, we extend our analysis to more general classifiers and\ndatasets, considering a wide range of loss-based sampling strategies and smooth\nconvex training loss functions. We propose a novel algorithm called\nAdaptive-Weight Sampling (AWS) that utilizes SGD with an adaptive step size\nthat achieves stochastic Polyak's step size in expectation. We establish\nconvergence rate results for AWS for smooth convex training loss functions. Our\nnumerical experiments demonstrate the efficiency of AWS on various datasets by\nusing either exact or estimated loss values.\n","authors":["Daniel Haimovich","Dima Karamshuk","Fridolin Linder","Niek Tax","Milan Vojnovic"],"pdf_url":"https://arxiv.org/pdf/2312.13927v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07409v1","updated":"2024-06-11T16:14:30Z","published":"2024-06-11T16:14:30Z","title":"Accelerating Ill-conditioned Hankel Matrix Recovery via Structured\n  Newton-like Descent","summary":"  This paper studies the robust Hankel recovery problem, which simultaneously\nremoves the sparse outliers and fulfills missing entries from the partial\nobservation. We propose a novel non-convex algorithm, coined Hankel Structured\nNewton-Like Descent (HSNLD), to tackle the robust Hankel recovery problem.\nHSNLD is highly efficient with linear convergence, and its convergence rate is\nindependent of the condition number of the underlying Hankel matrix. The\nrecovery guarantee has been established under some mild conditions. Numerical\nexperiments on both synthetic and real datasets show the superior performance\nof HSNLD against state-of-the-art algorithms.\n","authors":["HanQin Cai","Longxiu Huang","Xiliang Lu","Juntao You"],"pdf_url":"https://arxiv.org/pdf/2406.07409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07407v1","updated":"2024-06-11T16:13:09Z","published":"2024-06-11T16:13:09Z","title":"Private Geometric Median","summary":"  In this paper, we study differentially private (DP) algorithms for computing\nthe geometric median (GM) of a dataset: Given $n$ points, $x_1,\\dots,x_n$ in\n$\\mathbb{R}^d$, the goal is to find a point $\\theta$ that minimizes the sum of\nthe Euclidean distances to these points, i.e., $\\sum_{i=1}^{n} \\|\\theta -\nx_i\\|_2$. Off-the-shelf methods, such as DP-GD, require strong a priori\nknowledge locating the data within a ball of radius $R$, and the excess risk of\nthe algorithm depends linearly on $R$. In this paper, we ask: can we design an\nefficient and private algorithm with an excess error guarantee that scales with\nthe (unknown) radius containing the majority of the datapoints? Our main\ncontribution is a pair of polynomial-time DP algorithms for the task of private\nGM with an excess error guarantee that scales with the effective diameter of\nthe datapoints. Additionally, we propose an inefficient algorithm based on the\ninverse smooth sensitivity mechanism, which satisfies the more restrictive\nnotion of pure DP. We complement our results with a lower bound and demonstrate\nthe optimality of our polynomial-time algorithms in terms of sample complexity.\n","authors":["Mahdi Haghifam","Thomas Steinke","Jonathan Ullman"],"pdf_url":"https://arxiv.org/pdf/2406.07407v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2406.07404v1","updated":"2024-06-11T16:10:37Z","published":"2024-06-11T16:10:37Z","title":"Enhancing Tabular Data Optimization with a Flexible Graph-based\n  Reinforced Exploration Strategy","summary":"  Tabular data optimization methods aim to automatically find an optimal\nfeature transformation process that generates high-value features and improves\nthe performance of downstream machine learning tasks. Current frameworks for\nautomated feature transformation rely on iterative sequence generation tasks,\noptimizing decision strategies through performance feedback from downstream\ntasks. However, these approaches fail to effectively utilize historical\ndecision-making experiences and overlook potential relationships among\ngenerated features, thus limiting the depth of knowledge extraction. Moreover,\nthe granularity of the decision-making process lacks dynamic backtracking\ncapabilities for individual features, leading to insufficient adaptability when\nencountering inefficient pathways, adversely affecting overall robustness and\nexploration efficiency. To address the limitations observed in current\nautomatic feature engineering frameworks, we introduce a novel method that\nutilizes a feature-state transformation graph to effectively preserve the\nentire feature transformation journey, where each node represents a specific\ntransformation state. During exploration, three cascading agents iteratively\nselect nodes and idea mathematical operations to generate new transformation\nstates. This strategy leverages the inherent properties of the graph structure,\nallowing for the preservation and reuse of valuable transformations. It also\nenables backtracking capabilities through graph pruning techniques, which can\nrectify inefficient transformation paths. To validate the efficacy and\nflexibility of our approach, we conducted comprehensive experiments and\ndetailed case studies, demonstrating superior performance in diverse scenarios.\n","authors":["Xiaohan Huang","Dongjie Wang","Zhiyuan Ning","Ziyue Qiao","Qingqing Long","Haowei Zhu","Min Wu","Yuanchun Zhou","Meng Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.07404v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2406.07402v1","updated":"2024-06-11T16:08:39Z","published":"2024-06-11T16:08:39Z","title":"A Survey on Recent Random Walk-based Methods for Embedding Knowledge\n  Graphs","summary":"  Machine learning, deep learning, and NLP methods on knowledge graphs are\npresent in different fields and have important roles in various domains from\nself-driving cars to friend recommendations on social media platforms. However,\nto apply these methods to knowledge graphs, the data usually needs to be in an\nacceptable size and format. In fact, knowledge graphs normally have high\ndimensions and therefore we need to transform them to a low-dimensional vector\nspace. An embedding is a low-dimensional space into which you can translate\nhigh dimensional vectors in a way that intrinsic features of the input data are\npreserved. In this review, we first explain knowledge graphs and their\nembedding and then review some of the random walk-based embedding methods that\nhave been developed recently.\n","authors":["Elika Bozorgi","Sakher Khalil Alqaiidi","Afsaneh Shams","Hamid Reza Arabnia","Krzysztof Kochut"],"pdf_url":"https://arxiv.org/pdf/2406.07402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07400v1","updated":"2024-06-11T16:07:24Z","published":"2024-06-11T16:07:24Z","title":"Guiding LLM Temporal Logic Generation with Explicit Separation of Data\n  and Control","summary":"  Temporal logics are powerful tools that are widely used for the synthesis and\nverification of reactive systems. The recent progress on Large Language Models\n(LLMs) has the potential to make the process of writing such specifications\nmore accessible. However, writing specifications in temporal logics remains\nchallenging for all but the most expert users. A key question in using LLMs for\ntemporal logic specification engineering is to understand what kind of guidance\nis most helpful to the LLM and the users to easily produce specifications.\nLooking specifically at the problem of reactive program synthesis, we explore\nthe impact of providing an LLM with guidance on the separation of control and\ndata--making explicit for the LLM what functionality is relevant for the\nspecification, and treating the remaining functionality as an implementation\ndetail for a series of pre-defined functions and predicates. We present a\nbenchmark set and find that this separation of concerns improves specification\ngeneration. Our benchmark provides a test set against which to verify future\nwork in LLM generation of temporal logic specifications.\n","authors":["William Murphy","Nikolaus Holzer","Nathan Koenig","Leyi Cui","Raven Rothkopf","Feitong Qiao","Mark Santolucito"],"pdf_url":"https://arxiv.org/pdf/2406.07400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07399v1","updated":"2024-06-11T16:07:08Z","published":"2024-06-11T16:07:08Z","title":"Redefining Automotive Radar Imaging: A Domain-Informed 1D Deep Learning\n  Approach for High-Resolution and Efficient Performance","summary":"  Millimeter-wave (mmWave) radars are indispensable for perception tasks of\nautonomous vehicles, thanks to their resilience in challenging weather\nconditions. Yet, their deployment is often limited by insufficient spatial\nresolution for precise semantic scene interpretation. Classical\nsuper-resolution techniques adapted from optical imaging inadequately address\nthe distinct characteristics of radar signal data. In response, our study\nredefines radar imaging super-resolution as a one-dimensional (1D) signal\nsuper-resolution spectra estimation problem by harnessing the radar signal\nprocessing domain knowledge, introducing innovative data normalization and a\ndomain-informed signal-to-noise ratio (SNR)-guided loss function. Our tailored\ndeep learning network for automotive radar imaging exhibits remarkable\nscalability, parameter efficiency and fast inference speed, alongside enhanced\nperformance in terms of radar imaging quality and resolution. Extensive testing\nconfirms that our SR-SPECNet sets a new benchmark in producing high-resolution\nradar range-azimuth images, outperforming existing methods across varied\nantenna configurations and dataset sizes. Source code and new radar dataset\nwill be made publicly available online.\n","authors":["Ruxin Zheng","Shunqiao Sun","Holger Caesar","Honglei Chen","Jian Li"],"pdf_url":"https://arxiv.org/pdf/2406.07399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07398v1","updated":"2024-06-11T16:05:15Z","published":"2024-06-11T16:05:15Z","title":"Visual Representation Learning with Stochastic Frame Prediction","summary":"  Self-supervised learning of image representations by predicting future frames\nis a promising direction but still remains a challenge. This is because of the\nunder-determined nature of frame prediction; multiple potential futures can\narise from a single current frame. To tackle this challenge, in this paper, we\nrevisit the idea of stochastic video generation that learns to capture\nuncertainty in frame prediction and explore its effectiveness for\nrepresentation learning. Specifically, we design a framework that trains a\nstochastic frame prediction model to learn temporal information between frames.\nMoreover, to learn dense information within each frame, we introduce an\nauxiliary masked image modeling objective along with a shared decoder\narchitecture. We find this architecture allows for combining both objectives in\na synergistic and compute-efficient manner. We demonstrate the effectiveness of\nour framework on a variety of tasks from video label propagation and\nvision-based robot learning domains, such as video segmentation, pose tracking,\nvision-based robotic locomotion, and manipulation tasks. Code is available on\nthe project webpage: https://sites.google.com/view/2024rsp.\n","authors":["Huiwon Jang","Dongyoung Kim","Junsu Kim","Jinwoo Shin","Pieter Abbeel","Younggyo Seo"],"pdf_url":"https://arxiv.org/pdf/2406.07398v1.pdf","comment":"International Conference on Machine Learning (ICML) 2024"},{"id":"http://arxiv.org/abs/2309.11267v2","updated":"2024-06-11T15:55:48Z","published":"2023-09-20T12:50:52Z","title":"From Classification to Segmentation with Explainable AI: A Study on\n  Crack Detection and Growth Monitoring","summary":"  Monitoring surface cracks in infrastructure is crucial for structural health\nmonitoring. Automatic visual inspection offers an effective solution,\nespecially in hard-to-reach areas. Machine learning approaches have proven\ntheir effectiveness but typically require large annotated datasets for\nsupervised training. Once a crack is detected, monitoring its severity often\ndemands precise segmentation of the damage. However, pixel-level annotation of\nimages for segmentation is labor-intensive. To mitigate this cost, one can\nleverage explainable artificial intelligence (XAI) to derive segmentations from\nthe explanations of a classifier, requiring only weak image-level supervision.\nThis paper proposes applying this methodology to segment and monitor surface\ncracks. We evaluate the performance of various XAI methods and examine how this\napproach facilitates severity quantification and growth monitoring. Results\nreveal that while the resulting segmentation masks may exhibit lower quality\nthan those produced by supervised methods, they remain meaningful and enable\nseverity monitoring, thus reducing substantial labeling costs.\n","authors":["Florent Forest","Hugo Porta","Devis Tuia","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2309.11267v2.pdf","comment":"49 pages. Accepted for publication in Automation in Construction"},{"id":"http://arxiv.org/abs/2406.07381v1","updated":"2024-06-11T15:49:08Z","published":"2024-06-11T15:49:08Z","title":"World Models with Hints of Large Language Models for Goal Achieving","summary":"  Reinforcement learning struggles in the face of long-horizon tasks and sparse\ngoals due to the difficulty in manual reward specification. While existing\nmethods address this by adding intrinsic rewards, they may fail to provide\nmeaningful guidance in long-horizon decision-making tasks with large state and\naction spaces, lacking purposeful exploration. Inspired by human cognition, we\npropose a new multi-modal model-based RL approach named Dreaming with Large\nLanguage Models (DLLM). DLLM integrates the proposed hinting subgoals from the\nLLMs into the model rollouts to encourage goal discovery and reaching in\nchallenging tasks. By assigning higher intrinsic rewards to samples that align\nwith the hints outlined by the language model during model rollouts, DLLM\nguides the agent toward meaningful and efficient exploration. Extensive\nexperiments demonstrate that the DLLM outperforms recent methods in various\nchallenging, sparse-reward environments such as HomeGrid, Crafter, and\nMinecraft by 27.7\\%, 21.1\\%, and 9.9\\%, respectively.\n","authors":["Zeyuan Liu","Ziyu Huan","Xiyao Wang","Jiafei Lyu","Jian Tao","Xiu Li","Furong Huang","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2406.07381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02131v3","updated":"2024-06-11T15:49:07Z","published":"2024-06-04T09:18:20Z","title":"CondTSF: One-line Plugin of Dataset Condensation for Time Series\n  Forecasting","summary":"  Dataset condensation is a newborn technique that generates a small dataset\nthat can be used in training deep neural networks to lower training costs. The\nobjective of dataset condensation is to ensure that the model trained with the\nsynthetic dataset can perform comparably to the model trained with full\ndatasets. However, existing methods predominantly concentrate on classification\ntasks, posing challenges in their adaptation to time series forecasting\n(TS-forecasting). This challenge arises from disparities in the evaluation of\nsynthetic data. In classification, the synthetic data is considered\nwell-distilled if the model trained with the full dataset and the model trained\nwith the synthetic dataset yield identical labels for the same input,\nregardless of variations in output logits distribution. Conversely, in\nTS-forecasting, the effectiveness of synthetic data distillation is determined\nby the distance between predictions of the two models. The synthetic data is\ndeemed well-distilled only when all data points within the predictions are\nsimilar. Consequently, TS-forecasting has a more rigorous evaluation\nmethodology compared to classification. To mitigate this gap, we theoretically\nanalyze the optimization objective of dataset condensation for TS-forecasting\nand propose a new one-line plugin of dataset condensation designated as Dataset\nCondensation for Time Series Forecasting (CondTSF) based on our analysis.\nPlugging CondTSF into previous dataset condensation methods facilitates a\nreduction in the distance between the predictions of the model trained with the\nfull dataset and the model trained with the synthetic dataset, thereby\nenhancing performance. We conduct extensive experiments on eight commonly used\ntime series datasets. CondTSF consistently improves the performance of all\nprevious dataset condensation methods across all datasets, particularly at low\ncondensing ratios.\n","authors":["Jianrong Ding","Zhanyu Liu","Guanjie Zheng","Haiming Jin","Linghe Kong"],"pdf_url":"https://arxiv.org/pdf/2406.02131v3.pdf","comment":"23 pages, 13 figures"},{"id":"http://arxiv.org/abs/2406.07375v1","updated":"2024-06-11T15:41:56Z","published":"2024-06-11T15:41:56Z","title":"Improving the realism of robotic surgery simulation through injection of\n  learning-based estimated errors","summary":"  The development of algorithms for automation of subtasks during robotic\nsurgery can be accelerated by the availability of realistic simulation\nenvironments. In this work, we focus on one aspect of the realism of a surgical\nsimulator, which is the positional accuracy of the robot. In current\nsimulators, robots have perfect or near-perfect accuracy, which is not\nrepresentative of their physical counterparts. We therefore propose a pair of\nneural networks, trained by data collected from a physical robot, to estimate\nboth the controller error and the kinematic and non-kinematic error. These\nerror estimates are then injected within the simulator to produce a simulated\nrobot that has the characteristic performance of the physical robot. In this\nscenario, we believe it is sufficient for the estimated error used in the\nsimulation to have a statistically similar distribution to the actual error of\nthe physical robot. This is less stringent, and therefore more tenable, than\nthe requirement for error compensation of a physical robot, where the estimated\nerror should equal the actual error. Our results demonstrate that error\ninjection reduces the mean position and orientation differences between the\nsimulated and physical robots from 5.0 mm / 3.6 deg to 1.3 mm / 1.7 deg,\nrespectively, which represents reductions by factors of 3.8 and 2.1.\n","authors":["Juan Antonio Barragan","Hisashi Ishida","Adnan Munawar","Peter Kazanzides"],"pdf_url":"https://arxiv.org/pdf/2406.07375v1.pdf","comment":"6 page paper"},{"id":"http://arxiv.org/abs/2406.07373v1","updated":"2024-06-11T15:41:48Z","published":"2024-06-11T15:41:48Z","title":"Closing the Computational-Query Depth Gap in Parallel Stochastic Convex\n  Optimization","summary":"  We develop a new parallel algorithm for minimizing Lipschitz, convex\nfunctions with a stochastic subgradient oracle. The total number of queries\nmade and the query depth, i.e., the number of parallel rounds of queries, match\nthe prior state-of-the-art, [CJJLLST23], while improving upon the computational\ndepth by a polynomial factor for sufficiently small accuracy. When combined\nwith previous state-of-the-art methods our result closes a gap between the\nbest-known query depth and the best-known computational depth of parallel\nalgorithms.\n  Our method starts with a ball acceleration framework of previous parallel\nmethods, i.e., [CJJJLST20, ACJJS21], which reduce the problem to minimizing a\nregularized Gaussian convolution of the function constrained to Euclidean\nballs. By developing and leveraging new stability properties of the Hessian of\nthis induced function, we depart from prior parallel algorithms and reduce\nthese ball-constrained optimization problems to stochastic unconstrained\nquadratic minimization problems. Although we are unable to prove concentration\nof the asymmetric matrices that we use to approximate this Hessian, we\nnevertheless develop an efficient parallel method for solving these quadratics.\nInterestingly, our algorithms can be improved using fast matrix multiplication\nand use nearly-linear work if the matrix multiplication exponent is 2.\n","authors":["Arun Jambulapati","Aaron Sidford","Kevin Tian"],"pdf_url":"https://arxiv.org/pdf/2406.07373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16630v2","updated":"2024-06-11T15:40:43Z","published":"2023-07-31T13:08:16Z","title":"Text-CRS: A Generalized Certified Robustness Framework against Textual\n  Adversarial Attacks","summary":"  The language models, especially the basic text classification models, have\nbeen shown to be susceptible to textual adversarial attacks such as synonym\nsubstitution and word insertion attacks. To defend against such attacks, a\ngrowing body of research has been devoted to improving the model robustness.\nHowever, providing provable robustness guarantees instead of empirical\nrobustness is still widely unexplored. In this paper, we propose Text-CRS, a\ngeneralized certified robustness framework for natural language processing\n(NLP) based on randomized smoothing. To our best knowledge, existing certified\nschemes for NLP can only certify the robustness against $\\ell_0$ perturbations\nin synonym substitution attacks. Representing each word-level adversarial\noperation (i.e., synonym substitution, word reordering, insertion, and\ndeletion) as a combination of permutation and embedding transformation, we\npropose novel smoothing theorems to derive robustness bounds in both\npermutation and embedding space against such adversarial operations. To further\nimprove certified accuracy and radius, we consider the numerical relationships\nbetween discrete words and select proper noise distributions for the randomized\nsmoothing. Finally, we conduct substantial experiments on multiple language\nmodels and datasets. Text-CRS can address all four different word-level\nadversarial operations and achieve a significant accuracy improvement. We also\nprovide the first benchmark on certified accuracy and radius of four word-level\noperations, besides outperforming the state-of-the-art certification against\nsynonym substitution attacks.\n","authors":["Xinyu Zhang","Hanbin Hong","Yuan Hong","Peng Huang","Binghui Wang","Zhongjie Ba","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2307.16630v2.pdf","comment":"Published in the 2024 IEEE Symposium on Security and Privacy (SP)"},{"id":"http://arxiv.org/abs/2311.11961v2","updated":"2024-06-11T15:39:52Z","published":"2023-11-20T17:38:35Z","title":"NNG-Mix: Improving Semi-supervised Anomaly Detection with Pseudo-anomaly\n  Generation","summary":"  Anomaly detection (AD) is essential in identifying rare and often critical\nevents in complex systems, finding applications in fields such as network\nintrusion detection, financial fraud detection, and fault detection in\ninfrastructure and industrial systems. While AD is typically treated as an\nunsupervised learning task due to the high cost of label annotation, it is more\npractical to assume access to a small set of labeled anomaly samples from\ndomain experts, as is the case for semi-supervised anomaly detection.\nSemi-supervised and supervised approaches can leverage such labeled data,\nresulting in improved performance. In this paper, rather than proposing a new\nsemi-supervised or supervised approach for AD, we introduce a novel algorithm\nfor generating additional pseudo-anomalies on the basis of the limited labeled\nanomalies and a large volume of unlabeled data. This serves as an augmentation\nto facilitate the detection of new anomalies. Our proposed algorithm, named\nNearest Neighbor Gaussian Mixup (NNG-Mix), efficiently integrates information\nfrom both labeled and unlabeled data to generate pseudo-anomalies. We compare\nthe performance of this novel algorithm with commonly applied augmentation\ntechniques, such as Mixup and Cutout. We evaluate NNG-Mix by training various\nexisting semi-supervised and supervised anomaly detection algorithms on the\noriginal training data along with the generated pseudo-anomalies. Through\nextensive experiments on 57 benchmark datasets in ADBench, reflecting different\ndata types, we demonstrate that NNG-Mix outperforms other data augmentation\nmethods. It yields significant performance improvements compared to the\nbaselines trained exclusively on the original training data. Notably, NNG-Mix\nyields up to 16.4%, 8.8%, and 8.0% improvements on Classical, CV, and NLP\ndatasets in ADBench. Our source code is available at\nhttps://github.com/donghao51/NNG-Mix.\n","authors":["Hao Dong","Gaëtan Frusque","Yue Zhao","Eleni Chatzi","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2311.11961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17739v2","updated":"2024-06-11T15:35:20Z","published":"2024-02-27T18:18:23Z","title":"reBandit: Random Effects based Online RL algorithm for Reducing Cannabis\n  Use","summary":"  The escalating prevalence of cannabis use, and associated cannabis-use\ndisorder (CUD), poses a significant public health challenge globally. With a\nnotably wide treatment gap, especially among emerging adults (EAs; ages 18-25),\naddressing cannabis use and CUD remains a pivotal objective within the 2030\nUnited Nations Agenda for Sustainable Development Goals (SDG). In this work, we\ndevelop an online reinforcement learning (RL) algorithm called reBandit which\nwill be utilized in a mobile health study to deliver personalized mobile health\ninterventions aimed at reducing cannabis use among EAs. reBandit utilizes\nrandom effects and informative Bayesian priors to learn quickly and efficiently\nin noisy mobile health environments. Moreover, reBandit employs Empirical Bayes\nand optimization techniques to autonomously update its hyper-parameters online.\nTo evaluate the performance of our algorithm, we construct a simulation testbed\nusing data from a prior study, and compare against commonly used algorithms in\nmobile health studies. We show that reBandit performs equally well or better\nthan all the baseline algorithms, and the performance gap widens as population\nheterogeneity increases in the simulation environment, proving its adeptness to\nadapt to diverse population of study participants.\n","authors":["Susobhan Ghosh","Yongyi Guo","Pei-Yao Hung","Lara Coughlin","Erin Bonar","Inbal Nahum-Shani","Maureen Walton","Susan Murphy"],"pdf_url":"https://arxiv.org/pdf/2402.17739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07368v1","updated":"2024-06-11T15:34:43Z","published":"2024-06-11T15:34:43Z","title":"When Linear Attention Meets Autoregressive Decoding: Towards More\n  Effective and Efficient Linearized Large Language Models","summary":"  Autoregressive Large Language Models (LLMs) have achieved impressive\nperformance in language tasks but face two significant bottlenecks: (1)\nquadratic complexity in the attention module as the number of tokens increases,\nand (2) limited efficiency due to the sequential processing nature of\nautoregressive LLMs during generation. While linear attention and speculative\ndecoding offer potential solutions, their applicability and synergistic\npotential for enhancing autoregressive LLMs remain uncertain. We conduct the\nfirst comprehensive study on the efficacy of existing linear attention methods\nfor autoregressive LLMs, integrating them with speculative decoding. We\nintroduce an augmentation technique for linear attention that ensures\ncompatibility with speculative decoding, enabling more efficient training and\nserving of LLMs. Extensive experiments and ablation studies involving seven\nexisting linear attention models and five encoder/decoder-based LLMs\nconsistently validate the effectiveness of our augmented linearized LLMs.\nNotably, our approach achieves up to a 6.67 reduction in perplexity on the\nLLaMA model and up to a 2$\\times$ speedup during generation compared to prior\nlinear attention methods. Codes and models are available at\nhttps://github.com/GATECH-EIC/Linearized-LLM.\n","authors":["Haoran You","Yichao Fu","Zheng Wang","Amir Yazdanbakhsh"," Yingyan"," Lin"],"pdf_url":"https://arxiv.org/pdf/2406.07368v1.pdf","comment":"Accepted by ICML 2024; 17 pages; 10 figures; 16 tables"},{"id":"http://arxiv.org/abs/2306.06446v5","updated":"2024-06-11T15:34:06Z","published":"2023-06-10T13:53:41Z","title":"ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient\n  Vision Transformer","summary":"  Vision Transformers (ViTs) have shown impressive performance and have become\na unified backbone for multiple vision tasks. However, both the attention\nmechanism and multi-layer perceptrons (MLPs) in ViTs are not sufficiently\nefficient due to dense multiplications, leading to costly training and\ninference. To this end, we propose to reparameterize pre-trained ViTs with a\nmixture of multiplication primitives, e.g., bitwise shifts and additions,\ntowards a new type of multiplication-reduced model, dubbed\n$\\textbf{ShiftAddViT}$, which aims to achieve end-to-end inference speedups on\nGPUs without requiring training from scratch. Specifically, all\n$\\texttt{MatMuls}$ among queries, keys, and values are reparameterized using\nadditive kernels, after mapping queries and keys to binary codes in Hamming\nspace. The remaining MLPs or linear layers are then reparameterized with shift\nkernels. We utilize TVM to implement and optimize those customized kernels for\npractical hardware deployment on GPUs. We find that such a reparameterization\non attention maintains model accuracy, while inevitably leading to accuracy\ndrops when being applied to MLPs. To marry the best of both worlds, we further\npropose a new mixture of experts (MoE) framework to reparameterize MLPs by\ntaking multiplication or its primitives as experts, e.g., multiplication and\nshift, and designing a new latency-aware load-balancing loss. Such a loss helps\nto train a generic router for assigning a dynamic amount of input tokens to\ndifferent experts according to their latency. Extensive experiments on various\n2D/3D Transformer-based vision tasks consistently validate the effectiveness of\nour proposed ShiftAddViT, achieving up to $\\textbf{5.18$\\times$}$ latency\nreductions on GPUs and $\\textbf{42.9}$% energy savings, while maintaining a\ncomparable accuracy as original or efficient ViTs.\n","authors":["Haoran You","Huihong Shi","Yipin Guo"," Yingyan"," Lin"],"pdf_url":"https://arxiv.org/pdf/2306.06446v5.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2406.07361v1","updated":"2024-06-11T15:28:48Z","published":"2024-06-11T15:28:48Z","title":"Deep Implicit Optimization for Robust and Flexible Image Registration","summary":"  Deep Learning in Image Registration (DLIR) methods have been tremendously\nsuccessful in image registration due to their speed and ability to incorporate\nweak label supervision at training time. However, DLIR methods forego many of\nthe benefits of classical optimization-based methods. The functional nature of\ndeep networks do not guarantee that the predicted transformation is a local\nminima of the registration objective, the representation of the transformation\n(displacement/velocity field/affine) is fixed, and the networks are not robust\nto domain shift. Our method aims to bridge this gap between classical and\nlearning methods by incorporating optimization as a layer in a deep network. A\ndeep network is trained to predict multi-scale dense feature images that are\nregistered using a black box iterative optimization solver. This optimal warp\nis then used to minimize image and label alignment errors. By implicitly\ndifferentiating end-to-end through an iterative optimization solver, our\nlearned features are registration and label-aware, and the warp functions are\nguaranteed to be local minima of the registration objective in the feature\nspace. Our framework shows excellent performance on in-domain datasets, and is\nagnostic to domain shift such as anisotropy and varying intensity profiles. For\nthe first time, our method allows switching between arbitrary transformation\nrepresentations (free-form to diffeomorphic) at test time with zero retraining.\nEnd-to-end feature learning also facilitates interpretability of features, and\nout-of-the-box promptability using additional label-fidelity terms at\ninference.\n","authors":["Rohit Jena","Pratik Chaudhari","James C. Gee"],"pdf_url":"https://arxiv.org/pdf/2406.07361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07358v1","updated":"2024-06-11T15:26:57Z","published":"2024-06-11T15:26:57Z","title":"AI Sandbagging: Language Models can Strategically Underperform on\n  Evaluations","summary":"  Trustworthy capability evaluations are crucial for ensuring the safety of AI\nsystems, and are becoming a key component of AI regulation. However, the\ndevelopers of an AI system, or the AI system itself, may have incentives for\nevaluations to understate the AI's actual capability. These conflicting\ninterests lead to the problem of sandbagging $\\unicode{x2013}$ which we define\nas \"strategic underperformance on an evaluation\". In this paper we assess\nsandbagging capabilities in contemporary language models (LMs). We prompt\nfrontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on\ndangerous capability evaluations, while maintaining performance on general\n(harmless) capability evaluations. Moreover, we find that models can be\nfine-tuned, on a synthetic dataset, to hide specific capabilities unless given\na password. This behaviour generalizes to high-quality, held-out benchmarks\nsuch as WMDP. In addition, we show that both frontier and smaller models can be\nprompted, or password-locked, to target specific scores on a capability\nevaluation. Even more, we found that a capable password-locked model (Llama 3\n70b) is reasonably able to emulate a less capable model (Llama 2 7b). Overall,\nour results suggest that capability evaluations are vulnerable to sandbagging.\nThis vulnerability decreases the trustworthiness of evaluations, and thereby\nundermines important safety decisions regarding the development and deployment\nof advanced AI systems.\n","authors":["Teun van der Weij","Felix Hofstätter","Ollie Jaffe","Samuel F. Brown","Francis Rhys Ward"],"pdf_url":"https://arxiv.org/pdf/2406.07358v1.pdf","comment":"We publish our code and results\n  $\\href{https://github.com/your-repo/your-project}{here}$"},{"id":"http://arxiv.org/abs/2402.17233v2","updated":"2024-06-11T15:25:01Z","published":"2024-02-27T06:01:56Z","title":"Hybrid$^2$ Neural ODE Causal Modeling and an Application to Glycemic\n  Response","summary":"  Hybrid models composing mechanistic ODE-based dynamics with flexible and\nexpressive neural network components have grown rapidly in popularity,\nespecially in scientific domains where such ODE-based modeling offers important\ninterpretability and validated causal grounding (e.g., for counterfactual\nreasoning). The incorporation of mechanistic models also provides inductive\nbias in standard blackbox modeling approaches, critical when learning from\nsmall datasets or partially observed, complex systems. Unfortunately, as the\nhybrid models become more flexible, the causal grounding provided by the\nmechanistic model can quickly be lost. We address this problem by leveraging\nanother common source of domain knowledge: \\emph{ranking} of treatment effects\nfor a set of interventions, even if the precise treatment effect is unknown. We\nencode this information in a \\emph{causal loss} that we combine with the\nstandard predictive loss to arrive at a \\emph{hybrid loss} that biases our\nlearning towards causally valid hybrid models. We demonstrate our ability to\nachieve a win-win, state-of-the-art predictive performance \\emph{and} causal\nvalidity, in the challenging task of modeling glucose dynamics post-exercise in\nindividuals with type 1 diabetes.\n","authors":["Bob Junyi Zou","Matthew E. Levine","Dessi P. Zaharieva","Ramesh Johari","Emily B. Fox"],"pdf_url":"https://arxiv.org/pdf/2402.17233v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02174v5","updated":"2024-06-11T15:22:07Z","published":"2023-10-03T16:08:41Z","title":"Ask Again, Then Fail: Large Language Models' Vacillations in Judgment","summary":"  We observe that current conversational language models often waver in their\njudgments when faced with follow-up questions, even if the original judgment\nwas correct. This wavering presents a significant challenge for generating\nreliable responses and building user trust. To comprehensively assess this\nissue, we introduce a \\textsc{Follow-up Questioning Mechanism} along with two\nmetrics to quantify this inconsistency, confirming its widespread presence in\ncurrent language models. To mitigate this issue, we explore various prompting\nstrategies for closed-source models; moreover, we develop a training-based\nframework \\textsc{Unwavering-FQ} that teaches language models to maintain their\noriginally correct judgments through synthesized high-quality preference data.\nOur experimental results confirm the effectiveness of our framework and its\nability to enhance the general capabilities of models.\n","authors":["Qiming Xie","Zengzhi Wang","Yi Feng","Rui Xia"],"pdf_url":"https://arxiv.org/pdf/2310.02174v5.pdf","comment":"Accepted by ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2406.07348v1","updated":"2024-06-11T15:15:33Z","published":"2024-06-11T15:15:33Z","title":"DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented\n  Generation for Question-Answering","summary":"  Retrieval-Augmented Generation (RAG) has significantly demonstrated the\nperformance of Large Language Models (LLMs) in the knowledge-intensive tasks,\nsuch as Question-Answering (QA). RAG expands the query context by incorporating\nexternal knowledge bases to enhance the response accuracy. However, it would be\ninefficient to access LLMs multiple times for each query and unreliable to\nretrieve all the relevant documents by a single query. We find that even though\nthere is low relevance between some critical documents and query, it is\npossible to retrieve the remaining documents by combining parts of the\ndocuments with the query. To mine the relevance, a two-stage retrieval\nframework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is\nproposed to improve document retrieval recall and the accuracy of answers while\nmaintaining efficiency. Also, a small classifier is applied to two different\nselection strategies to determine the contribution of the retrieved documents\nto answering the query and retrieve the relatively relevant documents.\nMeanwhile, DR-RAG call the LLMs only once, which significantly improves the\nefficiency of the experiment. The experimental results on multi-hop QA datasets\nshow that DR-RAG can significantly improve the accuracy of the answers and\nachieve new progress in QA systems.\n","authors":["Zijian Hei","Weiling Wei","Wenjie Ou","Juyi Qiao","Junming Jiao","Zhiqing Zhu","Guowen Song"],"pdf_url":"https://arxiv.org/pdf/2406.07348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05981v2","updated":"2024-06-11T15:14:30Z","published":"2024-06-10T02:47:55Z","title":"ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training\n  Multiplication-Less Reparameterization","summary":"  Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM.\n","authors":["Haoran You","Yipin Guo","Yichao Fu","Wei Zhou","Huihong Shi","Xiaofan Zhang","Souvik Kundu","Amir Yazdanbakhsh"," Yingyan"," Lin"],"pdf_url":"https://arxiv.org/pdf/2406.05981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07337v1","updated":"2024-06-11T15:06:15Z","published":"2024-06-11T15:06:15Z","title":"Transferring Knowledge from Large Foundation Models to Small Downstream\n  Models","summary":"  How do we transfer the relevant knowledge from ever larger foundation models\ninto small, task-specific downstream models that can run at much lower costs?\nStandard transfer learning using pre-trained weights as the initialization\ntransfers limited information and commits us to often massive pre-trained\narchitectures. This procedure also precludes combining multiple pre-trained\nmodels that learn complementary information. To address these shortcomings, we\nintroduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT\noperates purely on features, thereby decoupling the choice of the pre-trained\nmodel from the smaller downstream model. Rather than indiscriminately\ncompressing all pre-trained features, AFT adaptively transfers pre-trained\nfeatures that are most useful for performing the downstream task, using a\nsimple regularization that adds minimal overhead. Across multiple vision,\nlanguage, and multi-modal datasets, AFT achieves significantly better\ndownstream performance compared to alternatives with a similar computational\ncost. Furthermore, AFT reliably translates improvement in pre-trained models\ninto improvement in downstream performance, even if the downstream model is\nover $50\\times$ smaller, and can effectively transfer complementary information\nlearned by multiple pre-trained models.\n","authors":["Shikai Qiu","Boran Han","Danielle C. Maddix","Shuai Zhang","Yuyang Wang","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2406.07337v1.pdf","comment":"ICML 2024. Code available at\n  https://github.com/amazon-science/adaptive-feature-transfer"},{"id":"http://arxiv.org/abs/2406.07328v1","updated":"2024-06-11T14:59:29Z","published":"2024-06-11T14:59:29Z","title":"Realistic Data Generation for 6D Pose Estimation of Surgical Instruments","summary":"  Automation in surgical robotics has the potential to improve patient safety\nand surgical efficiency, but it is difficult to achieve due to the need for\nrobust perception algorithms. In particular, 6D pose estimation of surgical\ninstruments is critical to enable the automatic execution of surgical maneuvers\nbased on visual feedback. In recent years, supervised deep learning algorithms\nhave shown increasingly better performance at 6D pose estimation tasks; yet,\ntheir success depends on the availability of large amounts of annotated data.\nIn household and industrial settings, synthetic data, generated with 3D\ncomputer graphics software, has been shown as an alternative to minimize\nannotation costs of 6D pose datasets. However, this strategy does not translate\nwell to surgical domains as commercial graphics software have limited tools to\ngenerate images depicting realistic instrument-tissue interactions. To address\nthese limitations, we propose an improved simulation environment for surgical\nrobotics that enables the automatic generation of large and diverse datasets\nfor 6D pose estimation of surgical instruments. Among the improvements, we\ndeveloped an automated data generation pipeline and an improved surgical scene.\nTo show the applicability of our system, we generated a dataset of 7.5k images\nwith pose annotations of a surgical needle that was used to evaluate a\nstate-of-the-art pose estimation network. The trained model obtained a mean\ntranslational error of 2.59mm on a challenging dataset that presented varying\nlevels of occlusion. These results highlight our pipeline's success in training\nand evaluating novel vision algorithms for surgical robotics applications.\n","authors":["Juan Antonio Barragan","Jintan Zhang","Haoying Zhou","Adnan Munawar","Peter Kazanzides"],"pdf_url":"https://arxiv.org/pdf/2406.07328v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2406.07327v1","updated":"2024-06-11T14:59:24Z","published":"2024-06-11T14:59:24Z","title":"3D-Properties: Identifying Challenges in DPO and Charting a Path Forward","summary":"  Aligning large language models (LLMs) with human preference has recently\ngained tremendous attention, with the canonical yet costly RLHF-PPO and the\nsimple and straightforward Direct Preference Optimization (DPO) as two\nexamples. Despite the efficiency, DPO has rarely be used in the\nstate-of-the-art production-level LLMs, implying its potential pathologies. In\nthis work, we revisit DPO with a comprehensive examination of its empirical\nefficacy and a systematic comparison with RLHF-PPO. We identify the\n\\textbf{3D}-properties of DPO's learning outcomes: the \\textbf{D}rastic drop in\nthe likelihood of rejected responses, the \\textbf{D}egradation into LLM\nunlearning, and the \\textbf{D}ispersion effect on unseen responses through\nexperiments with both a carefully designed toy model and practical LLMs on\ntasks including mathematical problem-solving and instruction following. These\nfindings inherently connect to some observations made by related works and we\nadditionally contribute a plausible theoretical explanation for them.\nAccordingly, we propose easy regularization methods to mitigate the issues\ncaused by \\textbf{3D}-properties, improving the training stability and final\nperformance of DPO. Our contributions also include an investigation into how\nthe distribution of the paired preference data impacts the effectiveness of\nDPO. We hope this work could offer research directions to narrow the gap\nbetween reward-free preference learning methods and reward-based ones.\n","authors":["Yuzi Yan","Yibo Miao","Jialian Li","Yipin Zhang","Jian Xie","Zhijie Deng","Dong Yan"],"pdf_url":"https://arxiv.org/pdf/2406.07327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07325v1","updated":"2024-06-11T14:59:18Z","published":"2024-06-11T14:59:18Z","title":"Beyond Training: Optimizing Reinforcement Learning Based Job Shop\n  Scheduling Through Adaptive Action Sampling","summary":"  Learned construction heuristics for scheduling problems have become\nincreasingly competitive with established solvers and heuristics in recent\nyears. In particular, significant improvements have been observed in solution\napproaches using deep reinforcement learning (DRL). While much attention has\nbeen paid to the design of network architectures and training algorithms to\nachieve state-of-the-art results, little research has investigated the optimal\nuse of trained DRL agents during inference. Our work is based on the hypothesis\nthat, similar to search algorithms, the utilization of trained DRL agents\nshould be dependent on the acceptable computational budget. We propose a simple\nyet effective parameterization, called $\\delta$-sampling that manipulates the\ntrained action vector to bias agent behavior towards exploration or\nexploitation during solution construction. By following this approach, we can\nachieve a more comprehensive coverage of the search space while still\ngenerating an acceptable number of solutions. In addition, we propose an\nalgorithm for obtaining the optimal parameterization for such a given number of\nsolutions and any given trained agent. Experiments extending existing training\nprotocols for job shop scheduling problems with our inference method validate\nour hypothesis and result in the expected improvements of the generated\nsolutions.\n","authors":["Constantin Waubert de Puiseau","Christian Dörpelkus","Jannik Peters","Hasan Tercan","Tobias Meisen"],"pdf_url":"https://arxiv.org/pdf/2406.07325v1.pdf","comment":"Presented Workshop Paper at ICAPS2024"},{"id":"http://arxiv.org/abs/2402.04866v3","updated":"2024-06-11T14:54:45Z","published":"2024-02-01T21:16:40Z","title":"Room Transfer Function Reconstruction Using Complex-valued Neural\n  Networks and Irregularly Distributed Microphones","summary":"  Reconstructing the room transfer functions needed to calculate the complex\nsound field in a room has several important real-world applications. However,\nan unpractical number of microphones is often required. Recently, in addition\nto classical signal processing methods, deep learning techniques have been\napplied to reconstruct the room transfer function starting from a very limited\nset of measurements at scattered points in the room. In this paper, we employ\ncomplex-valued neural networks to estimate room transfer functions in the\nfrequency range of the first room resonances, using a few irregularly\ndistributed microphones. To the best of our knowledge, this is the first time\nthat complex-valued neural networks are used to estimate room transfer\nfunctions. To analyze the benefits of applying complex-valued optimization to\nthe considered task, we compare the proposed technique with a state-of-the-art\nkernel-based signal processing approach for sound field reconstruction, showing\nthat the proposed technique exhibits relevant advantages in terms of phase\naccuracy and overall quality of the reconstructed sound field. For informative\npurposes, we also compare the model with a similarly-structured data-driven\napproach that, however, applies a real-valued neural network to reconstruct\nonly the magnitude of the sound field.\n","authors":["Francesca Ronchini","Luca Comanducci","Mirco Pezzoli","Fabio Antonacci","Augusto Sarti"],"pdf_url":"https://arxiv.org/pdf/2402.04866v3.pdf","comment":"Accepted at EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2406.07314v1","updated":"2024-06-11T14:44:37Z","published":"2024-06-11T14:44:37Z","title":"Rethinking the impact of noisy labels in graph classification: A utility\n  and privacy perspective","summary":"  Graph neural networks based on message-passing mechanisms have achieved\nadvanced results in graph classification tasks. However, their generalization\nperformance degrades when noisy labels are present in the training data. Most\nexisting noisy labeling approaches focus on the visual domain or graph node\nclassification tasks and analyze the impact of noisy labels only from a utility\nperspective. Unlike existing work, in this paper, we measure the effects of\nnoise labels on graph classification from data privacy and model utility\nperspectives. We find that noise labels degrade the model's generalization\nperformance and enhance the ability of membership inference attacks on graph\ndata privacy. To this end, we propose the robust graph neural network approach\nwith noisy labeled graph classification. Specifically, we first accurately\nfilter the noisy samples by high-confidence samples and the first feature\nprincipal component vector of each class. Then, the robust principal component\nvectors and the model output under data augmentation are utilized to achieve\nnoise label correction guided by dual spatial information. Finally, supervised\ngraph contrastive learning is introduced to enhance the embedding quality of\nthe model and protect the privacy of the training graph data. The utility and\nprivacy of the proposed method are validated by comparing twelve different\nmethods on eight real graph classification datasets. Compared with the\nstate-of-the-art methods, the RGLC method achieves at most and at least 7.8%\nand 0.8% performance gain at 30% noisy labeling rate, respectively, and reduces\nthe accuracy of privacy attacks to below 60%.\n","authors":["De Li","Xianxian Li","Zeming Gan","Qiyu Li","Bin Qu","Jinyan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16141v2","updated":"2024-06-11T14:33:23Z","published":"2024-05-25T09:21:43Z","title":"AIGB: Generative Auto-bidding via Diffusion Modeling","summary":"  Auto-bidding plays a crucial role in facilitating online advertising by\nautomatically providing bids for advertisers. Reinforcement learning (RL) has\ngained popularity for auto-bidding. However, most current RL auto-bidding\nmethods are modeled through the Markovian Decision Process (MDP), which assumes\nthe Markovian state transition. This assumption restricts the ability to\nperform in long horizon scenarios and makes the model unstable when dealing\nwith highly random online advertising environments. To tackle this issue, this\npaper introduces AI-Generated Bidding (AIGB), a novel paradigm for auto-bidding\nthrough generative modeling. In this paradigm, we propose DiffBid, a\nconditional diffusion modeling approach for bid generation. DiffBid directly\nmodels the correlation between the return and the entire trajectory,\neffectively avoiding error propagation across time steps in long horizons.\nAdditionally, DiffBid offers a versatile approach for generating trajectories\nthat maximize given targets while adhering to specific constraints. Extensive\nexperiments conducted on the real-world dataset and online A/B test on Alibaba\nadvertising platform demonstrate the effectiveness of DiffBid, achieving 2.81%\nincrease in GMV and 3.36% increase in ROI.\n","authors":["Jiayan Guo","Yusen Huo","Zhilin Zhang","Tianyu Wang","Chuan Yu","Jian Xu","Yan Zhang","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2405.16141v2.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2406.07302v1","updated":"2024-06-11T14:30:34Z","published":"2024-06-11T14:30:34Z","title":"BertaQA: How Much Do Language Models Know About Local Culture?","summary":"  Large Language Models (LLMs) exhibit extensive knowledge about the world, but\nmost evaluations have been limited to global or anglocentric subjects. This\nraises the question of how well these models perform on topics relevant to\nother cultures, whose presence on the web is not that prominent. To address\nthis gap, we introduce BertaQA, a multiple-choice trivia dataset that is\nparallel in English and Basque. The dataset consists of a local subset with\nquestions pertinent to the Basque culture, and a global subset with questions\nof broader interest. We find that state-of-the-art LLMs struggle with local\ncultural knowledge, even as they excel on global topics. However, we show that\ncontinued pre-training in Basque significantly improves the models' performance\non Basque culture, even when queried in English. To our knowledge, this is the\nfirst solid evidence of knowledge transfer from a low-resource to a\nhigh-resource language. Our analysis sheds light on the complex interplay\nbetween language and knowledge, and reveals that some prior findings do not\nfully hold when reassessed on local topics. Our dataset and evaluation code are\navailable under open licenses at https://github.com/juletx/BertaQA.\n","authors":["Julen Etxaniz","Gorka Azkune","Aitor Soroa","Oier Lopez de Lacalle","Mikel Artetxe"],"pdf_url":"https://arxiv.org/pdf/2406.07302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07295v1","updated":"2024-06-11T14:24:00Z","published":"2024-06-11T14:24:00Z","title":"Multi-objective Reinforcement learning from AI Feedback","summary":"  This paper presents Multi-Objective Reinforcement Learning from AI Feedback\n(MORLAIF), a novel approach to improving the alignment and performance of\nlanguage models trained using reinforcement learning from AI feedback (RLAIF).\nIn contrast to standard approaches that train a single preference model to\nrepresent all human preferences, MORLAIF decomposes this task into multiple\nsimpler principles, such as toxicity, factuality, and sycophancy. Separate\npreference models are trained for each principle using feedback from\nGPT-3.5-Turbo. These preference model scores are then combined using different\nscalarization functions to provide a reward signal for Proximal Policy\nOptimization (PPO) training of the target language model. Our experiments\nindicate that MORLAIF outperforms the standard RLAIF baselines and that MORLAIF\ncan be used to align larger language models using smaller ones. Surprisingly,\nthe choice of scalarization function does not appear to significantly impact\nthe results.\n","authors":["Marcus Williams"],"pdf_url":"https://arxiv.org/pdf/2406.07295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07291v1","updated":"2024-06-11T14:22:37Z","published":"2024-06-11T14:22:37Z","title":"Joint Learning of Context and Feedback Embeddings in Spoken Dialogue","summary":"  Short feedback responses, such as backchannels, play an important role in\nspoken dialogue. So far, most of the modeling of feedback responses has focused\non their timing, often neglecting how their lexical and prosodic form influence\ntheir contextual appropriateness and conversational function. In this paper, we\ninvestigate the possibility of embedding short dialogue contexts and feedback\nresponses in the same representation space using a contrastive learning\nobjective. In our evaluation, we primarily focus on how such embeddings can be\nused as a context-feedback appropriateness metric and thus for feedback\nresponse ranking in U.S. English dialogues. Our results show that the model\noutperforms humans given the same ranking task and that the learned embeddings\ncarry information about the conversational function of feedback responses.\n","authors":["Livia Qian","Gabriel Skantze"],"pdf_url":"https://arxiv.org/pdf/2406.07291v1.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2405.12926v2","updated":"2024-06-11T14:22:14Z","published":"2024-05-21T16:51:28Z","title":"Trusting Fair Data: Leveraging Quality in Fairness-Driven Data Removal\n  Techniques","summary":"  In this paper, we deal with bias mitigation techniques that remove specific\ndata points from the training set to aim for a fair representation of the\npopulation in that set. Machine learning models are trained on these\npre-processed datasets, and their predictions are expected to be fair. However,\nsuch approaches may exclude relevant data, making the attained subsets less\ntrustworthy for further usage. To enhance the trustworthiness of prior methods,\nwe propose additional requirements and objectives that the subsets must fulfill\nin addition to fairness: (1) group coverage, and (2) minimal data loss. While\nremoving entire groups may improve the measured fairness, this practice is very\nproblematic as failing to represent every group cannot be considered fair. In\nour second concern, we advocate for the retention of data while minimizing\ndiscrimination. By introducing a multi-objective optimization problem that\nconsiders fairness and data loss, we propose a methodology to find\nPareto-optimal solutions that balance these objectives. By identifying such\nsolutions, users can make informed decisions about the trade-off between\nfairness and data quality and select the most suitable subset for their\napplication.\n","authors":["Manh Khoi Duong","Stefan Conrad"],"pdf_url":"https://arxiv.org/pdf/2405.12926v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10553v2","updated":"2024-06-11T14:10:46Z","published":"2023-04-20T09:44:24Z","title":"Sparsity in neural networks can improve their privacy","summary":"  This article measures how sparsity can make neural networks more robust to\nmembership inference attacks. The obtained empirical results show that sparsity\nimproves the privacy of the network, while preserving comparable performances\non the task at hand. This empirical study completes and extends existing\nliterature.\n","authors":["Antoine Gonon","Léon Zheng","Clément Lalanne","Quoc-Tung Le","Guillaume Lauga","Can Pouliquen"],"pdf_url":"https://arxiv.org/pdf/2304.10553v2.pdf","comment":"arXiv admin note: duplicate of arXiv:2304.07234"},{"id":"http://arxiv.org/abs/2310.14838v2","updated":"2024-06-11T14:07:17Z","published":"2023-10-23T11:58:01Z","title":"Calibration of Time-Series Forecasting: Detecting and Adapting\n  Context-Driven Distribution Shift","summary":"  Recent years have witnessed the success of introducing deep learning models\nto time series forecasting. From a data generation perspective, we illustrate\nthat existing models are susceptible to distribution shifts driven by temporal\ncontexts, whether observed or unobserved. Such context-driven distribution\nshift (CDS) introduces biases in predictions within specific contexts and poses\nchallenges for conventional training paradigms. In this paper, we introduce a\nuniversal calibration methodology for the detection and adaptation of CDS with\na trained model. To this end, we propose a novel CDS detector, termed the\n\"residual-based CDS detector\" or \"Reconditionor\", which quantifies the model's\nvulnerability to CDS by evaluating the mutual information between prediction\nresiduals and their corresponding contexts. A high Reconditionor score\nindicates a severe susceptibility, thereby necessitating model adaptation. In\nthis circumstance, we put forth a straightforward yet potent adapter framework\nfor model calibration, termed the \"sample-level contextualized adapter\" or\n\"SOLID\". This framework involves the curation of a contextually similar dataset\nto the provided test sample and the subsequent fine-tuning of the model's\nprediction layer with a limited number of steps. Our theoretical analysis\ndemonstrates that this adaptation strategy can achieve an optimal bias-variance\ntrade-off. Notably, our proposed Reconditionor and SOLID are model-agnostic and\nreadily adaptable to a wide range of models. Extensive experiments show that\nSOLID consistently enhances the performance of current forecasting models on\nreal-world datasets, especially on cases with substantial CDS detected by the\nproposed Reconditionor, thus validating the effectiveness of the calibration\napproach.\n","authors":["Mouxiang Chen","Lefei Shen","Han Fu","Zhuo Li","Jianling Sun","Chenghao Liu"],"pdf_url":"https://arxiv.org/pdf/2310.14838v2.pdf","comment":"KDD'24 research paper"},{"id":"http://arxiv.org/abs/2402.14730v2","updated":"2024-06-11T14:05:40Z","published":"2024-02-22T17:42:15Z","title":"Clifford-Steerable Convolutional Neural Networks","summary":"  We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a\nnovel class of $\\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector\nfields on pseudo-Euclidean spaces $\\mathbb{R}^{p,q}$. They cover, for instance,\n$\\mathrm{E}(3)$-equivariance on $\\mathbb{R}^3$ and Poincar\\'e-equivariance on\nMinkowski spacetime $\\mathbb{R}^{1,3}$. Our approach is based on an implicit\nparametrization of $\\mathrm{O}(p,q)$-steerable kernels via Clifford group\nequivariant neural networks. We significantly and consistently outperform\nbaseline methods on fluid dynamics as well as relativistic electrodynamics\nforecasting tasks.\n","authors":["Maksim Zhdanov","David Ruhe","Maurice Weiler","Ana Lucic","Johannes Brandstetter","Patrick Forré"],"pdf_url":"https://arxiv.org/pdf/2402.14730v2.pdf","comment":"accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2401.04071v3","updated":"2024-06-11T14:01:18Z","published":"2024-01-08T18:18:02Z","title":"Fun with Flags: Robust Principal Directions via Flag Manifolds","summary":"  Principal component analysis (PCA), along with its extensions to manifolds\nand outlier contaminated data, have been indispensable in computer vision and\nmachine learning. In this work, we present a unifying formalism for PCA and its\nvariants, and introduce a framework based on the flags of linear subspaces, ie\na hierarchy of nested linear subspaces of increasing dimension, which not only\nallows for a common implementation but also yields novel variants, not explored\npreviously. We begin by generalizing traditional PCA methods that either\nmaximize variance or minimize reconstruction error. We expand these\ninterpretations to develop a wide array of new dimensionality reduction\nalgorithms by accounting for outliers and the data manifold. To devise a common\ncomputational approach, we recast robust and dual forms of PCA as optimization\nproblems on flag manifolds. We then integrate tangent space approximations of\nprincipal geodesic analysis (tangent-PCA) into this flag-based framework,\ncreating novel robust and dual geodesic PCA variations. The remarkable\nflexibility offered by the 'flagification' introduced here enables even more\nalgorithmic variants identified by specific flag types. Last but not least, we\npropose an effective convergent solver for these flag-formulations employing\nthe Stiefel manifold. Our empirical results on both real-world and synthetic\nscenarios, demonstrate the superiority of our novel algorithms, especially in\nterms of robustness to outliers on manifolds.\n","authors":["Nathan Mankovich","Gustau Camps-Valls","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2401.04071v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06701v2","updated":"2024-06-11T13:54:55Z","published":"2023-07-13T11:58:27Z","title":"S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized\n  Variational Autoencoder for Video Prediction","summary":"  We address the video prediction task by putting forth a novel model that\ncombines (i) our recently proposed hierarchical residual vector quantized\nvariational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN\n(ST-PixelCNN). We refer to this approach as a sequential hierarchical residual\nlearning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging\nthe intrinsic capabilities of HR-VQVAE at modeling still images with a\nparsimonious representation, combined with the ST-PixelCNN's ability at\nhandling spatiotemporal information, S-HR-VQVAE can better deal with chief\nchallenges in video prediction. These include learning spatiotemporal\ninformation, handling high dimensional data, combating blurry prediction, and\nimplicit modeling of physical characteristics. Extensive experimental results\non the KTH Human Action and Moving-MNIST tasks demonstrate that our model\ncompares favorably against top video prediction techniques both in quantitative\nand qualitative evaluations despite a much smaller model size. Finally, we\nboost S-HR-VQVAE by proposing a novel training method to jointly estimate the\nHR-VQVAE and ST-PixelCNN parameters.\n","authors":["Mohammad Adiban","Kalin Stefanov","Sabato Marco Siniscalchi","Giampiero Salvi"],"pdf_url":"https://arxiv.org/pdf/2307.06701v2.pdf","comment":"14 pages, 7 figures, 3 tables. Submitted to IEEE Transactions on\n  Pattern Analysis and Machine Intelligence on 2023-07-12"},{"id":"http://arxiv.org/abs/2406.07266v1","updated":"2024-06-11T13:51:51Z","published":"2024-06-11T13:51:51Z","title":"Efficient 3D Molecular Generation with Flow Matching and Scale Optimal\n  Transport","summary":"  Generative models for 3D drug design have gained prominence recently for\ntheir potential to design ligands directly within protein pockets. Current\napproaches, however, often suffer from very slow sampling times or generate\nmolecules with poor chemical validity. Addressing these limitations, we propose\nSemla, a scalable E(3)-equivariant message passing architecture. We further\nintroduce a molecular generation model, MolFlow, which is trained using flow\nmatching along with scale optimal transport, a novel extension of equivariant\noptimal transport. Our model produces state-of-the-art results on benchmark\ndatasets with just 100 sampling steps. Crucially, MolFlow samples high quality\nmolecules with as few as 20 steps, corresponding to a two order-of-magnitude\nspeed-up compared to state-of-the-art, without sacrificing performance.\nFurthermore, we highlight limitations of current evaluation methods for 3D\ngeneration and propose new benchmark metrics for unconditional molecular\ngenerators. Finally, using these new metrics, we compare our model's ability to\ngenerate high quality samples against current approaches and further\ndemonstrate MolFlow's strong performance.\n","authors":["Ross Irwin","Alessandro Tibo","Jon-Paul Janet","Simon Olsson"],"pdf_url":"https://arxiv.org/pdf/2406.07266v1.pdf","comment":"Preprint. Code to be released upon full publication"},{"id":"http://arxiv.org/abs/2406.07263v1","updated":"2024-06-11T13:42:49Z","published":"2024-06-11T13:42:49Z","title":"Active learning for affinity prediction of antibodies","summary":"  The primary objective of most lead optimization campaigns is to enhance the\nbinding affinity of ligands. For large molecules such as antibodies,\nidentifying mutations that enhance antibody affinity is particularly\nchallenging due to the combinatorial explosion of potential mutations. When the\nstructure of the antibody-antigen complex is available, relative binding free\nenergy (RBFE) methods can offer valuable insights into how different mutations\nwill impact the potency and selectivity of a drug candidate, thereby reducing\nthe reliance on costly and time-consuming wet-lab experiments. However,\naccurately simulating the physics of large molecules is computationally\nintensive. We present an active learning framework that iteratively proposes\npromising sequences for simulators to evaluate, thereby accelerating the search\nfor improved binders. We explore different modeling approaches to identify the\nmost effective surrogate model for this task, and evaluate our framework both\nusing pre-computed pools of data and in a realistic full-loop setting.\n","authors":["Alexandra Gessner","Sebastian W. Ober","Owen Vickery","Dino Oglić","Talip Uçar"],"pdf_url":"https://arxiv.org/pdf/2406.07263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07259v1","updated":"2024-06-11T13:39:07Z","published":"2024-06-11T13:39:07Z","title":"Scientific Computing with Large Language Models","summary":"  We provide an overview of the emergence of large language models for\nscientific computing applications. We highlight use cases that involve natural\nlanguage processing of scientific documents and specialized languages designed\nto describe physical systems. For the former, chatbot style applications appear\nin medicine, mathematics and physics and can be used iteratively with domain\nexperts for problem solving. We also review specialized languages within\nmolecular biology, the languages of molecules, proteins, and DNA where language\nmodels are being used to predict properties and even create novel physical\nsystems at much faster rates than traditional computing methods.\n","authors":["Christopher Culver","Peter Hicks","Mihailo Milenkovic","Sanjif Shanmugavelu","Tobias Becker"],"pdf_url":"https://arxiv.org/pdf/2406.07259v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2402.18512v2","updated":"2024-06-11T13:35:52Z","published":"2024-02-28T17:40:05Z","title":"Log Neural Controlled Differential Equations: The Lie Brackets Make a\n  Difference","summary":"  The vector field of a controlled differential equation (CDE) describes the\nrelationship between a control path and the evolution of a solution path.\nNeural CDEs (NCDEs) treat time series data as observations from a control path,\nparameterise a CDE's vector field using a neural network, and use the solution\npath as a continuously evolving hidden state. As their formulation makes them\nrobust to irregular sampling rates, NCDEs are a powerful approach for modelling\nreal-world data. Building on neural rough differential equations (NRDEs), we\nintroduce Log-NCDEs, a novel, effective, and efficient method for training\nNCDEs. The core component of Log-NCDEs is the Log-ODE method, a tool from the\nstudy of rough paths for approximating a CDE's solution. Log-NCDEs are shown to\noutperform NCDEs, NRDEs, the linear recurrent unit, S5, and MAMBA on a range of\nmultivariate time series datasets with up to $50{,}000$ observations.\n","authors":["Benjamin Walker","Andrew D. McLeod","Tiexin Qin","Yichuan Cheng","Haoliang Li","Terry Lyons"],"pdf_url":"https://arxiv.org/pdf/2402.18512v2.pdf","comment":"23 pages, 5 figures, International Conference on Machine Learning\n  2024"},{"id":"http://arxiv.org/abs/2406.07253v1","updated":"2024-06-11T13:34:05Z","published":"2024-06-11T13:34:05Z","title":"Hybrid Reinforcement Learning from Offline Observation Alone","summary":"  We consider the hybrid reinforcement learning setting where the agent has\naccess to both offline data and online interactive access. While Reinforcement\nLearning (RL) research typically assumes offline data contains complete action,\nreward and transition information, datasets with only state information (also\nknown as observation-only datasets) are more general, abundant and practical.\nThis motivates our study of the hybrid RL with observation-only offline dataset\nframework. While the task of competing with the best policy \"covered\" by the\noffline data can be solved if a reset model of the environment is provided\n(i.e., one that can be reset to any state), we show evidence of hardness when\nonly given the weaker trace model (i.e., one can only reset to the initial\nstates and must produce full traces through the environment), without further\nassumption of admissibility of the offline data. Under the admissibility\nassumptions -- that the offline data could actually be produced by the policy\nclass we consider -- we propose the first algorithm in the trace model setting\nthat provably matches the performance of algorithms that leverage a reset\nmodel. We also perform proof-of-concept experiments that suggest the\neffectiveness of our algorithm in practice.\n","authors":["Yuda Song","J. Andrew Bagnell","Aarti Singh"],"pdf_url":"https://arxiv.org/pdf/2406.07253v1.pdf","comment":"34 pages, 7 figures, published at ICML 2024"},{"id":"http://arxiv.org/abs/2406.04825v2","updated":"2024-06-11T13:33:16Z","published":"2024-06-07T10:50:03Z","title":"Graph Mining under Data scarcity","summary":"  Multitude of deep learning models have been proposed for node classification\nin graphs. However, they tend to perform poorly under labeled-data scarcity.\nAlthough Few-shot learning for graphs has been introduced to overcome this\nproblem, the existing models are not easily adaptable for generic graph\nlearning frameworks like Graph Neural Networks (GNNs). Our work proposes an\nUncertainty Estimator framework that can be applied on top of any generic GNN\nbackbone network (which are typically designed for supervised/semi-supervised\nnode classification) to improve the node classification performance. A neural\nnetwork is used to model the Uncertainty Estimator as a probability\ndistribution rather than probabilistic discrete scalar values. We train these\nmodels under the classic episodic learning paradigm in the $n$-way, $k$-shot\nfashion, in an end-to-end setting. Our work demonstrates that implementation of\nthe uncertainty estimator on a GNN backbone network improves the classification\naccuracy under Few-shot setting without any meta-learning specific\narchitecture. We conduct experiments on multiple datasets under different\nFew-shot settings and different GNN-based backbone networks. Our method\noutperforms the baselines, which demonstrates the efficacy of the Uncertainty\nEstimator for Few-shot node classification on graphs with a GNN.\n","authors":["Appan Rakaraddi","Lam Siew-Kei","Mahardhika Pratama","Marcus de Carvalho"],"pdf_url":"https://arxiv.org/pdf/2406.04825v2.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.07250v1","updated":"2024-06-11T13:32:40Z","published":"2024-06-11T13:32:40Z","title":"Description and Discussion on DCASE 2024 Challenge Task 2: First-Shot\n  Unsupervised Anomalous Sound Detection for Machine Condition Monitoring","summary":"  We present the task description of the Detection and Classification of\nAcoustic Scenes and Events (DCASE) 2024 Challenge Task 2: First-shot\nunsupervised anomalous sound detection (ASD) for machine condition monitoring.\nContinuing from last year's DCASE 2023 Challenge Task 2, we organize the task\nas a first-shot problem under domain generalization required settings. The main\ngoal of the first-shot problem is to enable rapid deployment of ASD systems for\nnew kinds of machines without the need for machine-specific hyperparameter\ntunings. This problem setting was realized by (1) giving only one section for\neach machine type and (2) having completely different machine types for the\ndevelopment and evaluation datasets. For the DCASE 2024 Challenge Task 2, data\nof completely new machine types were newly collected and provided as the\nevaluation dataset. In addition, attribute information such as the machine\noperation conditions were concealed for several machine types to mimic\nsituations where such information are unavailable. We will add challenge\nresults and analysis of the submissions after the challenge submission\ndeadline.\n","authors":["Tomoya Nishida","Noboru Harada","Daisuke Niizumi","Davide Albertini","Roberto Sannino","Simone Pradolini","Filippo Augusti","Keisuke Imoto","Kota Dohi","Harsh Purohit","Takashi Endo","Yohei Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2406.07250v1.pdf","comment":"anomaly detection, acoustic condition monitoring, domain shift,\n  first-shot problem, DCASE Challenge. arXiv admin note: text overlap with\n  arXiv:2305.07828"},{"id":"http://arxiv.org/abs/2406.07247v1","updated":"2024-06-11T13:29:34Z","published":"2024-06-11T13:29:34Z","title":"Dynamical Mean-Field Theory of Self-Attention Neural Networks","summary":"  Transformer-based models have demonstrated exceptional performance across\ndiverse domains, becoming the state-of-the-art solution for addressing\nsequential machine learning problems. Even though we have a general\nunderstanding of the fundamental components in the transformer architecture,\nlittle is known about how they operate or what are their expected dynamics.\nRecently, there has been an increasing interest in exploring the relationship\nbetween attention mechanisms and Hopfield networks, promising to shed light on\nthe statistical physics of transformer networks. However, to date, the\ndynamical regimes of transformer-like models have not been studied in depth. In\nthis paper, we address this gap by using methods for the study of asymmetric\nHopfield networks in nonequilibrium regimes --namely path integral methods over\ngenerating functionals, yielding dynamics governed by concurrent mean-field\nvariables. Assuming 1-bit tokens and weights, we derive analytical\napproximations for the behavior of large self-attention neural networks coupled\nto a softmax output, which become exact in the large limit size. Our findings\nreveal nontrivial dynamical phenomena, including nonequilibrium phase\ntransitions associated with chaotic bifurcations, even for very simple\nconfigurations with a few encoded features and a very short context window.\nFinally, we discuss the potential of our analytic approach to improve our\nunderstanding of the inner workings of transformer models, potentially reducing\ncomputational training costs and enhancing model interpretability.\n","authors":["Ángel Poc-López","Miguel Aguilera"],"pdf_url":"https://arxiv.org/pdf/2406.07247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07246v1","updated":"2024-06-11T13:28:43Z","published":"2024-06-11T13:28:43Z","title":"Marginalization Consistent Mixture of Separable Flows for Probabilistic\n  Irregular Time Series Forecasting","summary":"  Probabilistic forecasting models for joint distributions of targets in\nirregular time series are a heavily under-researched area in machine learning\nwith, to the best of our knowledge, only three models researched so far: GPR,\nthe Gaussian Process Regression model~\\citep{Durichen2015.Multitask}, TACTiS,\nthe Transformer-Attentional Copulas for Time Series~\\cite{Drouin2022.Tactis,\nashok2024tactis} and ProFITi \\citep{Yalavarthi2024.Probabilistica}, a\nmultivariate normalizing flow model based on invertible attention layers. While\nProFITi, thanks to using multivariate normalizing flows, is the more expressive\nmodel with better predictive performance, we will show that it suffers from\nmarginalization inconsistency: it does not guarantee that the marginal\ndistributions of a subset of variables in its predictive distributions coincide\nwith the directly predicted distributions of these variables. Also, TACTiS does\nnot provide any guarantees for marginalization consistency. We develop a novel\nprobabilistic irregular time series forecasting model, Marginalization\nConsistent Mixtures of Separable Flows (moses), that mixes several normalizing\nflows with (i) Gaussian Processes with full covariance matrix as source\ndistributions and (ii) a separable invertible transformation, aiming to combine\nthe expressivity of normalizing flows with the marginalization consistency of\nGaussians. In experiments on four different datasets we show that moses\noutperforms other state-of-the-art marginalization consistent models, performs\non par with ProFITi, but different from ProFITi, guarantee marginalization\nconsistency.\n","authors":["Vijaya Krishna Yalavarthi","Randolf Scholz","Kiran Madhusudhanan","Stefan Born","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2406.07246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18886v3","updated":"2024-06-11T13:25:53Z","published":"2024-04-29T17:19:40Z","title":"A Survey on Diffusion Models for Time Series and Spatio-Temporal Data","summary":"  The study of time series is crucial for understanding trends and anomalies\nover time, enabling predictive insights across various sectors. Spatio-temporal\ndata, on the other hand, is vital for analyzing phenomena in both space and\ntime, providing a dynamic perspective on complex system interactions. Recently,\ndiffusion models have seen widespread application in time series and\nspatio-temporal data mining. Not only do they enhance the generative and\ninferential capabilities for sequential and temporal data, but they also extend\nto other downstream tasks. In this survey, we comprehensively and thoroughly\nreview the use of diffusion models in time series and spatio-temporal data,\ncategorizing them by model category, task type, data modality, and practical\napplication domain. In detail, we categorize diffusion models into\nunconditioned and conditioned types and discuss time series and spatio-temporal\ndata separately. Unconditioned models, which operate unsupervised, are\nsubdivided into probability-based and score-based models, serving predictive\nand generative tasks such as forecasting, anomaly detection, classification,\nand imputation. Conditioned models, on the other hand, utilize extra\ninformation to enhance performance and are similarly divided for both\npredictive and generative tasks. Our survey extensively covers their\napplication in various fields, including healthcare, recommendation, climate,\nenergy, audio, and transportation, providing a foundational understanding of\nhow these models analyze and generate data. Through this structured overview,\nwe aim to provide researchers and practitioners with a comprehensive\nunderstanding of diffusion models for time series and spatio-temporal data\nanalysis, aiming to direct future innovations and applications by addressing\ntraditional challenges and exploring innovative solutions within the diffusion\nmodel framework.\n","authors":["Yiyuan Yang","Ming Jin","Haomin Wen","Chaoli Zhang","Yuxuan Liang","Lintao Ma","Yi Wang","Chenghao Liu","Bin Yang","Zenglin Xu","Jiang Bian","Shirui Pan","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2404.18886v3.pdf","comment":"Ongoing work & Under review; 27 pages, 8 figures, 2 tables; Github\n  Repo:\n  https://github.com/yyysjz1997/Awesome-TimeSeries-SpatioTemporal-Diffusion-Model"},{"id":"http://arxiv.org/abs/2406.03229v2","updated":"2024-06-11T13:22:47Z","published":"2024-06-05T13:06:17Z","title":"Global Clipper: Enhancing Safety and Reliability of Transformer-based\n  Object Detection Models","summary":"  As transformer-based object detection models progress, their impact in\ncritical sectors like autonomous vehicles and aviation is expected to grow.\nSoft errors causing bit flips during inference have significantly impacted DNN\nperformance, altering predictions. Traditional range restriction solutions for\nCNNs fall short for transformers. This study introduces the Global Clipper and\nGlobal Hybrid Clipper, effective mitigation strategies specifically designed\nfor transformer-based models. It significantly enhances their resilience to\nsoft errors and reduces faulty inferences to ~ 0\\%. We also detail extensive\ntesting across over 64 scenarios involving two transformer models (DINO-DETR\nand Lite-DETR) and two CNN models (YOLOv3 and SSD) using three datasets,\ntotalling approximately 3.3 million inferences, to assess model robustness\ncomprehensively. Moreover, the paper explores unique aspects of attention\nblocks in transformers and their operational differences from CNNs.\n","authors":["Qutub Syed Sha","Michael Paulitsch","Karthik Pattabiraman","Korbinian Hagn","Fabian Oboril","Cornelius Buerkle","Kay-Ulrich Scholl","Gereon Hinz","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2406.03229v2.pdf","comment":"Accepted at IJCAI-AISafety'24 Workshop"},{"id":"http://arxiv.org/abs/2406.03679v2","updated":"2024-06-11T13:19:38Z","published":"2024-06-06T01:49:29Z","title":"On the Effects of Data Scale on Computer Control Agents","summary":"  Autonomous agents that control computer interfaces to accomplish human tasks\nare emerging. Leveraging LLMs to power such agents has been of special\ninterest, but unless fine-tuned on human-collected task demonstrations,\nperformance is still relatively low. In this work we study whether fine-tuning\nalone is a viable approach for building real-world computer control agents. In\nparticularly, we investigate how performance measured on both high and\nlow-level tasks in domain and out of domain scales as more training data is\ncollected. To this end we collect and release a new dataset, AndroidControl,\nconsisting of 15,283 demonstrations of everyday tasks with Android apps.\nCompared to existing datasets, each AndroidControl task instance includes both\nhigh and low-level human-generated instructions, allowing us to explore the\nlevel of task complexity an agent can handle. Moreover, AndroidControl is the\nmost diverse computer control dataset to date, including 15,283 unique tasks\nover 833 Android apps, thus allowing us to conduct in-depth analysis of the\nmodel performance in and out of the domain of the training data. Using the\ndataset, we find that when tested in domain fine-tuned models outperform zero\nand few-shot baselines and scale in such a way that robust performance might\nfeasibly be obtained simply by collecting more data. Out of domain, performance\nscales significantly more slowly and suggests that in particular for high-level\ntasks, fine-tuning on more data alone may be insufficient for achieving robust\nout-of-domain performance.\n","authors":["Wei Li","William Bishop","Alice Li","Chris Rawles","Folawiyo Campbell-Ajala","Divya Tyamagundlu","Oriana Riva"],"pdf_url":"https://arxiv.org/pdf/2406.03679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00046v2","updated":"2024-06-11T13:18:14Z","published":"2024-05-28T13:09:22Z","title":"Hate Speech Detection with Generalizable Target-aware Fairness","summary":"  To counter the side effect brought by the proliferation of social media\nplatforms, hate speech detection (HSD) plays a vital role in halting the\ndissemination of toxic online posts at an early stage. However, given the\nubiquitous topical communities on social media, a trained HSD classifier easily\nbecomes biased towards specific targeted groups (e.g., female and black\npeople), where a high rate of false positive/negative results can significantly\nimpair public trust in the fairness of content moderation mechanisms, and\neventually harm the diversity of online society. Although existing\nfairness-aware HSD methods can smooth out some discrepancies across targeted\ngroups, they are mostly specific to a narrow selection of targets that are\nassumed to be known and fixed. This inevitably prevents those methods from\ngeneralizing to real-world use cases where new targeted groups constantly\nemerge over time. To tackle this defect, we propose Generalizable target-aware\nFairness (GetFair), a new method for fairly classifying each post that contains\ndiverse and even unseen targets during inference. To remove the HSD\nclassifier's spurious dependence on target-related features, GetFair trains a\nseries of filter functions in an adversarial pipeline, so as to deceive the\ndiscriminator that recovers the targeted group from filtered post embeddings.\nTo maintain scalability and generalizability, we innovatively parameterize all\nfilter functions via a hypernetwork that is regularized by the semantic\naffinity among targets. Taking a target's pretrained word embedding as input,\nthe hypernetwork generates the weights used by each target-specific filter\non-the-fly without storing dedicated filter parameters. Finally, comparative\nexperiments on two HSD datasets have shown advantageous performance of GetFair\non out-of-sample targets.\n","authors":["Tong Chen","Danny Wang","Xurong Liang","Marten Risius","Gianluca Demartini","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2406.00046v2.pdf","comment":"To appear in KDD 2024"},{"id":"http://arxiv.org/abs/2402.05894v4","updated":"2024-06-11T13:17:12Z","published":"2024-02-08T18:33:21Z","title":"Large Language Model Meets Graph Neural Network in Knowledge\n  Distillation","summary":"  In service-oriented architectures, accurately predicting the Quality of\nService (QoS) is crucial for maintaining reliability and enhancing user\nsatisfaction. However, significant challenges remain due to existing methods\nalways overlooking high-order latent collaborative relationships between users\nand services and failing to dynamically adjust feature learning for every\nspecific user-service invocation, which are critical for learning accurate\nfeatures. Additionally, reliance on RNNs for capturing QoS evolution hampers\nmodels' ability to detect long-term trends due to difficulties in managing\nlong-range dependencies. To address these challenges, we propose the\n\\underline{T}arget-Prompt \\underline{O}nline \\underline{G}raph\n\\underline{C}ollaborative \\underline{L}earning (TOGCL) framework for\ntemporal-aware QoS prediction. TOGCL leverages a dynamic user-service\ninvocation graph to model historical interactions, providing a comprehensive\nrepresentation of user-service relationships. Building on this graph, it\ndevelops a target-prompt graph attention network to extract online deep latent\nfeatures of users and services at each time slice, simultaneously considering\nimplicit collaborative relationships between target users/services and their\nneighbors, as well as relevant historical QoS values. Additionally, a\nmulti-layer Transformer encoder is employed to uncover temporal feature\nevolution patterns of users and services, leading to temporal-aware QoS\nprediction. Extensive experiments conducted on the WS-DREAM dataset demonstrate\nthat our proposed TOGCL framework significantly outperforms state-of-the-art\nmethods across multiple metrics, achieving improvements of up to 38.80\\%. These\nresults underscore the effectiveness of the TOGCL framework for precise\ntemporal QoS prediction.\n","authors":["Shengxiang Hu","Guobing Zou","Song Yang","Yanglan Gan","Bofeng Zhang","Yixin Chen"],"pdf_url":"https://arxiv.org/pdf/2402.05894v4.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2406.07236v1","updated":"2024-06-11T13:14:04Z","published":"2024-06-11T13:14:04Z","title":"Let Go of Your Labels with Unsupervised Transfer","summary":"  Foundation vision-language models have enabled remarkable zero-shot\ntransferability of the pre-trained representations to a wide range of\ndownstream tasks. However, to solve a new task, zero-shot transfer still\nnecessitates human guidance to define visual categories that appear in the\ndata. Here, we show that fully unsupervised transfer emerges when searching for\nthe labeling of a dataset that induces maximal margin classifiers in\nrepresentation spaces of different foundation models. We present TURTLE, a\nfully unsupervised method that effectively employs this guiding principle to\nuncover the underlying labeling of a downstream dataset without any supervision\nand task-specific representation learning. We evaluate TURTLE on a diverse\nbenchmark suite of 26 datasets and show that it achieves new state-of-the-art\nunsupervised performance. Furthermore, TURTLE, although being fully\nunsupervised, outperforms zero-shot transfer baselines on a wide range of\ndatasets. In particular, TURTLE matches the average performance of CLIP\nzero-shot on 26 datasets by employing the same representation space, spanning a\nwide range of architectures and model sizes. By guiding the search for the\nunderlying labeling using the representation spaces of two foundation models,\nTURTLE surpasses zero-shot transfer and unsupervised prompt tuning baselines,\ndemonstrating the surprising power and effectiveness of unsupervised transfer.\n","authors":["Artyom Gadetsky","Yulun Jiang","Maria Brbic"],"pdf_url":"https://arxiv.org/pdf/2406.07236v1.pdf","comment":"ICML 2024 camera-ready"},{"id":"http://arxiv.org/abs/2406.07234v1","updated":"2024-06-11T13:12:39Z","published":"2024-06-11T13:12:39Z","title":"OPFData: Large-scale datasets for AC optimal power flow with topological\n  perturbations","summary":"  Solving the AC optimal power flow problem (AC-OPF) is critical to the\nefficient and safe planning and operation of power grids. Small efficiency\nimprovements in this domain have the potential to lead to billions of dollars\nof cost savings, and significant reductions in emissions from fossil fuel\ngenerators. Recent work on data-driven solution methods for AC-OPF shows the\npotential for large speed improvements compared to traditional solvers;\nhowever, no large-scale open datasets for this problem exist. We present the\nlargest readily-available collection of solved AC-OPF problems to date. This\ncollection is orders of magnitude larger than existing readily-available\ndatasets, allowing training of high-capacity data-driven models. Uniquely, it\nincludes topological perturbations - a critical requirement for usage in\nrealistic power grid operations. We hope this resource will spur the community\nto scale research to larger grid sizes with variable topology.\n","authors":["Sean Lovett","Miha Zgubic","Sofia Liguori","Sephora Madjiheurem","Hamish Tomlinson","Sophie Elster","Chris Apps","Sims Witherspoon","Luis Piloto"],"pdf_url":"https://arxiv.org/pdf/2406.07234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17207v4","updated":"2024-06-11T13:06:15Z","published":"2023-09-29T12:59:28Z","title":"Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of\n  Agents","summary":"  Memory Gym presents a suite of 2D partially observable environments, namely\nMortar Mayhem, Mystery Path, and Searing Spotlights, designed to benchmark\nmemory capabilities in decision-making agents. These environments, originally\nwith finite tasks, are expanded into innovative, endless formats, mirroring the\nescalating challenges of cumulative memory games such as ``I packed my bag''.\nThis progression in task design shifts the focus from merely assessing sample\nefficiency to also probing the levels of memory effectiveness in dynamic,\nprolonged scenarios. To address the gap in available memory-based Deep\nReinforcement Learning baselines, we introduce an implementation that\nintegrates Transformer-XL (TrXL) with Proximal Policy Optimization. This\napproach utilizes TrXL as a form of episodic memory, employing a sliding window\ntechnique. Our comparative study between the Gated Recurrent Unit (GRU) and\nTrXL reveals varied performances across different settings. TrXL, on the finite\nenvironments, demonstrates superior sample efficiency in Mystery Path and\noutperforms in Mortar Mayhem. However, GRU is more efficient on Searing\nSpotlights. Most notably, in all endless tasks, GRU makes a remarkable\nresurgence, consistently outperforming TrXL by significant margins. Website and\nSource Code: https://github.com/MarcoMeter/endless-memory-gym/\n","authors":["Marco Pleines","Matthias Pallasch","Frank Zimmer","Mike Preuss"],"pdf_url":"https://arxiv.org/pdf/2309.17207v4.pdf","comment":"40 pages, 12 figures, 7 tables, under review"},{"id":"http://arxiv.org/abs/2406.07222v1","updated":"2024-06-11T13:01:50Z","published":"2024-06-11T13:01:50Z","title":"Improving Autoformalization using Type Checking","summary":"  Large language models show promise for autoformalization, the task of\nautomatically translating natural language into formal languages. However,\ncurrent autoformalization methods remain limited. The last reported\nstate-of-the-art performance on the ProofNet formalization benchmark for the\nLean proof assistant, achieved using Codex for Lean 3, only showed successful\nformalization of 16.1% of informal statements. Similarly, our evaluation of\nGPT-4o for Lean 4 only produces successful translations 34.9% of the time. Our\nanalysis shows that the performance of these models is largely limited by their\ninability to generate formal statements that successfully type-check (i.e., are\nsyntactically correct and consistent with types) - with a whopping 86.6% of\nGPT-4o errors starting from a type-check failure. In this work, we propose a\nmethod to fix this issue through decoding with type-check filtering, where we\ninitially sample a diverse set of candidate formalizations for an informal\nstatement, then use the Lean proof assistant to filter out candidates that do\nnot type-check. Using GPT-4o as a base model, and combining our method with\nself-consistency, we obtain a +18.3% absolute increase in formalization\naccuracy, and achieve a new state-of-the-art of 53.2% on ProofNet with Lean 4.\n","authors":["Auguste Poiroux","Gail Weiss","Viktor Kunčak","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2406.07222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02872v2","updated":"2024-06-11T12:58:51Z","published":"2024-02-05T10:39:32Z","title":"How do Large Language Models Learn In-Context? Query and Key Matrices of\n  In-Context Heads are Two Towers for Metric Learning","summary":"  We investigate the mechanism of in-context learning (ICL) on sentence\nclassification tasks with semantically-unrelated labels (\"foo\"/\"bar\"). We find\nintervening in only 1\\% heads (named \"in-context heads\") significantly affects\nICL accuracy from 87.6\\% to 24.4\\%. To understand this phenomenon, we analyze\nthe value-output vectors in these heads and discover that the vectors at each\nlabel position contain substantial information about the corresponding labels.\nFurthermore, we observe that the prediction shift from \"foo\" to \"bar\" is due to\nthe respective reduction and increase in these heads' attention scores at \"foo\"\nand \"bar\" positions. Therefore, we propose a hypothesis for ICL: in in-context\nheads, the value-output matrices extract label features, while the query-key\nmatrices compute the similarity between the features at the last position and\nthose at each label position. The query and key matrices can be considered as\ntwo towers that learn the similarity metric between the last position's\nfeatures and each demonstration at label positions. Using this hypothesis, we\nexplain the majority label bias and recency bias in ICL and propose two methods\nto reduce these biases by 22\\% and 17\\%, respectively.\n","authors":["Zeping Yu","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2402.02872v2.pdf","comment":"preprint (code and data will be released in final version)"},{"id":"http://arxiv.org/abs/2212.02724v3","updated":"2024-06-11T12:58:28Z","published":"2022-12-06T03:25:44Z","title":"Decentralized Stochastic Gradient Descent Ascent for Finite-Sum Minimax\n  Problems","summary":"  Minimax optimization problems have attracted significant attention in recent\nyears due to their widespread application in numerous machine learning models.\nTo solve the minimax problem, a wide variety of stochastic optimization methods\nhave been proposed. However, most of them ignore the distributed setting where\nthe training data is distributed on multiple workers. In this paper, we\ndeveloped a novel decentralized stochastic gradient descent ascent method for\nthe finite-sum minimax problem. In particular, by employing the\nvariance-reduced gradient, our method can achieve\n$O(\\frac{\\sqrt{n}\\kappa^3}{(1-\\lambda)^2\\epsilon^2})$ sample complexity and\n$O(\\frac{\\kappa^3}{(1-\\lambda)^2\\epsilon^2})$ communication complexity for the\nnonconvex-strongly-concave minimax problem. As far as we know, our work is the\nfirst one to achieve such theoretical complexities for this kind of minimax\nproblem. At last, we apply our method to AUC maximization, and the experimental\nresults confirm the effectiveness of our method.\n","authors":["Hongchang Gao"],"pdf_url":"https://arxiv.org/pdf/2212.02724v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00426v3","updated":"2024-06-11T12:53:03Z","published":"2024-06-01T12:48:11Z","title":"InterpreTabNet: Distilling Predictive Signals from Tabular Data by\n  Salient Feature Interpretation","summary":"  Tabular data are omnipresent in various sectors of industries. Neural\nnetworks for tabular data such as TabNet have been proposed to make predictions\nwhile leveraging the attention mechanism for interpretability. However, the\ninferred attention masks are often dense, making it challenging to come up with\nrationales about the predictive signal. To remedy this, we propose\nInterpreTabNet, a variant of the TabNet model that models the attention\nmechanism as a latent variable sampled from a Gumbel-Softmax distribution. This\nenables us to regularize the model to learn distinct concepts in the attention\nmasks via a KL Divergence regularizer. It prevents overlapping feature\nselection by promoting sparsity which maximizes the model's efficacy and\nimproves interpretability to determine the important features when predicting\nthe outcome. To assist in the interpretation of feature interdependencies from\nour model, we employ a large language model (GPT-4) and use prompt engineering\nto map from the learned feature mask onto natural language text describing the\nlearned signal. Through comprehensive experiments on real-world datasets, we\ndemonstrate that InterpreTabNet outperforms previous methods for interpreting\ntabular data while attaining competitive accuracy.\n","authors":["Jacob Si","Wendy Yusi Cheng","Michael Cooper","Rahul G. Krishnan"],"pdf_url":"https://arxiv.org/pdf/2406.00426v3.pdf","comment":"ICML 2024 Spotlight"},{"id":"http://arxiv.org/abs/2406.07217v1","updated":"2024-06-11T12:50:53Z","published":"2024-06-11T12:50:53Z","title":"A Synthetic Dataset for Personal Attribute Inference","summary":"  Recently, powerful Large Language Models (LLMs) have become easily accessible\nto hundreds of millions of users worldwide. However, their strong capabilities\nand vast world knowledge do not come without associated privacy risks. In this\nwork, we focus on the emerging privacy threat LLMs pose - the ability to\naccurately infer personal information from online texts. Despite the growing\nimportance of LLM-based author profiling, research in this area has been\nhampered by a lack of suitable public datasets, largely due to ethical and\nprivacy concerns associated with real personal data. In this work, we take two\nsteps to address this problem: (i) we construct a simulation framework for the\npopular social media platform Reddit using LLM agents seeded with synthetic\npersonal profiles; (ii) using this framework, we generate SynthPAI, a diverse\nsynthetic dataset of over 7800 comments manually labeled for personal\nattributes. We validate our dataset with a human study showing that humans\nbarely outperform random guessing on the task of distinguishing our synthetic\ncomments from real ones. Further, we verify that our dataset enables meaningful\npersonal attribute inference research by showing across 18 state-of-the-art\nLLMs that our synthetic comments allow us to draw the same conclusions as\nreal-world data. Together, this indicates that our dataset and pipeline provide\na strong and privacy-preserving basis for future research toward understanding\nand mitigating the inference-based privacy threats LLMs pose.\n","authors":["Hanna Yukhymenko","Robin Staab","Mark Vero","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2406.07217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05753v2","updated":"2024-06-11T12:45:08Z","published":"2024-06-09T12:16:30Z","title":"Grounding Continuous Representations in Geometry: Equivariant Neural\n  Fields","summary":"  Recently, Neural Fields have emerged as a powerful modelling paradigm to\nrepresent continuous signals. In a conditional neural field, a field is\nrepresented by a latent variable that conditions the NeF, whose parametrisation\nis otherwise shared over an entire dataset. We propose Equivariant Neural\nFields based on cross attention transformers, in which NeFs are conditioned on\na geometric conditioning variable, a latent point cloud, that enables an\nequivariant decoding from latent to field. Our equivariant approach induces a\nsteerability property by which both field and latent are grounded in geometry\nand amenable to transformation laws if the field transforms, the latent\nrepresents transforms accordingly and vice versa. Crucially, the equivariance\nrelation ensures that the latent is capable of (1) representing geometric\npatterns faitfhully, allowing for geometric reasoning in latent space, (2)\nweightsharing over spatially similar patterns, allowing for efficient learning\nof datasets of fields. These main properties are validated using classification\nexperiments and a verification of the capability of fitting entire datasets, in\ncomparison to other non-equivariant NeF approaches. We further validate the\npotential of ENFs by demonstrate unique local field editing properties.\n","authors":["David R Wessels","David M Knigge","Samuele Papa","Riccardo Valperga","Sharvaree Vadgama","Efstratios Gavves","Erik J Bekkers"],"pdf_url":"https://arxiv.org/pdf/2406.05753v2.pdf","comment":"Preprint for Neurips submission"},{"id":"http://arxiv.org/abs/2406.07213v1","updated":"2024-06-11T12:42:41Z","published":"2024-06-11T12:42:41Z","title":"Semantic-Aware Spectrum Sharing in Internet of Vehicles Based on Deep\n  Reinforcement Learning","summary":"  This work aims to investigate semantic communication in high-speed mobile\nInternet of vehicles (IoV) environments, with a focus on the spectrum sharing\nbetween vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I)\ncommunications. We specifically address spectrum scarcity and network traffic\nand then propose a semantic-aware spectrum sharing algorithm (SSS) based on the\ndeep reinforcement learning (DRL) soft actor-critic (SAC) approach. Firstly, we\ndelve into the extraction of semantic information. Secondly, we redefine\nmetrics for semantic information in V2V and V2I spectrum sharing in IoV\nenvironments, introducing high-speed semantic spectrum efficiency (HSSE) and\nsemantic transmission rate (HSR). Finally, we employ the SAC algorithm for\ndecision optimization in V2V and V2I spectrum sharing based on semantic\ninformation. This optimization encompasses the optimal link of V2V and V2I\nsharing strategies, the transmission power for vehicles sending semantic\ninformation and the length of transmitted semantic symbols, aiming at\nmaximizing HSSE of V2I and enhancing success rate of effective semantic\ninformation transmission (SRS) of V2V. Experimental results demonstrate that\nthe SSS algorithm outperforms other baseline algorithms, including other\ntraditional-communication-based spectrum sharing algorithms and spectrum\nsharing algorithm using other reinforcement learning approaches. The SSS\nalgorithm exhibits a 15% increase in HSSE and approximately a 7% increase in\nSRS.\n","authors":["Zhiyu Shao","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Jiangzhou Wang","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2406.07213v1.pdf","comment":"This paper has been submitted to IEEE Journal. The source code has\n  been released at:\n  https://github.com/qiongwu86/Semantic-Aware-Spectrum-Sharing-in-Internet-of-Vehicles-Based-on-Deep-Reinforcement-Learning"},{"id":"http://arxiv.org/abs/2403.10543v2","updated":"2024-06-11T12:35:27Z","published":"2024-03-11T08:48:54Z","title":"Mitigating Oversmoothing Through Reverse Process of GNNs for\n  Heterophilic Graphs","summary":"  Graph Neural Network (GNN) resembles the diffusion process, leading to the\nover-smoothing of learned representations when stacking many layers. Hence, the\nreverse process of message passing can produce the distinguishable node\nrepresentations by inverting the forward message propagation. The\ndistinguishable representations can help us to better classify neighboring\nnodes with different labels, such as in heterophilic graphs. In this work, we\napply the design principle of the reverse process to the three variants of the\nGNNs. Through the experiments on heterophilic graph data, where adjacent nodes\nneed to have different representations for successful classification, we show\nthat the reverse process significantly improves the prediction performance in\nmany cases. Additional analysis reveals that the reverse mechanism can mitigate\nthe over-smoothing over hundreds of layers. Our code is available at\nhttps://github.com/ml-postech/reverse-gnn.\n","authors":["MoonJeong Park","Jaeseung Heo","Dongwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2403.10543v2.pdf","comment":"Accepted by ICML 2024"},{"id":"http://arxiv.org/abs/2311.01434v2","updated":"2024-06-11T12:22:27Z","published":"2023-11-02T17:48:28Z","title":"Tailoring Mixup to Data for Calibration","summary":"  Among all data augmentation techniques proposed so far, linear interpolation\nof training samples, also called Mixup, has found to be effective for a large\npanel of applications. Along with improved performance, Mixup is also a good\ntechnique for improving calibration and predictive uncertainty. However, mixing\ndata carelessly can lead to manifold intrusion, i.e., conflicts between the\nsynthetic labels assigned and the true label distributions, which can\ndeteriorate calibration. In this work, we argue that the likelihood of manifold\nintrusion increases with the distance between data to mix. To this end, we\npropose to dynamically change the underlying distributions of interpolation\ncoefficients depending on the similarity between samples to mix, and define a\nflexible framework to do so without losing in diversity. We provide extensive\nexperiments for classification and regression tasks, showing that our proposed\nmethod improves performance and calibration of models, while being much more\nefficient. The code for our work is available at\nhttps://github.com/qbouniot/sim_kernel_mixup.\n","authors":["Quentin Bouniot","Pavlo Mozharovskyi","Florence d'Alché-Buc"],"pdf_url":"https://arxiv.org/pdf/2311.01434v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07082v2","updated":"2024-06-11T12:12:59Z","published":"2024-02-11T01:51:15Z","title":"Refined Sample Complexity for Markov Games with Independent Linear\n  Function Approximation","summary":"  Markov Games (MG) is an important model for Multi-Agent Reinforcement\nLearning (MARL). It was long believed that the \"curse of multi-agents\" (i.e.,\nthe algorithmic performance drops exponentially with the number of agents) is\nunavoidable until several recent works (Daskalakis et al., 2023; Cui et al.,\n2023; Wang et al., 2023). While these works resolved the curse of multi-agents,\nwhen the state spaces are prohibitively large and (linear) function\napproximations are deployed, they either had a slower convergence rate of\n$O(T^{-1/4})$ or brought a polynomial dependency on the number of actions\n$A_{\\max}$ -- which is avoidable in single-agent cases even when the loss\nfunctions can arbitrarily vary with time. This paper first refines the AVLPR\nframework by Wang et al. (2023), with an insight of designing *data-dependent*\n(i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a\nbroader choice of plug-in algorithms. When specialized to MGs with independent\nlinear function approximations, we propose novel *action-dependent bonuses* to\ncover occasionally extreme estimation errors. With the help of state-of-the-art\ntechniques from the single-agent RL literature, we give the first algorithm\nthat tackles the curse of multi-agents, attains the optimal $O(T^{-1/2})$\nconvergence rate, and avoids $\\text{poly}(A_{\\max})$ dependency simultaneously.\n","authors":["Yan Dai","Qiwen Cui","Simon S. Du"],"pdf_url":"https://arxiv.org/pdf/2402.07082v2.pdf","comment":"Accepted for presentation at the Conference on Learning Theory (COLT)\n  2024"},{"id":"http://arxiv.org/abs/2308.14507v2","updated":"2024-06-11T11:56:46Z","published":"2023-08-28T11:49:23Z","title":"Spectral Estimators for Structured Generalized Linear Models via\n  Approximate Message Passing","summary":"  We consider the problem of parameter estimation in a high-dimensional\ngeneralized linear model. Spectral methods obtained via the principal\neigenvector of a suitable data-dependent matrix provide a simple yet\nsurprisingly effective solution. However, despite their wide use, a rigorous\nperformance characterization, as well as a principled way to preprocess the\ndata, are available only for unstructured (i.i.d.\\ Gaussian and Haar\northogonal) designs. In contrast, real-world data matrices are highly\nstructured and exhibit non-trivial correlations. To address the problem, we\nconsider correlated Gaussian designs capturing the anisotropic nature of the\nfeatures via a covariance matrix $\\Sigma$. Our main result is a precise\nasymptotic characterization of the performance of spectral estimators. This\nallows us to identify the optimal preprocessing that minimizes the number of\nsamples needed for parameter estimation. Surprisingly, such preprocessing is\nuniversal across a broad set of statistical models, which partly addresses a\nconjecture on optimal spectral estimators for rotationally invariant designs.\nOur principled approach vastly improves upon previous heuristic methods,\nincluding for designs common in computational imaging and genetics. The\nproposed methodology, based on approximate message passing, is broadly\napplicable and opens the way to the precise characterization of spiked matrices\nand of the corresponding spectral methods in a variety of settings.\n","authors":["Yihan Zhang","Hong Chang Ji","Ramji Venkataramanan","Marco Mondelli"],"pdf_url":"https://arxiv.org/pdf/2308.14507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07177v1","updated":"2024-06-11T11:40:12Z","published":"2024-06-11T11:40:12Z","title":"TernaryLLM: Ternarized Large Language Model","summary":"  Large language models (LLMs) have achieved remarkable performance on Natural\nLanguage Processing (NLP) tasks, but they are hindered by high computational\ncosts and memory requirements. Ternarization, an extreme form of quantization,\noffers a solution by reducing memory usage and enabling energy-efficient\nfloating-point additions. However, applying ternarization to LLMs faces\nchallenges stemming from outliers in both weights and activations. In this\nwork, observing asymmetric outliers and non-zero means in weights, we introduce\nDual Learnable Ternarization (DLT), which enables both scales and shifts to be\nlearnable. We also propose Outlier-Friendly Feature Knowledge Distillation\n(OFF) to recover the information lost in extremely low-bit quantization. The\nproposed OFF can incorporate semantic information and is insensitive to\noutliers. At the core of OFF is maximizing the mutual information between\nfeatures in ternarized and floating-point models using cosine similarity.\nExtensive experiments demonstrate that our TernaryLLM surpasses previous\nlow-bit quantization methods on the standard text generation and zero-shot\nbenchmarks for different LLM families. Specifically, for one of the most\npowerful open-source models, LLaMA-3, our approach (W1.58A16) outperforms the\nprevious state-of-the-art method (W2A16) by 5.8 in terms of perplexity on C4\nand by 8.2% in terms of average accuracy on zero-shot tasks.\n","authors":["Tianqi Chen","Zhe Li","Weixiang Xu","Zeyu Zhu","Dong Li","Lu Tian","Emad Barsoum","Peisong Wang","Jian Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.07177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04761v4","updated":"2024-06-11T11:38:57Z","published":"2023-09-09T11:20:40Z","title":"A Comprehensive Survey on Deep Learning Techniques in Educational Data\n  Mining","summary":"  Educational Data Mining (EDM) has emerged as a vital field of research, which\nharnesses the power of computational techniques to analyze educational data.\nWith the increasing complexity and diversity of educational data, Deep Learning\ntechniques have shown significant advantages in addressing the challenges\nassociated with analyzing and modeling this data. This survey aims to\nsystematically review the state-of-the-art in EDM with Deep Learning. We begin\nby providing a brief introduction to EDM and Deep Learning, highlighting their\nrelevance in the context of modern education. Next, we present a detailed\nreview of Deep Learning techniques applied in four typical educational\nscenarios, including knowledge tracing, student behavior detection, performance\nprediction, and personalized recommendation. Furthermore, a comprehensive\noverview of public datasets and processing tools for EDM is provided. We then\nanalyze the practical challenges in EDM and propose targeted solutions.\nFinally, we point out emerging trends and future directions in this research\narea.\n","authors":["Yuanguo Lin","Hong Chen","Wei Xia","Fan Lin","Zongyue Wang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2309.04761v4.pdf","comment":"21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.05504v2","updated":"2024-06-11T11:37:35Z","published":"2024-06-08T16:04:33Z","title":"G-Transformer: Counterfactual Outcome Prediction under Dynamic and\n  Time-varying Treatment Regimes","summary":"  In the context of medical decision making, counterfactual prediction enables\nclinicians to predict treatment outcomes of interest under alternative courses\nof therapeutic actions given observed patient history. Prior machine learning\napproaches for counterfactual predictions under time-varying treatments focus\non static time-varying treatment regimes where treatments do not depend on\nprevious covariate history. In this work, we present G-Transformer, a\nTransformer-based framework supporting g-computation for counterfactual\nprediction under dynamic and time-varying treatment strategies. G-Transfomer\ncaptures complex, long-range dependencies in time-varying covariates using a\nTransformer architecture. G-Transformer estimates the conditional distribution\nof relevant covariates given covariate and treatment history at each time point\nusing an encoder architecture, then produces Monte Carlo estimates of\ncounterfactual outcomes by simulating forward patient trajectories under\ntreatment strategies of interest. We evaluate G-Transformer extensively using\ntwo simulated longitudinal datasets from mechanistic models, and a real-world\nsepsis ICU dataset from MIMIC-IV. G-Transformer outperforms both classical and\nstate-of-the-art counterfactual prediction models in these settings. To the\nbest of our knowledge, this is the first Transformer-based architecture for\ncounterfactual outcome prediction under dynamic and time-varying treatment\nstrategies. Code will be released upon publication of the paper.\n","authors":["Hong Xiong","Feng Wu","Leon Deng","Megan Su","Li-wei H Lehman"],"pdf_url":"https://arxiv.org/pdf/2406.05504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.12089v3","updated":"2024-06-11T11:18:57Z","published":"2022-10-21T16:29:14Z","title":"A Survey on Graph Counterfactual Explanations: Definitions, Methods,\n  Evaluation, and Research Challenges","summary":"  Graph Neural Networks (GNNs) perform well in community detection and molecule\nclassification. Counterfactual Explanations (CE) provide counter-examples to\novercome the transparency limitations of black-box models. Due to the growing\nattention in graph learning, we focus on the concepts of CE for GNNs. We\nanalysed the SoA to provide a taxonomy, a uniform notation, and the\nbenchmarking datasets and evaluation metrics. We discuss fourteen methods,\ntheir evaluation protocols, twenty-two datasets, and nineteen metrics. We\nintegrated the majority of methods into the GRETEL library to conduct an\nempirical evaluation to understand their strengths and pitfalls. We highlight\nopen challenges and future work.\n","authors":["Mario Alfonso Prado-Romero","Bardh Prenkaj","Giovanni Stilo","Fosca Giannotti"],"pdf_url":"https://arxiv.org/pdf/2210.12089v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04035v2","updated":"2024-06-11T11:14:56Z","published":"2024-06-06T13:03:51Z","title":"Spatio-temporal Early Prediction based on Multi-objective Reinforcement\n  Learning","summary":"  Accuracy and timeliness are indeed often conflicting goals in prediction\ntasks. Premature predictions may yield a higher rate of false alarms, whereas\ndelaying predictions to gather more information can render them too late to be\nuseful. In applications such as wildfires, crimes, and traffic jams, timely\npredictions are vital for safeguarding human life and property. Consequently,\nfinding a balance between accuracy and timeliness is crucial. In this paper, we\npropose a spatio-temporal early prediction model based on Multi-Objective\nreinforcement learning that can either implement an optimal policy given a\npreference or infer the preference based on a small number of samples. The\nmodel addresses two primary challenges: 1) enhancing the accuracy of early\npredictions and 2) providing the optimal policy for determining the most\nsuitable prediction time for each area. Our method demonstrates superior\nperformance on three large-scale real-world datasets, surpassing existing\nmethods in early spatio-temporal prediction tasks.\n","authors":["Wei Shao","Yufan Kang","Ziyan Peng","Xiao Xiao","Lei Wang","Yuhui Yang","Flora D Salim"],"pdf_url":"https://arxiv.org/pdf/2406.04035v2.pdf","comment":"Conference"},{"id":"http://arxiv.org/abs/2406.07160v1","updated":"2024-06-11T11:08:33Z","published":"2024-06-11T11:08:33Z","title":"Deep Learning-Based Approach for User Activity Detection with Grant-Free\n  Random Access in Cell-Free Massive MIMO","summary":"  Modern wireless networks must reliably support a wide array of connectivity\ndemands, encompassing various user needs across diverse scenarios. Machine-Type\nCommunication (mMTC) is pivotal in these networks, particularly given the\nchallenges posed by massive connectivity and sporadic device activation\npatterns. Traditional grant-based random access (GB-RA) protocols face\nlimitations due to constrained orthogonal preamble resources. In response, the\nadoption of grant-free random access (GF-RA) protocols offers a promising\nsolution. This paper explores the application of supervised machine learning\nmodels to tackle activity detection issues in scenarios where non-orthogonal\npreamble design is considered. We introduce a data-driven algorithm\nspecifically designed for user activity detection in Cell-Free Massive\nMultiple-Input Multiple-Output (CF-mMIMO) networks operating under GF-RA\nprotocols. Additionally, this study presents a novel clustering strategy that\nsimplifies and enhances activity detection accuracy, assesses the resilience of\nthe algorithm to input perturbations, and investigates the effects of adopting\nfloating-to-fixed-point conversion on algorithm performance. Simulations\nconducted adhere to 3GPP standards, ensuring accurate channel modeling, and\nemploy a deep learning approach to boost the detection capabilities of mMTC\nGF-RA devices. The results are compelling: the algorithm achieves an\nexceptional 99\\% accuracy rate, confirming its efficacy in real-world\napplications.\n","authors":["Ali Elkeshawy","HaÏfa Farès","Amor Nafkha"],"pdf_url":"https://arxiv.org/pdf/2406.07160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16375v4","updated":"2024-06-11T10:52:48Z","published":"2023-07-31T02:39:54Z","title":"UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed\n  Integer Quadratic Programming","summary":"  Distributed learning is commonly used for training deep learning models,\nespecially large models. In distributed learning, manual parallelism (MP)\nmethods demand considerable human effort and have limited flexibility. Hence,\nautomatic parallelism (AP) methods have recently been proposed for automating\nthe parallel strategy optimization process. Existing AP methods suffer from\nsub-optimal solutions because they do not jointly optimize the two categories\nof parallel strategies (i.e., inter-layer parallelism and intra-layer\nparallelism). In this paper, we propose a novel AP method called UniAP, which\nunifies inter- and intra-layer automatic parallelism by mixed integer quadratic\nprogramming. To the best of our knowledge, UniAP is the first parallel method\nthat can jointly optimize the two categories of parallel strategies to find an\noptimal solution. Experimental results show that UniAP outperforms\nstate-of-the-art methods by up to 3.80$\\times$ in throughput and reduces\nstrategy optimization time by up to 107$\\times$ across five Transformer-based\nmodels.\n","authors":["Hao Lin","Ke Wu","Jie Li","Jun Li","Wu-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2307.16375v4.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.07145v1","updated":"2024-06-11T10:45:41Z","published":"2024-06-11T10:45:41Z","title":"Failures Are Fated, But Can Be Faded: Characterizing and Mitigating\n  Unwanted Behaviors in Large-Scale Vision and Language Models","summary":"  In large deep neural networks that seem to perform surprisingly well on many\ntasks, we also observe a few failures related to accuracy, social biases, and\nalignment with human values, among others. Therefore, before deploying these\nmodels, it is crucial to characterize this failure landscape for engineers to\ndebug and legislative bodies to audit models. Nevertheless, it is infeasible to\nexhaustively test for all possible combinations of factors that could lead to a\nmodel's failure. In this paper, we introduce a post-hoc method that utilizes\n\\emph{deep reinforcement learning} to explore and construct the landscape of\nfailure modes in pre-trained discriminative and generative models. With the aid\nof limited human feedback, we then demonstrate how to restructure the failure\nlandscape to be more desirable by moving away from the discovered failure\nmodes. We empirically show the effectiveness of the proposed method across\ncommon Computer Vision, Natural Language Processing, and Vision-Language tasks.\n","authors":["Som Sagar","Aditya Taparia","Ransalu Senanayake"],"pdf_url":"https://arxiv.org/pdf/2406.07145v1.pdf","comment":"25 pages, 35 figures"},{"id":"http://arxiv.org/abs/2406.07141v1","updated":"2024-06-11T10:40:54Z","published":"2024-06-11T10:40:54Z","title":"Identifiable Object-Centric Representation Learning via Probabilistic\n  Slot Attention","summary":"  Learning modular object-centric representations is crucial for systematic\ngeneralization. Existing methods show promising object-binding capabilities\nempirically, but theoretical identifiability guarantees remain relatively\nunderdeveloped. Understanding when object-centric representations can\ntheoretically be identified is crucial for scaling slot-based methods to\nhigh-dimensional images with correctness guarantees. To that end, we propose a\nprobabilistic slot-attention algorithm that imposes an aggregate mixture prior\nover object-centric slot representations, thereby providing slot\nidentifiability guarantees without supervision, up to an equivalence relation.\nWe provide empirical verification of our theoretical identifiability result\nusing both simple 2-dimensional data and high-resolution imaging datasets.\n","authors":["Avinash Kori","Francesco Locatello","Ainkaran Santhirasekaram","Francesca Toni","Ben Glocker","Fabio De Sousa Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2406.07141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07126v1","updated":"2024-06-11T10:18:58Z","published":"2024-06-11T10:18:58Z","title":"Logical Distillation of Graph Neural Networks","summary":"  We present a logic based interpretable model for learning on graphs and an\nalgorithm to distill this model from a Graph Neural Network (GNN). Recent\nresults have shown connections between the expressivity of GNNs and the\ntwo-variable fragment of first-order logic with counting quantifiers (C2). We\nintroduce a decision-tree based model which leverages an extension of C2 to\ndistill interpretable logical classifiers from GNNs. We test our approach on\nmultiple GNN architectures. The distilled models are interpretable, succinct,\nand attain similar accuracy to the underlying GNN. Furthermore, when the ground\ntruth is expressible in C2, our approach outperforms the GNN.\n","authors":["Alexander Pluska","Pascal Welke","Thomas Gärtner","Sagar Malhotra"],"pdf_url":"https://arxiv.org/pdf/2406.07126v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.04607v2","updated":"2024-06-11T10:17:44Z","published":"2024-06-07T03:31:58Z","title":"MeGA: Merging Multiple Independently Trained Neural Networks Based on\n  Genetic Algorithm","summary":"  In this paper, we introduce a novel method for merging the weights of\nmultiple pre-trained neural networks using a genetic algorithm called MeGA.\nTraditional techniques, such as weight averaging and ensemble methods, often\nfail to fully harness the capabilities of pre-trained networks. Our approach\nleverages a genetic algorithm with tournament selection, crossover, and\nmutation to optimize weight combinations, creating a more effective fusion.\nThis technique allows the merged model to inherit advantageous features from\nboth parent models, resulting in enhanced accuracy and robustness. Through\nexperiments on the CIFAR-10 dataset, we demonstrate that our genetic\nalgorithm-based weight merging method improves test accuracy compared to\nindividual models and conventional methods. This approach provides a scalable\nsolution for integrating multiple pre-trained networks across various deep\nlearning applications. Github is available at:\nhttps://github.com/YUNBLAK/MeGA-Merging-Multiple-Independently-Trained-Neural-Networks-Based-on-Genetic-Algorithm\n","authors":["Daniel Yun"],"pdf_url":"https://arxiv.org/pdf/2406.04607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07125v1","updated":"2024-06-11T10:16:55Z","published":"2024-06-11T10:16:55Z","title":"CARACAS: vehiCular ArchitectuRe for detAiled Can Attacks Simulation","summary":"  Modern vehicles are increasingly vulnerable to attacks that exploit network\ninfrastructures, particularly the Controller Area Network (CAN) networks. To\neffectively counter such threats using contemporary tools like Intrusion\nDetection Systems (IDSs) based on data analysis and classification, large\ndatasets of CAN messages become imperative. This paper delves into the\nfeasibility of generating synthetic datasets by harnessing the modeling\ncapabilities of simulation frameworks such as Simulink coupled with a robust\nrepresentation of attack models to present CARACAS, a vehicular model,\nincluding component control via CAN messages and attack injection capabilities.\nCARACAS showcases the efficacy of this methodology, including a Battery\nElectric Vehicle (BEV) model, and focuses on attacks targeting torque control\nin two distinct scenarios.\n","authors":["Sadek Misto Kirdi","Nicola Scarano","Franco Oberti","Luca Mannella","Stefano Di Carlo","Alessandro Savino"],"pdf_url":"https://arxiv.org/pdf/2406.07125v1.pdf","comment":"6 pages, 8 figures, TrustAICyberSec workshop - IEEE ISCC 2024"},{"id":"http://arxiv.org/abs/2311.12078v3","updated":"2024-06-11T10:15:53Z","published":"2023-11-20T05:58:05Z","title":"Fast Controllable Diffusion Models for Undersampled MRI Reconstruction","summary":"  Supervised deep learning methods have shown promise in undersampled Magnetic\nResonance Imaging (MRI) reconstruction, but their requirement for paired data\nlimits their generalizability to the diverse MRI acquisition parameters.\nRecently, unsupervised controllable generative diffusion models have been\napplied to undersampled MRI reconstruction, without paired data or model\nretraining for different MRI acquisitions. However, diffusion models are\ngenerally slow in sampling and state-of-the-art acceleration techniques can\nlead to sub-optimal results when directly applied to the controllable\ngeneration process. This study introduces a new algorithm called\nPredictor-Projector-Noisor (PPN), which enhances and accelerates controllable\ngeneration of diffusion models for undersampled MRI reconstruction. Our results\ndemonstrate that PPN produces high-fidelity MR images that conform to\nundersampled k-space measurements with significantly shorter reconstruction\ntime than other controllable sampling methods. In addition, the unsupervised\nPPN accelerated diffusion models are adaptable to different MRI acquisition\nparameters, making them more practical for clinical use than supervised\nlearning techniques.\n","authors":["Wei Jiang","Zhuang Xiong","Feng Liu","Nan Ye","Hongfu Sun"],"pdf_url":"https://arxiv.org/pdf/2311.12078v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17467v3","updated":"2024-06-11T10:15:05Z","published":"2023-10-26T15:15:01Z","title":"The statistical thermodynamics of generative diffusion models: Phase\n  transitions, symmetry breaking and critical instability","summary":"  Generative diffusion models have achieved spectacular performance in many\nareas of machine learning and generative modeling. While the fundamental ideas\nbehind these models come from non-equilibrium physics, variational inference\nand stochastic calculus, in this paper we show that many aspects of these\nmodels can be understood using the tools of equilibrium statistical mechanics.\nUsing this reformulation, we show that generative diffusion models undergo\nsecond-order phase transitions corresponding to symmetry breaking phenomena. We\nshow that these phase-transitions are always in a mean-field universality\nclass, as they are the result of a self-consistency condition in the generative\ndynamics. We argue that the critical instability that arises from the phase\ntransitions lies at the heart of their generative capabilities, which are\ncharacterized by a set of mean-field critical exponents. Finally, we show that\nthe dynamic equation of the generative process can be interpreted as a\nstochastic adiabatic transformation that minimizes the free energy while\nkeeping the system in thermal equilibrium.\n","authors":["Luca Ambrogioni"],"pdf_url":"https://arxiv.org/pdf/2310.17467v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07124v1","updated":"2024-06-11T10:12:10Z","published":"2024-06-11T10:12:10Z","title":"CHARME: A chain-based reinforcement learning approach for the minor\n  embedding problem","summary":"  Quantum Annealing (QA) holds great potential for solving combinatorial\noptimization problems efficiently. However, the effectiveness of QA algorithms\nheavily relies on the embedding of problem instances, represented as logical\ngraphs, into the quantum unit processing (QPU) whose topology is in form of a\nlimited connectivity graph, known as the minor embedding Problem. Existing\nmethods for the minor embedding problem suffer from scalability issues when\nconfronted with larger problem sizes. In this paper, we propose a novel\napproach utilizing Reinforcement Learning (RL) techniques to address the minor\nembedding problem, named CHARME. CHARME includes three key components: a Graph\nNeural Network (GNN) architecture for policy modeling, a state transition\nalgorithm ensuring solution validity, and an order exploration strategy for\neffective training. Through comprehensive experiments on synthetic and\nreal-world instances, we demonstrate that the efficiency of our proposed order\nexploration strategy as well as our proposed RL framework, CHARME. In details,\nCHARME yields superior solutions compared to fast embedding methods such as\nMinorminer and ATOM. Moreover, our method surpasses the OCT-based approach,\nknown for its slower runtime but high-quality solutions, in several cases. In\naddition, our proposed exploration enhances the efficiency of the training of\nthe CHARME framework by providing better solutions compared to the greedy\nstrategy.\n","authors":["Hoang M. Ngo","Nguyen H K. Do","Minh N. Vu","Tamer Kahveci","My T. Thai"],"pdf_url":"https://arxiv.org/pdf/2406.07124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07117v1","updated":"2024-06-11T10:02:07Z","published":"2024-06-11T10:02:07Z","title":"Augmenting Offline RL with Unlabeled Data","summary":"  Recent advancements in offline Reinforcement Learning (Offline RL) have led\nto an increased focus on methods based on conservative policy updates to\naddress the Out-of-Distribution (OOD) issue. These methods typically involve\nadding behavior regularization or modifying the critic learning objective,\nfocusing primarily on states or actions with substantial dataset support.\nHowever, we challenge this prevailing notion by asserting that the absence of\nan action or state from a dataset does not necessarily imply its suboptimality.\nIn this paper, we propose a novel approach to tackle the OOD problem. We\nintroduce an offline RL teacher-student framework, complemented by a policy\nsimilarity measure. This framework enables the student policy to gain insights\nnot only from the offline RL dataset but also from the knowledge transferred by\na teacher policy. The teacher policy is trained using another dataset\nconsisting of state-action pairs, which can be viewed as practical domain\nknowledge acquired without direct interaction with the environment. We believe\nthis additional knowledge is key to effectively solving the OOD issue. This\nresearch represents a significant advancement in integrating a teacher-student\nnetwork into the actor-critic framework, opening new avenues for studies on\nknowledge transfer in offline RL and effectively addressing the OOD challenge.\n","authors":["Zhao Wang","Briti Gangopadhyay","Jia-Fong Yeh","Shingo Takamatsu"],"pdf_url":"https://arxiv.org/pdf/2406.07117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07115v1","updated":"2024-06-11T10:00:18Z","published":"2024-06-11T10:00:18Z","title":"Advancing Tool-Augmented Large Language Models: Integrating Insights\n  from Errors in Inference Trees","summary":"  Tool-augmented large language models (LLMs) leverage tools, often in the form\nof APIs, to enhance their reasoning capabilities on complex tasks, thus taking\non the role of intelligent agents interacting with the real world. The recently\nintroduced ToolLLaMA model by Qin et al. [2024] utilizes the depth-first\nsearch-based decision tree (DFSDT) method for reasoning with $16000+$\nreal-world APIs, which effectively improves the planning and inferencing\nperformance of tool-augmented LLMs compared to traditional chain reasoning\napproaches. However, their approach only employs successful paths from decision\ntrees (also called inference trees) for supervised fine-tuning (SFT) during\ntraining, which does not fully exploit the advantages of the tree of thought.\nIn this study, we propose an inference trajectory optimization framework based\non the preference data extracted from decision trees to address this\nlimitation. We first introduce a novel method for constructing preference data\nfrom the tree of thought, capitalizing on the failed explorations previously\noverlooked in the trees. Specifically, we generate an effective step-wise\npreference dataset, named ToolPreference, for tool use based on the ToolBench\ndataset. In the subsequent training phase, we first fine-tune the LLM with\ntool-usage expert trajectories and then use these step-wise preference pairs\nfor direct preference optimization (DPO) to update the policy of the LLM,\nresulting in our ToolPrefer-LLaMA (TP-LLaMA) model. Our experiments demonstrate\nthat by obtaining insights from errors in inference trees, TP-LLaMA\nsignificantly outperforms the baselines across almost all test scenarios by a\nlarge margin and exhibits better generalization capabilities with unseen APIs.\nAt the same time, TP-LLaMA has also demonstrated superior reasoning efficiency\ncompared to the baselines, making it more suitable for complex tool-usage\nreasoning tasks.\n","authors":["Sijia Chen","Yibo Wang","Yi-Feng Wu","Qing-Guo Chen","Zhao Xu","Weihua Luo","Kaifu Zhang","Lijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01349v2","updated":"2024-06-11T09:59:38Z","published":"2023-11-02T15:59:00Z","title":"Post-hoc Orthogonalization for Mitigation of Protected Feature Bias in\n  CXR Embeddings","summary":"  Purpose: To analyze and remove protected feature effects in chest radiograph\nembeddings of deep learning models. Methods: An orthogonalization is utilized\nto remove the influence of protected features (e.g., age, sex, race) in CXR\nembeddings, ensuring feature-independent results. To validate the efficacy of\nthe approach, we retrospectively study the MIMIC and CheXpert datasets using\nthree pre-trained models, namely a supervised contrastive, a self-supervised\ncontrastive, and a baseline classifier model. Our statistical analysis involves\ncomparing the original versus the orthogonalized embeddings by estimating\nprotected feature influences and evaluating the ability to predict race, age,\nor sex using the two types of embeddings. Results: Our experiments reveal a\nsignificant influence of protected features on predictions of pathologies.\nApplying orthogonalization removes these feature effects. Apart from removing\nany influence on pathology classification, while maintaining competitive\npredictive performance, orthogonalized embeddings further make it infeasible to\ndirectly predict protected attributes and mitigate subgroup disparities.\nConclusion: The presented work demonstrates the successful application and\nevaluation of the orthogonalization technique in the domain of chest X-ray\nimage classification.\n","authors":["Tobias Weber","Michael Ingrisch","Bernd Bischl","David Rügamer"],"pdf_url":"https://arxiv.org/pdf/2311.01349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12136v2","updated":"2024-06-11T09:57:23Z","published":"2023-07-22T18:05:28Z","title":"Using Reinforcement Learning for the Three-Dimensional Loading\n  Capacitated Vehicle Routing Problem","summary":"  Heavy goods vehicles are vital backbones of the supply chain delivery system\nbut also contribute significantly to carbon emissions with only 60% loading\nefficiency in the United Kingdom. Collaborative vehicle routing has been\nproposed as a solution to increase efficiency, but challenges remain to make\nthis a possibility. One key challenge is the efficient computation of viable\nsolutions for co-loading and routing. Current operations research methods\nsuffer from non-linear scaling with increasing problem size and are therefore\nbound to limited geographic areas to compute results in time for day-to-day\noperations. This only allows for local optima in routing and leaves global\noptimisation potential untouched. We develop a reinforcement learning model to\nsolve the three-dimensional loading capacitated vehicle routing problem in\napproximately linear time. While this problem has been studied extensively in\noperations research, no publications on solving it with reinforcement learning\nexist. We demonstrate the favourable scaling of our reinforcement learning\nmodel and benchmark our routing performance against state-of-the-art methods.\nThe model performs within an average gap of 3.83% to 8.10% compared to\nestablished methods. Our model not only represents a promising first step\ntowards large-scale logistics optimisation with reinforcement learning but also\nlays the foundation for this research stream. GitHub:\nhttps://github.com/if-loops/3L-CVRP\n","authors":["Stefan Schoepf","Stephen Mak","Julian Senoner","Liming Xu","Netland Torbjörn","Alexandra Brintrup"],"pdf_url":"https://arxiv.org/pdf/2307.12136v2.pdf","comment":"Presented at the IJCAI 2023 Workshop on Search and Planning with\n  Complex Objectives (WoSePCO)"},{"id":"http://arxiv.org/abs/2311.14772v2","updated":"2024-06-11T09:53:51Z","published":"2023-11-24T13:37:19Z","title":"Trainwreck: A damaging adversarial attack on image classifiers","summary":"  Adversarial attacks are an important security concern for computer vision\n(CV). As CV models are becoming increasingly valuable assets in applied\npractice, disrupting them is emerging as a form of economic sabotage. This\npaper opens up the exploration of damaging adversarial attacks (DAAs) that seek\nto damage target CV models. DAAs are formalized by defining the threat model,\nthe cost function DAAs maximize, and setting three requirements for success:\npotency, stealth, and customizability. As a pioneer DAA, this paper proposes\nTrainwreck, a train-time attack that conflates the data of similar classes in\nthe training data using stealthy ($\\epsilon \\leq 8/255$) class-pair universal\nperturbations obtained from a surrogate model. Trainwreck is a black-box,\ntransferable attack: it requires no knowledge of the target architecture, and a\nsingle poisoned dataset degrades the performance of any model trained on it.\nThe experimental evaluation on CIFAR-10 and CIFAR-100 and various model\narchitectures (EfficientNetV2, ResNeXt-101, and a finetuned ViT-L-16)\ndemonstrates Trainwreck's efficiency. Trainwreck achieves similar or better\npotency compared to the data poisoning state of the art and is fully\ncustomizable by the poison rate parameter. Finally, data redundancy with\nhashing is identified as a reliable defense against Trainwreck or similar DAAs.\nThe code is available at https://github.com/JanZahalka/trainwreck.\n","authors":["Jan Zahálka"],"pdf_url":"https://arxiv.org/pdf/2311.14772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07107v1","updated":"2024-06-11T09:49:00Z","published":"2024-06-11T09:49:00Z","title":"Agnostic Sharpness-Aware Minimization","summary":"  Sharpness-aware minimization (SAM) has been instrumental in improving deep\nneural network training by minimizing both the training loss and the sharpness\nof the loss landscape, leading the model into flatter minima that are\nassociated with better generalization properties. In another aspect,\nModel-Agnostic Meta-Learning (MAML) is a framework designed to improve the\nadaptability of models. MAML optimizes a set of meta-models that are\nspecifically tailored for quick adaptation to multiple tasks with minimal\nfine-tuning steps and can generalize well with limited data. In this work, we\nexplore the connection between SAM and MAML, particularly in terms of enhancing\nmodel generalization. We introduce Agnostic-SAM, a novel approach that combines\nthe principles of both SAM and MAML. Agnostic-SAM adapts the core idea of SAM\nby optimizing the model towards wider local minima using training data, while\nconcurrently maintaining low loss values on validation data. By doing so, it\nseeks flatter minima that are not only robust to small perturbations but also\nless vulnerable to data distributional shift problems. Our experimental results\ndemonstrate that Agnostic-SAM significantly improves generalization over\nbaselines across a range of datasets and under challenging conditions such as\nnoisy labels and data limitation.\n","authors":["Van-Anh Nguyen","Quyen Tran","Tuan Truong","Thanh-Toan Do","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2406.07107v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2406.07100v1","updated":"2024-06-11T09:42:03Z","published":"2024-06-11T09:42:03Z","title":"D-GRIL: End-to-End Topological Learning with 2-parameter Persistence","summary":"  End-to-end topological learning using 1-parameter persistence is well-known.\nWe show that the framework can be enhanced using 2-parameter persistence by\nadopting a recently introduced 2-parameter persistence based vectorization\ntechnique called GRIL. We establish a theoretical foundation of differentiating\nGRIL producing D-GRIL. We show that D-GRIL can be used to learn a bifiltration\nfunction on standard benchmark graph datasets. Further, we exhibit that this\nframework can be applied in the context of bio-activity prediction in drug\ndiscovery.\n","authors":["Soham Mukherjee","Shreyas N. Samaga","Cheng Xin","Steve Oudot","Tamal K. Dey"],"pdf_url":"https://arxiv.org/pdf/2406.07100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07096v1","updated":"2024-06-11T09:37:52Z","published":"2024-06-11T09:37:52Z","title":"Fast Context-Biasing for CTC and Transducer ASR models with CTC-based\n  Word Spotter","summary":"  Accurate recognition of rare and new words remains a pressing problem for\ncontextualized Automatic Speech Recognition (ASR) systems. Most context-biasing\nmethods involve modification of the ASR model or the beam-search decoding\nalgorithm, complicating model reuse and slowing down inference. This work\npresents a new approach to fast context-biasing with CTC-based Word Spotter\n(CTC-WS) for CTC and Transducer (RNN-T) ASR models. The proposed method matches\nCTC log-probabilities against a compact context graph to detect potential\ncontext-biasing candidates. The valid candidates then replace their greedy\nrecognition counterparts in corresponding frame intervals. A Hybrid\nTransducer-CTC model enables the CTC-WS application for the Transducer model.\nThe results demonstrate a significant acceleration of the context-biasing\nrecognition with a simultaneous improvement in F-score and WER compared to\nbaseline methods. The proposed method is publicly available in the NVIDIA NeMo\ntoolkit.\n","authors":["Andrei Andrusenko","Aleksandr Laptev","Vladimir Bataev","Vitaly Lavrukhin","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2406.07096v1.pdf","comment":"Accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2205.15049v5","updated":"2024-06-11T09:34:06Z","published":"2022-05-30T12:28:10Z","title":"Metrizing Fairness","summary":"  We study supervised learning problems that have significant effects on\nindividuals from two demographic groups, and we seek predictors that are fair\nwith respect to a group fairness criterion such as statistical parity (SP). A\npredictor is SP-fair if the distributions of predictions within the two groups\nare close in Kolmogorov distance, and fairness is achieved by penalizing the\ndissimilarity of these two distributions in the objective function of the\nlearning problem. In this paper, we identify conditions under which hard SP\nconstraints are guaranteed to improve predictive accuracy. We also showcase\nconceptual and computational benefits of measuring unfairness with integral\nprobability metrics (IPMs) other than the Kolmogorov distance. Conceptually, we\nshow that the generator of any IPM can be interpreted as a family of utility\nfunctions and that unfairness with respect to this IPM arises if individuals in\nthe two demographic groups have diverging expected utilities. We also prove\nthat the unfairness-regularized prediction loss admits unbiased gradient\nestimators, which are constructed from random mini-batches of training samples,\nif unfairness is measured by the squared $\\mathcal L^2$-distance or by a\nsquared maximum mean discrepancy. In this case, the fair learning problem is\nsusceptible to efficient stochastic gradient descent (SGD) algorithms.\nNumerical experiments on synthetic and real data show that these SGD algorithms\noutperform state-of-the-art methods for fair learning in that they achieve\nsuperior accuracy-unfairness trade-offs -- sometimes orders of magnitude\nfaster.\n","authors":["Yves Rychener","Bahar Taskesen","Daniel Kuhn"],"pdf_url":"https://arxiv.org/pdf/2205.15049v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07084v1","updated":"2024-06-11T09:21:50Z","published":"2024-06-11T09:21:50Z","title":"Leveraging Large Language Models for Efficient Failure Analysis in Game\n  Development","summary":"  In games, and more generally in the field of software development, early\ndetection of bugs is vital to maintain a high quality of the final product.\nAutomated tests are a powerful tool that can catch a problem earlier in\ndevelopment by executing periodically. As an example, when new code is\nsubmitted to the code base, a new automated test verifies these changes.\nHowever, identifying the specific change responsible for a test failure becomes\nharder when dealing with batches of changes -- especially in the case of a\nlarge-scale project such as a AAA game, where thousands of people contribute to\na single code base. This paper proposes a new approach to automatically\nidentify which change in the code caused a test to fail. The method leverages\nLarge Language Models (LLMs) to associate error messages with the corresponding\ncode changes causing the failure. We investigate the effectiveness of our\napproach with quantitative and qualitative evaluations. Our approach reaches an\naccuracy of 71% in our newly created dataset, which comprises issues reported\nby developers at EA over a period of one year. We further evaluated our model\nthrough a user study to assess the utility and usability of the tool from a\ndeveloper perspective, resulting in a significant reduction in time -- up to\n60% -- spent investigating issues.\n","authors":["Leonardo Marini","Linus Gisslén","Alessandro Sestini"],"pdf_url":"https://arxiv.org/pdf/2406.07084v1.pdf","comment":"Published at CoG 2024"},{"id":"http://arxiv.org/abs/2308.15323v2","updated":"2024-06-11T09:19:24Z","published":"2023-08-29T14:20:13Z","title":"Occlusion-Aware Deep Convolutional Neural Network via Homogeneous\n  Tanh-transforms for Face Parsing","summary":"  Face parsing infers a pixel-wise label map for each semantic facial\ncomponent. Previous methods generally work well for uncovered faces, however,\nthey overlook facial occlusion and ignore some contextual areas outside a\nsingle face, especially when facial occlusion has become a common situation\nduring the COVID-19 epidemic. Inspired by the lighting phenomena in everyday\nlife, where illumination from four distinct lamps provides a more uniform\ndistribution than a single central light source, we propose a novel homogeneous\ntanh-transform for image preprocessing, which is made up of four\ntanh-transforms. These transforms fuse the central vision and the peripheral\nvision together. Our proposed method addresses the dilemma of face parsing\nunder occlusion and compresses more information from the surrounding context.\nBased on homogeneous tanh-transforms, we propose an occlusion-aware\nconvolutional neural network for occluded face parsing. It combines information\nin both Tanh-polar space and Tanh-Cartesian space, capable of enhancing\nreceptive fields. Furthermore, we introduce an occlusion-aware loss to focus on\nthe boundaries of occluded regions. The network is simple, flexible, and can be\ntrained end-to-end. To facilitate future research of occluded face parsing, we\nalso contribute a new cleaned face parsing dataset. This dataset is manually\npurified from several academic or industrial datasets, including CelebAMask-HQ,\nShort-video Face Parsing, and the Helen dataset, and will be made public.\nExperiments demonstrate that our method surpasses state-of-the-art methods in\nface parsing under occlusion.\n","authors":["Jianhua Qiua","Weihua Liu","Chaochao Lin","Jiaojiao Li","Haoping Yu","Said Boumaraf"],"pdf_url":"https://arxiv.org/pdf/2308.15323v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07083v1","updated":"2024-06-11T09:16:43Z","published":"2024-06-11T09:16:43Z","title":"Efficient Mixture Learning in Black-Box Variational Inference","summary":"  Mixture variational distributions in black box variational inference (BBVI)\nhave demonstrated impressive results in challenging density estimation tasks.\nHowever, currently scaling the number of mixture components can lead to a\nlinear increase in the number of learnable parameters and a quadratic increase\nin inference time due to the evaluation of the evidence lower bound (ELBO). Our\ntwo key contributions address these limitations. First, we introduce the novel\nMultiple Importance Sampling Variational Autoencoder (MISVAE), which amortizes\nthe mapping from input to mixture-parameter space using one-hot encodings.\nFortunately, with MISVAE, each additional mixture component incurs a negligible\nincrease in network parameters. Second, we construct two new estimators of the\nELBO for mixtures in BBVI, enabling a tremendous reduction in inference time\nwith marginal or even improved impact on performance. Collectively, our\ncontributions enable scalability to hundreds of mixture components and provide\nsuperior estimation performance in shorter time, with fewer network parameters\ncompared to previous Mixture VAEs. Experimenting with MISVAE, we achieve\nastonishing, SOTA results on MNIST. Furthermore, we empirically validate our\nestimators in other BBVI settings, including Bayesian phylogenetic inference,\nwhere we improve inference times for the SOTA mixture model on eight data sets.\n","authors":["Alexandra Hotti","Oskar Kviman","Ricky Molén","Víctor Elvira","Jens Lagergren"],"pdf_url":"https://arxiv.org/pdf/2406.07083v1.pdf","comment":"In Proceedings of the 41 st International Conference on Machine\n  Learning (ICML), Vienna, Austria"},{"id":"http://arxiv.org/abs/2406.07072v1","updated":"2024-06-11T08:59:20Z","published":"2024-06-11T08:59:20Z","title":"On the relation between trainability and dequantization of variational\n  quantum learning models","summary":"  The quest for successful variational quantum machine learning (QML) relies on\nthe design of suitable parametrized quantum circuits (PQCs), as analogues to\nneural networks in classical machine learning. Successful QML models must\nfulfill the properties of trainability and non-dequantization, among others.\nRecent works have highlighted an intricate interplay between trainability and\ndequantization of such models, which is still unresolved. In this work we\ncontribute to this debate from the perspective of machine learning, proving a\nnumber of results identifying, among others when trainability and\nnon-dequantization are not mutually exclusive. We begin by providing a number\nof new somewhat broader definitions of the relevant concepts, compared to what\nis found in other literature, which are operationally motivated, and consistent\nwith prior art. With these precise definitions given and motivated, we then\nstudy the relation between trainability and dequantization of variational QML.\nNext, we also discuss the degrees of \"variationalness\" of QML models, where we\ndistinguish between models like the hardware efficient ansatz and quantum\nkernel methods. Finally, we introduce recipes for building PQC-based QML models\nwhich are both trainable and nondequantizable, and corresponding to different\ndegrees of variationalness. We do not address the practical utility for such\nmodels. Our work however does point toward a way forward for finding more\ngeneral constructions, for which finding applications may become feasible.\n","authors":["Elies Gil-Fuster","Casper Gyurik","Adrián Pérez-Salinas","Vedran Dunjko"],"pdf_url":"https://arxiv.org/pdf/2406.07072v1.pdf","comment":"17 pages (13+4), 3 figures"},{"id":"http://arxiv.org/abs/2406.07060v1","updated":"2024-06-11T08:41:21Z","published":"2024-06-11T08:41:21Z","title":"Reading Miscue Detection in Primary School through Automatic Speech\n  Recognition","summary":"  Automatic reading diagnosis systems can benefit both teachers for more\nefficient scoring of reading exercises and students for accessing reading\nexercises with feedback more easily. However, there are limited studies on\nAutomatic Speech Recognition (ASR) for child speech in languages other than\nEnglish, and limited research on ASR-based reading diagnosis systems. This\nstudy investigates how efficiently state-of-the-art (SOTA) pretrained ASR\nmodels recognize Dutch native children speech and manage to detect reading\nmiscues. We found that Hubert Large finetuned on Dutch speech achieves SOTA\nphoneme-level child speech recognition (PER at 23.1\\%), while Whisper (Faster\nWhisper Large-v2) achieves SOTA word-level performance (WER at 9.8\\%). Our\nfindings suggest that Wav2Vec2 Large and Whisper are the two best ASR models\nfor reading miscue detection. Specifically, Wav2Vec2 Large shows the highest\nrecall at 0.83, whereas Whisper exhibits the highest precision at 0.52 and an\nF1 score of 0.52.\n","authors":["Lingyun Gao","Cristian Tejedor-Garcia","Helmer Strik","Catia Cucchiarini"],"pdf_url":"https://arxiv.org/pdf/2406.07060v1.pdf","comment":"Proc. INTERSPEECH 2024, 1-5 September 2024. Kos Island, Greece"},{"id":"http://arxiv.org/abs/2406.07057v1","updated":"2024-06-11T08:38:13Z","published":"2024-06-11T08:38:13Z","title":"Benchmarking Trustworthiness of Multimodal Large Language Models: A\n  Comprehensive Study","summary":"  Despite the superior capabilities of Multimodal Large Language Models (MLLMs)\nacross diverse tasks, they still face significant trustworthiness challenges.\nYet, current literature on the assessment of trustworthy MLLMs remains limited,\nlacking a holistic evaluation to offer thorough insights into future\nimprovements. In this work, we establish MultiTrust, the first comprehensive\nand unified benchmark on the trustworthiness of MLLMs across five primary\naspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark\nemploys a rigorous evaluation strategy that addresses both multimodal risks and\ncross-modal impacts, encompassing 32 diverse tasks with self-curated datasets.\nExtensive experiments with 21 modern MLLMs reveal some previously unexplored\ntrustworthiness issues and risks, highlighting the complexities introduced by\nthe multimodality and underscoring the necessity for advanced methodologies to\nenhance their reliability. For instance, typical proprietary models still\nstruggle with the perception of visually confusing images and are vulnerable to\nmultimodal jailbreaking and adversarial attacks; MLLMs are more inclined to\ndisclose privacy in text and reveal ideological and cultural biases even when\npaired with irrelevant images in inference, indicating that the multimodality\namplifies the internal risks from base LLMs. Additionally, we release a\nscalable toolbox for standardized trustworthiness research, aiming to\nfacilitate future advancements in this important field. Code and resources are\npublicly available at: https://multi-trust.github.io/.\n","authors":["Yichi Zhang","Yao Huang","Yitong Sun","Chang Liu","Zhe Zhao","Zhengwei Fang","Yifan Wang","Huanran Chen","Xiao Yang","Xingxing Wei","Hang Su","Yinpeng Dong","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.07057v1.pdf","comment":"100 pages, 84 figures, 33 tables"},{"id":"http://arxiv.org/abs/2406.05482v2","updated":"2024-06-11T08:36:37Z","published":"2024-06-08T14:14:19Z","title":"Efficient Topology-aware Data Augmentation for High-Degree Graph Neural\n  Networks","summary":"  In recent years, graph neural networks (GNNs) have emerged as a potent tool\nfor learning on graph-structured data and won fruitful successes in varied\nfields. The majority of GNNs follow the message-passing paradigm, where\nrepresentations of each node are learned by recursively aggregating features of\nits neighbors. However, this mechanism brings severe over-smoothing and\nefficiency issues over high-degree graphs (HDGs), wherein most nodes have\ndozens (or even hundreds) of neighbors, such as social networks, transaction\ngraphs, power grids, etc. Additionally, such graphs usually encompass rich and\ncomplex structure semantics, which are hard to capture merely by feature\naggregations in GNNs. Motivated by the above limitations, we propose TADA, an\nefficient and effective front-mounted data augmentation framework for GNNs on\nHDGs. Under the hood, TADA includes two key modules: (i) feature expansion with\nstructure embeddings, and (ii) topology- and attribute-aware graph\nsparsification. The former obtains augmented node features and enhanced model\ncapacity by encoding the graph structure into high-quality structure embeddings\nwith our highly-efficient sketching method. Further, by exploiting\ntask-relevant features extracted from graph structures and attributes, the\nsecond module enables the accurate identification and reduction of numerous\nredundant/noisy edges from the input graph, thereby alleviating over-smoothing\nand facilitating faster feature aggregations over HDGs. Empirically, TADA\nconsiderably improves the predictive performance of mainstream GNN models on 8\nreal homophilic/heterophilic HDGs in terms of node classification, while\nachieving efficient training and inference processes.\n","authors":["Yurui Lai","Xiaoyang Lin","Renchi Yang","Hongtao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.05482v2.pdf","comment":"This is the technical report for the paper accepted to KDD 2024. 17\n  pages"},{"id":"http://arxiv.org/abs/2406.07053v1","updated":"2024-06-11T08:35:23Z","published":"2024-06-11T08:35:23Z","title":"TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation\n  and LLMs","summary":"  Large Language Models (LLMs) have immense potential to transform the\ntelecommunications industry. They could help professionals understand complex\nstandards, generate code, and accelerate development. However, traditional LLMs\nstruggle with the precision and source verification essential for telecom work.\nTo address this, specialized LLM-based solutions tailored to telecommunication\nstandards are needed. Retrieval-augmented generation (RAG) offers a way to\ncreate precise, fact-based answers. This paper proposes TelecomRAG, a framework\nfor a Telecommunication Standards Assistant that provides accurate, detailed,\nand verifiable responses. Our implementation, using a knowledge base built from\n3GPP Release 16 and Release 18 specification documents, demonstrates how this\nassistant surpasses generic LLMs, offering superior accuracy, technical depth,\nand verifiability, and thus significant value to the telecommunications field.\n","authors":["Girma M. Yilma","Jose A. Ayala-Romero","Andres Garcia-Saavedra","Xavier Costa-Perez"],"pdf_url":"https://arxiv.org/pdf/2406.07053v1.pdf","comment":"7 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2406.07049v1","updated":"2024-06-11T08:25:11Z","published":"2024-06-11T08:25:11Z","title":"GridPE: Unifying Positional Encoding in Transformers with a Grid\n  Cell-Inspired Framework","summary":"  Understanding spatial location and relationships is a fundamental capability\nfor modern artificial intelligence systems. Insights from human spatial\ncognition provide valuable guidance in this domain. Recent neuroscientific\ndiscoveries have highlighted the role of grid cells as a fundamental neural\ncomponent for spatial representation, including distance computation, path\nintegration, and scale discernment. In this paper, we introduce a novel\npositional encoding scheme inspired by Fourier analysis and the latest findings\nin computational neuroscience regarding grid cells. Assuming that grid cells\nencode spatial position through a summation of Fourier basis functions, we\ndemonstrate the translational invariance of the grid representation during\ninner product calculations. Additionally, we derive an optimal grid scale ratio\nfor multi-dimensional Euclidean spaces based on principles of biological\nefficiency. Utilizing these computational principles, we have developed a\n**Grid**-cell inspired **Positional Encoding** technique, termed **GridPE**,\nfor encoding locations within high-dimensional spaces. We integrated GridPE\ninto the Pyramid Vision Transformer architecture. Our theoretical analysis\nshows that GridPE provides a unifying framework for positional encoding in\narbitrary high-dimensional spaces. Experimental results demonstrate that GridPE\nsignificantly enhances the performance of transformers, underscoring the\nimportance of incorporating neuroscientific insights into the design of\nartificial intelligence systems.\n","authors":["Boyang Li","Yulin Wu","Nuoxian Huang"],"pdf_url":"https://arxiv.org/pdf/2406.07049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05376v2","updated":"2024-06-11T08:20:26Z","published":"2024-06-08T07:05:26Z","title":"Adversarial flows: A gradient flow characterization of adversarial\n  attacks","summary":"  A popular method to perform adversarial attacks on neuronal networks is the\nso-called fast gradient sign method and its iterative variant. In this paper,\nwe interpret this method as an explicit Euler discretization of a differential\ninclusion, where we also show convergence of the discretization to the\nassociated gradient flow. To do so, we consider the concept of p-curves of\nmaximal slope in the case $p=\\infty$. We prove existence of $\\infty$-curves of\nmaximum slope and derive an alternative characterization via differential\ninclusions. Furthermore, we also consider Wasserstein gradient flows for\npotential energies, where we show that curves in the Wasserstein space can be\ncharacterized by a representing measure on the space of curves in the\nunderlying Banach space, which fulfill the differential inclusion. The\napplication of our theory to the finite-dimensional setting is twofold: On the\none hand, we show that a whole class of normalized gradient descent methods (in\nparticular signed gradient descent) converge, up to subsequences, to the flow,\nwhen sending the step size to zero. On the other hand, in the distributional\nsetting, we show that the inner optimization task of adversarial training\nobjective can be characterized via $\\infty$-curves of maximum slope on an\nappropriate optimal transport space.\n","authors":["Lukas Weigand","Tim Roith","Martin Burger"],"pdf_url":"https://arxiv.org/pdf/2406.05376v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06433v2","updated":"2024-06-11T08:16:34Z","published":"2024-06-10T16:24:35Z","title":"DISCO: An End-to-End Bandit Framework for Personalised Discount\n  Allocation","summary":"  Personalised discount codes provide a powerful mechanism for managing\ncustomer relationships and operational spend in e-commerce. Bandits are well\nsuited for this product area, given the partial information nature of the\nproblem, as well as the need for adaptation to the changing business\nenvironment. Here, we introduce DISCO, an end-to-end contextual bandit\nframework for personalised discount code allocation at ASOS. DISCO adapts the\ntraditional Thompson Sampling algorithm by integrating it within an integer\nprogram, thereby allowing for operational cost control. Because bandit learning\nis often worse with high dimensional actions, we focused on building low\ndimensional action and context representations that were nonetheless capable of\ngood accuracy. Additionally, we sought to build a model that preserved the\nrelationship between price and sales, in which customers increasing their\npurchasing in response to lower prices (\"negative price elasticity\"). These\naims were achieved by using radial basis functions to represent the continuous\n(i.e. infinite armed) action space, in combination with context embeddings\nextracted from a neural network. These feature representations were used within\na Thompson Sampling framework to facilitate exploration, and further integrated\nwith an integer program to allocate discount codes across ASOS's customer base.\nThese modelling decisions result in a reward model that (a) enables pooled\nlearning across similar actions, (b) is highly accurate, including in\nextrapolation, and (c) preserves the expected negative price elasticity.\nThrough offline analysis, we show that DISCO is able to effectively enact\nexploration and improves its performance over time, despite the global\nconstraint. Finally, we subjected DISCO to a rigorous online A/B test, and find\nthat it achieves a significant improvement of >1% in average basket value,\nrelative to the legacy systems.\n","authors":["Jason Shuo Zhang","Benjamin Howson","Panayiota Savva","Eleanor Loh"],"pdf_url":"https://arxiv.org/pdf/2406.06433v2.pdf","comment":"Accepted at ECML/PKDD 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2406.07430v1","updated":"2024-06-11T16:34:02Z","published":"2024-06-11T16:34:02Z","title":"Learning Domain-Invariant Features for Out-of-Context News Detection","summary":"  Multimodal out-of-context news is a common type of misinformation on online\nmedia platforms. This involves posting a caption, alongside an invalid\nout-of-context news image. Reflecting its importance, researchers have\ndeveloped models to detect such misinformation. However, a common limitation of\nthese models is that they only consider the scenario where pre-labeled data is\navailable for each domain, failing to address the out-of-context news detection\non unlabeled domains (e.g., unverified news on new topics or agencies). In this\nwork, we therefore focus on domain adaptive out-of-context news detection. In\norder to effectively adapt the detection model to unlabeled news topics or\nagencies, we propose ConDA-TTA (Contrastive Domain Adaptation with Test-Time\nAdaptation) which applies contrastive learning and maximum mean discrepancy\n(MMD) to learn the domain-invariant feature. In addition, it leverages target\ndomain statistics during test-time to further assist domain adaptation.\nExperimental results show that our approach outperforms baselines in 5 out of 7\ndomain adaptation settings on two public datasets, by as much as 2.93% in F1\nand 2.08% in accuracy.\n","authors":["Yimeng Gu","Mengqi Zhang","Ignacio Castro","Shu Wu","Gareth Tyson"],"pdf_url":"https://arxiv.org/pdf/2406.07430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07383v1","updated":"2024-06-11T15:50:42Z","published":"2024-06-11T15:50:42Z","title":"Federated Multi-Agent DRL for Radio Resource Management in Industrial 6G\n  in-X subnetworks","summary":"  Recently, 6G in-X subnetworks have been proposed as low-power short-range\nradio cells to support localized extreme wireless connectivity inside entities\nsuch as industrial robots, vehicles, and the human body. Deployment of in-X\nsubnetworks within these entities may result in rapid changes in interference\nlevels and thus, varying link quality. This paper investigates distributed\ndynamic channel allocation to mitigate inter-subnetwork interference in dense\nin-factory deployments of 6G in-X subnetworks. This paper introduces two new\ntechniques, Federated Multi-Agent Double Deep Q-Network (F-MADDQN) and\nFederated Multi-Agent Deep Proximal Policy Optimization (F-MADPPO), for channel\nallocation in 6G in-X subnetworks. These techniques are based on a\nclient-to-server horizontal federated reinforcement learning framework. The\nmethods require sharing only local model weights with a centralized gNB for\nfederated aggregation thereby preserving local data privacy and security.\nSimulations were conducted using a practical indoor factory environment\nproposed by 5G-ACIA and 3GPP models for in-factory environments. The results\nshowed that the proposed methods achieved slightly better performance than\nbaseline schemes with significantly reduced signaling overhead compared to the\nbaseline solutions. The schemes also showed better robustness and\ngeneralization ability to changes in deployment densities and propagation\nparameters.\n","authors":["Bjarke Madsen","Ramoni Adeogun"],"pdf_url":"https://arxiv.org/pdf/2406.07383v1.pdf","comment":"Accepted for Workshop on Industrial Wireless Networks - IEEE\n  International Symposium on Personal, Indoor and Mobile Radio Communications"},{"id":"http://arxiv.org/abs/2306.09817v4","updated":"2024-06-11T15:28:31Z","published":"2023-06-16T12:59:51Z","title":"INDCOR white paper 4: Evaluation of Interactive Narrative Design For\n  Complexity Representations","summary":"  While a strength of Interactive Digital Narratives (IDN) is its support for\nmultiperspectivity, this also poses a substantial challenge to its evaluation.\nMoreover, evaluation has to assess the system's ability to represent a complex\nreality as well as the user's understanding of that complex reality as a result\nof the experience of interacting with the system. This is needed to measure an\nIDN's efficiency and effectiveness in representing the chosen complex\nphenomenon. We here present some empirical methods employed by INDCOR members\nin their research including UX toolkits and scales. Particularly, we consider\nthe impact of IDN on transformative learning and its evaluation through\nself-reporting and other alternatives.\n","authors":["Christian Roth","Breanne Pitt","Lāsma Šķestere","Jonathan Barbara","Agnes Karolina Bakk","Kirsty Dunlop","Maria del Mar Grandio","Miguel Barreda","Despoina Sampatakou","Michael Schlauch"],"pdf_url":"https://arxiv.org/pdf/2306.09817v4.pdf","comment":"arXiv admin note: text overlap with arXiv:2010.10135"},{"id":"http://arxiv.org/abs/2406.07268v1","updated":"2024-06-11T13:52:29Z","published":"2024-06-11T13:52:29Z","title":"Advancing Grounded Multimodal Named Entity Recognition via LLM-Based\n  Reformulation and Box-Based Segmentation","summary":"  Grounded Multimodal Named Entity Recognition (GMNER) task aims to identify\nnamed entities, entity types and their corresponding visual regions. GMNER task\nexhibits two challenging attributes: 1) The tenuous correlation between images\nand text on social media contributes to a notable proportion of named entities\nbeing ungroundable. 2) There exists a distinction between coarse-grained noun\nphrases used in similar tasks (e.g., phrase localization) and fine-grained\nnamed entities. In this paper, we propose RiVEG, a unified framework that\nreformulates GMNER into a joint MNER-VE-VG task by leveraging large language\nmodels (LLMs) as connecting bridges. This reformulation brings two benefits: 1)\nIt enables us to optimize the MNER module for optimal MNER performance and\neliminates the need to pre-extract region features using object detection\nmethods, thus naturally addressing the two major limitations of existing GMNER\nmethods. 2) The introduction of Entity Expansion Expression module and Visual\nEntailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG).\nThis endows the proposed framework with unlimited data and model scalability.\nFurthermore, to address the potential ambiguity stemming from the\ncoarse-grained bounding box output in GMNER, we further construct the new\nSegmented Multimodal Named Entity Recognition (SMNER) task and corresponding\nTwitter-SMNER dataset aimed at generating fine-grained segmentation masks, and\nexperimentally demonstrate the feasibility and effectiveness of using box\nprompt-based Segment Anything Model (SAM) to empower any GMNER model with the\nability to accomplish the SMNER task. Extensive experiments demonstrate that\nRiVEG significantly outperforms SoTA methods on four datasets across the MNER,\nGMNER, and SMNER tasks.\n","authors":["Jinyuan Li","Ziyan Li","Han Li","Jianfei Yu","Rui Xia","Di Sun","Gang Pan"],"pdf_url":"https://arxiv.org/pdf/2406.07268v1.pdf","comment":"Extension of our Findings of EMNLP 2023 & ACL 2024 paper"},{"id":"http://arxiv.org/abs/2406.03701v2","updated":"2024-06-11T13:27:40Z","published":"2024-06-06T02:50:59Z","title":"Recognizing Everything from All Modalities at Once: Grounded Multimodal\n  Universal Information Extraction","summary":"  In the field of information extraction (IE), tasks across a wide range of\nmodalities and their combinations have been traditionally studied in isolation,\nleaving a gap in deeply recognizing and analyzing cross-modal information. To\naddress this, this work for the first time introduces the concept of grounded\nMultimodal Universal Information Extraction (MUIE), providing a unified task\nframework to analyze any IE tasks over various modalities, along with their\nfine-grained groundings. To tackle MUIE, we tailor a multimodal large language\nmodel (MLLM), Reamo, capable of extracting and grounding information from all\nmodalities, i.e., recognizing everything from all modalities at once. Reamo is\nupdated via varied tuning strategies, equipping it with powerful capabilities\nfor information recognition and fine-grained multimodal grounding. To address\nthe absence of a suitable benchmark for grounded MUIE, we curate a\nhigh-quality, diverse, and challenging test set, which encompasses IE tasks\nacross 9 common modality combinations with the corresponding multimodal\ngroundings. The extensive comparison of Reamo with existing MLLMs integrated\ninto pipeline approaches demonstrates its advantages across all evaluation\ndimensions, establishing a strong benchmark for the follow-up research. Our\nresources are publicly released at https://haofei.vip/MUIE.\n","authors":["Meishan Zhang","Hao Fei","Bin Wang","Shengqiong Wu","Yixin Cao","Fei Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.03701v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07198v1","updated":"2024-06-11T12:18:18Z","published":"2024-06-11T12:18:18Z","title":"Target Speech Diarization with Multimodal Prompts","summary":"  Traditional speaker diarization seeks to detect ``who spoke when'' according\nto speaker characteristics. Extending to target speech diarization, we detect\n``when target event occurs'' according to the semantic characteristics of\nspeech. We propose a novel Multimodal Target Speech Diarization (MM-TSD)\nframework, which accommodates diverse and multi-modal prompts to specify target\nevents in a flexible and user-friendly manner, including semantic language\ndescription, pre-enrolled speech, pre-registered face image, and audio-language\nlogical prompts. We further propose a voice-face aligner module to project\nhuman voice and face representation into a shared space. We develop a\nmulti-modal dataset based on VoxCeleb2 for MM-TSD training and evaluation.\nAdditionally, we conduct comparative analysis and ablation studies for each\ncategory of prompts to validate the efficacy of each component in the proposed\nframework. Furthermore, our framework demonstrates versatility in performing\nvarious signal processing tasks, including speaker diarization and overlap\nspeech detection, using task-specific prompts. MM-TSD achieves robust and\ncomparable performance as a unified system compared to specialized models.\nMoreover, MM-TSD shows capability to handle complex conversations for\nreal-world dataset.\n","authors":["Yidi Jiang","Ruijie Tao","Zhengyang Chen","Yanmin Qian","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2406.07198v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.07162v1","updated":"2024-06-11T11:12:51Z","published":"2024-06-11T11:12:51Z","title":"EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and\n  Benchmark","summary":"  Speech emotion recognition (SER) is an important part of human-computer\ninteraction, receiving extensive attention from both industry and academia.\nHowever, the current research field of SER has long suffered from the following\nproblems: 1) There are few reasonable and universal splits of the datasets,\nmaking comparing different models and methods difficult. 2) No commonly used\nbenchmark covers numerous corpus and languages for researchers to refer to,\nmaking reproduction a burden. In this paper, we propose EmoBox, an\nout-of-the-box multilingual multi-corpus speech emotion recognition toolkit,\nalong with a benchmark for both intra-corpus and cross-corpus settings. For\nintra-corpus settings, we carefully designed the data partitioning for\ndifferent datasets. For cross-corpus settings, we employ a foundation SER\nmodel, emotion2vec, to mitigate annotation errors and obtain a test set that is\nfully balanced in speakers and emotions distributions. Based on EmoBox, we\npresent the intra-corpus SER results of 10 pre-trained speech models on 32\nemotion datasets with 14 languages, and the cross-corpus SER results on 4\ndatasets with the fully balanced test sets. To the best of our knowledge, this\nis the largest SER benchmark, across language scopes and quantity scales. We\nhope that our toolkit and benchmark can facilitate the research of SER in the\ncommunity.\n","authors":["Ziyang Ma","Mingjie Chen","Hezhao Zhang","Zhisheng Zheng","Wenxi Chen","Xiquan Li","Jiaxin Ye","Xie Chen","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2406.07162v1.pdf","comment":"Accepted by INTERSPEECH 2024. GitHub Repository:\n  https://github.com/emo-box/EmoBox"},{"id":"http://arxiv.org/abs/2406.07151v1","updated":"2024-06-11T10:52:17Z","published":"2024-06-11T10:52:17Z","title":"EEG-ImageNet: An Electroencephalogram Dataset and Benchmarks with Image\n  Visual Stimuli of Multi-Granularity Labels","summary":"  Identifying and reconstructing what we see from brain activity gives us a\nspecial insight into investigating how the biological visual system represents\nthe world. While recent efforts have achieved high-performance image\nclassification and high-quality image reconstruction from brain signals\ncollected by Functional Magnetic Resonance Imaging (fMRI) or\nmagnetoencephalogram (MEG), the expensiveness and bulkiness of these devices\nmake relevant applications difficult to generalize to practical applications.\nOn the other hand, Electroencephalography (EEG), despite its advantages of ease\nof use, cost-efficiency, high temporal resolution, and non-invasive nature, has\nnot been fully explored in relevant studies due to the lack of comprehensive\ndatasets. To address this gap, we introduce EEG-ImageNet, a novel EEG dataset\ncomprising recordings from 16 subjects exposed to 4000 images selected from the\nImageNet dataset. EEG-ImageNet consists of 5 times EEG-image pairs larger than\nexisting similar EEG benchmarks. EEG-ImageNet is collected with image stimuli\nof multi-granularity labels, i.e., 40 images with coarse-grained labels and 40\nwith fine-grained labels. Based on it, we establish benchmarks for object\nclassification and image reconstruction. Experiments with several commonly used\nmodels show that the best models can achieve object classification with\naccuracy around 60% and image reconstruction with two-way identification around\n64%. These results demonstrate the dataset's potential to advance EEG-based\nvisual brain-computer interfaces, understand the visual perception of\nbiological systems, and provide potential applications in improving machine\nvisual models.\n","authors":["Shuqi Zhu","Ziyi Ye","Qingyao Ai","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2406.07151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13478v2","updated":"2024-06-11T10:18:08Z","published":"2024-01-24T14:23:12Z","title":"SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval","summary":"  Multi-modal information retrieval (MMIR) is a rapidly evolving field, where\nsignificant progress, particularly in image-text pairing, has been made through\nadvanced representation learning and cross-modality alignment research.\nHowever, current benchmarks for evaluating MMIR performance in image-text\npairing within the scientific domain show a notable gap, where chart and table\nimages described in scholarly language usually do not play a significant role.\nTo bridge this gap, we develop a specialised scientific MMIR (SciMMIR)\nbenchmark by leveraging open-access paper collections to extract data relevant\nto the scientific domain. This benchmark comprises 530K meticulously curated\nimage-text pairs, extracted from figures and tables with detailed captions in\nscientific documents. We further annotate the image-text pairs with two-level\nsubset-subcategory hierarchy annotations to facilitate a more comprehensive\nevaluation of the baselines. We conducted zero-shot and fine-tuning evaluations\non prominent multi-modal image-captioning and visual language models, such as\nCLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific\ndomain, including the impact of pre-training and fine-tuning settings and the\ninfluence of the visual and textual encoders. All our data and checkpoints are\npublicly available at https://github.com/Wusiwei0410/SciMMIR.\n","authors":["Siwei Wu","Yizhi Li","Kang Zhu","Ge Zhang","Yiming Liang","Kaijing Ma","Chenghao Xiao","Haoran Zhang","Bohao Yang","Wenhu Chen","Wenhao Huang","Noura Al Moubayed","Jie Fu","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2401.13478v2.pdf","comment":"camera-ready version for ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.07087v1","updated":"2024-06-11T09:25:31Z","published":"2024-06-11T09:25:31Z","title":"Edge Rendering Architecture for multiuser XR Experiences and E2E\n  Performance Assessment","summary":"  Holographic communications are gaining ground among emerging eXtended-Reality\n(XR) applications due to their potential to revolutionize human communication.\nHowever, these technologies are characterized by higher requirements in terms\nof Quality of Service (QoS), such as high transmission data rates, very low\nlatency, and high computation capacity, challenging current achievable\ncapabilities. In this context, computation offloading techniques are being\ninvestigated, where resource-intensive computational tasks, like rendering XR\nexperiences, are shifted from user devices to a separate processor,\nspecifically an Edge Computing instance. This paper introduces an Edge\nRendering architecture for multiuser XR experiences, implements it on top of\nwidely employed XR and Web technologies, and proposes a method based on image\nand audio processing to evaluate its performance in terms of end-to-end media\nstreaming latency, inter-device, and intra-media synchronization when employing\ndifferent access networks.\n","authors":["Inhar Yeregui","Daniel Mejías","Guillermo Pacho","Roberto Viola","Jasone Astorga","Mario Montagud"],"pdf_url":"https://arxiv.org/pdf/2406.07087v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2406.06964v1","updated":"2024-06-11T05:47:16Z","published":"2024-06-11T05:47:16Z","title":"Missingness-resilient Video-enhanced Multimodal Disfluency Detection","summary":"  Most existing speech disfluency detection techniques only rely upon acoustic\ndata. In this work, we present a practical multimodal disfluency detection\napproach that leverages available video data together with audio. We curate an\naudiovisual dataset and propose a novel fusion technique with unified\nweight-sharing modality-agnostic encoders to learn the temporal and semantic\ncontext. Our resilient design accommodates real-world scenarios where the video\nmodality may sometimes be missing during inference. We also present alternative\nfusion strategies when both modalities are assured to be complete. In\nexperiments across five disfluency-detection tasks, our unified multimodal\napproach significantly outperforms Audio-only unimodal methods, yielding an\naverage absolute improvement of 10% (i.e., 10 percentage point increase) when\nboth video and audio modalities are always available, and 7% even when video\nmodality is missing in half of the samples.\n","authors":["Payal Mohapatra","Shamika Likhite","Subrata Biswas","Bashima Islam","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.06964v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2405.13049v2","updated":"2024-06-11T03:12:01Z","published":"2024-05-19T09:59:00Z","title":"SemEval-2024 Task 3: Multimodal Emotion Cause Analysis in Conversations","summary":"  The ability to understand emotions is an essential component of human-like\nartificial intelligence, as emotions greatly influence human cognition,\ndecision making, and social interactions. In addition to emotion recognition in\nconversations, the task of identifying the potential causes behind an\nindividual's emotional state in conversations, is of great importance in many\napplication scenarios. We organize SemEval-2024 Task 3, named Multimodal\nEmotion Cause Analysis in Conversations, which aims at extracting all pairs of\nemotions and their corresponding causes from conversations. Under different\nmodality settings, it consists of two subtasks: Textual Emotion-Cause Pair\nExtraction in Conversations (TECPE) and Multimodal Emotion-Cause Pair\nExtraction in Conversations (MECPE). The shared task has attracted 143\nregistrations and 216 successful submissions. In this paper, we introduce the\ntask, dataset and evaluation settings, summarize the systems of the top teams,\nand discuss the findings of the participants.\n","authors":["Fanfan Wang","Heqing Ma","Jianfei Yu","Rui Xia","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2405.13049v2.pdf","comment":"12 pages, 3 figures, 4 Tables"},{"id":"http://arxiv.org/abs/2406.06888v1","updated":"2024-06-11T02:07:47Z","published":"2024-06-11T02:07:47Z","title":"A Subjective Quality Evaluation of 3D Mesh with Dynamic Level of Detail\n  in Virtual Reality","summary":"  3D meshes are one of the main components of Virtual Reality applications.\nHowever, many network and computational resources are required to process 3D\nmeshes in real-time. A potential solution to this challenge is to dynamically\nadapt the Level of Detail (LoD) of a 3D mesh based on the object's position and\nthe user's viewpoint. In this paper, we conduct a subjective study to\ninvestigate users' quality perception of 3D meshes with dynamic Level of Detail\nin a Virtual Reality environment. The subjective experiment is carried out with\nfive 3D meshes of different characteristics, four Levels of Detail, and four\ndistance settings. The results of the experiment show that the impact of the\ndynamic level of detail depends on both the position of the 3D object in the\nvirtual world and the number of vertices of the original mesh. In addition, we\npresent a quality model that can accurately predict the MOS score of a LoD\nversion of a 3D mesh from the number of vertices and the distance from the\nviewpoint.\n","authors":["Duc Nguyen","Tran Thuy Hien","Truong Thu Huong"],"pdf_url":"https://arxiv.org/pdf/2406.06888v1.pdf","comment":"Acceped to ICIP 2024"},{"id":"http://arxiv.org/abs/2406.07676v1","updated":"2024-06-11T19:50:50Z","published":"2024-06-11T19:50:50Z","title":"FastAST: Accelerating Audio Spectrogram Transformer via Token Merging\n  and Cross-Model Knowledge Distillation","summary":"  Audio classification models, particularly the Audio Spectrogram Transformer\n(AST), play a crucial role in efficient audio analysis. However, optimizing\ntheir efficiency without compromising accuracy remains a challenge. In this\npaper, we introduce FastAST, a framework that integrates Token Merging (ToMe)\ninto the AST framework. FastAST enhances inference speed without requiring\nextensive retraining by merging similar tokens in audio spectrograms.\nFurthermore, during training, FastAST brings about significant speed\nimprovements. The experiments indicate that FastAST can increase audio\nclassification throughput with minimal impact on accuracy. To mitigate the\naccuracy impact, we integrate Cross-Model Knowledge Distillation (CMKD) into\nthe FastAST framework. Integrating ToMe and CMKD into AST results in improved\naccuracy compared to AST while maintaining faster inference speeds. FastAST\nrepresents a step towards real-time, resource-efficient audio analysis.\n","authors":["Swarup Ranjan Behera","Abhishek Dhiman","Karthik Gowda","Aalekhya Satya Narayani"],"pdf_url":"https://arxiv.org/pdf/2406.07676v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.07645v1","updated":"2024-06-11T18:20:39Z","published":"2024-06-11T18:20:39Z","title":"SSNVC: Single Stream Neural Video Compression with Implicit Temporal\n  Information","summary":"  Recently, Neural Video Compression (NVC) techniques have achieved remarkable\nperformance, even surpassing the best traditional lossy video codec. However,\nmost existing NVC methods heavily rely on transmitting Motion Vector (MV) to\ngenerate accurate contextual features, which has the following drawbacks. (1)\nCompressing and transmitting MV requires specialized MV encoder and decoder,\nwhich makes modules redundant. (2) Due to the existence of MV Encoder-Decoder,\nthe training strategy is complex. In this paper, we present a noval Single\nStream NVC framework (SSNVC), which removes complex MV Encoder-Decoder\nstructure and uses a one-stage training strategy. SSNVC implicitly use temporal\ninformation by adding previous entropy model feature to current entropy model\nand using previous two frame to generate predicted motion information at the\ndecoder side. Besides, we enhance the frame generator to generate higher\nquality reconstructed frame. Experiments demonstrate that SSNVC can achieve\nstate-of-the-art performance on multiple benchmarks, and can greatly simplify\ncompression process as well as training process.\n","authors":["Feng Wang","Haihang Ruan","Zhihuang Xie","Ronggang Wang","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2406.07645v1.pdf","comment":"Accepted by DCC 2024 as Poster. This is the full paper"},{"id":"http://arxiv.org/abs/2406.07588v1","updated":"2024-06-11T08:12:43Z","published":"2024-06-11T08:12:43Z","title":"AIM: Let Any Multi-modal Large Language Models Embrace Efficient\n  In-Context Learning","summary":"  In-context learning (ICL) facilitates Large Language Models (LLMs) exhibiting\nemergent ability on downstream tasks without updating billions of parameters.\nHowever, in the area of multi-modal Large Language Models (MLLMs), two problems\nhinder the application of multi-modal ICL: (1) Most primary MLLMs are only\ntrained on single-image datasets, making them unable to read multi-modal\ndemonstrations. (2) With the demonstrations increasing, thousands of visual\ntokens highly challenge hardware and degrade ICL performance. During\npreliminary explorations, we discovered that the inner LLM tends to focus more\non the linguistic modality within multi-modal demonstrations to generate\nresponses. Therefore, we propose a general and light-weighted framework\n\\textbf{AIM} to tackle the mentioned problems through \\textbf{A}ggregating\n\\textbf{I}mage information of \\textbf{M}ultimodal demonstrations to the dense\nlatent space of the corresponding linguistic part. Specifically, AIM first uses\nthe frozen backbone MLLM to read each image-text demonstration and extracts the\nvector representations on top of the text. These vectors naturally fuse the\ninformation of the image-text pair, and AIM transforms them into fused virtual\ntokens acceptable for the inner LLM via a trainable projection layer.\nUltimately, these fused tokens function as variants of multi-modal\ndemonstrations, fed into the MLLM to direct its response to the current query\nas usual. Because these fused tokens stem from the textual component of the\nimage-text pair, a multi-modal demonstration is nearly reduced to a pure\ntextual demonstration, thus seamlessly applying to any MLLMs. With its de facto\nMLLM frozen, AIM is parameter-efficient and we train it on public multi-modal\nweb corpora which have nothing to do with downstream test tasks.\n","authors":["Jun Gao","Qian Qiao","Ziqiang Cao","Zili Wang","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2406.07588v1.pdf","comment":null}]},"2024-06-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.05132v2","updated":"2024-06-12T17:59:58Z","published":"2024-06-07T17:59:59Z","title":"3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and\n  Less Hallucination","summary":"  The integration of language and 3D perception is crucial for developing\nembodied agents and robots that comprehend and interact with the physical\nworld. While large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, their adaptation to 3D environments\n(3D-LLMs) remains in its early stages. A primary challenge is the absence of\nlarge-scale datasets that provide dense grounding between language and 3D\nscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset\ncomprising 40,087 household scenes paired with 6.2 million densely-grounded\nscene-language instructions. Our results show that instruction tuning with\n3D-GRAND significantly enhances grounding capabilities and reduces\nhallucinations in 3D-LLMs. As part of our contributions, we propose a\ncomprehensive benchmark 3D-POPE to systematically evaluate hallucination in\n3D-LLMs, enabling fair comparisons among future models. Our experiments\nhighlight a scaling effect between dataset size and 3D-LLM performance,\nemphasizing the critical role of large-scale 3D-text datasets in advancing\nembodied AI research. Notably, our results demonstrate early signals for\neffective sim-to-real transfer, indicating that models trained on large\nsynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and\n3D-POPE, we aim to equip the embodied AI community with essential resources and\ninsights, setting the stage for more reliable and better-grounded 3D-LLMs.\nProject website: https://3d-grand.github.io\n","authors":["Jianing Yang","Xuweiyi Chen","Nikhil Madaan","Madhavan Iyengar","Shengyi Qian","David F. Fouhey","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2406.05132v2.pdf","comment":"Project website: https://3d-grand.github.io"},{"id":"http://arxiv.org/abs/2402.13254v4","updated":"2024-06-12T17:59:55Z","published":"2024-02-20T18:59:55Z","title":"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples","summary":"  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V. To facilitate future research, we release our code, dataset, benchmark,\nand checkpoints at https://countercurate.github.io.\n","authors":["Jianrui Zhang","Mu Cai","Tengyang Xie","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2402.13254v4.pdf","comment":"15 pages, 6 figures, 12 tables, Project Page:\n  https://countercurate.github.io/"},{"id":"http://arxiv.org/abs/2406.08482v1","updated":"2024-06-12T17:59:27Z","published":"2024-06-12T17:59:27Z","title":"Words Worth a Thousand Pictures: Measuring and Understanding Perceptual\n  Variability in Text-to-Image Generation","summary":"  Diffusion models are the state of the art in text-to-image generation, but\ntheir perceptual variability remains understudied. In this paper, we examine\nhow prompts affect image variability in black-box diffusion-based models. We\npropose W1KP, a human-calibrated measure of variability in a set of images,\nbootstrapped from existing image-pair perceptual distances. Current datasets do\nnot cover recent diffusion models, thus we curate three test sets for\nevaluation. Our best perceptual distance outperforms nine baselines by up to 18\npoints in accuracy, and our calibration matches graded human judgements 78% of\nthe time. Using W1KP, we study prompt reusability and show that Imagen prompts\ncan be reused for 10-50 random seeds before new images become too similar to\nalready generated images, while Stable Diffusion XL and DALL-E 3 can be reused\n50-200 times. Lastly, we analyze 56 linguistic features of real prompts,\nfinding that the prompt's length, CLIP embedding norm, concreteness, and word\nsenses influence variability most. As far as we are aware, we are the first to\nanalyze diffusion variability from a visuolinguistic perspective. Our project\npage is at http://w1kp.com\n","authors":["Raphael Tang","Xinyu Zhang","Lixinyu Xu","Yao Lu","Wenyan Li","Pontus Stenetorp","Jimmy Lin","Ferhan Ture"],"pdf_url":"https://arxiv.org/pdf/2406.08482v1.pdf","comment":"13 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.08478v1","updated":"2024-06-12T17:59:07Z","published":"2024-06-12T17:59:07Z","title":"What If We Recaption Billions of Web Images with LLaMA-3?","summary":"  Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate\nthat semantically aligning and enriching textual descriptions of these pairs\ncan significantly enhance model training across various vision-language tasks,\nparticularly text-to-image generation. However, large-scale investigations in\nthis area remain predominantly closed-source. Our paper aims to bridge this\ncommunity effort, leveraging the powerful and \\textit{open-sourced} LLaMA-3, a\nGPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a\nLLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images\nfrom the DataComp-1B dataset. Our empirical results confirm that this enhanced\ndataset, Recap-DataComp-1B, offers substantial benefits in training advanced\nvision-language models. For discriminative models like CLIP, we observe\nenhanced zero-shot performance in cross-modal retrieval tasks. For generative\nmodels like text-to-image Diffusion Transformers, the generated images exhibit\na significant improvement in alignment with users' text instructions,\nespecially in following complex queries. Our project page is\nhttps://www.haqtu.me/Recap-Datacomp-1B/\n","authors":["Xianhang Li","Haoqin Tu","Mude Hui","Zeyu Wang","Bingchen Zhao","Junfei Xiao","Sucheng Ren","Jieru Mei","Qing Liu","Huangjie Zheng","Yuyin Zhou","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2406.08478v1.pdf","comment":"* denotes equal contributions"},{"id":"http://arxiv.org/abs/2406.08464v1","updated":"2024-06-12T17:52:30Z","published":"2024-06-12T17:52:30Z","title":"Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs\n  with Nothing","summary":"  High-quality instruction data is critical for aligning large language models\n(LLMs). Although some models, such as Llama-3-Instruct, have open weights,\ntheir alignment data remain private, which hinders the democratization of AI.\nHigh human labor costs and a limited, predefined scope for prompting prevent\nexisting open-source data creation methods from scaling effectively,\npotentially limiting the diversity and quality of public alignment datasets. Is\nit possible to synthesize high-quality instruction data at scale by extracting\nit directly from an aligned LLM? We present a self-synthesis method for\ngenerating large-scale alignment data named Magpie. Our key observation is that\naligned LLMs like Llama-3-Instruct can generate a user query when we input only\nthe left-side templates up to the position reserved for user messages, thanks\nto their auto-regressive nature. We use this method to prompt Llama-3-Instruct\nand generate 4 million instructions along with their corresponding responses.\nWe perform a comprehensive analysis of the extracted data and select 300K\nhigh-quality instances. To compare Magpie data with other public instruction\ndatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the\nperformance of the fine-tuned models. Our results indicate that in some tasks,\nmodels fine-tuned with Magpie perform comparably to the official\nLlama-3-8B-Instruct, despite the latter being enhanced with 10 million data\npoints through supervised fine-tuning (SFT) and subsequent feedback learning.\nWe also show that using Magpie solely for SFT can surpass the performance of\nprevious public datasets utilized for both SFT and preference optimization,\nsuch as direct preference optimization with UltraFeedback. This advantage is\nevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.\n","authors":["Zhangchen Xu","Fengqing Jiang","Luyao Niu","Yuntian Deng","Radha Poovendran","Yejin Choi","Bill Yuchen Lin"],"pdf_url":"https://arxiv.org/pdf/2406.08464v1.pdf","comment":"Link: https://magpie-align.github.io/"},{"id":"http://arxiv.org/abs/2406.08447v1","updated":"2024-06-12T17:38:20Z","published":"2024-06-12T17:38:20Z","title":"The Impact of Initialization on LoRA Finetuning Dynamics","summary":"  In this paper, we study the role of initialization in Low Rank Adaptation\n(LoRA) as originally introduced in Hu et al. (2021). Essentially, to start from\nthe pretrained model as initialization for finetuning, one can either\ninitialize B to zero and A to random (default initialization in PEFT package),\nor vice-versa. In both cases, the product BA is equal to zero at\ninitialization, which makes finetuning starts from the pretrained model. These\ntwo initialization schemes are seemingly similar. They should in-principle\nyield the same performance and share the same optimal learning rate. We\ndemonstrate that this is an incorrect intuition and that the first scheme\n(initializing B to zero and A to random) on average yields better performance\ncompared to the other scheme. Our theoretical analysis shows that the reason\nbehind this might be that the first initialization allows the use of larger\nlearning rates (without causing output instability) compared to the second\ninitialization, resulting in more efficient learning of the first scheme. We\nvalidate our results with extensive experiments on LLMs.\n","authors":["Soufiane Hayou","Nikhil Ghosh","Bin Yu"],"pdf_url":"https://arxiv.org/pdf/2406.08447v1.pdf","comment":"TDLR: Different Initializations lead to completely different\n  finetuning dynamics. One initialization (set A random and B zero) is\n  generally better than the natural opposite initialization. arXiv admin note:\n  text overlap with arXiv:2402.12354"},{"id":"http://arxiv.org/abs/2406.08446v1","updated":"2024-06-12T17:37:09Z","published":"2024-06-12T17:37:09Z","title":"OLMES: A Standard for Language Model Evaluations","summary":"  Progress in AI is often demonstrated by new models claiming improved\nperformance on tasks measuring model capabilities. Evaluating language models\nin particular is challenging, as small changes to how a model is evaluated on a\ntask can lead to large changes in measured performance. There is no common\nstandard setup, so different models are evaluated on the same tasks in\ndifferent ways, leading to claims about which models perform best not being\nreproducible. We propose OLMES, a completely documented, practical, open\nstandard for reproducible LLM evaluations. In developing this standard, we\nidentify and review the varying factors in evaluation practices adopted by the\ncommunity - such as details of prompt formatting, choice of in-context\nexamples, probability normalizations, and task formulation. In particular,\nOLMES supports meaningful comparisons between smaller base models that require\nthe unnatural \"cloze\" formulation of multiple-choice questions against larger\nmodels that can utilize the original formulation. OLMES includes\nwell-considered recommendations guided by results from existing literature as\nwell as new experiments investigating open questions.\n","authors":["Yuling Gu","Oyvind Tafjord","Bailey Kuehl","Dany Haddad","Jesse Dodge","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2406.08446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08434v1","updated":"2024-06-12T17:21:21Z","published":"2024-06-12T17:21:21Z","title":"TasTe: Teaching Large Language Models to Translate through\n  Self-Reflection","summary":"  Large language models (LLMs) have exhibited remarkable performance in various\nnatural language processing tasks. Techniques like instruction tuning have\neffectively enhanced the proficiency of LLMs in the downstream task of machine\ntranslation. However, the existing approaches fail to yield satisfactory\ntranslation outputs that match the quality of supervised neural machine\ntranslation (NMT) systems. One plausible explanation for this discrepancy is\nthat the straightforward prompts employed in these methodologies are unable to\nfully exploit the acquired instruction-following capabilities. To this end, we\npropose the TasTe framework, which stands for translating through\nself-reflection. The self-reflection process includes two stages of inference.\nIn the first stage, LLMs are instructed to generate preliminary translations\nand conduct self-assessments on these translations simultaneously. In the\nsecond stage, LLMs are tasked to refine these preliminary translations\naccording to the evaluation results. The evaluation results in four language\ndirections on the WMT22 benchmark reveal the effectiveness of our approach\ncompared to existing methods. Our work presents a promising approach to unleash\nthe potential of LLMs and enhance their capabilities in MT. The codes and\ndatasets are open-sourced at https://github.com/YutongWang1216/ReflectionLLMMT.\n","authors":["Yutong Wang","Jiali Zeng","Xuebo Liu","Fandong Meng","Jie Zhou","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08434v1.pdf","comment":"This paper has been accepted to the ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2406.08426v1","updated":"2024-06-12T17:13:17Z","published":"2024-06-12T17:13:17Z","title":"Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL","summary":"  Generating accurate SQL according to natural language questions (text-to-SQL)\nis a long-standing problem since it is challenging in user question\nunderstanding, database schema comprehension, and SQL generation. Conventional\ntext-to-SQL systems include human engineering and deep neural networks.\nSubsequently, pre-trained language models (PLMs) have been developed and\nutilized for text-to-SQL tasks, achieving promising performance. As modern\ndatabases become more complex and corresponding user questions more\nchallenging, PLMs with limited comprehension capabilities can lead to incorrect\nSQL generation. This necessitates more sophisticated and tailored optimization\nmethods, which, in turn, restricts the applications of PLM-based systems. Most\nrecently, large language models (LLMs) have demonstrated significant abilities\nin natural language understanding as the model scale remains increasing.\nTherefore, integrating the LLM-based implementation can bring unique\nopportunities, challenges, and solutions to text-to-SQL research. In this\nsurvey, we present a comprehensive review of LLM-based text-to-SQL.\nSpecifically, we propose a brief overview of the current challenges and the\nevolutionary process of text-to-SQL. Then, we provide a detailed introduction\nto the datasets and metrics designed to evaluate text-to-SQL systems. After\nthat, we present a systematic analysis of recent advances in LLM-based\ntext-to-SQL. Finally, we discuss the remaining challenges in this field and\npropose expectations for future directions.\n","authors":["Zijin Hong","Zheng Yuan","Qinggang Zhang","Hao Chen","Junnan Dong","Feiran Huang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.08426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08411v1","updated":"2024-06-12T16:57:28Z","published":"2024-06-12T16:57:28Z","title":"Tailoring Generative AI Chatbots for Multiethnic Communities in Disaster\n  Preparedness Communication: Extending the CASA Paradigm","summary":"  This study is among the first to develop different prototypes of generative\nAI (GenAI) chatbots powered by GPT 4 to communicate hurricane preparedness\ninformation to diverse residents. Drawing from the Computers Are Social Actors\n(CASA) paradigm and the literature on disaster vulnerability and cultural\ntailoring, this study conducted a between-subjects experiment with 441 Black,\nHispanic, and Caucasian residents of Florida. A computational analysis of chat\nlogs (N = 7,848) shows that anthropomorphism and personalization are key\ncommunication topics in GenAI chatbot-user interactions. SEM results (N = 441)\nsuggest that GenAI chatbots varying in tone formality and cultural tailoring\nsignificantly predict bot perceptions and, subsequently, hurricane preparedness\noutcomes. These results highlight the potential of using GenAI chatbots to\nimprove diverse communities' disaster preparedness.\n","authors":["Xinyan Zhao","Yuan Sun","Wenlin Liu","Chau-Wai Wong"],"pdf_url":"https://arxiv.org/pdf/2406.08411v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2406.08407v1","updated":"2024-06-12T16:54:54Z","published":"2024-06-12T16:54:54Z","title":"MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation\n  in Videos","summary":"  Multimodal Language Language Models (MLLMs) demonstrate the emerging\nabilities of \"world models\" -- interpreting and reasoning about complex\nreal-world dynamics. To assess these abilities, we posit videos are the ideal\nmedium, as they encapsulate rich representations of real-world dynamics and\ncausalities. To this end, we introduce MMWorld, a new benchmark for\nmulti-discipline, multi-faceted multimodal video understanding. MMWorld\ndistinguishes itself from previous video understanding benchmarks with two\nunique advantages: (1) multi-discipline, covering various disciplines that\noften require domain expertise for comprehensive understanding; (2)\nmulti-faceted reasoning, including explanation, counterfactual thinking, future\nprediction, etc. MMWorld consists of a human-annotated dataset to evaluate\nMLLMs with questions about the whole videos and a synthetic dataset to analyze\nMLLMs within a single modality of perception. Together, MMWorld encompasses\n1,910 videos across seven broad disciplines and 69 subdisciplines, complete\nwith 6,627 question-answer pairs and associated captions. The evaluation\nincludes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld\n(e.g., GPT-4V performs the best with only 52.3\\% accuracy), showing large room\nfor improvement. Further ablation studies reveal other interesting findings\nsuch as models' different skill sets from humans. We hope MMWorld can serve as\nan essential step towards world model evaluation in videos.\n","authors":["Xuehai He","Weixi Feng","Kaizhi Zheng","Yujie Lu","Wanrong Zhu","Jiachen Li","Yue Fan","Jianfeng Wang","Linjie Li","Zhengyuan Yang","Kevin Lin","William Yang Wang","Lijuan Wang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08402v1","updated":"2024-06-12T16:51:54Z","published":"2024-06-12T16:51:54Z","title":"Understanding Sounds, Missing the Questions: The Challenge of Object\n  Hallucination in Large Audio-Language Models","summary":"  Large audio-language models (LALMs) enhance traditional large language models\nby integrating audio perception capabilities, allowing them to tackle\naudio-related tasks. Previous research has primarily focused on assessing the\nperformance of LALMs across various tasks, yet overlooking their reliability,\nparticularly concerning issues like object hallucination. In our study, we\nintroduce methods to assess the extent of object hallucination of publicly\navailable LALMs. Our findings reveal that LALMs are comparable to specialized\naudio captioning models in their understanding of audio content, but struggle\nto answer discriminative questions, specifically those requiring the\nidentification of the presence of particular object sounds within an audio\nclip. This limitation highlights a critical weakness in current LALMs: their\ninadequate understanding of discriminative queries. Moreover, we explore the\npotential of prompt engineering to enhance LALMs' performance on discriminative\nquestions.\n","authors":["Chun-Yi Kuan","Wei-Ping Huang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2406.08402v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.08398v1","updated":"2024-06-12T16:46:12Z","published":"2024-06-12T16:46:12Z","title":"cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations\n  in Scientific Papers","summary":"  An emerging area of research in situated and multimodal interactive\nconversations (SIMMC) includes interactions in scientific papers. Since\nscientific papers are primarily composed of text, equations, figures, and\ntables, SIMMC methods must be developed specifically for each component to\nsupport the depth of inquiry and interactions required by research scientists.\nThis work introduces Conversational Papers (cPAPERS), a dataset of\nconversational question-answer pairs from reviews of academic papers grounded\nin these paper components and their associated references from scientific\ndocuments available on arXiv. We present a data collection strategy to collect\nthese question-answer pairs from OpenReview and associate them with contextual\ninformation from LaTeX source files. Additionally, we present a series of\nbaseline approaches utilizing Large Language Models (LLMs) in both zero-shot\nand fine-tuned configurations to address the cPAPERS dataset.\n","authors":["Anirudh Sundar","Jin Xu","William Gay","Christopher Richardson","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2406.08398v1.pdf","comment":"14 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.08391v1","updated":"2024-06-12T16:41:31Z","published":"2024-06-12T16:41:31Z","title":"Large Language Models Must Be Taught to Know What They Don't Know","summary":"  When using large language models (LLMs) in high-stakes applications, we need\nto know when we can trust their predictions. Some works argue that prompting\nhigh-performance LLMs is sufficient to produce calibrated uncertainties, while\nothers introduce sampling methods that can be prohibitively expensive. In this\nwork, we first argue that prompting on its own is insufficient to achieve good\ncalibration and then show that fine-tuning on a small dataset of correct and\nincorrect answers can create an uncertainty estimate with good generalization\nand small computational overhead. We show that a thousand graded examples are\nsufficient to outperform baseline methods and that training through the\nfeatures of a model is necessary for good performance and tractable for large\nopen-source models when using LoRA. We also investigate the mechanisms that\nenable reliable LLM uncertainty estimation, finding that many models can be\nused as general-purpose uncertainty estimators, applicable not just to their\nown uncertainties but also the uncertainty of other models. Lastly, we show\nthat uncertainty estimates inform human use of LLMs in human-AI collaborative\nsettings through a user study.\n","authors":["Sanyam Kapoor","Nate Gruver","Manley Roberts","Katherine Collins","Arka Pal","Umang Bhatt","Adrian Weller","Samuel Dooley","Micah Goldblum","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2406.08391v1.pdf","comment":"Code available at:\n  https://github.com/activatedgeek/calibration-tuning"},{"id":"http://arxiv.org/abs/2406.08380v1","updated":"2024-06-12T16:30:58Z","published":"2024-06-12T16:30:58Z","title":"Towards Unsupervised Speech Recognition Without Pronunciation Models","summary":"  Recent advancements in supervised automatic speech recognition (ASR) have\nachieved remarkable performance, largely due to the growing availability of\nlarge transcribed speech corpora. However, most languages lack sufficient\npaired speech and text data to effectively train these systems. In this\narticle, we tackle the challenge of developing ASR systems without paired\nspeech and text corpora by proposing the removal of reliance on a phoneme\nlexicon. We explore a new research direction: word-level unsupervised ASR.\nUsing a curated speech corpus containing only high-frequency English words, our\nsystem achieves a word error rate of nearly 20% without parallel transcripts or\noracle word boundaries. Furthermore, we experimentally demonstrate that an\nunsupervised speech recognizer can emerge from joint speech-to-speech and\ntext-to-text masked token-infilling. This innovative model surpasses the\nperformance of previous unsupervised ASR models trained with direct\ndistribution matching.\n","authors":["Junrui Ni","Liming Wang","Yang Zhang","Kaizhi Qian","Heting Gao","Mark Hasegawa-Johnson","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2406.08380v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2405.11966v3","updated":"2024-06-12T16:05:40Z","published":"2024-05-20T11:47:13Z","title":"Multiple-Choice Questions are Efficient and Robust LLM Evaluators","summary":"  We present GSM-MC and MATH-MC, two multiple-choice (MC) datasets constructed\nby collecting answers and incorrect predictions on GSM8K and MATH from 60\nopen-source models. Through extensive experiments, we show that LLMs'\nperformance on the MC versions of these two popular benchmarks is strongly\ncorrelated with their performance on the original versions and is quite robust\nto distractor choices and option orders, while the evaluation time is reduced\nby a factor of up to 30. Following a similar procedure, we introduce PythonIO,\na new program output prediction MC dataset constructed from two other popular\nLLM evaluation benchmarks, HumanEval and MBPP. Our data and code are available\nat https://github.com/Geralt-Targaryen/MC-Evaluation.\n","authors":["Ziyin Zhang","Lizhen Xu","Zhaokun Jiang","Hongkun Hao","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2405.11966v3.pdf","comment":"data at https://github.com/Geralt-Targaryen/MC-Evaluation"},{"id":"http://arxiv.org/abs/2402.04978v2","updated":"2024-06-12T16:03:31Z","published":"2024-02-07T15:56:17Z","title":"An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge\n  Graph-Integrated Collaboration","summary":"  While Large Language Models (LLMs) demonstrate exceptional performance in a\nmultitude of Natural Language Processing (NLP) tasks, they encounter challenges\nin practical applications, including issues with hallucinations, inadequate\nknowledge updating, and limited transparency in the reasoning process. To\novercome these limitations, this study innovatively proposes a collaborative\ntraining-free reasoning scheme involving tight cooperation between Knowledge\nGraph (KG) and LLMs. This scheme first involves using LLMs to iteratively\nexplore KG, selectively retrieving a task-relevant knowledge subgraph to\nsupport reasoning. The LLMs are then guided to further combine inherent\nimplicit knowledge to reason on the subgraph while explicitly elucidating the\nreasoning process. Through such a cooperative approach, our scheme achieves\nmore reliable knowledge-based reasoning and facilitates the tracing of the\nreasoning results. Experimental results show that our scheme significantly\nprogressed across multiple datasets, notably achieving over a 10% improvement\non the QALD10 dataset compared to the best baseline and the fine-tuned\nstate-of-the-art (SOTA) work. Building on this success, this study hopes to\noffer a valuable reference for future research in the fusion of KG and LLMs,\nthereby enhancing LLMs' proficiency in solving complex issues.\n","authors":["Yihao Li","Ru Zhang","Jianyi Liu"],"pdf_url":"https://arxiv.org/pdf/2402.04978v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09151v2","updated":"2024-06-12T16:03:31Z","published":"2024-02-14T13:08:25Z","title":"Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for\n  Chinese Mental Health Text Analysis","summary":"  In the current environment, psychological issues are prevalent and\nwidespread, with social media serving as a key outlet for individuals to share\ntheir feelings. This results in the generation of vast quantities of data\ndaily, where negative emotions have the potential to precipitate crisis\nsituations. There is a recognized need for models capable of efficient\nanalysis. While pre-trained language models have demonstrated their\neffectiveness broadly, there's a noticeable gap in pre-trained models tailored\nfor specialized domains like psychology. To address this, we have collected a\nhuge dataset from Chinese social media platforms and enriched it with publicly\navailable datasets to create a comprehensive database encompassing 3.36 million\ntext entries. To enhance the model's applicability to psychological text\nanalysis, we integrated psychological lexicons into the pre-training masking\nmechanism. Building on an existing Chinese language model, we performed\nadaptive training to develop a model specialized for the psychological domain.\nWe evaluated our model's performance across six public datasets, where it\ndemonstrated improvements compared to eight other models. Additionally, in the\nqualitative comparison experiment, our model provided psychologically relevant\npredictions given the masked sentences. Due to concerns regarding data privacy,\nthe dataset will not be made publicly available. However, we have made the\npre-trained models and codes publicly accessible to the community via:\nhttps://github.com/zwzzzQAQ/Chinese-MentalBERT.\n","authors":["Wei Zhai","Hongzhi Qi","Qing Zhao","Jianqiang Li","Ziqi Wang","Han Wang","Bing Xiang Yang","Guanghui Fu"],"pdf_url":"https://arxiv.org/pdf/2402.09151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08353v1","updated":"2024-06-12T15:59:25Z","published":"2024-06-12T15:59:25Z","title":"Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study\n  on Word Error Rate and Fusion Techniques","summary":"  Text data is commonly utilized as a primary input to enhance Speech Emotion\nRecognition (SER) performance and reliability. However, the reliance on\nhuman-transcribed text in most studies impedes the development of practical SER\nsystems, creating a gap between in-lab research and real-world scenarios where\nAutomatic Speech Recognition (ASR) serves as the text source. Hence, this study\nbenchmarks SER performance using ASR transcripts with varying Word Error Rates\n(WERs) on well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our\nevaluation includes text-only and bimodal SER with diverse fusion techniques,\naiming for a comprehensive analysis that uncovers novel findings and challenges\nfaced by current SER research. Additionally, we propose a unified ASR\nerror-robust framework integrating ASR error correction and modality-gated\nfusion, achieving lower WER and higher SER results compared to the\nbest-performing ASR transcript. This research is expected to provide insights\ninto SER with ASR assistance, especially for real-world applications.\n","authors":["Yuanchao Li","Peter Bell","Catherine Lai"],"pdf_url":"https://arxiv.org/pdf/2406.08353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20267v3","updated":"2024-06-12T15:53:49Z","published":"2024-05-30T17:19:19Z","title":"Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles\n  and Committee Discussions","summary":"  As LLMs evolve on a daily basis, there is an urgent need for a trustworthy\nevaluation method that can provide robust evaluation results in a timely\nfashion. Currently, as static benchmarks are prone to contamination concerns,\nusers tend to trust human voting platforms, such as Chatbot Arena. However,\nhuman annotations require extensive manual efforts. To provide an automatic,\nrobust, and trustworthy evaluation framework, we innovatively propose the\nAuto-Arena of LLMs, which automates the entire evaluation process with LLM\nagents. Firstly, an examiner LLM devises queries. Then, a pair of candidate\nLLMs engage in a multi-round peer-battle around the query, during which the\nLLM's true performance gaps become visible. Finally, a committee of LLM judges\ncollectively discuss and determine the winner, which alleviates bias and\npromotes fairness. In our extensive experiment on the 17 newest LLMs,\nAuto-Arena shows the highest correlation with human preferences, providing a\npromising alternative to human evaluation platforms.\n","authors":["Ruochen Zhao","Wenxuan Zhang","Yew Ken Chia","Deli Zhao","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2405.20267v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06591v2","updated":"2024-06-12T15:19:46Z","published":"2024-06-05T16:11:55Z","title":"Exploring Multilingual Large Language Models for Enhanced TNM\n  classification of Radiology Report in lung cancer staging","summary":"  Background: Structured radiology reports remains underdeveloped due to\nlabor-intensive structuring and narrative-style reporting. Deep learning,\nparticularly large language models (LLMs) like GPT-3.5, offers promise in\nautomating the structuring of radiology reports in natural languages. However,\nalthough it has been reported that LLMs are less effective in languages other\nthan English, their radiological performance has not been extensively studied.\nPurpose: This study aimed to investigate the accuracy of TNM classification\nbased on radiology reports using GPT3.5-turbo (GPT3.5) and the utility of\nmultilingual LLMs in both Japanese and English. Material and Methods: Utilizing\nGPT3.5, we developed a system to automatically generate TNM classifications\nfrom chest CT reports for lung cancer and evaluate its performance. We\nstatistically analyzed the impact of providing full or partial TNM definitions\nin both languages using a Generalized Linear Mixed Model. Results: Highest\naccuracy was attained with full TNM definitions and radiology reports in\nEnglish (M = 94%, N = 80%, T = 47%, and ALL = 36%). Providing definitions for\neach of the T, N, and M factors statistically improved their respective\naccuracies (T: odds ratio (OR) = 2.35, p < 0.001; N: OR = 1.94, p < 0.01; M: OR\n= 2.50, p < 0.001). Japanese reports exhibited decreased N and M accuracies (N\naccuracy: OR = 0.74 and M accuracy: OR = 0.21). Conclusion: This study\nunderscores the potential of multilingual LLMs for automatic TNM classification\nin radiology reports. Even without additional model training, performance\nimprovements were evident with the provided TNM definitions, indicating LLMs'\nrelevance in radiology contexts.\n","authors":["Hidetoshi Matsuo","Mizuho Nishio","Takaaki Matsunaga","Koji Fujimoto","Takamichi Murakami"],"pdf_url":"https://arxiv.org/pdf/2406.06591v2.pdf","comment":"16 pages, 3figures"},{"id":"http://arxiv.org/abs/2406.08316v1","updated":"2024-06-12T15:16:40Z","published":"2024-06-12T15:16:40Z","title":"Is Programming by Example solved by LLMs?","summary":"  Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n`solved' PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short.\n","authors":["Wen-Ding Li","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2406.08316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06910v2","updated":"2024-06-12T15:05:40Z","published":"2024-06-11T03:09:20Z","title":"Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large\n  Language Models","summary":"  Simultaneous Machine Translation (SiMT) generates target translations while\nreading the source sentence. It relies on a policy to determine the optimal\ntiming for reading sentences and generating translations. Existing SiMT methods\ngenerally adopt the traditional Transformer architecture, which concurrently\ndetermines the policy and generates translations. While they excel at\ndetermining policies, their translation performance is suboptimal. Conversely,\nLarge Language Models (LLMs), trained on extensive corpora, possess superior\ngeneration capabilities, but it is difficult for them to acquire translation\npolicy through the training methods of SiMT. Therefore, we introduce\nAgent-SiMT, a framework combining the strengths of LLMs and traditional SiMT\nmethods. Agent-SiMT contains the policy-decision agent and the translation\nagent. The policy-decision agent is managed by a SiMT model, which determines\nthe translation policy using partial source sentence and translation. The\ntranslation agent, leveraging an LLM, generates translation based on the\npartial source sentence. The two agents collaborate to accomplish SiMT.\nExperiments demonstrate that Agent-SiMT attains state-of-the-art performance.\n","authors":["Shoutao Guo","Shaolei Zhang","Zhengrui Ma","Min Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2406.06910v2.pdf","comment":"18 pages, 8 figures, 7 tables. v2 of arXiv:2402.13036"},{"id":"http://arxiv.org/abs/2312.02219v2","updated":"2024-06-12T14:59:55Z","published":"2023-12-03T16:39:36Z","title":"Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large\n  Image-Language Models","summary":"  Large Vision and Language Models have enabled significant advances in fully\nsupervised and zero-shot visual tasks. These large architectures serve as the\nbaseline to what is currently known as Instruction Tuning Large Vision and\nLanguage models (IT-LVLMs). IT-LVLMs are general-purpose multi-modal assistants\nwhose responses are modulated by natural language instructions and visual data.\nDespite this versatility, IT-LVLM effectiveness in fundamental computer vision\nproblems remains unclear, primarily due to the absence of a standardized\nevaluation benchmark. This paper introduces a Multi-modal Evaluation Benchmark\nnamed MERLIM, a scalable test-bed to assess the capabilities of IT-LVLMs on\nfundamental computer vision tasks. MERLIM contains over 300K image-question\npairs and has a strong focus on detecting cross-modal \"hallucination\" events in\nIT-LVLMs. Our results bring important insights on the performance of\nstate-of-the-art IT-LVMLs including limitations at identifying fine-grained\nvisual concepts, object hallucinations across tasks, and biases towards the\nlanguage query. Our findings also suggest that these models have weak visual\ngrounding, but manage to make adequate guesses from global visual patterns or\nlanguage biases contained in the LLM component.\n","authors":["Andrés Villa","Juan Carlos León Alcázar","Alvaro Soto","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2312.02219v2.pdf","comment":"16 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.08255v1","updated":"2024-06-12T14:28:25Z","published":"2024-06-12T14:28:25Z","title":"M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine\n  Translation","summary":"  Document translation poses a challenge for Neural Machine Translation (NMT)\nsystems. Most document-level NMT systems rely on meticulously curated\nsentence-level parallel data, assuming flawless extraction of text from\ndocuments along with their precise reading order. These systems also tend to\ndisregard additional visual cues such as the document layout, deeming it\nirrelevant. However, real-world documents often possess intricate text layouts\nthat defy these assumptions. Extracting information from Optical Character\nRecognition (OCR) or heuristic rules can result in errors, and the layout\n(e.g., paragraphs, headers) may convey relationships between distant sections\nof text. This complexity is particularly evident in widely used PDF documents,\nwhich represent information visually. This paper addresses this gap by\nintroducing M3T, a novel benchmark dataset tailored to evaluate NMT systems on\nthe comprehensive task of translating semi-structured documents. This dataset\naims to bridge the evaluation gap in document-level NMT systems, acknowledging\nthe challenges posed by rich text layouts in real-world applications.\n","authors":["Benjamin Hsu","Xiaoyu Liu","Huayang Li","Yoshinari Fujinuma","Maria Nadejde","Xing Niu","Yair Kittenplon","Ron Litman","Raghavendra Pappagari"],"pdf_url":"https://arxiv.org/pdf/2406.08255v1.pdf","comment":"NAACL 2024, dataset at\n  https://github.com/amazon-science/m3t-multi-modal-translation-bench"},{"id":"http://arxiv.org/abs/2302.04914v3","updated":"2024-06-12T14:25:15Z","published":"2023-02-09T19:56:37Z","title":"Flexible, Model-Agnostic Method for Materials Data Extraction from Text\n  Using General Purpose Language Models","summary":"  Accurate and comprehensive material databases extracted from research papers\nare crucial for materials science and engineering, but their development\nrequires significant human effort. With large language models (LLMs)\ntransforming the way humans interact with text, LLMs provide an opportunity to\nrevolutionize data extraction. In this study, we demonstrate a simple and\nefficient method for extracting materials data from full-text research papers\nleveraging the capabilities of LLMs combined with human supervision. This\napproach is particularly suitable for mid-sized databases and requires minimal\nto no coding or prior knowledge about the extracted property. It offers high\nrecall and nearly perfect precision in the resulting database. The method is\neasily adaptable to new and superior language models, ensuring continued\nutility. We show this by evaluating and comparing its performance on GPT-3 and\nGPT-3.5/4 (which underlie ChatGPT), as well as free alternatives such as BART\nand DeBERTaV3. We provide a detailed analysis of the method's performance in\nextracting sentences containing bulk modulus data, achieving up to 90%\nprecision at 96% recall, depending on the amount of human effort involved. We\nfurther demonstrate the method's broader effectiveness by developing a database\nof critical cooling rates for metallic glasses over twice the size of previous\nhuman curated databases.\n","authors":["Maciej P. Polak","Shrey Modi","Anna Latosinska","Jinming Zhang","Ching-Wen Wang","Shaonan Wang","Ayan Deep Hazra","Dane Morgan"],"pdf_url":"https://arxiv.org/pdf/2302.04914v3.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.08246v1","updated":"2024-06-12T14:15:15Z","published":"2024-06-12T14:15:15Z","title":"Leveraging Large Language Models for Web Scraping","summary":"  Large Language Models (LLMs) demonstrate remarkable capabilities in\nreplicating human tasks and boosting productivity. However, their direct\napplication for data extraction presents limitations due to a prioritisation of\nfluency over factual accuracy and a restricted ability to manipulate specific\ninformation. Therefore to overcome these limitations, this research leverages\nthe knowledge representation power of pre-trained LLMs and the targeted\ninformation access enabled by RAG models, this research investigates a\ngeneral-purpose accurate data scraping recipe for RAG models designed for\nlanguage generation. To capture knowledge in a more modular and interpretable\nway, we use pre trained language models with a latent knowledge retriever,\nwhich allows the model to retrieve and attend over documents from a large\ncorpus. We utilised RAG model architecture and did an in-depth analysis of\ntheir capabilities under three tasks: (i) Semantic Classification of HTML\nelements, (ii) Chunking HTML text for effective understanding, and (iii)\ncomparing results from different LLMs and ranking algorithms. While previous\nwork has developed dedicated architectures and training procedures for HTML\nunderstanding and extraction, we show that LLMs pre-trained on standard natural\nlanguage with an addition of effective chunking, searching and ranking\nalgorithms, can prove to be efficient data scraping tool to extract complex\ndata from unstructured text. Future research directions include addressing the\nchallenges of provenance tracking and dynamic knowledge updates within the\nproposed RAG-based data extraction framework. By overcoming these limitations,\nthis approach holds the potential to revolutionise data extraction from vast\nrepositories of textual information.\n","authors":["Aman Ahluwalia","Suhrud Wani"],"pdf_url":"https://arxiv.org/pdf/2406.08246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17123v2","updated":"2024-06-12T14:12:17Z","published":"2024-04-26T02:40:03Z","title":"Text Sentiment Analysis and Classification Based on Bidirectional Gated\n  Recurrent Units (GRUs) Model","summary":"  This paper explores the importance of text sentiment analysis and\nclassification in the field of natural language processing, and proposes a new\napproach to sentiment analysis and classification based on the bidirectional\ngated recurrent units (GRUs) model. The study firstly analyses the word cloud\nmodel of the text with six sentiment labels, and then carries out data\npreprocessing, including the steps of removing special symbols, punctuation\nmarks, numbers, stop words and non-alphabetic parts. Subsequently, the data set\nis divided into training set and test set, and through model training and\ntesting, it is found that the accuracy of the validation set is increased from\n85% to 93% with training, which is an increase of 8%; at the same time, the\nloss value of the validation set decreases from 0.7 to 0.1 and tends to be\nstable, and the model is gradually close to the actual value, which can\neffectively classify the text emotions. The confusion matrix shows that the\naccuracy of the model on the test set reaches 94.8%, the precision is 95.9%,\nthe recall is 99.1%, and the F1 score is 97.4%, which proves that the model has\ngood generalisation ability and classification effect. Overall, the study\ndemonstrated an effective method for text sentiment analysis and classification\nwith satisfactory results.\n","authors":["Wei Xu","Jianlong Chen","Zhicheng Ding","Jinyin Wang"],"pdf_url":"https://arxiv.org/pdf/2404.17123v2.pdf","comment":"accepted by the 2nd International Conference on Software Engineering\n  and Machine Learning (CONF-SEML 2024)"},{"id":"http://arxiv.org/abs/2404.02677v2","updated":"2024-06-12T14:05:43Z","published":"2024-04-03T12:20:51Z","title":"The VoicePrivacy 2024 Challenge Evaluation Plan","summary":"  The task of the challenge is to develop a voice anonymization system for\nspeech data which conceals the speaker's voice identity while protecting\nlinguistic content and emotional states. The organizers provide development and\nevaluation datasets and evaluation scripts, as well as baseline anonymization\nsystems and a list of training resources formed on the basis of the\nparticipants' requests. Participants apply their developed anonymization\nsystems, run evaluation scripts and submit evaluation results and anonymized\nspeech data to the organizers. Results will be presented at a workshop held in\nconjunction with Interspeech 2024 to which all participants are invited to\npresent their challenge systems and to submit additional workshop papers.\n","authors":["Natalia Tomashenko","Xiaoxiao Miao","Pierre Champion","Sarina Meyer","Xin Wang","Emmanuel Vincent","Michele Panariello","Nicholas Evans","Junichi Yamagishi","Massimiliano Todisco"],"pdf_url":"https://arxiv.org/pdf/2404.02677v2.pdf","comment":"19 pages, https://www.voiceprivacychallenge.org/. arXiv admin note:\n  substantial text overlap with arXiv:2203.12468"},{"id":"http://arxiv.org/abs/2406.06581v2","updated":"2024-06-12T13:59:13Z","published":"2024-06-04T16:09:13Z","title":"Set-Based Prompting: Provably Solving the Language Model Order\n  Dependency Problem","summary":"  The development of generative language models that can create long and\ncoherent textual outputs via autoregression has lead to a proliferation of uses\nand a corresponding sweep of analyses as researches work to determine the\nlimitations of this new paradigm. Unlike humans, these 'Large Language Models'\n(LLMs) are highly sensitive to small changes in their inputs, leading to\nunwanted inconsistency in their behavior. One problematic inconsistency when\nLLMs are used to answer multiple-choice questions or analyze multiple inputs is\norder dependency: the output of an LLM can (and often does) change\nsignificantly when sub-sequences are swapped, despite both orderings being\nsemantically identical. In this paper we present Set-Based Prompting, a\ntechnique that guarantees the output of an LLM will not have order dependence\non a specified set of sub-sequences. We show that this method provably\neliminates order dependency, and that it can be applied to any\ntransformer-based LLM to enable text generation that is unaffected by\nre-orderings. Delving into the implications of our method, we show that,\ndespite our inputs being out of distribution, the impact on expected accuracy\nis small, where the expectation is over the order of uniformly chosen shuffling\nof the candidate responses, and usually significantly less in practice. Thus,\nSet-Based Prompting can be used as a 'dropped-in' method on fully trained\nmodels. Finally, we discuss how our method's success suggests that other strong\nguarantees can be obtained on LLM performance via modifying the input\nrepresentations.\n","authors":["Reid McIlroy-Young","Katrina Brown","Conlan Olson","Linjun Zhang","Cynthia Dwork"],"pdf_url":"https://arxiv.org/pdf/2406.06581v2.pdf","comment":"29 pages, 27 figures, code\n  https://github.com/reidmcy/set-based-prompting"},{"id":"http://arxiv.org/abs/2406.08223v1","updated":"2024-06-12T13:52:38Z","published":"2024-06-12T13:52:38Z","title":"Research Trends for the Interplay between Large Language Models and\n  Knowledge Graphs","summary":"  This survey investigates the synergistic relationship between Large Language\nModels (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's\ncapabilities in understanding, reasoning, and language processing. It aims to\naddress gaps in current research by exploring areas such as KG Question\nAnswering, ontology generation, KG validation, and the enhancement of KG\naccuracy and consistency through LLMs. The paper further examines the roles of\nLLMs in generating descriptive texts and natural language queries for KGs.\nThrough a structured analysis that includes categorizing LLM-KG interactions,\nexamining methodologies, and investigating collaborative uses and potential\nbiases, this study seeks to provide new insights into the combined potential of\nLLMs and KGs. It highlights the importance of their interaction for improving\nAI applications and outlines future research directions.\n","authors":["Hanieh Khorashadizadeh","Fatima Zahra Amara","Morteza Ezzabady","Frédéric Ieng","Sanju Tiwari","Nandana Mihindukulasooriya","Jinghua Groppe","Soror Sahri","Farah Benamara","Sven Groppe"],"pdf_url":"https://arxiv.org/pdf/2406.08223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08218v1","updated":"2024-06-12T13:49:38Z","published":"2024-06-12T13:49:38Z","title":"Figuratively Speaking: Authorship Attribution via Multi-Task Figurative\n  Language Modeling","summary":"  The identification of Figurative Language (FL) features in text is crucial\nfor various Natural Language Processing (NLP) tasks, where understanding of the\nauthor's intended meaning and its nuances is key for successful communication.\nAt the same time, the use of a specific blend of various FL forms most\naccurately reflects a writer's style, rather than the use of any single\nconstruct, such as just metaphors or irony. Thus, we postulate that FL features\ncould play an important role in Authorship Attribution (AA) tasks. We believe\nthat our is the first computational study of AA based on FL use. Accordingly,\nwe propose a Multi-task Figurative Language Model (MFLM) that learns to detect\nmultiple FL features in text at once. We demonstrate, through detailed\nevaluation across multiple test sets, that the our model tends to perform\nequally or outperform specialized binary models in FL detection. Subsequently,\nwe evaluate the predictive capability of joint FL features towards the AA task\non three datasets, observing improved AA performance through the integration of\nMFLM embeddings.\n","authors":["Gregorios A Katsios","Ning Sa","Tomek Strzalkowski"],"pdf_url":"https://arxiv.org/pdf/2406.08218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08215v1","updated":"2024-06-12T13:44:58Z","published":"2024-06-12T13:44:58Z","title":"SumHiS: Extractive Summarization Exploiting Hidden Structure","summary":"  Extractive summarization is a task of highlighting the most important parts\nof the text. We introduce a new approach to extractive summarization task using\nhidden clustering structure of the text. Experimental results on CNN/DailyMail\ndemonstrate that our approach generates more accurate summaries than both\nextractive and abstractive methods, achieving state-of-the-art results in terms\nof ROUGE-2 metric exceeding the previous approaches by 10%. Additionally, we\nshow that hidden structure of the text could be interpreted as aspects.\n","authors":["Tikhonov Pavel","Anastasiya Ianina","Valentin Malykh"],"pdf_url":"https://arxiv.org/pdf/2406.08215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08700v2","updated":"2024-06-12T13:44:10Z","published":"2024-04-10T18:08:59Z","title":"DyKnow:Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs","summary":"  LLMs acquire knowledge from massive data snapshots collected at different\ntimestamps. Their knowledge is then commonly evaluated using static benchmarks.\nHowever, factual knowledge is generally subject to time-sensitive changes, and\nstatic benchmarks cannot address those cases. We present an approach to\ndynamically evaluate the knowledge in LLMs and their time-sensitiveness against\nWikidata, a publicly available up-to-date knowledge graph. We evaluate the\ntime-sensitive knowledge in twenty-four private and open-source LLMs, as well\nas the effectiveness of four editing methods in updating the outdated facts.\nOur results show that 1) outdatedness is a critical problem across\nstate-of-the-art LLMs; 2) LLMs output inconsistent answers when prompted with\nslight variations of the question prompt; and 3) the performance of the\nstate-of-the-art knowledge editing algorithms is very limited, as they can not\nreduce the cases of outdatedness and output inconsistency.\n","authors":["Seyed Mahed Mousavi","Simone Alghisi","Giuseppe Riccardi"],"pdf_url":"https://arxiv.org/pdf/2404.08700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07497v2","updated":"2024-06-12T13:41:16Z","published":"2023-11-13T17:36:58Z","title":"Multilingual Nonce Dependency Treebanks: Understanding how Language\n  Models represent and process syntactic structure","summary":"  We introduce SPUD (Semantically Perturbed Universal Dependencies), a\nframework for creating nonce treebanks for the multilingual Universal\nDependencies (UD) corpora. SPUD data satisfies syntactic argument structure,\nprovides syntactic annotations, and ensures grammaticality via\nlanguage-specific rules. We create nonce data in Arabic, English, French,\nGerman, and Russian, and demonstrate two use cases of SPUD treebanks. First, we\ninvestigate the effect of nonce data on word co-occurrence statistics, as\nmeasured by perplexity scores of autoregressive (ALM) and masked language\nmodels (MLM). We find that ALM scores are significantly more affected by nonce\ndata than MLM scores. Second, we show how nonce data affects the performance of\nsyntactic dependency probes. We replicate the findings of M\\\"uller-Eberstein et\nal. (2022) on nonce test data and show that the performance declines on both\nMLMs and ALMs wrt. original test data. However, a majority of the performance\nis kept, suggesting that the probe indeed learns syntax independently from\nsemantics.\n","authors":["David Arps","Laura Kallmeyer","Younes Samih","Hassan Sajjad"],"pdf_url":"https://arxiv.org/pdf/2311.07497v2.pdf","comment":"NAACL 2024. Our software is available at\n  https://github.com/davidarps/spud"},{"id":"http://arxiv.org/abs/2406.08207v1","updated":"2024-06-12T13:39:44Z","published":"2024-06-12T13:39:44Z","title":"Transformer-based Model for ASR N-Best Rescoring and Rewriting","summary":"  Voice assistants increasingly use on-device Automatic Speech Recognition\n(ASR) to ensure speed and privacy. However, due to resource constraints on the\ndevice, queries pertaining to complex information domains often require further\nprocessing by a search engine. For such applications, we propose a novel\nTransformer based model capable of rescoring and rewriting, by exploring full\ncontext of the N-best hypotheses in parallel. We also propose a new\ndiscriminative sequence training objective that can work well for both rescore\nand rewrite tasks. We show that our Rescore+Rewrite model outperforms the\nRescore-only baseline, and achieves up to an average 8.6% relative Word Error\nRate (WER) reduction over the ASR system by itself.\n","authors":["Iwen E. Kang","Christophe Van Gysel","Man-Hung Siu"],"pdf_url":"https://arxiv.org/pdf/2406.08207v1.pdf","comment":"Interspeech '24"},{"id":"http://arxiv.org/abs/2406.08202v1","updated":"2024-06-12T13:35:10Z","published":"2024-06-12T13:35:10Z","title":"A Dialogue Game for Eliciting Balanced Collaboration","summary":"  Collaboration is an integral part of human dialogue. Typical task-oriented\ndialogue games assign asymmetric roles to the participants, which limits their\nability to elicit naturalistic role-taking in collaboration and its\nnegotiation. We present a novel and simple online setup that favors balanced\ncollaboration: a two-player 2D object placement game in which the players must\nnegotiate the goal state themselves. We show empirically that human players\nexhibit a variety of role distributions, and that balanced collaboration\nimproves task performance. We also present an LLM-based baseline agent which\ndemonstrates that automatic playing of our game is an interesting challenge for\nartificial systems.\n","authors":["Isidora Jeknić","David Schlangen","Alexander Koller"],"pdf_url":"https://arxiv.org/pdf/2406.08202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12692v3","updated":"2024-06-12T13:19:55Z","published":"2024-02-20T03:39:49Z","title":"FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning","summary":"  The application of formulas is a fundamental ability of humans when\naddressing numerical reasoning problems. However, existing numerical reasoning\ndatasets seldom explicitly indicate the formulas employed during the reasoning\nsteps. To bridge this gap, we construct a dataset for formula-based numerical\nreasoning called FormulaReasoning, which consists of 5,420 reasoning-based\nquestions. We employ it to conduct evaluations of LLMs with size ranging from\n7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thought\nmethods, and we further explore using retrieval-augmented LLMs provided with an\nexternal formula database associated with our dataset. We also experiment with\nsupervised methods where we divide the reasoning process into formula\ngeneration, parameter extraction, and numerical calculation, and perform data\naugmentation. Our empirical findings underscore the significant potential for\nimprovement in existing models when applied to our complex, formula-driven\nFormulaReasoning.\n","authors":["Xiao Li","Bolin Zhu","Sichen Liu","Yin Zhu","Yiwei Liu","Gong Cheng"],"pdf_url":"https://arxiv.org/pdf/2402.12692v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08183v1","updated":"2024-06-12T13:14:19Z","published":"2024-06-12T13:14:19Z","title":"Underneath the Numbers: Quantitative and Qualitative Gender Fairness in\n  LLMs for Depression Prediction","summary":"  Recent studies show bias in many machine learning models for depression\ndetection, but bias in LLMs for this task remains unexplored. This work\npresents the first attempt to investigate the degree of gender bias present in\nexisting LLMs (ChatGPT, LLaMA 2, and Bard) using both quantitative and\nqualitative approaches. From our quantitative evaluation, we found that ChatGPT\nperforms the best across various performance metrics and LLaMA 2 outperforms\nother LLMs in terms of group fairness metrics. As qualitative fairness\nevaluation remains an open research question we propose several strategies\n(e.g., word count, thematic analysis) to investigate whether and how a\nqualitative evaluation can provide valuable insights for bias analysis beyond\nwhat is possible with quantitative evaluation. We found that ChatGPT\nconsistently provides a more comprehensive, well-reasoned explanation for its\nprediction compared to LLaMA 2. We have also identified several themes adopted\nby LLMs to qualitatively evaluate gender fairness. We hope our results can be\nused as a stepping stone towards future attempts at improving qualitative\nevaluation of fairness for LLMs especially for high-stakes tasks such as\ndepression detection.\n","authors":["Micol Spitale","Jiaee Cheong","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2406.08183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08173v1","updated":"2024-06-12T13:05:27Z","published":"2024-06-12T13:05:27Z","title":"Semi-Supervised Spoken Language Glossification","summary":"  Spoken language glossification (SLG) aims to translate the spoken language\ntext into the sign language gloss, i.e., a written record of sign language. In\nthis work, we present a framework named $S$emi-$S$upervised $S$poken $L$anguage\n$G$lossification ($S^3$LG) for SLG. To tackle the bottleneck of limited\nparallel data in SLG, our $S^3$LG incorporates large-scale monolingual spoken\nlanguage text into SLG training. The proposed framework follows the\nself-training structure that iteratively annotates and learns from pseudo\nlabels. Considering the lexical similarity and syntactic difference between\nsign language and spoken language, our $S^3$LG adopts both the rule-based\nheuristic and model-based approach for auto-annotation. During training, we\nrandomly mix these complementary synthetic datasets and mark their differences\nwith a special token. As the synthetic data may be less quality, the $S^3$LG\nfurther leverages consistency regularization to reduce the negative impact of\nnoise in the synthetic data. Extensive experiments are conducted on public\nbenchmarks to demonstrate the effectiveness of the $S^3$LG. Our code is\navailable at \\url{https://github.com/yaohj11/S3LG}.\n","authors":["Huijie Yao","Wengang Zhou","Hao Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2406.08173v1.pdf","comment":"Accepted to ACL2024 main"},{"id":"http://arxiv.org/abs/2406.08155v1","updated":"2024-06-12T12:44:48Z","published":"2024-06-12T12:44:48Z","title":"Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark","summary":"  Large Language Models~(LLMs) have become foundational in the realm of natural\nlanguage processing, demonstrating performance improvements as model sizes\nincrease. The Mixture-of-Experts~(MoE) approach offers a promising way to scale\nLLMs more efficiently by using fewer computational FLOPs through sparse\nactivation. However, it suffers from significant memory overheads,\nnecessitating model compression techniques. Post-training quantization, a\npopular method for model compression, proves less effective when directly\napplied to MoE models due to MoE's overlooked inherent sparsity. This paper\nexplores several MoE structure-aware quantization heuristics, ranging from\ncoarse to fine granularity, from MoE block to individual linear weight. Our\ninvestigations reveal critical principles: different MoE structures (i.e.,\nblocks, experts, linear layers) require varying numbers of weight bits for\neffective and efficient quantization. Conclusions are supported by extensive\nbenchmarking across two representative MoE models and six tasks. We further\nintroduce novel enhancements to more accurately identify the most critical\nweights in MoE quantization that necessitate higher bit allocations, including\nthe linear weight outlier scorer and MoE block scorer. Additionally, subsequent\nexperiments validate our findings in the context of both weight and activation\nquantization.\n","authors":["Pingzhi Li","Xiaolong Jin","Yu Cheng","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08155v1.pdf","comment":"Our code for reproducing all our experiments is provided at\n  https://github.com/UNITES-Lab/moe-quantization"},{"id":"http://arxiv.org/abs/2311.08089v2","updated":"2024-06-12T12:25:51Z","published":"2023-11-14T11:24:08Z","title":"Improving In-context Learning of Multilingual Generative Language Models\n  with Cross-lingual Alignment","summary":"  Multilingual generative models obtain remarkable cross-lingual in-context\nlearning capabilities through pre-training on large-scale corpora. However,\nthey still exhibit a performance bias toward high-resource languages and learn\nisolated distributions of multilingual sentence representations, which may\nhinder knowledge transfer across languages. To bridge this gap, we propose a\nsimple yet effective cross-lingual alignment framework exploiting pairs of\ntranslation sentences. It aligns the internal sentence representations across\ndifferent languages via multilingual contrastive learning and aligns outputs by\nfollowing cross-lingual instructions in the target language. Experimental\nresults show that even with less than 0.1 {\\textperthousand} of pre-training\ntokens, our alignment framework significantly boosts the cross-lingual\nabilities of generative language models and mitigates the performance gap.\nFurther analyses reveal that it results in a better internal multilingual\nrepresentation distribution of multilingual models.\n","authors":["Chong Li","Shaonan Wang","Jiajun Zhang","Chengqing Zong"],"pdf_url":"https://arxiv.org/pdf/2311.08089v2.pdf","comment":"NAACL 2024; Our code is available at\n  https://github.com/chongli17/CrossLingualAlignment"},{"id":"http://arxiv.org/abs/2406.08124v1","updated":"2024-06-12T12:06:32Z","published":"2024-06-12T12:06:32Z","title":"Legend: Leveraging Representation Engineering to Annotate Safety Margin\n  for Preference Datasets","summary":"  The success of the reward model in distinguishing between responses with\nsubtle safety differences depends critically on the high-quality preference\ndataset, which should capture the fine-grained nuances of harmful and harmless\nresponses. This motivates the need to develop a dataset involving preference\nmargins, which accurately quantify how harmless one response is compared to\nanother. In this paper, we take the first step to propose an effective and\ncost-efficient framework to promote the margin-enhanced preference dataset\ndevelopment. Our framework, Legend, Leverages representation engineering to\nannotate preference datasets. It constructs the specific direction within the\nLLM's embedding space that represents safety. By leveraging this safety\ndirection, Legend can then leverage the semantic distances of paired responses\nalong this direction to annotate margins automatically. We experimentally\ndemonstrate our effectiveness in both reward modeling and harmless alignment\nfor LLMs. Legend also stands out for its efficiency, requiring only the\ninference time rather than additional training. This efficiency allows for\neasier implementation and scalability, making Legend particularly valuable for\npractical applications in aligning LLMs with safe conversations.\n","authors":["Duanyu Feng","Bowen Qin","Chen Huang","Youcheng Huang","Zheng Zhang","Wenqiang Lei"],"pdf_url":"https://arxiv.org/pdf/2406.08124v1.pdf","comment":"Our code is available at https://github.com/colfeng/Legend"},{"id":"http://arxiv.org/abs/2403.01193v3","updated":"2024-06-12T12:00:52Z","published":"2024-03-02T12:19:04Z","title":"RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots","summary":"  Large language models (LLMs) like ChatGPT demonstrate the remarkable progress\nof artificial intelligence. However, their tendency to hallucinate -- generate\nplausible but false information -- poses a significant challenge. This issue is\ncritical, as seen in recent court cases where ChatGPT's use led to citations of\nnon-existent legal rulings. This paper explores how Retrieval-Augmented\nGeneration (RAG) can counter hallucinations by integrating external knowledge\nwith prompts. We empirically evaluate RAG against standard LLMs using prompts\ndesigned to induce hallucinations. Our results show that RAG increases accuracy\nin some cases, but can still be misled when prompts directly contradict the\nmodel's pre-trained understanding. These findings highlight the complex nature\nof hallucinations and the need for more robust solutions to ensure LLM\nreliability in real-world applications. We offer practical recommendations for\nRAG deployment and discuss implications for the development of more trustworthy\nLLMs.\n","authors":["Philip Feldman","James R. Foulds","Shimei Pan"],"pdf_url":"https://arxiv.org/pdf/2403.01193v3.pdf","comment":"7 Pages, 1 Figure, 1 Table"},{"id":"http://arxiv.org/abs/2406.08116v1","updated":"2024-06-12T11:52:35Z","published":"2024-06-12T11:52:35Z","title":"Supportiveness-based Knowledge Rewriting for Retrieval-augmented\n  Language Modeling","summary":"  Retrieval-augmented language models (RALMs) have recently shown great\npotential in mitigating the limitations of implicit knowledge in LLMs, such as\nuntimely updating of the latest expertise and unreliable retention of long-tail\nknowledge. However, since the external knowledge base, as well as the\nretriever, can not guarantee reliability, potentially leading to the knowledge\nretrieved not being helpful or even misleading for LLM generation. In this\npaper, we introduce Supportiveness-based Knowledge Rewriting (SKR), a robust\nand pluggable knowledge rewriter inherently optimized for LLM generation.\nSpecifically, we introduce the novel concept of \"supportiveness\"--which\nrepresents how effectively a knowledge piece facilitates downstream tasks--by\nconsidering the perplexity impact of augmented knowledge on the response text\nof a white-box LLM. Based on knowledge supportiveness, we first design a\ntraining data curation strategy for our rewriter model, effectively identifying\nand filtering out poor or irrelevant rewrites (e.g., with low supportiveness\nscores) to improve data efficacy. We then introduce the direct preference\noptimization (DPO) algorithm to align the generated rewrites to optimal\nsupportiveness, guiding the rewriter model to summarize augmented content that\nbetter improves the final response. Comprehensive evaluations across six\npopular knowledge-intensive tasks and four LLMs have demonstrated the\neffectiveness and superiority of SKR. With only 7B parameters, SKR has shown\nbetter knowledge rewriting capability over GPT-4, the current state-of-the-art\ngeneral-purpose LLM.\n","authors":["Zile Qiao","Wei Ye","Yong Jiang","Tong Mo","Pengjun Xie","Weiping Li","Fei Huang","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14517v2","updated":"2024-06-12T11:41:31Z","published":"2023-11-24T14:45:53Z","title":"tinyCLAP: Distilling Constrastive Language-Audio Pretrained Models","summary":"  Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in\nthe field of audio and speech processing. Its employment ranges from sound\nevent detection to text-to-audio generation. However, one of the main\nlimitations is the considerable amount of data required in the training process\nand the overall computational complexity during inference. This paper\ninvestigates how we can reduce the complexity of contrastive language-audio\npre-trained models, yielding an efficient model that we call tinyCLAP. We\nderive an unimodal distillation loss from first principles and explore how the\ndimensionality of the shared, multimodal latent space can be reduced via\npruning. TinyCLAP uses only 6% of the original Microsoft CLAP parameters with a\nminimal reduction (less than 5%) in zero-shot classification performance across\nthe three sound event detection datasets on which it was tested\n","authors":["Francesco Paissan","Elisabetta Farella"],"pdf_url":"https://arxiv.org/pdf/2311.14517v2.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.08101v1","updated":"2024-06-12T11:27:10Z","published":"2024-06-12T11:27:10Z","title":"CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI\n  Systems","summary":"  Conversational explainable artificial intelligence (ConvXAI) systems based on\nlarge language models (LLMs) have garnered significant interest from the\nresearch community in natural language processing (NLP) and human-computer\ninteraction (HCI). Such systems can provide answers to user questions about\nexplanations, have the potential to enhance users' comprehension and offer more\ninformation about the decision-making and generation processes of LLMs.\nCurrently available ConvXAI systems are based on intent recognition rather than\nfree chat. Thus, reliably grasping users' intentions in ConvXAI systems still\npresents a challenge, because there is a broad range of XAI methods to map\nrequests onto and each of them can have multiple slots to take care of. In\norder to bridge this gap, we present CoXQL, the first dataset for user intent\nrecognition in ConvXAI, covering 31 intents, seven of which require filling\nadditional slots. Subsequently, we enhance an existing parsing approach by\nincorporating template validations, and conduct an evaluation of several LLMs\non CoXQL using different parsing strategies. We conclude that the improved\nparsing approach (MP+) surpasses the performance of previous approaches. We\nalso discover that intents with multiple slots remain highly challenging for\nLLMs.\n","authors":["Qianli Wang","Tatiana Anikina","Nils Feldhus","Simon Ostermann","Sebastian Möller"],"pdf_url":"https://arxiv.org/pdf/2406.08101v1.pdf","comment":"4 pages, short paper"},{"id":"http://arxiv.org/abs/2406.08100v1","updated":"2024-06-12T11:27:03Z","published":"2024-06-12T11:27:03Z","title":"Multimodal Table Understanding","summary":"  Although great progress has been made by previous table understanding methods\nincluding recent approaches based on large language models (LLMs), they rely\nheavily on the premise that given tables must be converted into a certain text\nsequence (such as Markdown or HTML) to serve as model input. However, it is\ndifficult to access such high-quality textual table representations in some\nreal-world scenarios, and table images are much more accessible. Therefore, how\nto directly understand tables using intuitive visual information is a crucial\nand urgent challenge for developing more practical applications. In this paper,\nwe propose a new problem, multimodal table understanding, where the model needs\nto generate correct responses to various table-related requests based on the\ngiven table image. To facilitate both the model training and evaluation, we\nconstruct a large-scale dataset named MMTab, which covers a wide spectrum of\ntable images, instructions and tasks. On this basis, we develop Table-LLaVA, a\ngeneralist tabular multimodal large language model (MLLM), which significantly\noutperforms recent open-source MLLM baselines on 23 benchmarks under held-in\nand held-out settings. The code and data is available at this\nhttps://github.com/SpursGoZmy/Table-LLaVA\n","authors":["Mingyu Zheng","Xinwei Feng","Qingyi Si","Qiaoqiao She","Zheng Lin","Wenbin Jiang","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08100v1.pdf","comment":"23 pages, 16 figures, ACL 2024 main conference, camera-ready version"},{"id":"http://arxiv.org/abs/2406.06590v2","updated":"2024-06-12T11:18:40Z","published":"2024-06-05T15:23:11Z","title":"Are LLMs classical or nonmonotonic reasoners? Lessons from generics","summary":"  Recent scholarship on reasoning in LLMs has supplied evidence of impressive\nperformance and flexible adaptation to machine generated or human feedback.\nNonmonotonic reasoning, crucial to human cognition for navigating the real\nworld, remains a challenging, yet understudied task. In this work, we study\nnonmonotonic reasoning capabilities of seven state-of-the-art LLMs in one\nabstract and one commonsense reasoning task featuring generics, such as 'Birds\nfly', and exceptions, 'Penguins don't fly' (see Fig. 1). While LLMs exhibit\nreasoning patterns in accordance with human nonmonotonic reasoning abilities,\nthey fail to maintain stable beliefs on truth conditions of generics at the\naddition of supporting examples ('Owls fly') or unrelated information ('Lions\nhave manes'). Our findings highlight pitfalls in attributing human reasoning\nbehaviours to LLMs, as well as assessing general capabilities, while consistent\nreasoning remains elusive.\n","authors":["Alina Leidinger","Robert van Rooij","Ekaterina Shutova"],"pdf_url":"https://arxiv.org/pdf/2406.06590v2.pdf","comment":"Accepted at ACL 2024 (main)"},{"id":"http://arxiv.org/abs/2403.16554v2","updated":"2024-06-12T11:17:47Z","published":"2024-03-25T09:04:14Z","title":"PE: A Poincare Explanation Method for Fast Text Hierarchy Generation","summary":"  The black-box nature of deep learning models in NLP hinders their widespread\napplication. The research focus has shifted to Hierarchical Attribution (HA)\nfor its ability to model feature interactions. Recent works model\nnon-contiguous combinations with a time-costly greedy search in Eculidean\nspaces, neglecting underlying linguistic information in feature\nrepresentations. In this work, we introduce a novel method, namely Poincare\nExplanation (PE), for modeling feature interactions with hyperbolic spaces in a\ntime efficient manner. Specifically, we take building text hierarchies as\nfinding spanning trees in hyperbolic spaces. First we project the embeddings\ninto hyperbolic spaces to elicit inherit semantic and syntax hierarchical\nstructures. Then we propose a simple yet effective strategy to calculate\nShapley score. Finally we build the the hierarchy with proving the constructing\nprocess in the projected space could be viewed as building a minimum spanning\ntree and introduce a time efficient building algorithm. Experimental results\ndemonstrate the effectiveness of our approach.\n","authors":["Qian Chen","Dongyang Li","Xiaofeng He","Hongzhao Li","Hongyu Yi"],"pdf_url":"https://arxiv.org/pdf/2403.16554v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08092v1","updated":"2024-06-12T11:16:30Z","published":"2024-06-12T11:16:30Z","title":"Languages Transferred Within the Encoder: On Representation Transfer in\n  Zero-Shot Multilingual Translation","summary":"  Understanding representation transfer in multilingual neural machine\ntranslation can reveal the representational issue causing the zero-shot\ntranslation deficiency. In this work, we introduce the identity pair, a\nsentence translated into itself, to address the lack of the base measure in\nmultilingual investigations, as the identity pair represents the optimal state\nof representation among any language transfers. In our analysis, we demonstrate\nthat the encoder transfers the source language to the representational subspace\nof the target language instead of the language-agnostic state. Thus, the\nzero-shot translation deficiency arises because representations are entangled\nwith other languages and are not transferred effectively to the target\nlanguage. Based on our findings, we propose two methods: 1) low-rank\nlanguage-specific embedding at the encoder, and 2) language-specific\ncontrastive learning of the representation at the decoder. The experimental\nresults on Europarl-15, TED-19, and OPUS-100 datasets show that our methods\nsubstantially enhance the performance of zero-shot translations by improving\nlanguage transfer capacity, thereby providing practical evidence to support our\nconclusions.\n","authors":["Zhi Qu","Chenchen Ding","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2406.08092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03239v2","updated":"2024-06-12T11:11:48Z","published":"2024-06-05T13:16:46Z","title":"Document-level Claim Extraction and Decontextualisation for\n  Fact-Checking","summary":"  Selecting which claims to check is a time-consuming task for human\nfact-checkers, especially from documents consisting of multiple sentences and\ncontaining multiple claims. However, existing claim extraction approaches focus\nmore on identifying and extracting claims from individual sentences, e.g.,\nidentifying whether a sentence contains a claim or the exact boundaries of the\nclaim within a sentence. In this paper, we propose a method for document-level\nclaim extraction for fact-checking, which aims to extract check-worthy claims\nfrom documents and decontextualise them so that they can be understood out of\ncontext. Specifically, we first recast claim extraction as extractive\nsummarization in order to identify central sentences from documents, then\nrewrite them to include necessary context from the originating document through\nsentence decontextualisation. Evaluation with both automatic metrics and a\nfact-checking professional shows that our method is able to extract\ncheck-worthy claims from documents more accurately than previous work, while\nalso improving evidence retrieval.\n","authors":["Zhenyun Deng","Michael Schlichtkrull","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2406.03239v2.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.08080v1","updated":"2024-06-12T11:04:11Z","published":"2024-06-12T11:04:11Z","title":"AustroTox: A Dataset for Target-Based Austrian German Offensive Language\n  Detection","summary":"  Model interpretability in toxicity detection greatly profits from token-level\nannotations. However, currently such annotations are only available in English.\nWe introduce a dataset annotated for offensive language detection sourced from\na news forum, notable for its incorporation of the Austrian German dialect,\ncomprising 4,562 user comments. In addition to binary offensiveness\nclassification, we identify spans within each comment constituting vulgar\nlanguage or representing targets of offensive statements. We evaluate\nfine-tuned language models as well as large language models in a zero- and\nfew-shot fashion. The results indicate that while fine-tuned models excel in\ndetecting linguistic peculiarities such as vulgar dialect, large language\nmodels demonstrate superior performance in detecting offensiveness in\nAustroTox. We publish the data and code.\n","authors":["Pia Pachinger","Janis Goldzycher","Anna Maria Planitzer","Wojciech Kusa","Allan Hanbury","Julia Neidhardt"],"pdf_url":"https://arxiv.org/pdf/2406.08080v1.pdf","comment":"Accepted to Findings of the Association for Computational\n  Linguistics: ACL 2024"},{"id":"http://arxiv.org/abs/2406.08074v1","updated":"2024-06-12T10:48:53Z","published":"2024-06-12T10:48:53Z","title":"A Concept-Based Explainability Framework for Large Multimodal Models","summary":"  Large multimodal models (LMMs) combine unimodal encoders and large language\nmodels (LLMs) to perform multimodal tasks. Despite recent advancements towards\nthe interpretability of these models, understanding internal representations of\nLMMs remains largely a mystery. In this paper, we present a novel framework for\nthe interpretation of LMMs. We propose a dictionary learning based approach,\napplied to the representation of tokens. The elements of the learned dictionary\ncorrespond to our proposed concepts. We show that these concepts are well\nsemantically grounded in both vision and text. Thus we refer to these as\n\"multi-modal concepts\". We qualitatively and quantitatively evaluate the\nresults of the learnt concepts. We show that the extracted multimodal concepts\nare useful to interpret representations of test samples. Finally, we evaluate\nthe disentanglement between different concepts and the quality of grounding\nconcepts visually and textually. We will publicly release our code.\n","authors":["Jayneel Parekh","Pegah Khayatan","Mustafa Shukor","Alasdair Newson","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2406.08074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07494v2","updated":"2024-06-12T10:47:09Z","published":"2024-06-11T17:30:22Z","title":"CADS: A Systematic Literature Review on the Challenges of Abstractive\n  Dialogue Summarization","summary":"  Abstractive dialogue summarization is the task of distilling conversations\ninto informative and concise summaries. Although reviews have been conducted on\nthis topic, there is a lack of comprehensive work detailing the challenges of\ndialogue summarization, unifying the differing understanding of the task, and\naligning proposed techniques, datasets, and evaluation metrics with the\nchallenges. This article summarizes the research on Transformer-based\nabstractive summarization for English dialogues by systematically reviewing\n1262 unique research papers published between 2019 and 2024, relying on the\nSemantic Scholar and DBLP databases. We cover the main challenges present in\ndialog summarization (i.e., language, structure, comprehension, speaker,\nsalience, and factuality) and link them to corresponding techniques such as\ngraph-based approaches, additional training tasks, and planning strategies,\nwhich typically overly rely on BART-based encoder-decoder models. We find that\nwhile some challenges, like language, have seen considerable progress, mainly\ndue to training methods, others, such as comprehension, factuality, and\nsalience, remain difficult and hold significant research opportunities. We\ninvestigate how these approaches are typically assessed, covering the datasets\nfor the subdomains of dialogue (e.g., meeting, medical), the established\nautomatic metrics and human evaluation approaches for assessing scores and\nannotator agreement. We observe that only a few datasets span across all\nsubdomains. The ROUGE metric is the most used, while human evaluation is\nfrequently reported without sufficient detail on inner-annotator agreement and\nannotation guidelines. Additionally, we discuss the possible implications of\nthe recently explored large language models and conclude that despite a\npotential shift in relevance and difficulty, our described challenge taxonomy\nremains relevant.\n","authors":["Frederic Kirstein","Jan Philip Wahle","Bela Gipp","Terry Ruas"],"pdf_url":"https://arxiv.org/pdf/2406.07494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08068v1","updated":"2024-06-12T10:36:27Z","published":"2024-06-12T10:36:27Z","title":"Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A\n  Survey","summary":"  Compared to traditional sentiment analysis, which only considers text,\nmultimodal sentiment analysis needs to consider emotional signals from\nmultimodal sources simultaneously and is therefore more consistent with the way\nhow humans process sentiment in real-world scenarios. It involves processing\nemotional information from various sources such as natural language, images,\nvideos, audio, physiological signals, etc. However, although other modalities\nalso contain diverse emotional cues, natural language usually contains richer\ncontextual information and therefore always occupies a crucial position in\nmultimodal sentiment analysis. The emergence of ChatGPT has opened up immense\npotential for applying large language models (LLMs) to text-centric multimodal\ntasks. However, it is still unclear how existing LLMs can adapt better to\ntext-centric multimodal sentiment analysis tasks. This survey aims to (1)\npresent a comprehensive review of recent research in text-centric multimodal\nsentiment analysis tasks, (2) examine the potential of LLMs for text-centric\nmultimodal sentiment analysis, outlining their approaches, advantages, and\nlimitations, (3) summarize the application scenarios of LLM-based multimodal\nsentiment analysis technology, and (4) explore the challenges and potential\nresearch directions for multimodal sentiment analysis in the future.\n","authors":["Hao Yang","Yanyan Zhao","Yang Wu","Shilong Wang","Tian Zheng","Hongbo Zhang","Wanxiang Che","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2406.08068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09835v3","updated":"2024-06-12T10:31:57Z","published":"2023-11-16T12:03:21Z","title":"ML-Bench: Evaluating Large Language Models and Agents for Machine\n  Learning Tasks on Repository-Level Code","summary":"  Despite Large Language Models (LLMs) like GPT-4 achieving impressive results\nin function-level code generation, they struggle with repository-scale code\nunderstanding (e.g., coming up with the right arguments for calling routines),\nrequiring a deeper comprehension of complex file interactions. Also, recently,\npeople have developed LLM agents that attempt to interact with repository code\n(e.g., compiling and evaluating its execution), prompting the need to evaluate\ntheir performance. These gaps have motivated our development of ML-Bench, a\nbenchmark rooted in real-world programming applications that leverage existing\ncode repositories to perform tasks. Addressing the need for LLMs to interpret\nlong code contexts and translate instructions into precise, executable scripts,\nML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories,\nchallenging LLMs to accommodate user-specified arguments and documentation\nintricacies effectively. To evaluate both LLMs and AI agents, two setups are\nemployed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a\npredefined deployment environment, and ML-Agent-Bench for testing autonomous\nagents in an end-to-end task execution within a Linux sandbox environment. Our\nfindings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%,\nthere remains significant scope for improvement, highlighted by issues such as\nhallucinated outputs and difficulties with bash script generation. Notably, in\nthe more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate,\nreflecting the efficacy of iterative action and feedback in complex task\nresolution.\n","authors":["Xiangru Tang","Yuliang Liu","Zefan Cai","Yanjun Shao","Junjie Lu","Yichi Zhang","Zexuan Deng","Helan Hu","Kaikai An","Ruijun Huang","Shuzheng Si","Sheng Chen","Haozhe Zhao","Liang Chen","Yan Wang","Tianyu Liu","Zhiwei Jiang","Baobao Chang","Yin Fang","Yujia Qin","Wangchunshu Zhou","Yilun Zhao","Arman Cohan","Mark Gerstein"],"pdf_url":"https://arxiv.org/pdf/2311.09835v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08055v1","updated":"2024-06-12T10:12:52Z","published":"2024-06-12T10:12:52Z","title":"Learning Job Title Representation from Job Description Aggregation\n  Network","summary":"  Learning job title representation is a vital process for developing automatic\nhuman resource tools. To do so, existing methods primarily rely on learning the\ntitle representation through skills extracted from the job description,\nneglecting the rich and diverse content within. Thus, we propose an alternative\nframework for learning job titles through their respective job description (JD)\nand utilize a Job Description Aggregator component to handle the lengthy\ndescription and bidirectional contrastive loss to account for the bidirectional\nrelationship between the job title and its description. We evaluated the\nperformance of our method on both in-domain and out-of-domain settings,\nachieving a superior performance over the skill-based approach.\n","authors":["Napat Laosaengpha","Thanit Tativannarat","Chawan Piansaddhayanon","Attapol Rutherford","Ekapol Chuangsuwanich"],"pdf_url":"https://arxiv.org/pdf/2406.08055v1.pdf","comment":"to be published in Findings of the Association for Computational\n  Linguistics: ACL 2024"},{"id":"http://arxiv.org/abs/2406.08050v1","updated":"2024-06-12T10:02:27Z","published":"2024-06-12T10:02:27Z","title":"Adversarial Evasion Attack Efficiency against Large Language Models","summary":"  Large Language Models (LLMs) are valuable for text classification, but their\nvulnerabilities must not be disregarded. They lack robustness against\nadversarial examples, so it is pertinent to understand the impacts of different\ntypes of perturbations, and assess if those attacks could be replicated by\ncommon users with a small amount of perturbations and a small number of queries\nto a deployed LLM. This work presents an analysis of the effectiveness,\nefficiency, and practicality of three different types of adversarial attacks\nagainst five different LLMs in a sentiment classification task. The obtained\nresults demonstrated the very distinct impacts of the word-level and\ncharacter-level attacks. The word attacks were more effective, but the\ncharacter and more constrained attacks were more practical and required a\nreduced number of perturbations and queries. These differences need to be\nconsidered during the development of adversarial defense strategies to train\nmore robust LLMs for intelligent text classification applications.\n","authors":["João Vitorino","Eva Maia","Isabel Praça"],"pdf_url":"https://arxiv.org/pdf/2406.08050v1.pdf","comment":"9 pages, 1 table, 2 figures, DCAI 2024 conference"},{"id":"http://arxiv.org/abs/2404.11932v2","updated":"2024-06-12T09:35:48Z","published":"2024-04-18T06:20:50Z","title":"CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual\n  Knowledge Alignment","summary":"  Multilingual proficiency presents a significant challenge for large language\nmodels (LLMs). English-centric models are usually suboptimal in other\nlanguages, particularly those that are linguistically distant from English.\nThis performance discrepancy mainly stems from the imbalanced distribution of\ntraining data across languages during pre-training and instruction tuning\nstages. To address this problem, we propose a novel approach called CrossIn,\nwhich utilizes a mixed composition of cross-lingual instruction tuning data.\nOur method leverages the compressed representation shared by various languages\nto efficiently enhance the model's task-solving capabilities and multilingual\nproficiency within a single process. In addition, we introduce a multi-task and\nmulti-faceted benchmark to evaluate the effectiveness of CrossIn. Experimental\nresults demonstrate that our method substantially improves performance across\ntasks and languages, and we provide extensive insights into the impact of\ncross-lingual data volume and the integration of translation data on enhancing\nmultilingual consistency and accuracy.\n","authors":["Geyu Lin","Bin Wang","Zhengyuan Liu","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2404.11932v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2306.08658v2","updated":"2024-06-12T09:33:29Z","published":"2023-06-14T17:53:06Z","title":"Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language\n  Representations","summary":"  Vision-and-language (VL) models with separate encoders for each modality\n(e.g., CLIP) have become the go-to models for zero-shot image classification\nand image-text retrieval. They are, however, mostly evaluated in English as\nmultilingual benchmarks are limited in availability. We introduce\nBabel-ImageNet, a massively multilingual benchmark that offers (partial)\ntranslations of ImageNet labels to 100 languages, built without machine\ntranslation or manual annotation. We instead automatically obtain reliable\ntranslations by linking them -- via shared WordNet synsets -- to BabelNet, a\nmassively multilingual lexico-semantic network. We evaluate 11 public\nmultilingual CLIP models on zero-shot image classification (ZS-IC) on our\nbenchmark, demonstrating a significant gap between English ImageNet performance\nand that of high-resource languages (e.g., German or Chinese), and an even\nbigger gap for low-resource languages (e.g., Sinhala or Lao). Crucially, we\nshow that the models' ZS-IC performance highly correlates with their\nperformance in image-text retrieval, validating the use of Babel-ImageNet to\nevaluate multilingual models for the vast majority of languages without gold\nimage-text data. Finally, we show that the performance of multilingual CLIP can\nbe drastically improved for low-resource languages with parameter-efficient\nlanguage-specific training. We make our code and data publicly available:\n\\url{https://github.com/gregor-ge/Babel-ImageNet}\n","authors":["Gregor Geigle","Radu Timofte","Goran Glavaš"],"pdf_url":"https://arxiv.org/pdf/2306.08658v2.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2405.14179v2","updated":"2024-06-12T09:16:46Z","published":"2024-05-23T05:06:55Z","title":"UzMorphAnalyser: A Morphological Analysis Model for the Uzbek Language\n  Using Inflectional Endings","summary":"  As Uzbek language is agglutinative, has many morphological features which\nwords formed by combining root and affixes. Affixes play an important role in\nthe morphological analysis of words, by adding additional meanings and\ngrammatical functions to words. Inflectional endings are utilized to express\nvarious morphological features within the language. This feature introduces\nnumerous possibilities for word endings, thereby significantly expanding the\nword vocabulary and exacerbating issues related to data sparsity in statistical\nmodels. This paper present modeling of the morphological analysis of Uzbek\nwords, including stemming, lemmatizing, and the extraction of morphological\ninformation while considering morpho-phonetic exceptions. Main steps of the\nmodel involve developing a complete set of word-ending with assigned\nmorphological information, and additional datasets for morphological analysis.\nThe proposed model was evaluated using a curated test set comprising 5.3K\nwords. Through manual verification of stemming, lemmatizing, and morphological\nfeature corrections carried out by linguistic specialists, it obtained a\nword-level accuracy of over 91%. The developed tool based on the proposed model\nis available as a web-based application and an open-source Python library.\n","authors":["Ulugbek Salaev"],"pdf_url":"https://arxiv.org/pdf/2405.14179v2.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.12486v2","updated":"2024-06-12T09:09:47Z","published":"2024-02-19T19:49:29Z","title":"Do Pre-Trained Language Models Detect and Understand Semantic\n  Underspecification? Ask the DUST!","summary":"  In everyday language use, speakers frequently utter and interpret sentences\nthat are semantically underspecified, namely, whose content is insufficient to\nfully convey their message or interpret them univocally. For example, to\ninterpret the underspecified sentence \"Don't spend too much\", which leaves\nimplicit what (not) to spend, additional linguistic context or outside\nknowledge is needed. In this work, we propose a novel Dataset of semantically\nUnderspecified Sentences grouped by Type (DUST) and use it to study whether\npre-trained language models (LMs) correctly identify and interpret\nunderspecified sentences. We find that newer LMs are reasonably able to\nidentify underspecified sentences when explicitly prompted. However,\ninterpreting them correctly is much harder for any LMs. Our experiments show\nthat when interpreting underspecified sentences, LMs exhibit little\nuncertainty, contrary to what theoretical accounts of underspecification would\npredict. Overall, our study reveals limitations in current models' processing\nof sentence semantics and highlights the importance of using naturalistic data\nand communicative scenarios when evaluating LMs' language capabilities.\n","authors":["Frank Wildenburg","Michael Hanna","Sandro Pezzelle"],"pdf_url":"https://arxiv.org/pdf/2402.12486v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07358v2","updated":"2024-06-12T09:06:13Z","published":"2024-06-11T15:26:57Z","title":"AI Sandbagging: Language Models can Strategically Underperform on\n  Evaluations","summary":"  Trustworthy capability evaluations are crucial for ensuring the safety of AI\nsystems, and are becoming a key component of AI regulation. However, the\ndevelopers of an AI system, or the AI system itself, may have incentives for\nevaluations to understate the AI's actual capability. These conflicting\ninterests lead to the problem of sandbagging $\\unicode{x2013}$ which we define\nas \"strategic underperformance on an evaluation\". In this paper we assess\nsandbagging capabilities in contemporary language models (LMs). We prompt\nfrontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on\ndangerous capability evaluations, while maintaining performance on general\n(harmless) capability evaluations. Moreover, we find that models can be\nfine-tuned, on a synthetic dataset, to hide specific capabilities unless given\na password. This behaviour generalizes to high-quality, held-out benchmarks\nsuch as WMDP. In addition, we show that both frontier and smaller models can be\nprompted, or password-locked, to target specific scores on a capability\nevaluation. Even more, we found that a capable password-locked model (Llama 3\n70b) is reasonably able to emulate a less capable model (Llama 2 7b). Overall,\nour results suggest that capability evaluations are vulnerable to sandbagging.\nThis vulnerability decreases the trustworthiness of evaluations, and thereby\nundermines important safety decisions regarding the development and deployment\nof advanced AI systems.\n","authors":["Teun van der Weij","Felix Hofstätter","Ollie Jaffe","Samuel F. Brown","Francis Rhys Ward"],"pdf_url":"https://arxiv.org/pdf/2406.07358v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09074v4","updated":"2024-06-12T08:55:13Z","published":"2024-01-17T09:23:59Z","title":"Code Simulation Challenges for Large Language Models","summary":"  Many reasoning, planning, and problem-solving tasks share an intrinsic\nalgorithmic nature: correctly simulating each step is a sufficient condition to\nsolve them correctly. This work studies to what extent Large Language Models\n(LLMs) can simulate coding and algorithmic tasks to provide insights into\ngeneral capabilities in such algorithmic reasoning tasks. We introduce\nbenchmarks for straight-line programs, code that contains critical paths, and\napproximate and redundant instructions. We further assess the simulation\ncapabilities of LLMs with sorting algorithms and nested loops and show that a\nroutine's computational complexity directly affects an LLM's ability to\nsimulate its execution. While the most powerful LLMs exhibit relatively strong\nsimulation capabilities, the process is fragile, seems to rely heavily on\npattern recognition, and is affected by memorisation. We propose a novel\noff-the-shelf prompting method, Chain of Simulation (CoSm), which instructs\nLLMs to simulate code execution line by line/follow the computation pattern of\ncompilers. CoSm efficiently helps LLMs reduce memorisation and shallow pattern\nrecognition while improving simulation performance. We consider the success of\nCoSm in code simulation to be inspirational for other general routine\nsimulation reasoning tasks.\n","authors":["Emanuele La Malfa","Christoph Weinhuber","Orazio Torre","Fangru Lin","Samuele Marro","Anthony Cohn","Nigel Shadbolt","Michael Wooldridge"],"pdf_url":"https://arxiv.org/pdf/2401.09074v4.pdf","comment":"Code: https://github.com/EmanueleLM/CodeSimulation"},{"id":"http://arxiv.org/abs/2309.02726v3","updated":"2024-06-12T08:40:15Z","published":"2023-09-06T05:19:41Z","title":"Large Language Models for Automated Open-domain Scientific Hypotheses\n  Discovery","summary":"  Hypothetical induction is recognized as the main reasoning type when\nscientists make observations about the world and try to propose hypotheses to\nexplain those observations. Past research on hypothetical induction is under a\nconstrained setting: (1) the observation annotations in the dataset are\ncarefully manually handpicked sentences (resulting in a close-domain setting);\nand (2) the ground truth hypotheses are mostly commonsense knowledge, making\nthe task less challenging. In this work, we tackle these problems by proposing\nthe first dataset for social science academic hypotheses discovery, with the\nfinal goal to create systems that automatically generate valid, novel, and\nhelpful scientific hypotheses, given only a pile of raw web corpus. Unlike\nprevious settings, the new dataset requires (1) using open-domain data (raw web\ncorpus) as observations; and (2) proposing hypotheses even new to humanity. A\nmulti-module framework is developed for the task, including three different\nfeedback mechanisms to boost performance, which exhibits superior performance\nin terms of both GPT-4 based and expert-based evaluation. To the best of our\nknowledge, this is the first work showing that LLMs are able to generate novel\n(''not existing in literature'') and valid (''reflecting reality'') scientific\nhypotheses.\n","authors":["Zonglin Yang","Xinya Du","Junxian Li","Jie Zheng","Soujanya Poria","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2309.02726v3.pdf","comment":"Accepted by ACL 2024 (findings)"},{"id":"http://arxiv.org/abs/2401.07103v2","updated":"2024-06-12T08:31:58Z","published":"2024-01-13T15:59:09Z","title":"Leveraging Large Language Models for NLG Evaluation: Advances and\n  Challenges","summary":"  In the rapidly evolving domain of Natural Language Generation (NLG)\nevaluation, introducing Large Language Models (LLMs) has opened new avenues for\nassessing generated content quality, e.g., coherence, creativity, and context\nrelevance. This paper aims to provide a thorough overview of leveraging LLMs\nfor NLG evaluation, a burgeoning area that lacks a systematic analysis. We\npropose a coherent taxonomy for organizing existing LLM-based evaluation\nmetrics, offering a structured framework to understand and compare these\nmethods. Our detailed exploration includes critically assessing various\nLLM-based methodologies, as well as comparing their strengths and limitations\nin evaluating NLG outputs. By discussing unresolved challenges, including bias,\nrobustness, domain-specificity, and unified evaluation, this paper seeks to\noffer insights to researchers and advocate for fairer and more advanced NLG\nevaluation techniques.\n","authors":["Zhen Li","Xiaohan Xu","Tao Shen","Can Xu","Jia-Chen Gu","Yuxuan Lai","Chongyang Tao","Shuai Ma"],"pdf_url":"https://arxiv.org/pdf/2401.07103v2.pdf","comment":"21 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.09096v2","updated":"2024-06-12T08:28:15Z","published":"2023-11-15T16:42:29Z","title":"Defending Large Language Models Against Jailbreaking Attacks Through\n  Goal Prioritization","summary":"  While significant attention has been dedicated to exploiting weaknesses in\nLLMs through jailbreaking attacks, there remains a paucity of effort in\ndefending against these attacks. We point out a pivotal factor contributing to\nthe success of jailbreaks: the intrinsic conflict between the goals of being\nhelpful and ensuring safety. Accordingly, we propose to integrate goal\nprioritization at both training and inference stages to counteract.\nImplementing goal prioritization during inference substantially diminishes the\nAttack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And\nintegrating goal prioritization into model training reduces the ASR from 71.0%\nto 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking\nsamples are included during training, our approach slashes the ASR by half.\nAdditionally, our findings reveal that while stronger LLMs face greater safety\nrisks, they also possess a greater capacity to be steered towards defending\nagainst such attacks, both because of their stronger ability in instruction\nfollowing. Our work thus contributes to the comprehension of jailbreaking\nattacks and defenses, and sheds light on the relationship between LLMs'\ncapability and safety. Our code is available at\n\\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.\n","authors":["Zhexin Zhang","Junxiao Yang","Pei Ke","Fei Mi","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2311.09096v2.pdf","comment":"ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2406.07990v1","updated":"2024-06-12T08:26:30Z","published":"2024-06-12T08:26:30Z","title":"Blowfish: Topological and statistical signatures for quantifying\n  ambiguity in semantic search","summary":"  This works reports evidence for the topological signatures of ambiguity in\nsentence embeddings that could be leveraged for ranking and/or explanation\npurposes in the context of vector search and Retrieval Augmented Generation\n(RAG) systems. We proposed a working definition of ambiguity and designed an\nexperiment where we have broken down a proprietary dataset into collections of\nchunks of varying size - 3, 5, and 10 lines and used the different collections\nsuccessively as queries and answers sets. It allowed us to test the signatures\nof ambiguity with removal of confounding factors. Our results show that proxy\nambiguous queries (size 10 queries against size 3 documents) display different\ndistributions of homologies 0 and 1 based features than proxy clear queries\n(size 5 queries against size 10 documents). We then discuss those results in\nterms increased manifold complexity and/or approximately discontinuous\nembedding submanifolds. Finally we propose a strategy to leverage those\nfindings as a new scoring strategy of semantic similarities.\n","authors":["Thomas Roland Barillot","Alex De Castro"],"pdf_url":"https://arxiv.org/pdf/2406.07990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10548v3","updated":"2024-06-12T08:00:19Z","published":"2024-05-17T05:20:49Z","title":"Language Models can Exploit Cross-Task In-context Learning for\n  Data-Scarce Novel Tasks","summary":"  Large Language Models (LLMs) have transformed NLP with their remarkable\nIn-context Learning (ICL) capabilities. Automated assistants based on LLMs are\ngaining popularity; however, adapting them to novel tasks is still challenging.\nWhile colossal models excel in zero-shot performance, their computational\ndemands limit widespread use, and smaller language models struggle without\ncontext. This paper investigates whether LLMs can generalize from labeled\nexamples of predefined tasks to novel tasks. Drawing inspiration from\nbiological neurons and the mechanistic interpretation of the Transformer\narchitecture, we explore the potential for information sharing across tasks. We\ndesign a cross-task prompting setup with three LLMs and show that LLMs achieve\nsignificant performance improvements despite no examples from the target task\nin the context. Cross-task prompting leads to a remarkable performance boost of\n107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B, and 3.2% for GPT 3.5 on average\nover zero-shot prompting, and performs comparable to standard in-context\nlearning. The effectiveness of generating pseudo-labels for in-task examples is\ndemonstrated, and our analyses reveal a strong correlation between the effect\nof cross-task examples and model activation similarities in source and target\ninput tokens. This paper offers a first-of-its-kind exploration of LLMs'\nability to solve novel tasks based on contextual signals from different task\nexamples.\n","authors":["Anwoy Chatterjee","Eshaan Tanwar","Subhabrata Dutta","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2405.10548v3.pdf","comment":"Accepted at ACL 2024 Main"},{"id":"http://arxiv.org/abs/2406.07971v1","updated":"2024-06-12T07:52:17Z","published":"2024-06-12T07:52:17Z","title":"It Takes Two: On the Seamlessness between Reward and Policy Model in\n  RLHF","summary":"  Reinforcement Learning from Human Feedback (RLHF) involves training policy\nmodels (PMs) and reward models (RMs) to align language models with human\npreferences. Instead of focusing solely on PMs and RMs independently, we\npropose to examine their interactions during fine-tuning, introducing the\nconcept of seamlessness. Our study starts with observing the saturation\nphenomenon, where continual improvements in RM and PM do not translate into\nRLHF progress. Our analysis shows that RMs fail to assign proper scores to PM\nresponses, resulting in a 35% mismatch rate with human preferences,\nhighlighting a significant discrepancy between PM and RM. To measure\nseamlessness between PM and RM without human effort, we propose an automatic\nmetric, SEAM. SEAM quantifies the discrepancies between PM and RM judgments\ninduced by data samples. We validate the effectiveness of SEAM in data\nselection and model augmentation. Our experiments demonstrate that (1) using\nSEAM-filtered data for RL training improves RLHF performance by 4.5%, and (2)\nSEAM-guided model augmentation results in a 4% performance improvement over\nstandard augmentation methods.\n","authors":["Taiming Lu","Lingfeng Shen","Xinyu Yang","Weiting Tan","Beidi Chen","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2406.07971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07970v1","updated":"2024-06-12T07:49:36Z","published":"2024-06-12T07:49:36Z","title":"Guiding In-Context Learning of LLMs through Quality Estimation for\n  Machine Translation","summary":"  The quality of output from large language models (LLMs), particularly in\nmachine translation (MT), is closely tied to the quality of in-context examples\n(ICEs) provided along with the query, i.e., the text to translate. The\neffectiveness of these ICEs is influenced by various factors, such as the\ndomain of the source text, the order in which the ICEs are presented, the\nnumber of these examples, and the prompt templates used. Naturally, selecting\nthe most impactful ICEs depends on understanding how these affect the resulting\ntranslation quality, which ultimately relies on translation references or human\njudgment. This paper presents a novel methodology for in-context learning (ICL)\nthat relies on a search algorithm guided by domain-specific quality estimation\n(QE). Leveraging the XGLM model, our methodology estimates the resulting\ntranslation quality without the need for translation references, selecting\neffective ICEs for MT to maximize translation quality. Our results demonstrate\nsignificant improvements over existing ICL methods and higher translation\nperformance compared to fine-tuning a pre-trained language model (PLM),\nspecifically mBART-50.\n","authors":["Javad Pourmostafa Roshan Sharami","Dimitar Shterionov","Pieter Spronck"],"pdf_url":"https://arxiv.org/pdf/2406.07970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07969v1","updated":"2024-06-12T07:49:21Z","published":"2024-06-12T07:49:21Z","title":"LibriTTS-P: A Corpus with Speaking Style and Speaker Identity Prompts\n  for Text-to-Speech and Style Captioning","summary":"  We introduce LibriTTS-P, a new corpus based on LibriTTS-R that includes\nutterance-level descriptions (i.e., prompts) of speaking style and\nspeaker-level prompts of speaker characteristics. We employ a hybrid approach\nto construct prompt annotations: (1) manual annotations that capture human\nperceptions of speaker characteristics and (2) synthetic annotations on\nspeaking style. Compared to existing English prompt datasets, our corpus\nprovides more diverse prompt annotations for all speakers of LibriTTS-R.\nExperimental results for prompt-based controllable TTS demonstrate that the TTS\nmodel trained with LibriTTS-P achieves higher naturalness than the model using\nthe conventional dataset. Furthermore, the results for style captioning tasks\nshow that the model utilizing LibriTTS-P generates 2.5 times more accurate\nwords than the model using a conventional dataset. Our corpus, LibriTTS-P, is\navailable at https://github.com/line/LibriTTS-P.\n","authors":["Masaya Kawamura","Ryuichi Yamamoto","Yuma Shirahata","Takuya Hasumi","Kentaro Tachibana"],"pdf_url":"https://arxiv.org/pdf/2406.07969v1.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.07967v1","updated":"2024-06-12T07:44:36Z","published":"2024-06-12T07:44:36Z","title":"Better than Random: Reliable NLG Human Evaluation with Constrained\n  Active Sampling","summary":"  Human evaluation is viewed as a reliable evaluation method for NLG which is\nexpensive and time-consuming. To save labor and costs, researchers usually\nperform human evaluation on a small subset of data sampled from the whole\ndataset in practice. However, different selection subsets will lead to\ndifferent rankings of the systems. To give a more correct inter-system ranking\nand make the gold standard human evaluation more reliable, we propose a\nConstrained Active Sampling Framework (CASF) for reliable human judgment. CASF\noperates through a Learner, a Systematic Sampler and a Constrained Controller\nto select representative samples for getting a more correct inter-system\nranking.Experiment results on 137 real NLG evaluation setups with 44 human\nevaluation metrics across 16 datasets and 5 NLG tasks demonstrate CASF receives\n93.18% top-ranked system recognition accuracy and ranks first or ranks second\non 90.91% of the human metrics with 0.83 overall inter-system ranking Kendall\ncorrelation.Code and data are publicly available online.\n","authors":["Jie Ruan","Xiao Pu","Mingqi Gao","Xiaojun Wan","Yuesheng Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.07967v1.pdf","comment":"With Appendix"},{"id":"http://arxiv.org/abs/2406.07964v1","updated":"2024-06-12T07:42:12Z","published":"2024-06-12T07:42:12Z","title":"Political Leaning Inference through Plurinational Scenarios","summary":"  Social media users express their political preferences via interaction with\nother users, by spontaneous declarations or by participation in communities\nwithin the network. This makes a social network such as Twitter a valuable data\nsource to study computational science approaches to political learning\ninference. In this work we focus on three diverse regions in Spain (Basque\nCountry, Catalonia and Galicia) to explore various methods for multi-party\ncategorization, required to analyze evolving and complex political landscapes,\nand compare it with binary left-right approaches. We use a two-step method\ninvolving unsupervised user representations obtained from the retweets and\ntheir subsequent use for political leaning detection. Comprehensive\nexperimentation on a newly collected and curated dataset comprising labeled\nusers and their interactions demonstrate the effectiveness of using Relational\nEmbeddings as representation method for political ideology detection in both\nbinary and multi-party frameworks, even with limited training data. Finally,\ndata visualization illustrates the ability of the Relational Embeddings to\ncapture intricate intra-group and inter-group political affinities.\n","authors":["Joseba Fernandez de Landa","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2406.07964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07962v1","updated":"2024-06-12T07:41:44Z","published":"2024-06-12T07:41:44Z","title":"Toward a Method to Generate Capability Ontologies from Natural Language\n  Descriptions","summary":"  To achieve a flexible and adaptable system, capability ontologies are\nincreasingly leveraged to describe functions in a machine-interpretable way.\nHowever, modeling such complex ontological descriptions is still a manual and\nerror-prone task that requires a significant amount of effort and ontology\nexpertise. This contribution presents an innovative method to automate\ncapability ontology modeling using Large Language Models (LLMs), which have\nproven to be well suited for such tasks. Our approach requires only a natural\nlanguage description of a capability, which is then automatically inserted into\na predefined prompt using a few-shot prompting technique. After prompting an\nLLM, the resulting capability ontology is automatically verified through\nvarious steps in a loop with the LLM to check the overall correctness of the\ncapability ontology. First, a syntax check is performed, then a check for\ncontradictions, and finally a check for hallucinations and missing ontology\nelements. Our method greatly reduces manual effort, as only the initial natural\nlanguage description and a final human review and possible correction are\nnecessary, thereby streamlining the capability ontology generation process.\n","authors":["Luis Miguel Vieira da Silva","Aljosha Köcher","Felix Gehlhoff","Alexander Fay"],"pdf_url":"https://arxiv.org/pdf/2406.07962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.12519v3","updated":"2024-06-12T07:37:35Z","published":"2023-05-21T17:26:16Z","title":"DPIC: Decoupling Prompt and Intrinsic Characteristics for LLM Generated\n  Text Detection","summary":"  Large language models (LLMs) have the potential to generate texts that pose\nrisks of misuse, such as plagiarism, planting fake reviews on e-commerce\nplatforms, or creating inflammatory false tweets. Consequently, detecting\nwhether a text is generated by LLMs has become increasingly important. Existing\nhigh-quality detection methods usually require access to the interior of the\nmodel to extract the intrinsic characteristics. However, since we do not have\naccess to the interior of the black-box model, we must resort to surrogate\nmodels, which impacts detection quality. In order to achieve high-quality\ndetection of black-box models, we would like to extract deep intrinsic\ncharacteristics of the black-box model generated texts. We view the generation\nprocess as a coupled process of prompt and intrinsic characteristics of the\ngenerative model. Based on this insight, we propose to decouple prompt and\nintrinsic characteristics (DPIC) for LLM-generated text detection method.\nSpecifically, given a candidate text, DPIC employs an auxiliary LLM to\nreconstruct the prompt corresponding to the candidate text, then uses the\nprompt to regenerate text by the auxiliary LLM, which makes the candidate text\nand the regenerated text align with their prompts, respectively. Then, the\nsimilarity between the candidate text and the regenerated text is used as a\ndetection feature, thus eliminating the prompt in the detection process, which\nallows the detector to focus on the intrinsic characteristics of the generative\nmodel. Compared to the baselines, DPIC has achieved an average improvement of\n6.76\\% and 2.91\\% in detecting texts from different domains generated by GPT4\nand Claude3, respectively.\n","authors":["Xiao Yu","Yuang Qi","Kejiang Chen","Guoqiang Chen","Xi Yang","Pengyuan Zhu","Xiuwei Shang","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2305.12519v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02567v2","updated":"2024-06-12T07:13:01Z","published":"2024-03-05T00:48:56Z","title":"Eliciting Better Multilingual Structured Reasoning from LLMs through\n  Code","summary":"  The development of large language models (LLM) has shown progress on\nreasoning, though studies have largely considered either English or simple\nreasoning tasks. To address this, we introduce a multilingual structured\nreasoning and explanation dataset, termed xSTREET, that covers four tasks\nacross six languages. xSTREET exposes a gap in base LLM performance between\nEnglish and non-English reasoning tasks.\n  We then propose two methods to remedy this gap, building on the insight that\nLLMs trained on code are better reasoners. First, at training time, we augment\na code dataset with multilingual comments using machine translation while\nkeeping program code as-is. Second, at inference time, we bridge the gap\nbetween training and inference by employing a prompt structure that\nincorporates step-by-step code primitives to derive new facts and find a\nsolution. Our methods show improved multilingual performance on xSTREET, most\nnotably on the scientific commonsense reasoning subtask. Furthermore, the\nmodels show no regression on non-reasoning tasks, thus demonstrating our\ntechniques maintain general-purpose abilities.\n","authors":["Bryan Li","Tamer Alkhouli","Daniele Bonadiman","Nikolaos Pappas","Saab Mansour"],"pdf_url":"https://arxiv.org/pdf/2403.02567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01753v3","updated":"2024-06-12T07:12:36Z","published":"2024-04-02T09:11:58Z","title":"M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets","summary":"  In recent years, multimodal natural language processing, aimed at learning\nfrom diverse data types, has garnered significant attention. However, there\nneeds to be more clarity when it comes to analysing multimodal tasks in\nmulti-lingual contexts. While prior studies on sentiment analysis of tweets\nhave predominantly focused on the English language, this paper addresses this\ngap by transforming an existing textual Twitter sentiment dataset into a\nmultimodal format through a straightforward curation process. Our work opens up\nnew avenues for sentiment-related research within the research community.\nAdditionally, we conduct baseline experiments utilising this augmented dataset\nand report the findings. Notably, our evaluations reveal that when comparing\nunimodal and multimodal configurations, using a sentiment-tuned large language\nmodel as a text encoder performs exceptionally well.\n","authors":["Gaurish Thakkar","Sherzod Hakimov","Marko Tadić"],"pdf_url":"https://arxiv.org/pdf/2404.01753v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07935v1","updated":"2024-06-12T06:59:31Z","published":"2024-06-12T06:59:31Z","title":"Defining and Detecting Vulnerability in Human Evaluation Guidelines: A\n  Preliminary Study Towards Reliable NLG Evaluation","summary":"  Human evaluation serves as the gold standard for assessing the quality of\nNatural Language Generation (NLG) systems. Nevertheless, the evaluation\nguideline, as a pivotal element ensuring reliable and reproducible human\nassessment, has received limited attention.Our investigation revealed that only\n29.84% of recent papers involving human evaluation at top conferences release\ntheir evaluation guidelines, with vulnerabilities identified in 77.09% of these\nguidelines. Unreliable evaluation guidelines can yield inaccurate assessment\noutcomes, potentially impeding the advancement of NLG in the right direction.\nTo address these challenges, we take an initial step towards reliable\nevaluation guidelines and propose the first human evaluation guideline dataset\nby collecting annotations of guidelines extracted from existing papers as well\nas generated via Large Language Models (LLMs). We then introduce a taxonomy of\neight vulnerabilities and formulate a principle for composing evaluation\nguidelines. Furthermore, a method for detecting guideline vulnerabilities has\nbeen explored using LLMs, and we offer a set of recommendations to enhance\nreliability in human evaluation. The annotated human evaluation guideline\ndataset and code for the vulnerability detection method are publicly available\nonline.\n","authors":["Jie Ruan","Wenqing Wang","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2406.07935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07933v1","updated":"2024-06-12T06:56:20Z","published":"2024-06-12T06:56:20Z","title":"Large Language Model Unlearning via Embedding-Corrupted Prompts","summary":"  Large language models (LLMs) have advanced to encompass extensive knowledge\nacross diverse domains. Yet controlling what a large language model should not\nknow is important for ensuring alignment and thus safe use. However, accurately\nand efficiently unlearning knowledge from an LLM remains challenging due to the\npotential collateral damage caused by the fuzzy boundary between retention and\nforgetting, and the large computational requirements for optimization across\nstate-of-the-art models with hundreds of billions of parameters. In this work,\nwe present Embedding-COrrupted (ECO) Prompts, a lightweight unlearning\nframework for large language models to address both the challenges of knowledge\nentanglement and unlearning efficiency. Instead of relying on the LLM itself to\nunlearn, we enforce an unlearned state during inference by employing a prompt\nclassifier to identify and safeguard prompts to forget. We learn corruptions\nadded to prompt embeddings via zeroth order optimization toward the unlearning\nobjective offline and corrupt prompts flagged by the classifier during\ninference. We find that these embedding-corrupted prompts not only lead to\ndesirable outputs that satisfy the unlearning objective but also closely\napproximate the output from a model that has never been trained on the data\nintended for forgetting. Through extensive experiments on unlearning, we\ndemonstrate the superiority of our method in achieving promising unlearning at\nnearly zero side effects in general domains and domains closely related to the\nunlearned ones. Additionally, we highlight the scalability of our method to 100\nLLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the\nnumber of parameters increases.\n","authors":["Chris Yuhao Liu","Yaxuan Wang","Jeffrey Flanigan","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.07933v1.pdf","comment":"55 pages, 4 figures, 66 tables"},{"id":"http://arxiv.org/abs/2406.07922v1","updated":"2024-06-12T06:44:05Z","published":"2024-06-12T06:44:05Z","title":"Automated Information Extraction from Thyroid Operation Narrative: A\n  Comparative Study of GPT-4 and Fine-tuned KoELECTRA","summary":"  In the rapidly evolving field of healthcare, the integration of artificial\nintelligence (AI) has become a pivotal component in the automation of clinical\nworkflows, ushering in a new era of efficiency and accuracy. This study focuses\non the transformative capabilities of the fine-tuned KoELECTRA model in\ncomparison to the GPT-4 model, aiming to facilitate automated information\nextraction from thyroid operation narratives. The current research landscape is\ndominated by traditional methods heavily reliant on regular expressions, which\noften face challenges in processing free-style text formats containing critical\ndetails of operation records, including frozen biopsy reports. Addressing this,\nthe study leverages advanced natural language processing (NLP) techniques to\nfoster a paradigm shift towards more sophisticated data processing systems.\nThrough this comparative study, we aspire to unveil a more streamlined,\nprecise, and efficient approach to document processing in the healthcare\ndomain, potentially revolutionizing the way medical data is handled and\nanalyzed.\n","authors":["Dongsuk Jang","Hyeryun Park","Jiye Son","Hyeonuk Hwang","Sujin Kim","Jinwook Choi"],"pdf_url":"https://arxiv.org/pdf/2406.07922v1.pdf","comment":"9 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2406.07913v1","updated":"2024-06-12T06:33:54Z","published":"2024-06-12T06:33:54Z","title":"DeTriever: Decoder-representation-based Retriever for Improving NL2SQL\n  In-Context Learning","summary":"  While in-context Learning (ICL) has proven to be an effective technique to\nimprove the performance of Large Language Models (LLMs) in a variety of complex\ntasks, notably in translating natural language questions into Structured Query\nLanguage (NL2SQL), the question of how to select the most beneficial\ndemonstration examples remains an open research problem. While prior works\noften adapted off-the-shelf encoders to retrieve examples dynamically, an\ninherent discrepancy exists in the representational capacities between the\nexternal retrievers and the LLMs. Further, optimizing the selection of examples\nis a non-trivial task, since there are no straightforward methods to assess the\nrelative benefits of examples without performing pairwise inference. To address\nthese shortcomings, we propose DeTriever, a novel demonstration retrieval\nframework that learns a weighted combination of LLM hidden states, where rich\nsemantic information is encoded. To train the model, we propose a proxy score\nthat estimates the relative benefits of examples based on the similarities\nbetween output queries. Experiments on two popular NL2SQL benchmarks\ndemonstrate that our method significantly outperforms the state-of-the-art\nbaselines on one-shot NL2SQL tasks.\n","authors":["Yuxi Feng","Raymond Li","Zhenan Fan","Giuseppe Carenini","Mohammadreza Pourreza","Weiwei Zhang","Yong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08369v3","updated":"2024-06-12T06:30:39Z","published":"2023-11-14T18:32:52Z","title":"How You Prompt Matters! Even Task-Oriented Constraints in Instructions\n  Affect LLM-Generated Text Detection","summary":"  To combat the misuse of Large Language Models (LLMs), many recent studies\nhave presented LLM-generated-text detectors with promising performance. When\nusers instruct LLMs to generate texts, the instruction can include different\nconstraints depending on the user's need. However, most recent studies do not\ncover such diverse instruction patterns when creating datasets for LLM\ndetection. In this paper, we reveal that even task-oriented constraints --\nconstraints that would naturally be included in an instruction and are not\nrelated to detection-evasion -- cause existing powerful detectors to have a\nlarge variance in detection performance. We focus on student essay writing as a\nrealistic domain and manually create task-oriented constraints based on several\nfactors for essay quality. Our experiments show that the standard deviation\n(SD) of current detector performance on texts generated by an instruction with\nsuch a constraint is significantly larger (up to an SD of 14.4 F1-score) than\nthat by generating texts multiple times or paraphrasing the instruction. We\nalso observe an overall trend where the constraints can make LLM detection more\nchallenging than without them. Finally, our analysis indicates that the high\ninstruction-following ability of LLMs fosters the large impact of such\nconstraints on detection performance.\n","authors":["Ryuto Koike","Masahiro Kaneko","Naoaki Okazaki"],"pdf_url":"https://arxiv.org/pdf/2311.08369v3.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2305.14225v2","updated":"2024-06-12T06:25:15Z","published":"2023-05-23T16:40:07Z","title":"ManiTweet: A New Benchmark for Identifying Manipulation of News on\n  Social Media","summary":"  Considerable advancements have been made to tackle the misrepresentation of\ninformation derived from reference articles in the domains of fact-checking and\nfaithful summarization. However, an unaddressed aspect remains - the\nidentification of social media posts that manipulate information within\nassociated news articles. This task presents a significant challenge, primarily\ndue to the prevalence of personal opinions in such posts. We present a novel\ntask, identifying manipulation of news on social media, which aims to detect\nmanipulation in social media posts and identify manipulated or inserted\ninformation. To study this task, we have proposed a data collection schema and\ncurated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and\ncorresponding articles. Our analysis demonstrates that this task is highly\nchallenging, with large language models (LLMs) yielding unsatisfactory\nperformance. Additionally, we have developed a simple yet effective basic model\nthat outperforms LLMs significantly on the ManiTweet dataset. Finally, we have\nconducted an exploratory analysis of human-written tweets, unveiling intriguing\nconnections between manipulation and the domain and factuality of news\narticles, as well as revealing that manipulated sentences are more likely to\nencapsulate the main story or consequences of a news outlet.\n","authors":["Kung-Hsiang Huang","Hou Pong Chan","Kathleen McKeown","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2305.14225v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07909v1","updated":"2024-06-12T06:22:52Z","published":"2024-06-12T06:22:52Z","title":"Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation","summary":"  Transformer encoder with connectionist temporal classification (CTC)\nframework is widely used for automatic speech recognition (ASR). However,\nknowledge distillation (KD) for ASR displays a problem of disagreement between\nteacher-student models in frame-level alignment which ultimately hinders it\nfrom improving the student model's performance. In order to resolve this\nproblem, this paper introduces a self-knowledge distillation (SKD) method that\nguides the frame-level alignment during the training time. In contrast to the\nconventional method using separate teacher and student models, this study\nintroduces a simple and effective method sharing encoder layers and applying\nthe sub-model as the student model. Overall, our approach is effective in\nimproving both the resource efficiency as well as performance. We also\nconducted an experimental analysis of the spike timings to illustrate that the\nproposed method improves performance by reducing the alignment disagreement.\n","authors":["Eungbeom Kim","Hantae Kim","Kyogu Lee"],"pdf_url":"https://arxiv.org/pdf/2406.07909v1.pdf","comment":"Accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.02856v2","updated":"2024-06-12T06:12:41Z","published":"2024-06-05T02:12:06Z","title":"Xmodel-LM Technical Report","summary":"  We introduce Xmodel-LM, a compact and efficient 1.1B language model\npre-trained on around 2 trillion tokens. Trained on our self-built dataset\n(Xdata), which balances Chinese and English corpora based on downstream task\noptimization, Xmodel-LM exhibits remarkable performance despite its smaller\nsize. It notably surpasses existing open-source language models of similar\nscale. Our model checkpoints and code are publicly accessible on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM.\n","authors":["Yichuan Wang","Yang Liu","Yu Yan","Qun Wang","Shulei Wu","Xucheng Huang","Ling Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.02856v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07900v1","updated":"2024-06-12T06:06:55Z","published":"2024-06-12T06:06:55Z","title":"Exploring Self-Supervised Multi-view Contrastive Learning for Speech\n  Emotion Recognition with Limited Annotations","summary":"  Recent advancements in Deep and Self-Supervised Learning (SSL) have led to\nsubstantial improvements in Speech Emotion Recognition (SER) performance,\nreaching unprecedented levels. However, obtaining sufficient amounts of\naccurately labeled data for training or fine-tuning the models remains a costly\nand challenging task. In this paper, we propose a multi-view SSL pre-training\ntechnique that can be applied to various representations of speech, including\nthe ones generated by large speech models, to improve SER performance in\nscenarios where annotations are limited. Our experiments, based on wav2vec 2.0,\nspectral and paralinguistic features, demonstrate that the proposed framework\nboosts the SER performance, by up to 10% in Unweighted Average Recall, in\nsettings with extremely sparse data annotations.\n","authors":["Bulat Khaertdinov","Pedro Jeuris","Annanda Sousa","Enrique Hortal"],"pdf_url":"https://arxiv.org/pdf/2406.07900v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.07890v1","updated":"2024-06-12T05:41:01Z","published":"2024-06-12T05:41:01Z","title":"Exploring Speech Foundation Models for Speaker Diarization in\n  Child-Adult Dyadic Interactions","summary":"  Speech foundation models, trained on vast datasets, have opened unique\nopportunities in addressing challenging low-resource speech understanding, such\nas child speech. In this work, we explore the capabilities of speech foundation\nmodels on child-adult speaker diarization. We show that exemplary foundation\nmodels can achieve 39.5% and 62.3% relative reductions in Diarization Error\nRate and Speaker Confusion Rate, respectively, compared to previous speaker\ndiarization methods. In addition, we benchmark and evaluate the speaker\ndiarization results of the speech foundation models with varying the input\naudio window size, speaker demographics, and training data ratio. Our results\nhighlight promising pathways for understanding and adopting speech foundation\nmodels to facilitate child speech understanding.\n","authors":["Anfeng Xu","Kevin Huang","Tiantian Feng","Lue Shen","Helen Tager-Flusberg","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2406.07890v1.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.07887v1","updated":"2024-06-12T05:25:15Z","published":"2024-06-12T05:25:15Z","title":"An Empirical Study of Mamba-based Language Models","summary":"  Selective state-space models (SSMs) like Mamba overcome some of the\nshortcomings of Transformers, such as quadratic computational complexity with\nsequence length and large inference-time memory requirements from the key-value\ncache. Moreover, recent studies have shown that SSMs can match or exceed the\nlanguage modeling capabilities of Transformers, making them an attractive\nalternative. In a controlled setting (e.g., same data), however, studies so far\nhave only presented small scale experiments comparing SSMs to Transformers. To\nunderstand the strengths and weaknesses of these architectures at larger\nscales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and\nTransformer models trained on the same datasets of up to 3.5T tokens. We also\ncompare these models to a hybrid architecture consisting of 43% Mamba-2, 7%\nattention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks,\nwe answer the question of whether Mamba models can match Transformers at larger\ntraining budgets. Our results show that while pure SSMs match or exceed\nTransformers on many tasks, they lag behind Transformers on tasks which require\nstrong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook)\nor long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid\nexceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points\non average) and is predicted to be up to 8x faster when generating tokens at\ninference time. To validate long-context capabilities, we provide additional\nexperiments evaluating variants of the Mamba-2-Hybrid and Transformer extended\nto support 16K, 32K, and 128K sequences. On an additional 23 long-context\ntasks, the hybrid model continues to closely match or exceed the Transformer on\naverage. To enable further study, we release the checkpoints as well as the\ncode used to train our models as part of NVIDIA's Megatron-LM project.\n","authors":["Roger Waleffe","Wonmin Byeon","Duncan Riach","Brandon Norick","Vijay Korthikanti","Tri Dao","Albert Gu","Ali Hatamizadeh","Sudhakar Singh","Deepak Narayanan","Garvit Kulshreshtha","Vartika Singh","Jared Casper","Jan Kautz","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2406.07887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07886v1","updated":"2024-06-12T05:24:58Z","published":"2024-06-12T05:24:58Z","title":"Label-aware Hard Negative Sampling Strategies with Momentum Contrastive\n  Learning for Implicit Hate Speech Detection","summary":"  Detecting implicit hate speech that is not directly hateful remains a\nchallenge. Recent research has attempted to detect implicit hate speech by\napplying contrastive learning to pre-trained language models such as BERT and\nRoBERTa, but the proposed models still do not have a significant advantage over\ncross-entropy loss-based learning. We found that contrastive learning based on\nrandomly sampled batch data does not encourage the model to learn hard negative\nsamples. In this work, we propose Label-aware Hard Negative sampling strategies\n(LAHN) that encourage the model to learn detailed features from hard negative\nsamples, instead of naive negative samples in random batch, using\nmomentum-integrated contrastive learning. LAHN outperforms the existing models\nfor implicit hate speech detection both in- and cross-datasets. The code is\navailable at https://github.com/Hanyang-HCC-Lab/LAHN\n","authors":["Jaehoon Kim","Seungwan Jin","Sohyun Park","Someen Park","Kyungsik Han"],"pdf_url":"https://arxiv.org/pdf/2406.07886v1.pdf","comment":"Accepted to ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.07882v1","updated":"2024-06-12T05:20:16Z","published":"2024-06-12T05:20:16Z","title":"Designing a Dashboard for Transparency and Control of Conversational AI","summary":"  Conversational LLMs function as black box systems, leaving users guessing\nabout why they see the output they do. This lack of transparency is potentially\nproblematic, especially given concerns around bias and truthfulness. To address\nthis issue, we present an end-to-end prototype-connecting interpretability\ntechniques with user experience design-that seeks to make chatbots more\ntransparent. We begin by showing evidence that a prominent open-source LLM has\na \"user model\": examining the internal state of the system, we can extract data\nrelated to a user's age, gender, educational level, and socioeconomic status.\nNext, we describe the design of a dashboard that accompanies the chatbot\ninterface, displaying this user model in real time. The dashboard can also be\nused to control the user model and the system's behavior. Finally, we discuss a\nstudy in which users conversed with the instrumented system. Our results\nsuggest that users appreciate seeing internal states, which helped them expose\nbiased behavior and increased their sense of control. Participants also made\nvaluable suggestions that point to future directions for both design and\nmachine learning research. The project page and video demo of our TalkTuner\nsystem are available at https://bit.ly/talktuner-project-page\n","authors":["Yida Chen","Aoyu Wu","Trevor DePodesta","Catherine Yeh","Kenneth Li","Nicholas Castillo Marin","Oam Patel","Jan Riecke","Shivam Raval","Olivia Seow","Martin Wattenberg","Fernanda Viégas"],"pdf_url":"https://arxiv.org/pdf/2406.07882v1.pdf","comment":"Project page: https://bit.ly/talktuner-project-page 38 pages, 23\n  figures"},{"id":"http://arxiv.org/abs/2404.17991v2","updated":"2024-06-12T04:47:01Z","published":"2024-04-27T19:42:51Z","title":"Enhancing Pre-Trained Generative Language Models with Question Attended\n  Span Extraction on Machine Reading Comprehension","summary":"  Machine Reading Comprehension (MRC) poses a significant challenge in the\nfield of Natural Language Processing (NLP). While mainstream MRC methods\npredominantly leverage extractive strategies using encoder-only models such as\nBERT, generative approaches face the issue of out-of-control generation -- a\ncritical problem where answers generated are often incorrect, irrelevant, or\nunfaithful to the source text. To address these limitations in generative\nmodels for MRC, we introduce the Question-Attended Span Extraction (QASE)\nmodule. Integrated during the fine-tuning phase of pre-trained generative\nlanguage models (PLMs), QASE significantly enhances their performance, allowing\nthem to surpass the extractive capabilities of advanced Large Language Models\n(LLMs) such as GPT-4 in few-shot settings. Notably, these gains in performance\ndo not come with an increase in computational demands. The efficacy of the QASE\nmodule has been rigorously tested across various datasets, consistently\nachieving or even surpassing state-of-the-art (SOTA) results, thereby bridging\nthe gap between generative and extractive models in extractive MRC tasks.\n","authors":["Lin Ai","Zheng Hui","Zizhou Liu","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2404.17991v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2403.04771"},{"id":"http://arxiv.org/abs/2405.07863v2","updated":"2024-06-12T04:40:53Z","published":"2024-05-13T15:50:39Z","title":"RLHF Workflow: From Reward Modeling to Online RLHF","summary":"  We present the workflow of Online Iterative Reinforcement Learning from Human\nFeedback (RLHF) in this technical report, which is widely reported to\noutperform its offline counterpart by a large margin in the recent large\nlanguage model (LLM) literature. However, existing open-source RLHF projects\nare still largely confined to the offline learning setting. In this technical\nreport, we aim to fill in this gap and provide a detailed recipe that is easy\nto reproduce for online iterative RLHF. In particular, since online human\nfeedback is usually infeasible for open-source communities with limited\nresources, we start by constructing preference models using a diverse set of\nopen-source datasets and use the constructed proxy preference model to\napproximate human feedback. Then, we discuss the theoretical insights and\nalgorithmic principles behind online iterative RLHF, followed by a detailed\npractical implementation. Our trained LLM, LLaMA-3-8B-SFR-Iterative-DPO-R,\nachieves impressive performance on LLM chatbot benchmarks, including\nAlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks\nsuch as HumanEval and TruthfulQA. We have shown that supervised fine-tuning\n(SFT) and iterative RLHF can obtain state-of-the-art performance with fully\nopen-source datasets. Further, we have made our models, curated datasets, and\ncomprehensive step-by-step code guidebooks publicly available. Please refer to\nhttps://github.com/RLHFlow/RLHF-Reward-Modeling and\nhttps://github.com/RLHFlow/Online-RLHF for more detailed information.\n","authors":["Hanze Dong","Wei Xiong","Bo Pang","Haoxiang Wang","Han Zhao","Yingbo Zhou","Nan Jiang","Doyen Sahoo","Caiming Xiong","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.07863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07860v1","updated":"2024-06-12T04:22:27Z","published":"2024-06-12T04:22:27Z","title":"BookSQL: A Large Scale Text-to-SQL Dataset for Accounting Domain","summary":"  Several large-scale datasets (e.g., WikiSQL, Spider) for developing natural\nlanguage interfaces to databases have recently been proposed. These datasets\ncover a wide breadth of domains but fall short on some essential domains, such\nas finance and accounting. Given that accounting databases are used worldwide,\nparticularly by non-technical people, there is an imminent need to develop\nmodels that could help extract information from accounting databases via\nnatural language queries. In this resource paper, we aim to fill this gap by\nproposing a new large-scale Text-to-SQL dataset for the accounting and\nfinancial domain: BookSQL. The dataset consists of 100k natural language\nqueries-SQL pairs, and accounting databases of 1 million records. We experiment\nwith and analyze existing state-of-the-art models (including GPT-4) for the\nText-to-SQL task on BookSQL. We find significant performance gaps, thus\npointing towards developing more focused models for this domain.\n","authors":["Rahul Kumar","Amar Raja Dibbu","Shrutendra Harsola","Vignesh Subrahmaniam","Ashutosh Modi"],"pdf_url":"https://arxiv.org/pdf/2406.07860v1.pdf","comment":"Accepted at NAACL 2024; 20 Pages (main + appendix)"},{"id":"http://arxiv.org/abs/2402.10073v3","updated":"2024-06-12T04:13:17Z","published":"2024-02-15T16:36:04Z","title":"Both Matter: Enhancing the Emotional Intelligence of Large Language\n  Models without Compromising the General Intelligence","summary":"  Emotional Intelligence (EI), consisting of emotion perception, emotion\ncognition and emotion expression, plays the critical roles in improving user\ninteraction experience for the current large language model (LLM) based\nconversational general AI assistants. Previous works mainly focus on raising\nthe emotion perception ability of them via naive fine-tuning on EI-related\nclassification or regression tasks. However, this leads to the incomplete\nenhancement of EI and catastrophic forgetting of the general intelligence (GI).\nTo this end, we first introduce \\textsc{EiBench}, a large-scale collection of\nEI-related tasks in the text-to-text formation with task instructions that\ncovers all three aspects of EI, which lays a solid foundation for the\ncomprehensive EI enhancement of LLMs. Then a novel \\underline{\\textbf{Mo}}dular\n\\underline{\\textbf{E}}motional \\underline{\\textbf{I}}ntelligence enhancement\nmethod (\\textbf{MoEI}), consisting of Modular Parameter Expansion and\nintra-inter modulation, is proposed to comprehensively enhance the EI of LLMs\nwithout compromise their GI. Extensive experiments on two representative\nLLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness\nof MoEI to improving EI while maintain GI.\n","authors":["Weixiang Zhao","Zhuojun Li","Shilong Wang","Yang Wang","Yulin Hu","Yanyan Zhao","Chen Wei","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2402.10073v3.pdf","comment":"To appear at Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2406.07855v1","updated":"2024-06-12T04:09:44Z","published":"2024-06-12T04:09:44Z","title":"VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via\n  Monotonic Alignment","summary":"  With the help of discrete neural audio codecs, large language models (LLM)\nhave increasingly been recognized as a promising methodology for zero-shot\nText-to-Speech (TTS) synthesis. However, sampling based decoding strategies\nbring astonishing diversity to generation, but also pose robustness issues such\nas typos, omissions and repetition. In addition, the high sampling rate of\naudio also brings huge computational overhead to the inference process of\nautoregression. To address these issues, we propose VALL-E R, a robust and\nefficient zero-shot TTS system, building upon the foundation of VALL-E.\nSpecifically, we introduce a phoneme monotonic alignment strategy to strengthen\nthe connection between phonemes and acoustic sequence, ensuring a more precise\nalignment by constraining the acoustic tokens to match their associated\nphonemes. Furthermore, we employ a codec-merging approach to downsample the\ndiscrete codes in shallow quantization layer, thereby accelerating the decoding\nspeed while preserving the high quality of speech output. Benefiting from these\nstrategies, VALL-E R obtains controllablity over phonemes and demonstrates its\nstrong robustness by approaching the WER of ground truth. In addition, it\nrequires fewer autoregressive steps, with over 60% time reduction during\ninference. This research has the potential to be applied to meaningful\nprojects, including the creation of speech for those affected by aphasia. Audio\nsamples will be available at: https://aka.ms/valler.\n","authors":["Bing Han","Long Zhou","Shujie Liu","Sanyuan Chen","Lingwei Meng","Yanming Qian","Yanqing Liu","Sheng Zhao","Jinyu Li","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2406.07855v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.07850v1","updated":"2024-06-12T03:38:45Z","published":"2024-06-12T03:38:45Z","title":"Dynamic Stochastic Decoding Strategy for Open-Domain Dialogue Generation","summary":"  Stochastic sampling strategies such as top-k and top-p have been widely used\nin dialogue generation task. However, as an open-domain chatting system, there\nwill be two different conversation scenarios, i.e. chit-chat and\nknowledge-based question answering. In the former situation, responses\ndiversity is essential due to the one-to-many nature in dialogue. The latter,\non the other hand, requires less randomness given that stochastic decoding\nstrategy entails the risk of generating incorrect information. As a result, an\nadaptive and flexible decoding strategy is needed to cope with these two\nscenarios simultaneously. To this end, we propose the dynamic decoding strategy\n(DDS), which can adjust the decoding space w.r.t. different contexts. In DDS,\nboth sequence-level and token-level adaptive search can be achieved to adjust\nthe decoding process in a unified framework. Besides, our adaptive algorithm\ncan not only be used during model inference, but it can also be applied during\nthe model training stage to further enhance the performance. Comprehensive\nexperiments indicate that the proposed decoding strategy can consistently\nimprove the performance of pre-trained dialogue models when coupled with four\nwell-used stochastic decoding algorithms.\n","authors":["Yiwei Li","Fei Mi","Yitong Li","Yasheng Wang","Bin Sun","Shaoxiong Feng","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2406.07850v1.pdf","comment":"ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2402.18150v2","updated":"2024-06-12T03:21:15Z","published":"2024-02-28T08:24:38Z","title":"Unsupervised Information Refinement Training of Large Language Models\n  for Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating additional information from retrieval. However, studies have\nshown that LLMs still face challenges in effectively using the retrieved\ninformation, even ignoring it or being misled by it. The key reason is that the\ntraining of LLMs does not clearly make LLMs learn how to utilize input\nretrieved texts with varied quality. In this paper, we propose a novel\nperspective that considers the role of LLMs in RAG as ``Information Refiner'',\nwhich means that regardless of correctness, completeness, or usefulness of\nretrieved texts, LLMs can consistently integrate knowledge within the retrieved\ntexts and model parameters to generate the texts that are more concise,\naccurate, and complete than the retrieved texts. To this end, we propose an\ninformation refinement training method named InFO-RAG that optimizes LLMs for\nRAG in an unsupervised manner. InFO-RAG is low-cost and general across various\ntasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse\ntasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,\nand Code Generation show that InFO-RAG improves the performance of LLaMA2 by an\naverage of 9.39\\% relative points. InFO-RAG also shows advantages in in-context\nlearning and robustness of RAG.\n","authors":["Shicheng Xu","Liang Pang","Mo Yu","Fandong Meng","Huawei Shen","Xueqi Cheng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.18150v2.pdf","comment":"ACL 2024 Main"},{"id":"http://arxiv.org/abs/2406.07842v1","updated":"2024-06-12T03:17:57Z","published":"2024-06-12T03:17:57Z","title":"Dual-Pipeline with Low-Rank Adaptation for New Language Integration in\n  Multilingual ASR","summary":"  This paper addresses challenges in integrating new languages into a\npre-trained multilingual automatic speech recognition (mASR) system,\nparticularly in scenarios where training data for existing languages is limited\nor unavailable. The proposed method employs a dual-pipeline with low-rank\nadaptation (LoRA). It maintains two data flow pipelines-one for existing\nlanguages and another for new languages. The primary pipeline follows the\nstandard flow through the pre-trained parameters of mASR, while the secondary\npipeline additionally utilizes language-specific parameters represented by LoRA\nand a separate output decoder module. Importantly, the proposed approach\nminimizes the performance degradation of existing languages and enables a\nlanguage-agnostic operation mode, facilitated by a decoder selection strategy.\nWe validate the effectiveness of the proposed method by extending the\npre-trained Whisper model to 19 new languages from the FLEURS dataset\n","authors":["Yerbolat Khassanov","Zhipeng Chen","Tianfeng Chen","Tze Yuang Chong","Wei Li","Jun Zhang","Lu Lu","Yuxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07842v1.pdf","comment":"5 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.07841v1","updated":"2024-06-12T03:16:45Z","published":"2024-06-12T03:16:45Z","title":"Labeling Comic Mischief Content in Online Videos with a Multimodal\n  Hierarchical-Cross-Attention Model","summary":"  We address the challenge of detecting questionable content in online media,\nspecifically the subcategory of comic mischief. This type of content combines\nelements such as violence, adult content, or sarcasm with humor, making it\ndifficult to detect. Employing a multimodal approach is vital to capture the\nsubtle details inherent in comic mischief content. To tackle this problem, we\npropose a novel end-to-end multimodal system for the task of comic mischief\ndetection. As part of this contribution, we release a novel dataset for the\ntargeted task consisting of three modalities: video, text (video captions and\nsubtitles), and audio. We also design a HIerarchical Cross-attention model with\nCAPtions (HICCAP) to capture the intricate relationships among these\nmodalities. The results show that the proposed approach makes a significant\nimprovement over robust baselines and state-of-the-art models for comic\nmischief detection and its type classification. This emphasizes the potential\nof our system to empower users, to make informed decisions about the online\ncontent they choose to see. In addition, we conduct experiments on the UCF101,\nHMDB51, and XD-Violence datasets, comparing our model against other\nstate-of-the-art approaches showcasing the outstanding performance of our\nproposed model in various scenarios.\n","authors":["Elaheh Baharlouei","Mahsa Shafaei","Yigeng Zhang","Hugo Jair Escalante","Thamar Solorio"],"pdf_url":"https://arxiv.org/pdf/2406.07841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04319v3","updated":"2024-06-12T03:02:45Z","published":"2024-01-09T02:25:23Z","title":"Know Your Needs Better: Towards Structured Understanding of Marketer\n  Demands with Analogical Reasoning Augmented LLMs","summary":"  In this paper, we explore a new way for user targeting, where non-expert\nmarketers could select their target users solely given demands in natural\nlanguage form. The key to this issue is how to transform natural languages into\npractical structured logical languages, i.e., the structured understanding of\nmarketer demands. In practical scenarios, the demands of non-expert marketers\nare often abstract and diverse. Considering the impressive natural language\nprocessing ability of large language models (LLMs), we try to leverage LLMs to\nsolve this issue. To stimulate the LLMs' reasoning ability, the\nchain-of-thought (CoT) prompting method is widely used, but existing methods\nstill have some limitations in our scenario: (1) Previous methods either use\nsimple \"Let's think step by step\" spells or provide fixed examples in\ndemonstrations without considering compatibility between prompts and concrete\nquestions, making LLMs ineffective when the marketers' demands are abstract and\ndiverse. (2) Previous methods are often implemented in closed-source models or\nexcessively large models, which is not suitable in industrial practical\nscenarios. Based on these, we propose ARALLM (i.e., Analogical Reasoning\nAugmented Large Language Models) consisting of two modules: Analogical\nReasoning based Prompting and Reasoning-Augmented Multi-Task Model\nDistillation. Part of our data and code can be found at\nhttps://github.com/alipay/Analogic-Reasoning-Augmented-Large-Language-Model.\n","authors":["Junjie Wang","Dan Yang","Binbin Hu","Yue Shen","Wen Zhang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2401.04319v3.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2406.07823v1","updated":"2024-06-12T02:46:17Z","published":"2024-06-12T02:46:17Z","title":"PRoDeliberation: Parallel Robust Deliberation for End-to-End Spoken\n  Language Understanding","summary":"  Spoken Language Understanding (SLU) is a critical component of voice\nassistants; it consists of converting speech to semantic parses for task\nexecution. Previous works have explored end-to-end models to improve the\nquality and robustness of SLU models with Deliberation, however these models\nhave remained autoregressive, resulting in higher latencies. In this work we\nintroduce PRoDeliberation, a novel method leveraging a Connectionist Temporal\nClassification-based decoding strategy as well as a denoising objective to\ntrain robust non-autoregressive deliberation models. We show that\nPRoDeliberation achieves the latency reduction of parallel decoding (2-10x\nimprovement over autoregressive models) while retaining the ability to correct\nAutomatic Speech Recognition (ASR) mistranscriptions of autoregressive\ndeliberation systems. We further show that the design of the denoising training\nallows PRoDeliberation to overcome the limitations of small ASR devices, and we\nprovide analysis on the necessity of each component of the system.\n","authors":["Trang Le","Daniel Lazar","Suyoun Kim","Shan Jiang","Duc Le","Adithya Sagar","Aleksandr Livshits","Ahmed Aly","Akshat Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2406.07823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02060v3","updated":"2024-06-12T02:46:16Z","published":"2024-04-02T15:59:11Z","title":"Long-context LLMs Struggle with Long In-context Learning","summary":"  Large Language Models (LLMs) have made significant strides in handling long\nsequences. Some models like Gemini could even to be capable of dealing with\nmillions of tokens. However, their performance evaluation has largely been\nconfined to metrics like perplexity and synthetic tasks, which may not fully\ncapture their true abilities in more challenging, real-world scenarios. We\nintroduce a benchmark (LongICLBench) for long in-context learning in\nextreme-label classification using six datasets with 28 to 174 classes and\ninput lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend\nthe entire input to recognize the massive label spaces to make correct\npredictions. We evaluate on 15 long-context LLMs and find that they perform\nwell on less challenging classification tasks with smaller label space and\nshorter demonstrations. However, they struggle with more challenging task like\nDiscovery with 174 labels, suggesting a gap in their ability to process long,\ncontext-rich sequences. Further analysis reveals a bias towards labels\npresented later in the sequence and a need for improved reasoning over multiple\npieces of information. Our study reveals that long context understanding and\nreasoning is still a challenging task for the existing LLMs. We believe\nLongICLBench could serve as a more realistic evaluation for the future\nlong-context LLMs.\n","authors":["Tianle Li","Ge Zhang","Quy Duc Do","Xiang Yue","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2404.02060v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07822v1","updated":"2024-06-12T02:43:19Z","published":"2024-06-12T02:43:19Z","title":"Tell Me What's Next: Textual Foresight for Generic UI Representations","summary":"  Mobile app user interfaces (UIs) are rich with action, text, structure, and\nimage content that can be utilized to learn generic UI representations for\ntasks like automating user commands, summarizing content, and evaluating the\naccessibility of user interfaces. Prior work has learned strong visual\nrepresentations with local or global captioning losses, but fails to retain\nboth granularities. To combat this, we propose Textual Foresight, a novel\npretraining objective for learning UI screen representations. Textual Foresight\ngenerates global text descriptions of future UI states given a current UI and\nlocal action taken. Our approach requires joint reasoning over elements and\nentire screens, resulting in improved UI features: on generation tasks, UI\nagents trained with Textual Foresight outperform state-of-the-art by 2% with\n28x fewer images. We train with our newly constructed mobile app dataset,\nOpenApp, which results in the first public dataset for app UI representation\nlearning. OpenApp enables new baselines, and we find Textual Foresight improves\naverage task performance over them by 5.7% while having access to 2x less data.\n","authors":["Andrea Burns","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2406.07822v1.pdf","comment":"Accepted to ACL 2024 Findings. Data and code to be released at\n  https://github.com/aburns4/textualforesight"},{"id":"http://arxiv.org/abs/2402.06853v2","updated":"2024-06-12T02:28:12Z","published":"2024-02-10T01:18:15Z","title":"History, Development, and Principles of Large Language Models-An\n  Introductory Survey","summary":"  Language models serve as a cornerstone in natural language processing (NLP),\nutilizing mathematical methods to generalize language laws and knowledge for\nprediction and generation. Over extensive research spanning decades, language\nmodeling has progressed from initial statistical language models (SLMs) to the\ncontemporary landscape of large language models (LLMs). Notably, the swift\nevolution of LLMs has reached the ability to process, understand, and generate\nhuman-level text. Nevertheless, despite the significant advantages that LLMs\noffer in improving both work and personal lives, the limited understanding\namong general practitioners about the background and principles of these models\nhampers their full potential. Notably, most LLMs reviews focus on specific\naspects and utilize specialized language, posing a challenge for practitioners\nlacking relevant background knowledge. In light of this, this survey aims to\npresent a comprehensible overview of LLMs to assist a broader audience. It\nstrives to facilitate a comprehensive understanding by exploring the historical\nbackground of language models and tracing their evolution over time. The survey\nfurther investigates the factors influencing the development of LLMs,\nemphasizing key contributions. Additionally, it concentrates on elucidating the\nunderlying principles of LLMs, equipping audiences with essential theoretical\nknowledge. The survey also highlights the limitations of existing work and\npoints out promising future directions.\n","authors":["Zhibo Chu","Shiwen Ni","Zichong Wang","Xi Feng","Min Yang","Wenbin Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.06853v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07816v1","updated":"2024-06-12T02:23:57Z","published":"2024-06-12T02:23:57Z","title":"Spoof Diarization: \"What Spoofed When\" in Partially Spoofed Audio","summary":"  This paper defines Spoof Diarization as a novel task in the Partial Spoof\n(PS) scenario. It aims to determine what spoofed when, which includes not only\nlocating spoof regions but also clustering them according to different spoofing\nmethods. As a pioneering study in spoof diarization, we focus on defining the\ntask, establishing evaluation metrics, and proposing a benchmark model, namely\nthe Countermeasure-Condition Clustering (3C) model. Utilizing this model, we\nfirst explore how to effectively train countermeasures to support spoof\ndiarization using three labeling schemes. We then utilize spoof localization\npredictions to enhance the diarization performance. This first study reveals\nthe high complexity of the task, even in restricted scenarios where only a\nsingle speaker per audio file and an oracle number of spoofing methods are\nconsidered. Our code is available at\nhttps://github.com/nii-yamagishilab/PartialSpoof.\n","authors":["Lin Zhang","Xin Wang","Erica Cooper","Mireia Diez","Federico Landini","Nicholas Evans","Junichi Yamagishi"],"pdf_url":"https://arxiv.org/pdf/2406.07816v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.07815v1","updated":"2024-06-12T02:23:51Z","published":"2024-06-12T02:23:51Z","title":"Are Large Language Models Good Statisticians?","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of scientific tasks including mathematics, physics, and chemistry.\nDespite their successes, the effectiveness of LLMs in handling complex\nstatistical tasks remains systematically under-explored. To bridge this gap, we\nintroduce StatQA, a new benchmark designed for statistical analysis tasks.\nStatQA comprises 11,623 examples tailored to evaluate LLMs' proficiency in\nspecialized statistical tasks and their applicability assessment capabilities,\nparticularly for hypothesis testing methods. We systematically experiment with\nrepresentative LLMs using various prompting strategies and show that even\nstate-of-the-art models such as GPT-4o achieve a best performance of only\n64.83%, indicating significant room for improvement. Notably, while open-source\nLLMs (e.g. LLaMA-3) show limited capability, those fine-tuned ones exhibit\nmarked improvements, outperforming all in-context learning-based methods (e.g.\nGPT-4o). Moreover, our comparative human experiments highlight a striking\ncontrast in error types between LLMs and humans: LLMs primarily make\napplicability errors, whereas humans mostly make statistical task confusion\nerrors. This divergence highlights distinct areas of proficiency and\ndeficiency, suggesting that combining LLM and human expertise could lead to\ncomplementary strengths, inviting further investigation into their\ncollaborative potential.\n","authors":["Yizhang Zhu","Shiyin Du","Boyan Li","Yuyu Luo","Nan Tang"],"pdf_url":"https://arxiv.org/pdf/2406.07815v1.pdf","comment":"31 pages, 10 figures,19 tables. Work in progress"},{"id":"http://arxiv.org/abs/2406.07814v1","updated":"2024-06-12T02:20:46Z","published":"2024-06-12T02:20:46Z","title":"Collective Constitutional AI: Aligning a Language Model with Public\n  Input","summary":"  There is growing consensus that language model (LM) developers should not be\nthe sole deciders of LM behavior, creating a need for methods that enable the\nbroader public to collectively shape the behavior of LM systems that affect\nthem. To address this need, we present Collective Constitutional AI (CCAI): a\nmulti-stage process for sourcing and integrating public input into LMs-from\nidentifying a target population to sourcing principles to training and\nevaluating a model. We demonstrate the real-world practicality of this approach\nby creating what is, to our knowledge, the first LM fine-tuned with\ncollectively sourced public input and evaluating this model against a baseline\nmodel trained with established principles from a LM developer. Our quantitative\nevaluations demonstrate several benefits of our approach: the CCAI-trained\nmodel shows lower bias across nine social dimensions compared to the baseline\nmodel, while maintaining equivalent performance on language, math, and\nhelpful-harmless evaluations. Qualitative comparisons of the models suggest\nthat the models differ on the basis of their respective constitutions, e.g.,\nwhen prompted with contentious topics, the CCAI-trained model tends to generate\nresponses that reframe the matter positively instead of a refusal. These\nresults demonstrate a promising, tractable pathway toward publicly informed\ndevelopment of language models.\n","authors":["Saffron Huang","Divya Siddarth","Liane Lovitt","Thomas I. Liao","Esin Durmus","Alex Tamkin","Deep Ganguli"],"pdf_url":"https://arxiv.org/pdf/2406.07814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07812v1","updated":"2024-06-12T02:08:45Z","published":"2024-06-12T02:08:45Z","title":"To be Continuous, or to be Discrete, Those are Bits of Questions","summary":"  Recently, binary representation has been proposed as a novel representation\nthat lies between continuous and discrete representations. It exhibits\nconsiderable information-preserving capability when being used to replace\ncontinuous input vectors. In this paper, we investigate the feasibility of\nfurther introducing it to the output side, aiming to allow models to output\nbinary labels instead. To preserve the structural information on the output\nside along with label information, we extend the previous contrastive hashing\nmethod as structured contrastive hashing. More specifically, we upgrade CKY\nfrom label-level to bit-level, define a new similarity function with span\nmarginal probabilities, and introduce a novel contrastive loss function with a\ncarefully designed instance selection strategy. Our model achieves competitive\nperformance on various structured prediction tasks, and demonstrates that\nbinary representation can be considered a novel representation that further\nbridges the gap between the continuous nature of deep learning and the discrete\nintrinsic property of natural languages.\n","authors":["Yiran Wang","Masao Utiyama"],"pdf_url":"https://arxiv.org/pdf/2406.07812v1.pdf","comment":"ACL-2024"},{"id":"http://arxiv.org/abs/2402.09025v4","updated":"2024-06-12T01:40:12Z","published":"2024-02-14T09:01:13Z","title":"SLEB: Streamlining LLMs through Redundancy Verification and Elimination\n  of Transformer Blocks","summary":"  Large language models (LLMs) have proven to be highly effective across\nvarious natural language processing tasks. However, their large number of\nparameters poses significant challenges for practical deployment. Pruning, a\ntechnique aimed at reducing the size and complexity of LLMs, offers a potential\nsolution by removing redundant components from the network. Despite the promise\nof pruning, existing methods often struggle to achieve substantial end-to-end\nLLM inference speedup. In this paper, we introduce SLEB, a novel approach\ndesigned to streamline LLMs by eliminating redundant transformer blocks. We\nchoose the transformer block as the fundamental unit for pruning, because LLMs\nexhibit block-level redundancy with high similarity between the outputs of\nneighboring blocks. This choice allows us to effectively enhance the processing\nspeed of LLMs. Our experimental results demonstrate that SLEB outperforms\nprevious LLM pruning methods in accelerating LLM inference while also\nmaintaining superior perplexity and accuracy, making SLEB as a promising\ntechnique for enhancing the efficiency of LLMs. The code is available at:\nhttps://github.com/jiwonsong-dev/SLEB.\n","authors":["Jiwon Song","Kyungseok Oh","Taesu Kim","Hyungjun Kim","Yulhwa Kim","Jae-Joon Kim"],"pdf_url":"https://arxiv.org/pdf/2402.09025v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07801v1","updated":"2024-06-12T01:35:46Z","published":"2024-06-12T01:35:46Z","title":"PolySpeech: Exploring Unified Multitask Speech Models for\n  Competitiveness with Single-task Models","summary":"  Recently, there have been attempts to integrate various speech processing\ntasks into a unified model. However, few previous works directly demonstrated\nthat joint optimization of diverse tasks in multitask speech models has\npositive influence on the performance of individual tasks. In this paper we\npresent a multitask speech model -- PolySpeech, which supports speech\nrecognition, speech synthesis, and two speech classification tasks. PolySpeech\ntakes multi-modal language model as its core structure and uses semantic\nrepresentations as speech inputs. We introduce semantic speech embedding\ntokenization and speech reconstruction methods to PolySpeech, enabling\nefficient generation of high-quality speech for any given speaker. PolySpeech\nshows competitiveness across various tasks compared to single-task models. In\nour experiments, multitask optimization achieves performance comparable to\nsingle-task optimization and is especially beneficial for specific tasks.\n","authors":["Runyan Yang","Huibao Yang","Xiqing Zhang","Tiantian Ye","Ying Liu","Yingying Gao","Shilei Zhang","Chao Deng","Junlan Feng"],"pdf_url":"https://arxiv.org/pdf/2406.07801v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.02009v2","updated":"2024-06-12T01:33:56Z","published":"2024-06-04T06:43:34Z","title":"Phonetic Enhanced Language Modeling for Text-to-Speech Synthesis","summary":"  Recent language model-based text-to-speech (TTS) frameworks demonstrate\nscalability and in-context learning capabilities. However, they suffer from\nrobustness issues due to the accumulation of errors in speech unit predictions\nduring autoregressive language modeling. In this paper, we propose a phonetic\nenhanced language modeling method to improve the performance of TTS models. We\nleverage self-supervised representations that are phonetically rich as the\ntraining target for the autoregressive language model. Subsequently, a\nnon-autoregressive model is employed to predict discrete acoustic codecs that\ncontain fine-grained acoustic details. The TTS model focuses solely on\nlinguistic modeling during autoregressive training, thereby reducing the error\npropagation that occurs in non-autoregressive training. Both objective and\nsubjective evaluations validate the effectiveness of our proposed method.\n","authors":["Kun Zhou","Shengkui Zhao","Yukun Ma","Chong Zhang","Hao Wang","Dianwen Ng","Chongjia Ni","Nguyen Trung Hieu","Jia Qi Yip","Bin Ma"],"pdf_url":"https://arxiv.org/pdf/2406.02009v2.pdf","comment":"Accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2401.05054v2","updated":"2024-06-12T01:27:32Z","published":"2024-01-10T10:23:41Z","title":"Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding","summary":"  One of the most important challenges in text generation systems is to produce\noutputs that are not only correct but also diverse. Recently, Minimum\nBayes-Risk (MBR) decoding has gained prominence for generating sentences of the\nhighest quality among the decoding algorithms. However, existing algorithms\nproposed for generating diverse outputs are predominantly based on beam search\nor random sampling, thus their output quality is capped by these underlying\nmethods. In this paper, we investigate an alternative approach -- we develop\ndiversity-promoting decoding algorithms by enforcing diversity objectives to\nMBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and\n$k$-medoids MBR (KMBR), methods to generate a set of sentences with high\nquality and diversity. We evaluate DMBR and KMBR on a variety of directed text\ngeneration tasks using encoder-decoder models and a large language model with\nprompting. The experimental results show that the proposed method achieves a\nbetter trade-off than the diverse beam search and sampling algorithms.\n","authors":["Yuu Jinnai","Ukyo Honda","Tetsuro Morimura","Peinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.05054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07794v1","updated":"2024-06-12T01:18:04Z","published":"2024-06-12T01:18:04Z","title":"IndirectRequests: Making Task-Oriented Dialogue Datasets More Natural by\n  Synthetically Generating Indirect User Requests","summary":"  Existing benchmark corpora of task-oriented dialogue are collected either\nusing a \"machines talking to machines\" approach or by giving template-based\ngoal descriptions to crowdworkers. These methods, however, often produce\nutterances that are markedly different from natural human conversations in\nwhich people often convey their preferences in indirect ways, such as through\nsmall talk. We term such utterances as Indirect User Requests (IURs).\nUnderstanding such utterances demands considerable world knowledge and\nreasoning capabilities on the listener's part. Our study introduces an\nLLM-based pipeline to automatically generate realistic, high-quality IURs for a\ngiven domain, with the ultimate goal of supporting research in natural language\nunderstanding (NLU) and dialogue state tracking (DST) for task-oriented\ndialogue systems. Our findings show that while large LLMs such as GPT-3.5 and\nGPT-4 generate high-quality IURs, achieving similar quality with smaller models\nis more challenging. We release IndirectRequests, a dataset of IURs that\nadvances beyond the initial Schema-Guided Dialog (SGD) dataset in that it\nprovides a challenging testbed for testing the \"in the wild\" performance of NLU\nand DST models.\n","authors":["Amogh Mannekote","Jinseok Nam","Ziming Li","Jian Gao","Kristy Elizabeth Boyer","Bonnie J. Dorr"],"pdf_url":"https://arxiv.org/pdf/2406.07794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02749v2","updated":"2024-06-12T01:14:45Z","published":"2024-01-05T11:02:08Z","title":"Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding","summary":"  Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to\nbeam search decoding for a wide range of text generation tasks. However, MBR\nrequires a huge amount of time for inference to compute the MBR objective,\nwhich makes the method infeasible in many situations where response time is\ncritical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently\nbeen proposed to reduce the inference time in machine translation tasks.\nAlthough it is shown to significantly reduce the amount of computation, it\nrequires hyperparameter tuning using a development set to be effective. To this\nend, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a\nhyperparameter-free method to run MBR decoding approximately. AMBR is derived\nfrom the observation that the problem of computing the sample-based MBR\nobjective is the medoid identification problem. AMBR uses the Correlated\nSequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best\napproximation algorithm to date for the medoid identification problem, to\ncompute the sample-based MBR objective. We evaluate AMBR on machine\ntranslation, text summarization, and image captioning tasks. The results show\nthat AMBR achieves on par with CBP, with CBP selecting hyperparameters through\nan Oracle for each given computation budget.\n","authors":["Yuu Jinnai","Kaito Ariu"],"pdf_url":"https://arxiv.org/pdf/2401.02749v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07791v1","updated":"2024-06-12T01:12:28Z","published":"2024-06-12T01:12:28Z","title":"Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs","summary":"  LLM-as-a-Judge offers a promising alternative to human judges across various\ntasks, yet inherent biases, particularly position bias - a systematic\npreference for answers based on their position in the prompt - compromise its\neffectiveness. Our study investigates this issue by developing a framework to\nsystematically study and quantify position bias using metrics such as\nrepetitional consistency, positional consistency, and positional fairness. We\nconduct experiments with 9 judge models across 22 tasks from the MTBench and\nDevBench benchmarks and nearly 40 answer-generating models, generating\napproximately 80,000 evaluation instances. This comprehensive assessment\nreveals significant variations in bias across judges and tasks. Although GPT-4\noften excels in positional consistency and fairness, some more cost-effective\nmodels perform comparably or even better in specific tasks, highlighting\nessential trade-offs between consistency, fairness, and cost. Our results also\ndemonstrate high consistency of judgment across repetitions, confirming that\nposition bias is not due to random variations. This research significantly\ncontributes to the field by introducing new concepts for understanding position\nbias and providing a multi-dimensional framework for evaluation. These insights\nguide the selection of optimal judge models, enhance benchmark design, and lay\nthe foundation for future research into effective debiasing strategies,\nultimately enhancing the reliability of LLM evaluators.\n","authors":["Lin Shi","Weicheng Ma","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2406.07791v1.pdf","comment":"70 pages, around 200 figures and subfigures"},{"id":"http://arxiv.org/abs/2406.07348v2","updated":"2024-06-12T01:06:59Z","published":"2024-06-11T15:15:33Z","title":"DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented\n  Generation for Question-Answering","summary":"  Retrieval-Augmented Generation (RAG) has significantly demonstrated the\nperformance of Large Language Models (LLMs) in the knowledge-intensive tasks,\nsuch as Question-Answering (QA). RAG expands the query context by incorporating\nexternal knowledge bases to enhance the response accuracy. However, it would be\ninefficient to access LLMs multiple times for each query and unreliable to\nretrieve all the relevant documents by a single query. We find that even though\nthere is low relevance between some critical documents and query, it is\npossible to retrieve the remaining documents by combining parts of the\ndocuments with the query. To mine the relevance, a two-stage retrieval\nframework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is\nproposed to improve document retrieval recall and the accuracy of answers while\nmaintaining efficiency. Also, a small classifier is applied to two different\nselection strategies to determine the contribution of the retrieved documents\nto answering the query and retrieve the relatively relevant documents.\nMeanwhile, DR-RAG call the LLMs only once, which significantly improves the\nefficiency of the experiment. The experimental results on multi-hop QA datasets\nshow that DR-RAG can significantly improve the accuracy of the answers and\nachieve new progress in QA systems.\n","authors":["Zijian Hei","Weiling Liu","Wenjie Ou","Juyi Qiao","Junming Jiao","Zhiqing Zhu","Guowen Song"],"pdf_url":"https://arxiv.org/pdf/2406.07348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05263v2","updated":"2024-06-12T01:00:09Z","published":"2023-11-09T10:46:09Z","title":"Model-Based Minimum Bayes Risk Decoding for Text Generation","summary":"  Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative\nto beam search decoding in a variety of text generation tasks. MBR decoding\nselects a hypothesis from a pool of hypotheses that has the least expected risk\nunder a probability model according to a given utility function. Since it is\nimpractical to compute the expected risk exactly over all possible hypotheses,\ntwo approximations are commonly used in MBR. First, it integrates over a\nsampled set of hypotheses rather than over all possible hypotheses. Second, it\nestimates the probability of each hypothesis using a Monte Carlo estimator.\nWhile the first approximation is necessary to make it computationally feasible,\nthe second is not essential since we typically have access to the model\nprobability at inference time. We propose Model-Based MBR (MBMBR), a variant of\nMBR that uses the model probability itself as the estimate of the probability\ndistribution instead of the Monte Carlo estimate. We show analytically and\nempirically that the model-based estimate is more promising than the Monte\nCarlo estimate in text generation tasks. Our experiments show that MBMBR\noutperforms MBR in several text generation tasks, both with encoder-decoder\nmodels and with large language models.\n","authors":["Yuu Jinnai","Tetsuro Morimura","Ukyo Honda","Kaito Ariu","Kenshi Abe"],"pdf_url":"https://arxiv.org/pdf/2311.05263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14348v3","updated":"2024-06-12T00:27:06Z","published":"2023-09-18T02:07:22Z","title":"Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM","summary":"  Recently, Large Language Models (LLMs) have made significant advancements and\nare now widely used across various domains. Unfortunately, there has been a\nrising concern that LLMs can be misused to generate harmful or malicious\ncontent. Though a line of research has focused on aligning LLMs with human\nvalues and preventing them from producing inappropriate content, such\nalignments are usually vulnerable and can be bypassed by alignment-breaking\nattacks via adversarially optimized or handcrafted jailbreaking prompts. In\nthis work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against\npotential alignment-breaking attacks. RA-LLM can be directly constructed upon\nan existing aligned LLM with a robust alignment checking function, without\nrequiring any expensive retraining or fine-tuning process of the original LLM.\nFurthermore, we also provide a theoretical analysis for RA-LLM to verify its\neffectiveness in defending against alignment-breaking attacks. Through\nreal-world experiments on open-source large language models, we demonstrate\nthat RA-LLM can successfully defend against both state-of-the-art adversarial\nprompts and popular handcrafted jailbreaking prompts by reducing their attack\nsuccess rates from nearly 100% to around 10% or less.\n","authors":["Bochuan Cao","Yuanpu Cao","Lu Lin","Jinghui Chen"],"pdf_url":"https://arxiv.org/pdf/2309.14348v3.pdf","comment":"19 Pages, 5 Figures, 8 Tables. Accepted by ACL 2024"},{"id":"http://arxiv.org/abs/2406.07780v1","updated":"2024-06-12T00:19:40Z","published":"2024-06-12T00:19:40Z","title":"A Critical Look At Tokenwise Reward-Guided Text Generation","summary":"  Large language models (LLMs) can significantly be improved by aligning to\nhuman preferences -- the so-called reinforcement learning from human feedback\n(RLHF). However, the cost of fine-tuning an LLM is prohibitive for many users.\nDue to their ability to bypass LLM finetuning, tokenwise reward-guided text\ngeneration (RGTG) methods have recently been proposed. They use a reward model\ntrained on full sequences to score partial sequences during a tokenwise\ndecoding, in a bid to steer the generation towards sequences with high rewards.\nHowever, these methods have so far been only heuristically motivated and poorly\nanalyzed. In this work, we show that reward models trained on full sequences\nare not compatible with scoring partial sequences. To alleviate this issue, we\npropose to explicitly train a Bradley-Terry reward model on partial sequences,\nand autoregressively sample from the implied tokenwise policy during decoding\ntime. We study the property of this reward model and the implied policy. In\nparticular, we show that this policy is proportional to the ratio of two\ndistinct RLHF policies. We show that our simple approach outperforms previous\nRGTG methods and achieves similar performance as strong offline baselines but\nwithout large-scale LLM finetuning.\n","authors":["Ahmad Rashid","Ruotian Wu","Julia Grosse","Agustinus Kristiadi","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2406.07780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07778v1","updated":"2024-06-12T00:01:32Z","published":"2024-06-12T00:01:32Z","title":"On Trojans in Refined Language Models","summary":"  A Trojan in a language model can be inserted when the model is refined for a\nparticular application such as determining the sentiment of product reviews. In\nthis paper, we clarify and empirically explore variations of the data-poisoning\nthreat model. We then empirically assess two simple defenses each for a\ndifferent defense scenario. Finally, we provide a brief survey of related\nattacks and defenses.\n","authors":["Jayaram Raghuram","George Kesidis","David J. Miller"],"pdf_url":"https://arxiv.org/pdf/2406.07778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08680v1","updated":"2024-06-12T22:43:38Z","published":"2024-06-12T22:43:38Z","title":"Analyzing Large Language Models for Classroom Discussion Assessment","summary":"  Automatically assessing classroom discussion quality is becoming increasingly\nfeasible with the help of new NLP advancements such as large language models\n(LLMs). In this work, we examine how the assessment performance of 2 LLMs\ninteracts with 3 factors that may affect performance: task formulation, context\nlength, and few-shot examples. We also explore the computational efficiency and\npredictive consistency of the 2 LLMs. Our results suggest that the 3\naforementioned factors do affect the performance of the tested LLMs and there\nis a relation between consistency and performance. We recommend a LLM-based\nassessment approach that has a good balance in terms of predictive performance,\ncomputational efficiency, and consistency.\n","authors":["Nhat Tran","Benjamin Pierce","Diane Litman","Richard Correnti","Lindsay Clare Matsumura"],"pdf_url":"https://arxiv.org/pdf/2406.08680v1.pdf","comment":"EDM 2024 Short Paper"},{"id":"http://arxiv.org/abs/2406.08673v1","updated":"2024-06-12T22:28:08Z","published":"2024-06-12T22:28:08Z","title":"HelpSteer2: Open-source dataset for training top-performing reward\n  models","summary":"  High-quality preference datasets are essential for training reward models\nthat can effectively guide large language models (LLMs) in generating\nhigh-quality responses aligned with human preferences. As LLMs become stronger\nand better aligned, permissively licensed preference datasets, such as Open\nAssistant, HH-RLHF, and HelpSteer need to be updated to remain effective for\nreward modeling. Methods that distil preference data from proprietary LLMs such\nas GPT-4 have restrictions on commercial usage imposed by model providers. To\nimprove upon both generated responses and attribute labeling quality, we\nrelease HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0).\nUsing a powerful internal base model trained on HelpSteer2, we are able to\nachieve the SOTA score (92.0%) on Reward-Bench's primary dataset, outperforming\ncurrently listed open and proprietary models, as of June 12th, 2024. Notably,\nHelpSteer2 consists of only ten thousand response pairs, an order of magnitude\nfewer than existing preference datasets (e.g., HH-RLHF), which makes it highly\nefficient for training reward models. Our extensive experiments demonstrate\nthat reward models trained with HelpSteer2 are effective in aligning LLMs. In\nparticular, we propose SteerLM 2.0, a model alignment approach that can\neffectively make use of the rich multi-attribute score predicted by our reward\nmodels. HelpSteer2 is available at\nhttps://huggingface.co/datasets/nvidia/HelpSteer2 and code is available at\nhttps://github.com/NVIDIA/NeMo-Aligner\n","authors":["Zhilin Wang","Yi Dong","Olivier Delalleau","Jiaqi Zeng","Gerald Shen","Daniel Egert","Jimmy J. Zhang","Makesh Narsimhan Sreedhar","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2406.08673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08660v1","updated":"2024-06-12T21:46:13Z","published":"2024-06-12T21:46:13Z","title":"Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot\n  Generative AI Models in Text Classification","summary":"  Generative AI offers a simple, prompt-based alternative to fine-tuning\nsmaller BERT-style LLMs for text classification tasks. This promises to\neliminate the need for manually labeled training data and task-specific model\ntraining. However, it remains an open question whether tools like ChatGPT can\ndeliver on this promise. In this paper, we show that smaller, fine-tuned LLMs\n(still) consistently and significantly outperform larger, zero-shot prompted\nmodels in text classification. We compare three major generative AI models\n(ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs\nacross a diverse set of classification tasks (sentiment, approval/disapproval,\nemotions, party positions) and text categories (news, tweets, speeches). We\nfind that fine-tuning with application-specific training data achieves superior\nperformance in all cases. To make this approach more accessible to a broader\naudience, we provide an easy-to-use toolkit alongside this paper. Our toolkit,\naccompanied by non-technical step-by-step guidance, enables users to select and\nfine-tune BERT-like LLMs for any classification task with minimal technical and\ncomputational effort.\n","authors":["Martin Juan José Bucher","Marco Martini"],"pdf_url":"https://arxiv.org/pdf/2406.08660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08657v1","updated":"2024-06-12T21:42:13Z","published":"2024-06-12T21:42:13Z","title":"Mistral-C2F: Coarse to Fine Actor for Analytical and Reasoning\n  Enhancement in RLHF and Effective-Merged LLMs","summary":"  Despite the advances in Large Language Models (LLMs), exemplified by models\nlike GPT-4 and Claude, smaller-scale LLMs such as Llama and Mistral often\nstruggle with generating in-depth and coherent dialogues. This paper presents a\nnovel two-step Coarse-to-Fine Actor model to address the inherent limitations\nin conversational and analytical capabilities of small-sized LLMs. Our approach\nbegins with the Policy-based Coarse Actor, employing a technique we term\n\"Continuous Maximization\". The Coarse Actor establishes an enhanced,\nknowledge-rich pool adept at aligning with human preference styles in analysis\nand reasoning. Through the RLHF process, it employs Continuous Maximization, a\nstrategy that dynamically and adaptively extends the output length limit,\nenabling the generation of more detailed and analytical content. Subsequently,\nthe Fine Actor refines this analytical content, addressing the generation of\nexcessively redundant information from the Coarse Actor. We introduce a\n\"Knowledge Residue Merger\" approach, refining the content from the Coarse Actor\nand merging it with an existing Instruction model to improve quality,\ncorrectness, and reduce redundancies. We applied our methodology to the popular\nMistral model, creating Mistral-C2F, which has demonstrated exceptional\nperformance across 11 general language tasks and the MT-Bench Dialogue task,\noutperforming similar-scale models and even larger models with 13B and 30B\nparameters. Our model has significantly improved conversational and analytical\nreasoning abilities.\n","authors":["Chen Zheng","Ke Sun","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.08657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08656v1","updated":"2024-06-12T21:41:32Z","published":"2024-06-12T21:41:32Z","title":"TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and\n  Image-to-Video Generation","summary":"  Video generation has many unique challenges beyond those of image generation.\nThe temporal dimension introduces extensive possible variations across frames,\nover which consistency and continuity may be violated. In this study, we move\nbeyond evaluating simple actions and argue that generated videos should\nincorporate the emergence of new concepts and their relation transitions like\nin real-world videos as time progresses. To assess the Temporal\nCompositionality of video generation models, we propose TC-Bench, a benchmark\nof meticulously crafted text prompts, corresponding ground truth videos, and\nrobust evaluation metrics. The prompts articulate the initial and final states\nof scenes, effectively reducing ambiguities for frame development and\nsimplifying the assessment of transition completion. In addition, by collecting\naligned real-world videos corresponding to the prompts, we expand TC-Bench's\napplicability from text-conditional models to image-conditional ones that can\nperform generative frame interpolation. We also develop new metrics to measure\nthe completeness of component transitions in generated videos, which\ndemonstrate significantly higher correlations with human judgments than\nexisting metrics. Our comprehensive experimental results reveal that most video\ngenerators achieve less than 20% of the compositional changes, highlighting\nenormous space for future improvement. Our analysis indicates that current\nvideo generation models struggle to interpret descriptions of compositional\nchanges and synthesize various components across different time steps.\n","authors":["Weixi Feng","Jiachen Li","Michael Saxon","Tsu-jui Fu","Wenhu Chen","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15610v2","updated":"2024-06-12T21:09:39Z","published":"2024-02-23T21:16:52Z","title":"Selective \"Selective Prediction\": Reducing Unnecessary Abstention in\n  Vision-Language Reasoning","summary":"  Selective prediction minimizes incorrect predictions from vision-language\nmodels (VLMs) by allowing them to abstain from answering when uncertain.\nHowever, when deploying a vision-language system with low tolerance for\ninaccurate predictions, selective prediction may be over-cautious and abstain\ntoo frequently, even on many correct predictions. We introduce ReCoVERR, an\ninference-time algorithm to reduce the over-abstention of a selective\nvision-language system without increasing the error rate of the system's\npredictions. When the VLM makes a low-confidence prediction, instead of\nabstaining ReCoVERR tries to find relevant clues in the image that provide\nadditional evidence for the prediction. ReCoVERR uses an LLM to pose related\nquestions to the VLM, collects high-confidence evidences, and if enough\nevidence confirms the prediction the system makes a prediction instead of\nabstaining. ReCoVERR enables three VLMs (BLIP2, InstructBLIP, and LLaVA-1.5) to\nanswer up to 20% more questions on the VQAv2 and A-OKVQA tasks without\ndecreasing system accuracy, thus improving overall system reliability. Our code\nis available at https://github.com/tejas1995/ReCoVERR.\n","authors":["Tejas Srinivasan","Jack Hessel","Tanmay Gupta","Bill Yuchen Lin","Yejin Choi","Jesse Thomason","Khyathi Raghavi Chandu"],"pdf_url":"https://arxiv.org/pdf/2402.15610v2.pdf","comment":"Accepted to ACL Findings 2024"},{"id":"http://arxiv.org/abs/2402.05889v2","updated":"2024-06-12T21:03:33Z","published":"2024-02-08T18:27:22Z","title":"CREMA: Generalizable and Efficient Video-Language Reasoning via\n  Multimodal Modular Fusion","summary":"  Despite impressive advancements in recent multimodal reasoning approaches,\nthey are still limited in flexibility and efficiency, as these models typically\nprocess only a few fixed modality inputs and require updates to numerous\nparameters. This paper tackles these critical challenges and proposes CREMA, a\ngeneralizable, highly efficient, and modular modality-fusion framework that can\nincorporate any new modality to enhance video reasoning. We first augment\nmultiple informative modalities (such as optical flow, 3D point cloud, audio,\nthermal heatmap, and touch map) from given videos without extra human\nannotation by leveraging sensors or existing pre-trained models. Next, we\nintroduce a query transformer with multiple parameter-efficient modules\nassociated with each accessible modality. It projects diverse modality features\nto the LLM token embedding space, allowing the model to integrate different\ndata types for response generation. Furthermore, we propose a novel progressive\nmultimodal fusion design supported by a lightweight fusion module and\nmodality-sequential training strategy. It helps compress information across\nvarious assisting modalities, maintaining computational efficiency in the LLM\nwhile improving performance. We validate our method on 7 video-language\nreasoning tasks assisted by diverse modalities, including conventional VideoQA\nand Video-Audio/3D/Touch/Thermal QA, and achieve better/equivalent performance\nagainst strong multimodal LLMs, including OneLLM, BLIP-2, and SeViLA while\nreducing over 90% trainable parameters. We provide extensive analyses of CREMA,\nincluding the impact of each modality on reasoning domains, the design of the\nfusion module, and example visualizations.\n","authors":["Shoubin Yu","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2402.05889v2.pdf","comment":"first two authors contributed equally. Project page:\n  https://CREMA-VideoLLM.github.io/"},{"id":"http://arxiv.org/abs/2406.08641v1","updated":"2024-06-12T21:01:26Z","published":"2024-06-12T21:01:26Z","title":"ML-SUPERB 2.0: Benchmarking Multilingual Speech Models Across Modeling\n  Constraints, Languages, and Datasets","summary":"  ML-SUPERB evaluates self-supervised learning (SSL) models on the tasks of\nlanguage identification and automatic speech recognition (ASR). This benchmark\ntreats the models as feature extractors and uses a single shallow downstream\nmodel, which can be fine-tuned for a downstream task. However, real-world use\ncases may require different configurations. This paper presents ML-SUPERB~2.0,\nwhich is a new benchmark for evaluating pre-trained SSL and supervised speech\nmodels across downstream models, fine-tuning setups, and efficient model\nadaptation approaches. We find performance improvements over the setup of\nML-SUPERB. However, performance depends on the downstream model design. Also,\nwe find large performance differences between languages and datasets,\nsuggesting the need for more targeted approaches to improve multilingual ASR\nperformance.\n","authors":["Jiatong Shi","Shih-Heng Wang","William Chen","Martijn Bartelds","Vanya Bannihatti Kumar","Jinchuan Tian","Xuankai Chang","Dan Jurafsky","Karen Livescu","Hung-yi Lee","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2406.08641v1.pdf","comment":"Accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2310.07815v3","updated":"2024-06-12T20:36:22Z","published":"2023-10-11T18:56:15Z","title":"Language Models As Semantic Indexers","summary":"  Semantic identifier (ID) is an important concept in information retrieval\nthat aims to preserve the semantics of objects such as documents and items\ninside their IDs. Previous studies typically adopt a two-stage pipeline to\nlearn semantic IDs by first procuring embeddings using off-the-shelf text\nencoders and then deriving IDs based on the embeddings. However, each step\nintroduces potential information loss, and there is usually an inherent\nmismatch between the distribution of embeddings within the latent space\nproduced by text encoders and the anticipated distribution required for\nsemantic indexing. It is non-trivial to design a method that can learn the\ndocument's semantic representations and its hierarchical structure\nsimultaneously, given that semantic IDs are discrete and sequentially\nstructured, and the semantic supervision is deficient. In this paper, we\nintroduce LMIndexer, a self-supervised framework to learn semantic IDs with a\ngenerative language model. We tackle the challenge of sequential discrete ID by\nintroducing a semantic indexer capable of generating neural sequential discrete\nrepresentations with progressive training and contrastive learning. In response\nto the semantic supervision deficiency, we propose to train the model with a\nself-supervised document reconstruction objective. We show the high quality of\nthe learned IDs and demonstrate their effectiveness on three tasks including\nrecommendation, product search, and document retrieval on five datasets from\nvarious domains. Code is available at\nhttps://github.com/PeterGriffinJin/LMIndexer.\n","authors":["Bowen Jin","Hansi Zeng","Guoyin Wang","Xiusi Chen","Tianxin Wei","Ruirui Li","Zhengyang Wang","Zheng Li","Yang Li","Hanqing Lu","Suhang Wang","Jiawei Han","Xianfeng Tang"],"pdf_url":"https://arxiv.org/pdf/2310.07815v3.pdf","comment":"10 pages, 5 appendix pages"},{"id":"http://arxiv.org/abs/2406.08633v1","updated":"2024-06-12T20:30:34Z","published":"2024-06-12T20:30:34Z","title":"Unraveling Code-Mixing Patterns in Migration Discourse: Automated\n  Detection and Analysis of Online Conversations on Reddit","summary":"  The surge in global migration patterns underscores the imperative of\nintegrating migrants seamlessly into host communities, necessitating inclusive\nand trustworthy public services. Despite the Nordic countries' robust public\nsector infrastructure, recent immigrants often encounter barriers to accessing\nthese services, exacerbating social disparities and eroding trust. Addressing\ndigital inequalities and linguistic diversity is paramount in this endeavor.\nThis paper explores the utilization of code-mixing, a communication strategy\nprevalent among multilingual speakers, in migration-related discourse on social\nmedia platforms such as Reddit. We present Ensemble Learning for Multilingual\nIdentification of Code-mixed Texts (ELMICT), a novel approach designed to\nautomatically detect code-mixed messages in migration-related discussions.\nLeveraging ensemble learning techniques for combining multiple tokenizers'\noutputs and pre-trained language models, ELMICT demonstrates high performance\n(with F1 more than 0.95) in identifying code-mixing across various languages\nand contexts, particularly in cross-lingual zero-shot conditions (with avg. F1\nmore than 0.70). Moreover, the utilization of ELMICT helps to analyze the\nprevalence of code-mixing in migration-related threads compared to other\nthematic categories on Reddit, shedding light on the topics of concern to\nmigrant communities. Our findings reveal insights into the communicative\nstrategies employed by migrants on social media platforms, offering\nimplications for the development of inclusive digital public services and\nconversational systems. By addressing the research questions posed in this\nstudy, we contribute to the understanding of linguistic diversity in migration\ndiscourse and pave the way for more effective tools for building trust in\nmulticultural societies.\n","authors":["Fedor Vitiugin","Sunok Lee","Henna Paakki","Anastasiia Chizhikova","Nitin Sawhney"],"pdf_url":"https://arxiv.org/pdf/2406.08633v1.pdf","comment":"10 pages, 3 figures, Workshop Proceedings of the 18th International\n  AAAI Conference on Web and Social Media"},{"id":"http://arxiv.org/abs/2406.08627v1","updated":"2024-06-12T20:20:09Z","published":"2024-06-12T20:20:09Z","title":"Time-MMD: A New Multi-Domain Multimodal Dataset for Time Series Analysis","summary":"  Time series data are ubiquitous across a wide range of real-world domains.\nWhile real-world time series analysis (TSA) requires human experts to integrate\nnumerical series data with multimodal domain-specific knowledge, most existing\nTSA models rely solely on numerical data, overlooking the significance of\ninformation beyond numerical series. This oversight is due to the untapped\npotential of textual series data and the absence of a comprehensive,\nhigh-quality multimodal dataset. To overcome this obstacle, we introduce\nTime-MMD, the first multi-domain, multimodal time series dataset covering 9\nprimary data domains. Time-MMD ensures fine-grained modality alignment,\neliminates data contamination, and provides high usability. Additionally, we\ndevelop MM-TSFlib, the first multimodal time-series forecasting (TSF) library,\nseamlessly pipelining multimodal TSF evaluations based on Time-MMD for in-depth\nanalyses. Extensive experiments conducted on Time-MMD through MM-TSFlib\ndemonstrate significant performance enhancements by extending unimodal TSF to\nmultimodality, evidenced by over 15% mean squared error reduction in general,\nand up to 40% in domains with rich textual data. More importantly, our datasets\nand library revolutionize broader applications, impacts, research topics to\nadvance TSA. The dataset and library are available at\nhttps://github.com/AdityaLab/Time-MMD and\nhttps://github.com/AdityaLab/MM-TSFlib.\n","authors":["Haoxin Liu","Shangqing Xu","Zhiyuan Zhao","Lingkai Kong","Harshavardhan Kamarthi","Aditya B. Sasanur","Megha Sharma","Jiaming Cui","Qingsong Wen","Chao Zhang","B. Aditya Prakash"],"pdf_url":"https://arxiv.org/pdf/2406.08627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08619v1","updated":"2024-06-12T20:04:44Z","published":"2024-06-12T20:04:44Z","title":"Self-Supervised Speech Representations are More Phonetic than Semantic","summary":"  Self-supervised speech models (S3Ms) have become an effective backbone for\nspeech applications. Various analyses suggest that S3Ms encode linguistic\nproperties. In this work, we seek a more fine-grained analysis of the\nword-level linguistic properties encoded in S3Ms. Specifically, we curate a\nnovel dataset of near homophone (phonetically similar) and synonym\n(semantically similar) word pairs and measure the similarities between S3M word\nrepresentation pairs. Our study reveals that S3M representations consistently\nand significantly exhibit more phonetic than semantic similarity. Further, we\nquestion whether widely used intent classification datasets such as Fluent\nSpeech Commands and Snips Smartlights are adequate for measuring semantic\nabilities. Our simple baseline, using only the word identity, surpasses\nS3M-based models. This corroborates our findings and suggests that high scores\non these datasets do not necessarily guarantee the presence of semantic\ncontent.\n","authors":["Kwanghee Choi","Ankita Pasad","Tomohiko Nakamura","Satoru Fukayama","Karen Livescu","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2406.08619v1.pdf","comment":"Accepted to Interspeech 2024. Source code at\n  https://github.com/juice500ml/phonetic_semantic_probing"},{"id":"http://arxiv.org/abs/2406.06764v2","updated":"2024-06-12T19:31:16Z","published":"2024-06-10T19:50:16Z","title":"$Classi|Q\\rangle$ Towards a Translation Framework To Bridge The\n  Classical-Quantum Programming Gap","summary":"  Quantum computing, albeit readily available as hardware or emulated on the\ncloud, is still far from being available in general regarding complex\nprogramming paradigms and learning curves. This vision paper introduces\n$Classi|Q\\rangle$, a translation framework idea to bridge Classical and Quantum\nComputing by translating high-level programming languages, e.g., Python or C++,\ninto a low-level language, e.g., Quantum Assembly. Our idea paper serves as a\nblueprint for ongoing efforts in quantum software engineering, offering a\nroadmap for further $Classi|Q\\rangle$ development to meet the diverse needs of\nresearchers and practitioners. $Classi|Q\\rangle$ is designed to empower\nresearchers and practitioners with no prior quantum experience to harness the\npotential of hybrid quantum computation. We also discuss future enhancements to\n$Classi|Q\\rangle$, including support for additional quantum languages, improved\noptimization strategies, and integration with emerging quantum computing\nplatforms.\n","authors":["Matteo Esposito","Maryam Tavassoli Sabzevari","Boshuai Ye","Davide Falessi","Arif Ali Khan","Davide Taibi"],"pdf_url":"https://arxiv.org/pdf/2406.06764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08607v1","updated":"2024-06-12T19:26:35Z","published":"2024-06-12T19:26:35Z","title":"Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning\n  Framework from Logit Difference","summary":"  As Large Language Models (LLMs) demonstrate extensive capability in learning\nfrom documents, LLM unlearning becomes an increasingly important research area\nto address concerns of LLMs in terms of privacy, copyright, etc. A conventional\nLLM unlearning task typically involves two goals: (1) The target LLM should\nforget the knowledge in the specified forget documents, and (2) it should\nretain the other knowledge that the LLM possesses, for which we assume access\nto a small number of retain documents. To achieve both goals, a mainstream\nclass of LLM unlearning methods introduces an optimization framework with a\ncombination of two objectives - maximizing the prediction loss on the forget\ndocuments while minimizing that on the retain documents, which suffers from two\nchallenges, degenerated output and catastrophic forgetting. In this paper, we\npropose a novel unlearning framework called Unlearning from Logit Difference\n(ULD), which introduces an assistant LLM that aims to achieve the opposite of\nthe unlearning goals: remembering the forget documents and forgetting the\nretain knowledge. ULD then derives the unlearned LLM by computing the logit\ndifference between the target and the assistant LLMs. We show that such\nreversed objectives would naturally resolve both aforementioned challenges\nwhile significantly improving the training efficiency. Extensive experiments\ndemonstrate that our method efficiently achieves the intended forgetting while\npreserving the LLM's overall capabilities, reducing training time by more than\nthreefold. Notably, our method loses 0% of model utility on the ToFU benchmark,\nwhereas baseline methods may sacrifice 17% of utility on average to achieve\ncomparable forget quality. Our code will be publicly available at\nhttps://github.com/UCSB-NLP-Chang/ULD.\n","authors":["Jiabao Ji","Yujian Liu","Yang Zhang","Gaowen Liu","Ramana Rao Kompella","Sijia Liu","Shiyu Chang"],"pdf_url":"https://arxiv.org/pdf/2406.08607v1.pdf","comment":"21 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.08606v1","updated":"2024-06-12T19:22:29Z","published":"2024-06-12T19:22:29Z","title":"End-to-End Argument Mining as Augmented Natural Language Generation","summary":"  Argument Mining (AM) is a crucial aspect of computational argumentation,\nwhich deals with the identification and extraction of Argumentative Components\n(ACs) and their corresponding Argumentative Relations (ARs). Most prior works\nhave solved these problems by dividing them into multiple subtasks. And the\navailable end-to-end setups are mostly based on the dependency parsing\napproach. This work proposes a unified end-to-end framework based on a\ngenerative paradigm, in which the argumentative structures are framed into\nlabel-augmented text, called Augmented Natural Language (ANL). Additionally, we\nexplore the role of different types of markers in solving AM tasks. Through\ndifferent marker-based fine-tuning strategies, we present an extensive study by\nintegrating marker knowledge into our generative model. The proposed framework\nachieves competitive results to the state-of-the-art (SoTA) model and\noutperforms several baselines.\n","authors":["Nilmadhab Das","Vishal Choudhary","V. Vijaya Saradhi","Ashish Anand"],"pdf_url":"https://arxiv.org/pdf/2406.08606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03304v2","updated":"2024-06-12T19:21:33Z","published":"2024-03-05T20:07:42Z","title":"Large Language Models for Document-Level Event-Argument Data\n  Augmentation for Challenging Role Types","summary":"  Event Argument Extraction (EAE) is an extremely difficult information\nextraction problem -- with significant limitations in few-shot cross-domain\n(FSCD) settings. A common solution to FSCD modeling is data augmentation.\nUnfortunately, existing augmentation methods are not well-suited to a variety\nof real-world EAE contexts including (i) The need to model long documents (10+\nsentences) (ii) The need to model zero and few-shot roles (i.e. event roles\nwith little to no training representation). In this work, we introduce two\nnovel LLM-powered data augmentation frameworks for synthesizing extractive\ndocument-level EAE samples using zero in-domain training data. Our highest\nperforming methods provide a 16-pt increase in F1 score on extraction of zero\nshot role types.\n  To better facilitate analysis of cross-domain EAE, we additionally introduce\na new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify\nroles in the target domain which are semantic outliers with respect to roles\nobserved in the source domain. Our experiments show that LLM-based augmentation\ncan boost RDF1 performance by up to 11 F1 points compared to baseline methods.\n","authors":["Joseph Gatto","Parker Seegmiller","Omar Sharif","Sarah M. Preum"],"pdf_url":"https://arxiv.org/pdf/2403.03304v2.pdf","comment":"Paper in submission (8 pages)"},{"id":"http://arxiv.org/abs/2406.08598v1","updated":"2024-06-12T19:05:43Z","published":"2024-06-12T19:05:43Z","title":"Language Model Council: Benchmarking Foundation Models on Highly\n  Subjective Tasks by Consensus","summary":"  The rapid advancement of Large Language Models (LLMs) necessitates robust and\nchallenging benchmarks. Leaderboards like Chatbot Arena rank LLMs based on how\nwell their responses align with human preferences. However, many tasks such as\nthose related to emotional intelligence, creative writing, or persuasiveness,\nare highly subjective and often lack majoritarian human agreement. Judges may\nhave irreconcilable disagreements about what constitutes a better response. To\naddress the challenge of ranking LLMs on highly subjective tasks, we propose a\nnovel benchmarking framework, the Language Model Council (LMC). The LMC\noperates through a democratic process to: 1) formulate a test set through equal\nparticipation, 2) administer the test among council members, and 3) evaluate\nresponses as a collective jury. We deploy a council of 20 newest LLMs on an\nopen-ended emotional intelligence task: responding to interpersonal dilemmas.\nOur results show that the LMC produces rankings that are more separable,\nrobust, and less biased than those from any individual LLM judge, and is more\nconsistent with a human-established leaderboard compared to other benchmarks.\n","authors":["Justin Zhao","Flor Miriam Plaza-del-Arco","Amanda Cercas Curry"],"pdf_url":"https://arxiv.org/pdf/2406.08598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08587v1","updated":"2024-06-12T18:47:28Z","published":"2024-06-12T18:47:28Z","title":"CS-Bench: A Comprehensive Benchmark for Large Language Models towards\n  Computer Science Mastery","summary":"  Computer Science (CS) stands as a testament to the intricacies of human\nintelligence, profoundly advancing the development of artificial intelligence\nand modern society. However, the current community of large language models\n(LLMs) overly focuses on benchmarks for analyzing specific foundational skills\n(e.g. mathematics and code generation), neglecting an all-round evaluation of\nthe computer science field. To bridge this gap, we introduce CS-Bench, the\nfirst bilingual (Chinese-English) benchmark dedicated to evaluating the\nperformance of LLMs in computer science. CS-Bench comprises approximately 5K\nmeticulously curated test samples, covering 26 subfields across 4 key areas of\ncomputer science, encompassing various task forms and divisions of knowledge\nand reasoning. Utilizing CS-Bench, we conduct a comprehensive evaluation of\nover 30 mainstream LLMs, revealing the relationship between CS performance and\nmodel scales. We also quantitatively analyze the reasons for failures in\nexisting LLMs and highlight directions for improvements, including knowledge\nsupplementation and CS-specific reasoning. Further cross-capability experiments\nshow a high correlation between LLMs' capabilities in computer science and\ntheir abilities in mathematics and coding. Moreover, expert LLMs specialized in\nmathematics and coding also demonstrate strong performances in several CS\nsubfields. Looking ahead, we envision CS-Bench serving as a cornerstone for LLM\napplications in the CS field and paving new avenues in assessing LLMs' diverse\nreasoning capabilities. The CS-Bench data and evaluation code are available at\nhttps://github.com/csbench/csbench.\n","authors":["Xiaoshuai Song","Muxi Diao","Guanting Dong","Zhengyang Wang","Yujia Fu","Runqi Qiao","Zhexu Wang","Dayuan Fu","Huangxuan Wu","Bin Liang","Weihao Zeng","Yejie Wang","Zhuoma GongQue","Jianing Yu","Qiuna Tan","Weiran Xu"],"pdf_url":"https://arxiv.org/pdf/2406.08587v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2406.08582v1","updated":"2024-06-12T18:38:40Z","published":"2024-06-12T18:38:40Z","title":"Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An\n  Experimental Study and Quality Assessment Methods","summary":"  There are various methods for adapting LLMs to different domains. The most\ncommon methods are prompting, finetuning, and RAG. In this work, we explore the\npossibility of adapting a model using one of the PEFT methods - QLoRA. The\nexperiment aims to simulate human responses based on their interviews. The\nsimulation quality is assessed by comparing the quality of the style and the\nquality of the generated facts.\n","authors":["Eugene Vyborov","Oleksiy Osypenko","Serge Sotnyk"],"pdf_url":"https://arxiv.org/pdf/2406.08582v1.pdf","comment":"16 pages, 5 tables"},{"id":"http://arxiv.org/abs/2406.06870v2","updated":"2024-06-12T18:38:13Z","published":"2024-06-11T01:10:40Z","title":"What's in an embedding? Would a rose by any embedding smell as sweet?","summary":"  Large Language Models (LLMs) are often criticized for lacking true\n\"understanding\" and an ability to \"reason\" with their knowledge, being seen\nmerely as advanced autocomplete systems. We believe that this perspective might\nbe missing an important insight. We suggest that LLMs do develop a kind of\nempirical \"understanding\" that is \"geometry\"-like, which seems quite sufficient\nfor a range of applications in NLP, computer vision, coding assistance, etc.\nHowever, this \"geometric\" understanding, built from incomplete and noisy data,\nmakes them unreliable, difficult to generalize, and lacking in inference\ncapabilities and explanations, similar to the challenges faced by\nheuristics-based expert systems decades ago.\n  To overcome these limitations, we suggest that LLMs should be integrated with\nan \"algebraic\" representation of knowledge that includes symbolic AI elements\nused in expert systems. This integration aims to create large knowledge models\n(LKMs) that not only possess \"deep\" knowledge grounded in first principles, but\nalso have the ability to reason and explain, mimicking human expert\ncapabilities. To harness the full potential of generative AI safely and\neffectively, a paradigm shift from LLMs to the more comprehensive LKMs is\nneeded.\n","authors":["Venkat Venkatasubramanian"],"pdf_url":"https://arxiv.org/pdf/2406.06870v2.pdf","comment":"7 pages, 9 images"},{"id":"http://arxiv.org/abs/2305.12057v4","updated":"2024-06-12T18:28:01Z","published":"2023-05-20T01:53:03Z","title":"Accurate Knowledge Distillation with n-best Reranking","summary":"  We propose utilizing n-best reranking to enhance Sequence-Level Knowledge\nDistillation (Kim and Rush, 2016) where we extract pseudo-labels for student\nmodel's training data from top n-best hypotheses and leverage a diverse set of\nmodels with different inductive biases, objective functions or architectures,\nincluding some publicly-available large language models, to pick the\nhighest-quality hypotheses as labels. The effectiveness of our proposal is\nvalidated through experiments on the WMT'21 German-English and Chinese-English\ntranslation tasks. Our results demonstrate that utilizing pseudo-labels\ngenerated by our n-best reranker leads to a significantly more accurate student\nmodel. In fact, our best student model achieves comparable accuracy to a large\ntranslation model from (Tran et al., 2021) with 4.7 billion parameters, while\nhaving two orders of magnitude fewer parameters.\n","authors":["Hendra Setiawan"],"pdf_url":"https://arxiv.org/pdf/2305.12057v4.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.05132v2","updated":"2024-06-12T17:59:58Z","published":"2024-06-07T17:59:59Z","title":"3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and\n  Less Hallucination","summary":"  The integration of language and 3D perception is crucial for developing\nembodied agents and robots that comprehend and interact with the physical\nworld. While large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, their adaptation to 3D environments\n(3D-LLMs) remains in its early stages. A primary challenge is the absence of\nlarge-scale datasets that provide dense grounding between language and 3D\nscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset\ncomprising 40,087 household scenes paired with 6.2 million densely-grounded\nscene-language instructions. Our results show that instruction tuning with\n3D-GRAND significantly enhances grounding capabilities and reduces\nhallucinations in 3D-LLMs. As part of our contributions, we propose a\ncomprehensive benchmark 3D-POPE to systematically evaluate hallucination in\n3D-LLMs, enabling fair comparisons among future models. Our experiments\nhighlight a scaling effect between dataset size and 3D-LLM performance,\nemphasizing the critical role of large-scale 3D-text datasets in advancing\nembodied AI research. Notably, our results demonstrate early signals for\neffective sim-to-real transfer, indicating that models trained on large\nsynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and\n3D-POPE, we aim to equip the embodied AI community with essential resources and\ninsights, setting the stage for more reliable and better-grounded 3D-LLMs.\nProject website: https://3d-grand.github.io\n","authors":["Jianing Yang","Xuweiyi Chen","Nikhil Madaan","Madhavan Iyengar","Shengyi Qian","David F. Fouhey","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2406.05132v2.pdf","comment":"Project website: https://3d-grand.github.io"},{"id":"http://arxiv.org/abs/2402.13254v4","updated":"2024-06-12T17:59:55Z","published":"2024-02-20T18:59:55Z","title":"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples","summary":"  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V. To facilitate future research, we release our code, dataset, benchmark,\nand checkpoints at https://countercurate.github.io.\n","authors":["Jianrui Zhang","Mu Cai","Tengyang Xie","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2402.13254v4.pdf","comment":"15 pages, 6 figures, 12 tables, Project Page:\n  https://countercurate.github.io/"},{"id":"http://arxiv.org/abs/2406.08488v1","updated":"2024-06-12T17:59:52Z","published":"2024-06-12T17:59:52Z","title":"ICE-G: Image Conditional Editing of 3D Gaussian Splats","summary":"  Recently many techniques have emerged to create high quality 3D assets and\nscenes. When it comes to editing of these objects, however, existing approaches\nare either slow, compromise on quality, or do not provide enough customization.\nWe introduce a novel approach to quickly edit a 3D model from a single\nreference view. Our technique first segments the edit image, and then matches\nsemantically corresponding regions across chosen segmented dataset views using\nDINO features. A color or texture change from a particular region of the edit\nimage can then be applied to other views automatically in a semantically\nsensible manner. These edited views act as an updated dataset to further train\nand re-style the 3D scene. The end-result is therefore an edited 3D model. Our\nframework enables a wide variety of editing tasks such as manual local edits,\ncorrespondence based style transfer from any example image, and a combination\nof different styles from multiple example images. We use Gaussian Splats as our\nprimary 3D representation due to their speed and ease of local editing, but our\ntechnique works for other methods such as NeRFs as well. We show through\nmultiple examples that our method produces higher quality results while\noffering fine-grained control of editing. Project page: ice-gaussian.github.io\n","authors":["Vishnu Jaganathan","Hannah Hanyun Huang","Muhammad Zubair Irshad","Varun Jampani","Amit Raj","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2406.08488v1.pdf","comment":"Accepted to CVPR AI4CC Workshop 2024. Project page:\n  https://ice-gaussian.github.io"},{"id":"http://arxiv.org/abs/2406.08487v1","updated":"2024-06-12T17:59:49Z","published":"2024-06-12T17:59:49Z","title":"Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models","summary":"  Seeing clearly with high resolution is a foundation of Large Multimodal\nModels (LMMs), which has been proven to be vital for visual perception and\nreasoning. Existing works usually employ a straightforward resolution upscaling\nmethod, where the image consists of global and local branches, with the latter\nbeing the sliced image patches but resized to the same resolution as the\nformer. This means that higher resolution requires more local patches,\nresulting in exorbitant computational expenses, and meanwhile, the dominance of\nlocal image tokens may diminish the global context. In this paper, we dive into\nthe problems and propose a new framework as well as an elaborate optimization\nstrategy. Specifically, we extract contextual information from the global view\nusing a mixture of adapters, based on the observation that different adapters\nexcel at different tasks. With regard to local patches, learnable query\nembeddings are introduced to reduce image tokens, the most important tokens\naccounting for the user question will be further selected by a similarity-based\nselector. Our empirical results demonstrate a `less is more' pattern, where\n\\textit{utilizing fewer but more informative local image tokens leads to\nimproved performance}. Besides, a significant challenge lies in the training\nstrategy, as simultaneous end-to-end training of the global mining block and\nlocal compression block does not yield optimal results. We thus advocate for an\nalternating training way, ensuring balanced learning between global and local\naspects. Finally, we also introduce a challenging dataset with high\nrequirements for image detail, enhancing the training of the local compression\nlayer. The proposed method, termed LMM with Sophisticated Tasks, Local image\ncompression, and Mixture of global Experts (SliME), achieves leading\nperformance across various benchmarks with only 2 million training data.\n","authors":["Yi-Fan Zhang","Qingsong Wen","Chaoyou Fu","Xue Wang","Zhang Zhang","Liang Wang","Rong Jin"],"pdf_url":"https://arxiv.org/pdf/2406.08487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08486v1","updated":"2024-06-12T17:59:42Z","published":"2024-06-12T17:59:42Z","title":"On Evaluating Adversarial Robustness of Volumetric Medical Segmentation\n  Models","summary":"  Volumetric medical segmentation models have achieved significant success on\norgan and tumor-based segmentation tasks in recent years. However, their\nvulnerability to adversarial attacks remains largely unexplored, raising\nserious concerns regarding the real-world deployment of tools employing such\nmodels in the healthcare sector. This underscores the importance of\ninvestigating the robustness of existing models. In this context, our work aims\nto empirically examine the adversarial robustness across current volumetric\nsegmentation architectures, encompassing Convolutional, Transformer, and\nMamba-based models. We extend this investigation across four volumetric\nsegmentation datasets, evaluating robustness under both white box and black box\nadversarial attacks. Overall, we observe that while both pixel and\nfrequency-based attacks perform reasonably well under white box setting, the\nlatter performs significantly better under transfer-based black box attacks.\nAcross our experiments, we observe transformer-based models show higher\nrobustness than convolution-based models with Mamba-based models being the most\nvulnerable. Additionally, we show that large-scale training of volumetric\nsegmentation models improves the model's robustness against adversarial\nattacks. The code and pretrained models will be made available at\nhttps://github.com/HashmatShadab/Robustness-of-Volumetric-Medical-Segmentation-Models.\n","authors":["Hashmat Shadab Malik","Numan Saeed","Asif Hanif","Muzammal Naseer","Mohammad Yaqub","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2406.08486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08482v1","updated":"2024-06-12T17:59:27Z","published":"2024-06-12T17:59:27Z","title":"Words Worth a Thousand Pictures: Measuring and Understanding Perceptual\n  Variability in Text-to-Image Generation","summary":"  Diffusion models are the state of the art in text-to-image generation, but\ntheir perceptual variability remains understudied. In this paper, we examine\nhow prompts affect image variability in black-box diffusion-based models. We\npropose W1KP, a human-calibrated measure of variability in a set of images,\nbootstrapped from existing image-pair perceptual distances. Current datasets do\nnot cover recent diffusion models, thus we curate three test sets for\nevaluation. Our best perceptual distance outperforms nine baselines by up to 18\npoints in accuracy, and our calibration matches graded human judgements 78% of\nthe time. Using W1KP, we study prompt reusability and show that Imagen prompts\ncan be reused for 10-50 random seeds before new images become too similar to\nalready generated images, while Stable Diffusion XL and DALL-E 3 can be reused\n50-200 times. Lastly, we analyze 56 linguistic features of real prompts,\nfinding that the prompt's length, CLIP embedding norm, concreteness, and word\nsenses influence variability most. As far as we are aware, we are the first to\nanalyze diffusion variability from a visuolinguistic perspective. Our project\npage is at http://w1kp.com\n","authors":["Raphael Tang","Xinyu Zhang","Lixinyu Xu","Yao Lu","Wenyan Li","Pontus Stenetorp","Jimmy Lin","Ferhan Ture"],"pdf_url":"https://arxiv.org/pdf/2406.08482v1.pdf","comment":"13 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.08481v1","updated":"2024-06-12T17:59:21Z","published":"2024-06-12T17:59:21Z","title":"Enhancing End-to-End Autonomous Driving with Latent World Model","summary":"  End-to-end autonomous driving has garnered widespread attention. Current\nend-to-end approaches largely rely on the supervision from perception tasks\nsuch as detection, tracking, and map segmentation to aid in learning scene\nrepresentations. However, these methods require extensive annotations,\nhindering the data scalability. To address this challenge, we propose a novel\nself-supervised method to enhance end-to-end driving without the need for\ncostly labels. Specifically, our framework \\textbf{LAW} uses a LAtent World\nmodel to predict future latent features based on the predicted ego actions and\nthe latent feature of the current frame. The predicted latent features are\nsupervised by the actually observed features in the future. This supervision\njointly optimizes the latent feature learning and action prediction, which\ngreatly enhances the driving performance. As a result, our approach achieves\nstate-of-the-art performance in both open-loop and closed-loop benchmarks\nwithout costly annotations.\n","authors":["Yingyan Li","Lue Fan","Jiawei He","Yuqi Wang","Yuntao Chen","Zhaoxiang Zhang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2406.08481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08479v1","updated":"2024-06-12T17:59:08Z","published":"2024-06-12T17:59:08Z","title":"Real3D: Scaling Up Large Reconstruction Models with Real-World Images","summary":"  The default strategy for training single-view Large Reconstruction Models\n(LRMs) follows the fully supervised route using large-scale datasets of\nsynthetic 3D assets or multi-view captures. Although these resources simplify\nthe training procedure, they are hard to scale up beyond the existing datasets\nand they are not necessarily representative of the real distribution of object\nshapes. To address these limitations, in this paper, we introduce Real3D, the\nfirst LRM system that can be trained using single-view real-world images.\nReal3D introduces a novel self-training framework that can benefit from both\nthe existing synthetic data and diverse single-view real images. We propose two\nunsupervised losses that allow us to supervise LRMs at the pixel- and\nsemantic-level, even for training examples without ground-truth 3D or novel\nviews. To further improve performance and scale up the image data, we develop\nan automatic data curation approach to collect high-quality examples from\nin-the-wild images. Our experiments show that Real3D consistently outperforms\nprior work in four diverse evaluation settings that include real and synthetic\ndata, as well as both in-domain and out-of-domain shapes. Code and model can be\nfound here: https://hwjiang1510.github.io/Real3D/\n","authors":["Hanwen Jiang","Qixing Huang","Georgios Pavlakos"],"pdf_url":"https://arxiv.org/pdf/2406.08479v1.pdf","comment":"Project page: https://hwjiang1510.github.io/Real3D/"},{"id":"http://arxiv.org/abs/2406.08478v1","updated":"2024-06-12T17:59:07Z","published":"2024-06-12T17:59:07Z","title":"What If We Recaption Billions of Web Images with LLaMA-3?","summary":"  Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate\nthat semantically aligning and enriching textual descriptions of these pairs\ncan significantly enhance model training across various vision-language tasks,\nparticularly text-to-image generation. However, large-scale investigations in\nthis area remain predominantly closed-source. Our paper aims to bridge this\ncommunity effort, leveraging the powerful and \\textit{open-sourced} LLaMA-3, a\nGPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a\nLLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images\nfrom the DataComp-1B dataset. Our empirical results confirm that this enhanced\ndataset, Recap-DataComp-1B, offers substantial benefits in training advanced\nvision-language models. For discriminative models like CLIP, we observe\nenhanced zero-shot performance in cross-modal retrieval tasks. For generative\nmodels like text-to-image Diffusion Transformers, the generated images exhibit\na significant improvement in alignment with users' text instructions,\nespecially in following complex queries. Our project page is\nhttps://www.haqtu.me/Recap-Datacomp-1B/\n","authors":["Xianhang Li","Haoqin Tu","Mude Hui","Zeyu Wang","Bingchen Zhao","Junfei Xiao","Sucheng Ren","Jieru Mei","Qing Liu","Huangjie Zheng","Yuyin Zhou","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2406.08478v1.pdf","comment":"* denotes equal contributions"},{"id":"http://arxiv.org/abs/2406.08476v1","updated":"2024-06-12T17:59:04Z","published":"2024-06-12T17:59:04Z","title":"RMem: Restricted Memory Banks Improve Video Object Segmentation","summary":"  With recent video object segmentation (VOS) benchmarks evolving to\nchallenging scenarios, we revisit a simple but overlooked strategy: restricting\nthe size of memory banks. This diverges from the prevalent practice of\nexpanding memory banks to accommodate extensive historical information. Our\nspecially designed \"memory deciphering\" study offers a pivotal insight\nunderpinning such a strategy: expanding memory banks, while seemingly\nbeneficial, actually increases the difficulty for VOS modules to decode\nrelevant features due to the confusion from redundant information. By\nrestricting memory banks to a limited number of essential frames, we achieve a\nnotable improvement in VOS accuracy. This process balances the importance and\nfreshness of frames to maintain an informative memory bank within a bounded\ncapacity. Additionally, restricted memory banks reduce the training-inference\ndiscrepancy in memory lengths compared with continuous expansion. This fosters\nnew opportunities in temporal reasoning and enables us to introduce the\npreviously overlooked \"temporal positional embedding.\" Finally, our insights\nare embodied in \"RMem\" (\"R\" for restricted), a simple yet effective VOS\nmodification that excels at challenging VOS scenarios and establishes new state\nof the art for object state changes (on the VOST dataset) and long videos (on\nthe Long Videos dataset). Our code and demo are available at\nhttps://restricted-memory.github.io/.\n","authors":["Junbao Zhou","Ziqi Pang","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08476v1.pdf","comment":"CVPR 2024, Project Page: https://restricted-memory.github.io/"},{"id":"http://arxiv.org/abs/2406.08475v1","updated":"2024-06-12T17:57:25Z","published":"2024-06-12T17:57:25Z","title":"Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent\n  Diffusion Models","summary":"  Creating realistic avatars from a single RGB image is an attractive yet\nchallenging problem. Due to its ill-posed nature, recent works leverage\npowerful prior from 2D diffusion models pretrained on large datasets. Although\n2D diffusion models demonstrate strong generalization capability, they cannot\nprovide multi-view shape priors with guaranteed 3D consistency. We propose\nHuman 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent\nDiffusion. Our key insight is that 2D multi-view diffusion and 3D\nreconstruction models provide complementary information for each other, and by\ncoupling them in a tight manner, we can fully leverage the potential of both\nmodels. We introduce a novel image-conditioned generative 3D Gaussian Splats\nreconstruction model that leverages the priors from 2D multi-view diffusion\nmodels, and provides an explicit 3D representation, which further guides the 2D\nreverse sampling process to have better 3D consistency. Experiments show that\nour proposed framework outperforms state-of-the-art methods and enables the\ncreation of realistic avatars from a single RGB image, achieving high-fidelity\nin both geometry and appearance. Extensive ablations also validate the efficacy\nof our design, (1) multi-view 2D priors conditioning in generative 3D\nreconstruction and (2) consistency refinement of sampling trajectory via the\nexplicit 3D representation. Our code and models will be released on\nhttps://yuxuan-xue.com/human-3diffusion.\n","authors":["Yuxuan Xue","Xianghui Xie","Riccardo Marin","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2406.08475v1.pdf","comment":"Project Page: https://yuxuan-xue.com/human-3diffusion"},{"id":"http://arxiv.org/abs/2406.08474v1","updated":"2024-06-12T17:57:06Z","published":"2024-06-12T17:57:06Z","title":"Real2Code: Reconstruct Articulated Objects via Code Generation","summary":"  We present Real2Code, a novel approach to reconstructing articulated objects\nvia code generation. Given visual observations of an object, we first\nreconstruct its part geometry using an image segmentation model and a shape\ncompletion model. We then represent the object parts with oriented bounding\nboxes, which are input to a fine-tuned large language model (LLM) to predict\njoint articulation as code. By leveraging pre-trained vision and language\nmodels, our approach scales elegantly with the number of articulated parts, and\ngeneralizes from synthetic training data to real world objects in unstructured\nenvironments. Experimental results demonstrate that Real2Code significantly\noutperforms previous state-of-the-art in reconstruction accuracy, and is the\nfirst approach to extrapolate beyond objects' structural complexity in the\ntraining set, and reconstructs objects with up to 10 articulated parts. When\nincorporated with a stereo reconstruction model, Real2Code also generalizes to\nreal world objects from a handful of multi-view RGB images, without the need\nfor depth or camera information.\n","authors":["Zhao Mandi","Yijia Weng","Dominik Bauer","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2406.08474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08463v1","updated":"2024-06-12T17:51:53Z","published":"2024-06-12T17:51:53Z","title":"Self-supervised Learning of Neural Implicit Feature Fields for Camera\n  Pose Refinement","summary":"  Visual localization techniques rely upon some underlying scene representation\nto localize against. These representations can be explicit such as 3D SFM map\nor implicit, such as a neural network that learns to encode the scene. The\nformer requires sparse feature extractors and matchers to build the scene\nrepresentation. The latter might lack geometric grounding not capturing the 3D\nstructure of the scene well enough. This paper proposes to jointly learn the\nscene representation along with a 3D dense feature field and a 2D feature\nextractor whose outputs are embedded in the same metric space. Through a\ncontrastive framework we align this volumetric field with the image-based\nextractor and regularize the latter with a ranking loss from learned surface\ninformation. We learn the underlying geometry of the scene with an implicit\nfield through volumetric rendering and design our feature field to leverage\nintermediate geometric information encoded in the implicit field. The resulting\nfeatures are discriminative and robust to viewpoint change while maintaining\nrich encoded information. Visual localization is then achieved by aligning the\nimage-based features and the rendered volumetric features. We show the\neffectiveness of our approach on real-world scenes, demonstrating that our\napproach outperforms prior and concurrent work on leveraging implicit scene\nrepresentations for localization.\n","authors":["Maxime Pietrantoni","Gabriela Csurka","Martin Humenberger","Torsten Sattler"],"pdf_url":"https://arxiv.org/pdf/2406.08463v1.pdf","comment":"Published in 3DV24 (highlight)"},{"id":"http://arxiv.org/abs/2406.08457v1","updated":"2024-06-12T17:49:26Z","published":"2024-06-12T17:49:26Z","title":"ConceptHash: Interpretable Fine-Grained Hashing via Concept Discovery","summary":"  Existing fine-grained hashing methods typically lack code interpretability as\nthey compute hash code bits holistically using both global and local features.\nTo address this limitation, we propose ConceptHash, a novel method that\nachieves sub-code level interpretability. In ConceptHash, each sub-code\ncorresponds to a human-understandable concept, such as an object part, and\nthese concepts are automatically discovered without human annotations.\nSpecifically, we leverage a Vision Transformer architecture and introduce\nconcept tokens as visual prompts, along with image patch tokens as model\ninputs. Each concept is then mapped to a specific sub-code at the model output,\nproviding natural sub-code interpretability. To capture subtle visual\ndifferences among highly similar sub-categories (e.g., bird species), we\nincorporate language guidance to ensure that the learned hash codes are\ndistinguishable within fine-grained object classes while maintaining semantic\nalignment. This approach allows us to develop hash codes that exhibit\nsimilarity within families of species while remaining distinct from species in\nother families. Extensive experiments on four fine-grained image retrieval\nbenchmarks demonstrate that ConceptHash outperforms previous methods by a\nsignificant margin, offering unique sub-code interpretability as an additional\nbenefit. Code at: https://github.com/kamwoh/concepthash.\n","authors":["Kam Woh Ng","Xiatian Zhu","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2406.08457v1.pdf","comment":"CVPRW 2024 - FGVC11 best paper award"},{"id":"http://arxiv.org/abs/2406.08451v1","updated":"2024-06-12T17:44:26Z","published":"2024-06-12T17:44:26Z","title":"GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on\n  Mobile Devices","summary":"  Smartphone users often navigate across multiple applications (apps) to\ncomplete tasks such as sharing content between social media platforms.\nAutonomous Graphical User Interface (GUI) navigation agents can enhance user\nexperience in communication, entertainment, and productivity by streamlining\nworkflows and reducing manual intervention. However, prior GUI agents often\ntrained with datasets comprising simple tasks that can be completed within a\nsingle app, leading to poor performance in cross-app navigation. To address\nthis problem, we introduce GUI Odyssey, a comprehensive dataset for training\nand evaluating cross-app navigation agents. GUI Odyssey consists of 7,735\nepisodes from 6 mobile devices, spanning 6 types of cross-app tasks, 201 apps,\nand 1.4K app combos. Leveraging GUI Odyssey, we developed OdysseyAgent, a\nmultimodal cross-app navigation agent by fine-tuning the Qwen-VL model with a\nhistory resampling module. Extensive experiments demonstrate OdysseyAgent's\nsuperior accuracy compared to existing models. For instance, OdysseyAgent\nsurpasses fine-tuned Qwen-VL and zero-shot GPT-4V by 1.44\\% and 55.49\\%\nin-domain accuracy, and 2.29\\% and 48.14\\% out-of-domain accuracy on average.\nThe dataset and code will be released in\n\\url{https://github.com/OpenGVLab/GUI-Odyssey}.\n","authors":["Quanfeng Lu","Wenqi Shao","Zitao Liu","Fanqing Meng","Boxuan Li","Botong Chen","Siyuan Huang","Kaipeng Zhang","Yu Qiao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2406.08451v1.pdf","comment":"16 pages, 8 figures, a cross-app GUI navigation dataset"},{"id":"http://arxiv.org/abs/2406.08444v1","updated":"2024-06-12T17:34:38Z","published":"2024-06-12T17:34:38Z","title":"PixMamba: Leveraging State Space Models in a Dual-Level Architecture for\n  Underwater Image Enhancement","summary":"  Underwater Image Enhancement (UIE) is critical for marine research and\nexploration but hindered by complex color distortions and severe blurring.\nRecent deep learning-based methods have achieved remarkable results, yet these\nmethods struggle with high computational costs and insufficient global\nmodeling, resulting in locally under- or over- adjusted regions. We present\nPixMamba, a novel architecture, designed to overcome these challenges by\nleveraging State Space Models (SSMs) for efficient global dependency modeling.\nUnlike convolutional neural networks (CNNs) with limited receptive fields and\ntransformer networks with high computational costs, PixMamba efficiently\ncaptures global contextual information while maintaining computational\nefficiency. Our dual-level strategy features the patch-level Efficient Mamba\nNet (EMNet) for reconstructing enhanced image feature and the pixel-level\nPixMamba Net (PixNet) to ensure fine-grained feature capturing and global\nconsistency of enhanced image that were previously difficult to obtain.\nPixMamba achieves state-of-the-art performance across various underwater image\ndatasets and delivers visually superior results. Code is available at:\nhttps://github.com/weitunglin/pixmamba.\n","authors":["Wei-Tung Lin","Yong-Xiang Lin","Jyun-Wei Chen","Kai-Lung Hua"],"pdf_url":"https://arxiv.org/pdf/2406.08444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08443v1","updated":"2024-06-12T17:31:36Z","published":"2024-06-12T17:31:36Z","title":"Transformation-Dependent Adversarial Attacks","summary":"  We introduce transformation-dependent adversarial attacks, a new class of\nthreats where a single additive perturbation can trigger diverse, controllable\nmis-predictions by systematically transforming the input (e.g., scaling,\nblurring, compression). Unlike traditional attacks with static effects, our\nperturbations embed metamorphic properties to enable different adversarial\nattacks as a function of the transformation parameters. We demonstrate the\ntransformation-dependent vulnerability across models (e.g., convolutional\nnetworks and vision transformers) and vision tasks (e.g., image classification\nand object detection). Our proposed geometric and photometric transformations\nenable a range of targeted errors from one crafted input (e.g., higher than 90%\nattack success rate for classifiers). We analyze effects of model architecture\nand type/variety of transformations on attack effectiveness. This work forces a\nparadigm shift by redefining adversarial inputs as dynamic, controllable\nthreats. We highlight the need for robust defenses against such multifaceted,\nchameleon-like perturbations that current techniques are ill-prepared for.\n","authors":["Yaoteng Tan","Zikui Cai","M. Salman Asif"],"pdf_url":"https://arxiv.org/pdf/2406.08443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08439v1","updated":"2024-06-12T17:24:41Z","published":"2024-06-12T17:24:41Z","title":"Coherent Optical Modems for Full-Wavefield Lidar","summary":"  The advent of the digital age has driven the development of coherent optical\nmodems -- devices that modulate the amplitude and phase of light in multiple\npolarization states. These modems transmit data through fiber optic cables that\nare thousands of kilometers in length at data rates exceeding one terabit per\nsecond. This remarkable technology is made possible through near-THz-rate\nprogrammable control and sensing of the full optical wavefield. While coherent\noptical modems form the backbone of telecommunications networks around the\nworld, their extraordinary capabilities also provide unique opportunities for\nimaging. Here, we introduce full-wavefield lidar: a new imaging modality that\nrepurposes off-the-shelf coherent optical modems to simultaneously measure\ndistance, axial velocity, and polarization. We demonstrate this modality by\ncombining a 74 GHz-bandwidth coherent optical modem with free-space coupling\noptics and scanning mirrors. We develop a time-resolved image formation model\nfor this system and formulate a maximum-likelihood reconstruction algorithm to\nrecover depth, velocity, and polarization information at each scene point from\nthe modem's raw transmitted and received symbols. Compared to existing lidars,\nfull-wavefield lidar promises improved mm-scale ranging accuracy from brief,\nmicrosecond exposure times, reliable velocimetry, and robustness to intererence\nfrom ambient light or other lidar signals.\n","authors":["Parsa Mirdehghan","Brandon Buscaino","Maxx Wu","Doug Charlton","Mohammad E. Mousa-Pasandi","Kiriakos N. Kutulakos","David B. Lindell"],"pdf_url":"https://arxiv.org/pdf/2406.08439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07647v4","updated":"2024-06-12T17:16:39Z","published":"2023-04-15T22:24:05Z","title":"LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene\n  Graphs with Weak Supervision","summary":"  We propose LASER, a neuro-symbolic approach to learn semantic video\nrepresentations that capture rich spatial and temporal properties in video data\nby leveraging high-level logic specifications. In particular, we formulate the\nproblem in terms of alignment between raw videos and spatio-temporal logic\nspecifications. The alignment algorithm leverages a differentiable symbolic\nreasoner and a combination of contrastive, temporal, and semantics losses. It\neffectively and efficiently trains low-level perception models to extract a\nfine-grained video representation in the form of a spatio-temporal scene graph\nthat conforms to the desired high-level specification. To practically reduce\nthe manual effort of obtaining ground truth labels, we derive logic\nspecifications from captions by employing a large language model with a generic\nprompting template. In doing so, we explore a novel methodology that weakly\nsupervises the learning of spatio-temporal scene graphs with widely accessible\nvideo-caption data. We evaluate our method on three datasets with rich spatial\nand temporal specifications: 20BN-Something-Something, MUGEN, and OpenPVSG. We\ndemonstrate that our method learns better fine-grained video semantics than\nexisting baselines.\n","authors":["Jiani Huang","Ziyang Li","Mayur Naik","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2304.07647v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08431v1","updated":"2024-06-12T17:16:16Z","published":"2024-06-12T17:16:16Z","title":"Diffusion Soup: Model Merging for Text-to-Image Diffusion Models","summary":"  We present Diffusion Soup, a compartmentalization method for Text-to-Image\nGeneration that averages the weights of diffusion models trained on sharded\ndata. By construction, our approach enables training-free continual learning\nand unlearning with no additional memory or inference costs, since models\ncorresponding to data shards can be added or removed by re-averaging. We show\nthat Diffusion Soup samples from a point in weight space that approximates the\ngeometric mean of the distributions of constituent datasets, which offers\nanti-memorization guarantees and enables zero-shot style mixing. Empirically,\nDiffusion Soup outperforms a paragon model trained on the union of all data\nshards and achieves a 30% improvement in Image Reward (.34 $\\to$ .44) on domain\nsharded data, and a 59% improvement in IR (.37 $\\to$ .59) on aesthetic data. In\nboth cases, souping also prevails in TIFA score (respectively, 85.5 $\\to$ 86.5\nand 85.6 $\\to$ 86.8). We demonstrate robust unlearning -- removing any\nindividual domain shard only lowers performance by 1% in IR (.45 $\\to$ .44) --\nand validate our theoretical insights on anti-memorization using real data.\nFinally, we showcase Diffusion Soup's ability to blend the distinct styles of\nmodels finetuned on different shards, resulting in the zero-shot generation of\nhybrid styles.\n","authors":["Benjamin Biggs","Arjun Seshadri","Yang Zou","Achin Jain","Aditya Golatkar","Yusheng Xie","Alessandro Achille","Ashwin Swaminathan","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2406.08431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08425v1","updated":"2024-06-12T17:10:27Z","published":"2024-06-12T17:10:27Z","title":"AWGUNET: Attention-Aided Wavelet Guided U-Net for Nuclei Segmentation in\n  Histopathology Images","summary":"  Accurate nuclei segmentation in histopathological images is crucial for\ncancer diagnosis. Automating this process offers valuable support to clinical\nexperts, as manual annotation is time-consuming and prone to human errors.\nHowever, automating nuclei segmentation presents challenges due to uncertain\ncell boundaries, intricate staining, and diverse structures. In this paper, we\npresent a segmentation approach that combines the U-Net architecture with a\nDenseNet-121 backbone, harnessing the strengths of both to capture\ncomprehensive contextual and spatial information. Our model introduces the\nWavelet-guided channel attention module to enhance cell boundary delineation,\nalong with a learnable weighted global attention module for channel-specific\nattention. The decoder module, composed of an upsample block and convolution\nblock, further refines segmentation in handling staining patterns. The\nexperimental results conducted on two publicly accessible histopathology\ndatasets, namely Monuseg and TNBC, underscore the superiority of our proposed\nmodel, demonstrating its potential to advance histopathological image analysis\nand cancer diagnosis. The code is made available at:\nhttps://github.com/AyushRoy2001/AWGUNET.\n","authors":["Ayush Roy","Payel Pramanik","Dmitrii Kaplun","Sergei Antonov","Ram Sarkar"],"pdf_url":"https://arxiv.org/pdf/2406.08425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08421v1","updated":"2024-06-12T17:05:08Z","published":"2024-06-12T17:05:08Z","title":"PRIBOOT: A New Data-Driven Expert for Improved Driving Simulations","summary":"  The development of Autonomous Driving (AD) systems in simulated environments\nlike CARLA is crucial for advancing real-world automotive technologies. To\ndrive innovation, CARLA introduced Leaderboard 2.0, significantly more\nchallenging than its predecessor. However, current AD methods have struggled to\nachieve satisfactory outcomes due to a lack of sufficient ground truth data.\nHuman driving logs provided by CARLA are insufficient, and previously\nsuccessful expert agents like Autopilot and Roach, used for collecting\ndatasets, have seen reduced effectiveness under these more demanding\nconditions. To overcome these data limitations, we introduce PRIBOOT, an expert\nagent that leverages limited human logs with privileged information. We have\ndeveloped a novel BEV representation specifically tailored to meet the demands\nof this new benchmark and processed it as an RGB image to facilitate the\napplication of transfer learning techniques, instead of using a set of masks.\nAdditionally, we propose the Infraction Rate Score (IRS), a new evaluation\nmetric designed to provide a more balanced assessment of driving performance\nover extended routes. PRIBOOT is the first model to achieve a Route Completion\n(RC) of 75% in Leaderboard 2.0, along with a Driving Score (DS) and IRS of 20%\nand 45%, respectively. With PRIBOOT, researchers can now generate extensive\ndatasets, potentially solving the data availability issues that have hindered\nprogress in this benchmark.\n","authors":["Daniel Coelho","Miguel Oliveira","Vitor Santos","Antonio M. Lopez"],"pdf_url":"https://arxiv.org/pdf/2406.08421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08418v1","updated":"2024-06-12T17:01:04Z","published":"2024-06-12T17:01:04Z","title":"OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images\n  Interleaved with Text","summary":"  Image-text interleaved data, consisting of multiple images and texts arranged\nin a natural document format, aligns with the presentation paradigm of internet\ndata and closely resembles human reading habits. Recent studies have shown that\nsuch data aids multimodal in-context learning and maintains the capabilities of\nlarge language models during multimodal fine-tuning. However, the limited scale\nand diversity of current image-text interleaved data restrict the development\nof multimodal large language models. In this paper, we introduce OmniCorpus, a\n10 billion-scale image-text interleaved dataset. Using an efficient data\nengine, we filter and extract large-scale high-quality documents, which contain\n8.6 billion images and 1,696 billion text tokens. Compared to counterparts\n(e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while\nmaintaining good data quality; 2) features more diverse sources, including both\nEnglish and non-English websites as well as video-centric websites; 3) is more\nflexible, easily degradable from an image-text interleaved format to pure text\ncorpus and image-text pairs. Through comprehensive analysis and experiments, we\nvalidate the quality, usability, and effectiveness of the proposed dataset. We\nhope this could provide a solid data foundation for future multimodal model\nresearch. Code and data are released at\nhttps://github.com/OpenGVLab/OmniCorpus.\n","authors":["Qingyun Li","Zhe Chen","Weiyun Wang","Wenhai Wang","Shenglong Ye","Zhenjiang Jin","Guanzhou Chen","Yinan He","Zhangwei Gao","Erfei Cui","Jiashuo Yu","Hao Tian","Jiasheng Zhou","Chao Xu","Bin Wang","Xingjian Wei","Wei Li","Wenjian Zhang","Bo Zhang","Pinlong Cai","Licheng Wen","Xiangchao Yan","Pei Chu","Yi Wang","Min Dou","Changyao Tian","Xizhou Zhu","Lewei Lu","Yushi Chen","Junjun He","Tong Lu","Yali Wang","Limin Wang","Dahua Lin","Yu Qiao","Botian Shi","Conghui He","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2406.08418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08407v1","updated":"2024-06-12T16:54:54Z","published":"2024-06-12T16:54:54Z","title":"MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation\n  in Videos","summary":"  Multimodal Language Language Models (MLLMs) demonstrate the emerging\nabilities of \"world models\" -- interpreting and reasoning about complex\nreal-world dynamics. To assess these abilities, we posit videos are the ideal\nmedium, as they encapsulate rich representations of real-world dynamics and\ncausalities. To this end, we introduce MMWorld, a new benchmark for\nmulti-discipline, multi-faceted multimodal video understanding. MMWorld\ndistinguishes itself from previous video understanding benchmarks with two\nunique advantages: (1) multi-discipline, covering various disciplines that\noften require domain expertise for comprehensive understanding; (2)\nmulti-faceted reasoning, including explanation, counterfactual thinking, future\nprediction, etc. MMWorld consists of a human-annotated dataset to evaluate\nMLLMs with questions about the whole videos and a synthetic dataset to analyze\nMLLMs within a single modality of perception. Together, MMWorld encompasses\n1,910 videos across seven broad disciplines and 69 subdisciplines, complete\nwith 6,627 question-answer pairs and associated captions. The evaluation\nincludes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld\n(e.g., GPT-4V performs the best with only 52.3\\% accuracy), showing large room\nfor improvement. Further ablation studies reveal other interesting findings\nsuch as models' different skill sets from humans. We hope MMWorld can serve as\nan essential step towards world model evaluation in videos.\n","authors":["Xuehai He","Weixi Feng","Kaizhi Zheng","Yujie Lu","Wanrong Zhu","Jiachen Li","Yue Fan","Jianfeng Wang","Linjie Li","Zhengyuan Yang","Kevin Lin","William Yang Wang","Lijuan Wang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08394v1","updated":"2024-06-12T16:44:50Z","published":"2024-06-12T16:44:50Z","title":"VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model\n  for Hundreds of Vision-Language Tasks","summary":"  We present VisionLLM v2, an end-to-end generalist multimodal large model\n(MLLM) that unifies visual perception, understanding, and generation within a\nsingle framework. Unlike traditional MLLMs limited to text output, VisionLLM v2\nsignificantly broadens its application scope. It excels not only in\nconventional visual question answering (VQA) but also in open-ended,\ncross-domain vision tasks such as object localization, pose estimation, and\nimage generation and editing. To this end, we propose a new information\ntransmission mechanism termed \"super link\", as a medium to connect MLLM with\ntask-specific decoders. It not only allows flexible transmission of task\ninformation and gradient feedback between the MLLM and multiple downstream\ndecoders but also effectively resolves training conflicts in multi-tasking\nscenarios. In addition, to support the diverse range of tasks, we carefully\ncollected and combed training data from hundreds of public vision and\nvision-language tasks. In this way, our model can be joint-trained end-to-end\non hundreds of vision language tasks and generalize to these tasks using a set\nof shared parameters through different user prompts, achieving performance\ncomparable to task-specific models. We believe VisionLLM v2 will offer a new\nperspective on the generalization of MLLMs.\n","authors":["Jiannan Wu","Muyan Zhong","Sen Xing","Zeqiang Lai","Zhaoyang Liu","Wenhai Wang","Zhe Chen","Xizhou Zhu","Lewei Lu","Tong Lu","Ping Luo","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2406.08394v1.pdf","comment":"43 pages"},{"id":"http://arxiv.org/abs/2406.08392v1","updated":"2024-06-12T16:43:47Z","published":"2024-06-12T16:43:47Z","title":"FontStudio: Shape-Adaptive Diffusion Model for Coherent and Consistent\n  Font Effect Generation","summary":"  Recently, the application of modern diffusion-based text-to-image generation\nmodels for creating artistic fonts, traditionally the domain of professional\ndesigners, has garnered significant interest. Diverging from the majority of\nexisting studies that concentrate on generating artistic typography, our\nresearch aims to tackle a novel and more demanding challenge: the generation of\ntext effects for multilingual fonts. This task essentially requires generating\ncoherent and consistent visual content within the confines of a font-shaped\ncanvas, as opposed to a traditional rectangular canvas. To address this task,\nwe introduce a novel shape-adaptive diffusion model capable of interpreting the\ngiven shape and strategically planning pixel distributions within the irregular\ncanvas. To achieve this, we curate a high-quality shape-adaptive image-text\ndataset and incorporate the segmentation mask as a visual condition to steer\nthe image generation process within the irregular-canvas. This approach enables\nthe traditionally rectangle canvas-based diffusion model to produce the desired\nconcepts in accordance with the provided geometric shapes. Second, to maintain\nconsistency across multiple letters, we also present a training-free,\nshape-adaptive effect transfer method for transferring textures from a\ngenerated reference letter to others. The key insights are building a font\neffect noise prior and propagating the font effect information in a\nconcatenated latent space. The efficacy of our FontStudio system is confirmed\nthrough user preference studies, which show a marked preference (78% win-rates\non aesthetics) for our system even when compared to the latest unrivaled\ncommercial product, Adobe Firefly.\n","authors":["Xinzhi Mu","Li Chen","Bohan Chen","Shuyang Gu","Jianmin Bao","Dong Chen","Ji Li","Yuhui Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.08392v1.pdf","comment":"Project-page: https://font-studio.github.io/"},{"id":"http://arxiv.org/abs/2405.18842v2","updated":"2024-06-12T16:42:26Z","published":"2024-05-29T07:49:15Z","title":"Descriptive Image Quality Assessment in the Wild","summary":"  With the rapid advancement of Vision Language Models (VLMs), VLM-based Image\nQuality Assessment (IQA) seeks to describe image quality linguistically to\nalign with human expression and capture the multifaceted nature of IQA tasks.\nHowever, current methods are still far from practical usage. First, prior works\nfocus narrowly on specific sub-tasks or settings, which do not align with\ndiverse real-world applications. Second, their performance is sub-optimal due\nto limitations in dataset coverage, scale, and quality. To overcome these\nchallenges, we introduce Depicted image Quality Assessment in the Wild\n(DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that\nencompasses both assessment and comparison tasks, brief and detailed responses,\nfull-reference and non-reference scenarios. We introduce a\nground-truth-informed dataset construction approach to enhance data quality,\nand scale up the dataset to 495K under the brief-detail joint framework.\nConsequently, we construct a comprehensive, large-scale, and high-quality\ndataset, named DQ-495K. We also retain image resolution during training to\nbetter handle resolution-related quality issues, and estimate a confidence\nscore that is helpful to filter out low-quality responses. Experimental results\ndemonstrate that DepictQA-Wild significantly outperforms traditional\nscore-based methods, prior VLM-based IQA models, and proprietary GPT-4V in\ndistortion identification, instant rating, and reasoning tasks. Our advantages\nare further confirmed by real-world applications including assessing the\nweb-downloaded images and ranking model-processed images. Datasets and codes\nwill be released in https://depictqa.github.io/depictqa-wild/.\n","authors":["Zhiyuan You","Jinjin Gu","Zheyuan Li","Xin Cai","Kaiwen Zhu","Chao Dong","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2405.18842v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08381v1","updated":"2024-06-12T16:31:06Z","published":"2024-06-12T16:31:06Z","title":"LaneCPP: Continuous 3D Lane Detection using Physical Priors","summary":"  Monocular 3D lane detection has become a fundamental problem in the context\nof autonomous driving, which comprises the tasks of finding the road surface\nand locating lane markings. One major challenge lies in a flexible but robust\nline representation capable of modeling complex lane structures, while still\navoiding unpredictable behavior. While previous methods rely on fully\ndata-driven approaches, we instead introduce a novel approach LaneCPP that uses\na continuous 3D lane detection model leveraging physical prior knowledge about\nthe lane structure and road geometry. While our sophisticated lane model is\ncapable of modeling complex road structures, it also shows robust behavior\nsince physical constraints are incorporated by means of a regularization scheme\nthat can be analytically applied to our parametric representation. Moreover, we\nincorporate prior knowledge about the road geometry into the 3D feature space\nby modeling geometry-aware spatial features, guiding the network to learn an\ninternal road surface representation. In our experiments, we show the benefits\nof our contributions and prove the meaningfulness of using priors to make 3D\nlane detection more robust. The results show that LaneCPP achieves\nstate-of-the-art performance in terms of F-Score and geometric errors.\n","authors":["Maximilian Pittner","Joel Janai","Alexandru P. Condurache"],"pdf_url":"https://arxiv.org/pdf/2406.08381v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2406.08379v1","updated":"2024-06-12T16:29:45Z","published":"2024-06-12T16:29:45Z","title":"Eyes Wide Unshut: Unsupervised Mistake Detection in Egocentric Video by\n  Detecting Unpredictable Gaze","summary":"  In this paper, we address the challenge of unsupervised mistake detection in\negocentric video through the analysis of gaze signals, a critical component for\nadvancing user assistance in smart glasses. Traditional supervised methods,\nreliant on manually labeled mistakes, suffer from domain-dependence and\nscalability issues. This research introduces an unsupervised method for\ndetecting mistakes in videos of human activities, overcoming the challenges of\ndomain-specific requirements and the necessity for annotated data. By analyzing\nunusual gaze patterns that signal user disorientation during tasks, we propose\na gaze completion model that forecasts eye gaze trajectories from incomplete\ninputs. The difference between the anticipated and observed gaze paths acts as\nan indicator for identifying errors. Our method is validated on the EPIC-Tent\ndataset, showing its superiority compared to current one-class supervised and\nunsupervised techniques.\n","authors":["Michele Mazzamuto","Antonino Furnari","Giovanni Maria Farinella"],"pdf_url":"https://arxiv.org/pdf/2406.08379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08377v1","updated":"2024-06-12T16:26:56Z","published":"2024-06-12T16:26:56Z","title":"DDR: Exploiting Deep Degradation Response as Flexible Image Descriptor","summary":"  Image deep features extracted by pre-trained networks are known to contain\nrich and informative representations. In this paper, we present Deep\nDegradation Response (DDR), a method to quantify changes in image deep features\nunder varying degradation conditions. Specifically, our approach facilitates\nflexible and adaptive degradation, enabling the controlled synthesis of image\ndegradation through text-driven prompts. Extensive evaluations demonstrate the\nversatility of DDR as an image descriptor, with strong correlations observed\nwith key image attributes such as complexity, colorfulness, sharpness, and\noverall quality. Moreover, we demonstrate the efficacy of DDR across a spectrum\nof applications. It excels as a blind image quality assessment metric,\noutperforming existing methodologies across multiple datasets. Additionally,\nDDR serves as an effective unsupervised learning objective in image restoration\ntasks, yielding notable advancements in image deblurring and single-image\nsuper-resolution. Our code will be made available.\n","authors":["Juncheng Wu","Zhangkai Ni","Hanli Wang","Wenhan Yang","Yuyin Zhou","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08374v1","updated":"2024-06-12T16:22:41Z","published":"2024-06-12T16:22:41Z","title":"2.5D Multi-view Averaging Diffusion Model for 3D Medical Image\n  Translation: Application to Low-count PET Reconstruction with CT-less\n  Attenuation Correction","summary":"  Positron Emission Tomography (PET) is an important clinical imaging tool but\ninevitably introduces radiation hazards to patients and healthcare providers.\nReducing the tracer injection dose and eliminating the CT acquisition for\nattenuation correction can reduce the overall radiation dose, but often results\nin PET with high noise and bias. Thus, it is desirable to develop 3D methods to\ntranslate the non-attenuation-corrected low-dose PET (NAC-LDPET) into\nattenuation-corrected standard-dose PET (AC-SDPET). Recently, diffusion models\nhave emerged as a new state-of-the-art deep learning method for image-to-image\ntranslation, better than traditional CNN-based methods. However, due to the\nhigh computation cost and memory burden, it is largely limited to 2D\napplications. To address these challenges, we developed a novel 2.5D Multi-view\nAveraging Diffusion Model (MADM) for 3D image-to-image translation with\napplication on NAC-LDPET to AC-SDPET translation. Specifically, MADM employs\nseparate diffusion models for axial, coronal, and sagittal views, whose outputs\nare averaged in each sampling step to ensure the 3D generation quality from\nmultiple views. To accelerate the 3D sampling process, we also proposed a\nstrategy to use the CNN-based 3D generation as a prior for the diffusion model.\nOur experimental results on human patient studies suggested that MADM can\ngenerate high-quality 3D translation images, outperforming previous CNN-based\nand Diffusion-based baseline methods.\n","authors":["Tianqi Chen","Jun Hou","Yinchi Zhou","Huidong Xie","Xiongchao Chen","Qiong Liu","Xueqi Guo","Menghua Xia","James S. Duncan","Chi Liu","Bo Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.08374v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.08372v1","updated":"2024-06-12T16:20:58Z","published":"2024-06-12T16:20:58Z","title":"APSeg: Auto-Prompt Network for Cross-Domain Few-Shot Semantic\n  Segmentatio","summary":"  Few-shot semantic segmentation (FSS) endeavors to segment unseen classes with\nonly a few labeled samples. Current FSS methods are commonly built on the\nassumption that their training and application scenarios share similar domains,\nand their performances degrade significantly while applied to a distinct\ndomain. To this end, we propose to leverage the cutting-edge foundation model,\nthe Segment Anything Model (SAM), for generalization enhancement. The SAM\nhowever performs unsatisfactorily on domains that are distinct from its\ntraining data, which primarily comprise natural scene images, and it does not\nsupport automatic segmentation of specific semantics due to its interactive\nprompting mechanism. In our work, we introduce APSeg, a novel auto-prompt\nnetwork for cross-domain few-shot semantic segmentation (CD-FSS), which is\ndesigned to be auto-prompted for guiding cross-domain segmentation.\nSpecifically, we propose a Dual Prototype Anchor Transformation (DPAT) module\nthat fuses pseudo query prototypes extracted based on cycle-consistency with\nsupport prototypes, allowing features to be transformed into a more stable\ndomain-agnostic space. Additionally, a Meta Prompt Generator (MPG) module is\nintroduced to automatically generate prompt embeddings, eliminating the need\nfor manual visual prompts. We build an efficient model which can be applied\ndirectly to target domains without fine-tuning. Extensive experiments on four\ncross-domain datasets show that our model outperforms the state-of-the-art\nCD-FSS method by 5.24% and 3.10% in average accuracy on 1-shot and 5-shot\nsettings, respectively.\n","authors":["Weizhao He","Yang Zhang","Wei Zhuo","Linlin Shen","Jiaqi Yang","Songhe Deng","Liang Sun"],"pdf_url":"https://arxiv.org/pdf/2406.08372v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.08358v1","updated":"2024-06-12T16:02:28Z","published":"2024-06-12T16:02:28Z","title":"From a Social Cognitive Perspective: Context-aware Visual Social\n  Relationship Recognition","summary":"  People's social relationships are often manifested through their\nsurroundings, with certain objects or interactions acting as symbols for\nspecific relationships, e.g., wedding rings, roses, hugs, or holding hands.\nThis brings unique challenges to recognizing social relationships, requiring\nunderstanding and capturing the essence of these contexts from visual\nappearances. However, current methods of social relationship understanding rely\non the basic classification paradigm of detected persons and objects, which\nfails to understand the comprehensive context and often overlooks decisive\nsocial factors, especially subtle visual cues. To highlight the social-aware\ncontext and intricate details, we propose a novel approach that recognizes\n\\textbf{Con}textual \\textbf{So}cial \\textbf{R}elationships (\\textbf{ConSoR})\nfrom a social cognitive perspective. Specifically, to incorporate social-aware\nsemantics, we build a lightweight adapter upon the frozen CLIP to learn social\nconcepts via our novel multi-modal side adapter tuning mechanism. Further, we\nconstruct social-aware descriptive language prompts (e.g., scene, activity,\nobjects, emotions) with social relationships for each image, and then compel\nConSoR to concentrate more intensively on the decisive visual social factors\nvia visual-linguistic contrasting. Impressively, ConSoR outperforms previous\nmethods with a 12.2\\% gain on the People-in-Social-Context (PISC) dataset and a\n9.8\\% increase on the People-in-Photo-Album (PIPA) benchmark. Furthermore, we\nobserve that ConSoR excels at finding critical visual evidence to reveal social\nrelationships.\n","authors":["Shiwei Wu","Chao Zhang","Joya Chen","Tong Xu","Likang Wu","Yao Hu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08354v1","updated":"2024-06-12T16:00:16Z","published":"2024-06-12T16:00:16Z","title":"DocSynthv2: A Practical Autoregressive Modeling for Document Generation","summary":"  While the generation of document layouts has been extensively explored,\ncomprehensive document generation encompassing both layout and content presents\na more complex challenge. This paper delves into this advanced domain,\nproposing a novel approach called DocSynthv2 through the development of a\nsimple yet effective autoregressive structured model. Our model, distinct in\nits integration of both layout and textual cues, marks a step beyond existing\nlayout-generation approaches. By focusing on the relationship between the\nstructural elements and the textual content within documents, we aim to\ngenerate cohesive and contextually relevant documents without any reliance on\nvisual components. Through experimental studies on our curated benchmark for\nthe new task, we demonstrate the ability of our model combining layout and\ntextual information in enhancing the generation quality and relevance of\ndocuments, opening new pathways for research in document creation and automated\ndesign. Our findings emphasize the effectiveness of autoregressive models in\nhandling complex document generation tasks.\n","authors":["Sanket Biswas","Rajiv Jain","Vlad I. Morariu","Jiuxiang Gu","Puneet Mathur","Curtis Wigington","Tong Sun","Josep Lladós"],"pdf_url":"https://arxiv.org/pdf/2406.08354v1.pdf","comment":"Spotlight (Oral) Acceptance to CVPR 2024 Workshop for Graphic Design\n  Understanding and Generation (GDUG)"},{"id":"http://arxiv.org/abs/2406.08344v1","updated":"2024-06-12T15:51:39Z","published":"2024-06-12T15:51:39Z","title":"Blind Image Deblurring using FFT-ReLU with Deep Learning Pipeline\n  Integration","summary":"  Blind image deblurring is the process of deriving a sharp image and a blur\nkernel from a blurred image. Blurry images are typically modeled as the\nconvolution of a sharp image with a blur kernel, necessitating the estimation\nof the unknown blur kernel to perform blind image deblurring effectively.\nExisting approaches primarily focus on domain-specific features of images, such\nas salient edges, dark channels, and light streaks. These features serve as\nprobabilistic priors to enhance the estimation of the blur kernel. For improved\ngenerality, we propose a novel prior (ReLU sparsity prior) that estimates blur\nkernel effectively across all distributions of images (natural, facial, text,\nlow-light, saturated etc). Our approach demonstrates superior efficiency, with\ninference times up to three times faster, while maintaining high accuracy in\nPSNR, SSIM, and error ratio metrics. We also observe noticeable improvement in\nthe performance of the state-of-the-art architectures (in terms of\naforementioned metrics) in deep learning based approaches when our method is\nused as a post-processing unit.\n","authors":["Abdul Mohaimen Al Radi","Prothito Shovon Majumder","Syed Mumtahin Mahmud","Mahdi Mohd Hossain Noki","Md. Haider Ali","Md. Mosaddek Khan"],"pdf_url":"https://arxiv.org/pdf/2406.08344v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2406.08337v1","updated":"2024-06-12T15:42:52Z","published":"2024-06-12T15:42:52Z","title":"WMAdapter: Adding WaterMark Control to Latent Diffusion Models","summary":"  Watermarking is crucial for protecting the copyright of AI-generated images.\nWe propose WMAdapter, a diffusion model watermark plugin that takes\nuser-specified watermark information and allows for seamless watermark\nimprinting during the diffusion generation process. WMAdapter is efficient and\nrobust, with a strong emphasis on high generation quality. To achieve this, we\nmake two key designs: (1) We develop a contextual adapter structure that is\nlightweight and enables effective knowledge transfer from heavily pretrained\npost-hoc watermarking models. (2) We introduce an extra finetuning step and\ndesign a hybrid finetuning strategy to further improve image quality and\neliminate tiny artifacts. Empirical results demonstrate that WMAdapter offers\nstrong flexibility, exceptional image generation quality and competitive\nwatermark robustness.\n","authors":["Hai Ci","Yiren Song","Pei Yang","Jinheng Xie","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2406.08337v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2406.08336v1","updated":"2024-06-12T15:42:21Z","published":"2024-06-12T15:42:21Z","title":"CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal\n  Dysarthric Speech Reconstruction","summary":"  Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech\ninto normal speech. It still suffers from low speaker similarity and poor\nprosody naturalness. In this paper, we propose a multi-modal DSR model by\nleveraging neural codec language modeling to improve the reconstruction\nresults, especially for the speaker similarity and prosody naturalness. Our\nproposed model consists of: (i) a multi-modal content encoder to extract robust\nphoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a\nspeaker codec encoder to extract and normalize the speaker-aware codecs from\nthe dysarthric speech, in order to provide original timbre and normal prosody;\n(iii) a codec language model based speech decoder to reconstruct the speech\nbased on the extracted phoneme embeddings and normalized codecs. Evaluations on\nthe commonly used UASpeech corpus show that our proposed model can achieve\nsignificant improvements in terms of speaker similarity and prosody\nnaturalness.\n","authors":["Xueyuan Chen","Dongchao Yang","Dingdong Wang","Xixin Wu","Zhiyong Wu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2406.08336v1.pdf","comment":"Accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.08332v1","updated":"2024-06-12T15:36:30Z","published":"2024-06-12T15:36:30Z","title":"UDON: Universal Dynamic Online distillatioN for generic image\n  representations","summary":"  Universal image representations are critical in enabling real-world\nfine-grained and instance-level recognition applications, where objects and\nentities from any domain must be identified at large scale. Despite recent\nadvances, existing methods fail to capture important domain-specific knowledge,\nwhile also ignoring differences in data distribution across different domains.\nThis leads to a large performance gap between efficient universal solutions and\nexpensive approaches utilising a collection of specialist models, one for each\ndomain. In this work, we make significant strides towards closing this gap, by\nintroducing a new learning technique, dubbed UDON (Universal Dynamic Online\nDistillatioN). UDON employs multi-teacher distillation, where each teacher is\nspecialized in one domain, to transfer detailed domain-specific knowledge into\nthe student universal embedding. UDON's distillation approach is not only\neffective, but also very efficient, by sharing most model parameters between\nthe student and all teachers, where all models are jointly trained in an online\nmanner. UDON also comprises a sampling technique which adapts the training\nprocess to dynamically allocate batches to domains which are learned slower and\nrequire more frequent processing. This boosts significantly the learning of\ncomplex domains which are characterised by a large number of classes and\nlong-tail distributions. With comprehensive experiments, we validate each\ncomponent of UDON, and showcase significant improvements over the state of the\nart in the recent UnED benchmark. Code: https://github.com/nikosips/UDON .\n","authors":["Nikolaos-Antonios Ypsilantis","Kaifeng Chen","André Araujo","Ondřej Chum"],"pdf_url":"https://arxiv.org/pdf/2406.08332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01887v2","updated":"2024-06-12T15:36:12Z","published":"2024-01-03T18:57:27Z","title":"LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry","summary":"  Visual odometry estimates the motion of a moving camera based on visual\ninput. Existing methods, mostly focusing on two-view point tracking, often\nignore the rich temporal context in the image sequence, thereby overlooking the\nglobal motion patterns and providing no assessment of the full trajectory\nreliability. These shortcomings hinder performance in scenarios with occlusion,\ndynamic objects, and low-texture areas. To address these challenges, we present\nthe Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively\ncombines visual, inter-track, and temporal cues with mindfully selected anchors\nfor dynamic track estimation. Moreover, LEAP's temporal probabilistic\nformulation integrates distribution updates into a learnable iterative\nrefinement module to reason about point-wise uncertainty. Based on these\ntraits, we develop LEAP-VO, a robust visual odometry system adept at handling\nocclusions and dynamic scenes. Our mindful integration showcases a novel\npractice by employing long-term point tracking as the front-end. Extensive\nexperiments demonstrate that the proposed pipeline significantly outperforms\nexisting baselines across various visual odometry benchmarks.\n","authors":["Weirong Chen","Le Chen","Rui Wang","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2401.01887v2.pdf","comment":"Accepted to CVPR 2024. Project page:\n  https://chiaki530.github.io/projects/leapvo"},{"id":"http://arxiv.org/abs/2406.08324v1","updated":"2024-06-12T15:24:09Z","published":"2024-06-12T15:24:09Z","title":"LaMOT: Language-Guided Multi-Object Tracking","summary":"  Vision-Language MOT is a crucial tracking problem and has drawn increasing\nattention recently. It aims to track objects based on human language commands,\nreplacing the traditional use of templates or pre-set information from training\nsets in conventional tracking tasks. Despite various efforts, a key challenge\nlies in the lack of a clear understanding of why language is used for tracking,\nwhich hinders further development in this field. In this paper, we address this\nchallenge by introducing Language-Guided MOT, a unified task framework, along\nwith a corresponding large-scale benchmark, termed LaMOT, which encompasses\ndiverse scenarios and language descriptions. Specially, LaMOT comprises 1,660\nsequences from 4 different datasets and aims to unify various Vision-Language\nMOT tasks while providing a standardized evaluation platform. To ensure\nhigh-quality annotations, we manually assign appropriate descriptive texts to\neach target in every video and conduct careful inspection and correction. To\nthe best of our knowledge, LaMOT is the first benchmark dedicated to\nLanguage-Guided MOT. Additionally, we propose a simple yet effective tracker,\ntermed LaMOTer. By establishing a unified task framework, providing challenging\nbenchmarks, and offering insights for future algorithm design and evaluation,\nwe expect to contribute to the advancement of research in Vision-Language MOT.\nWe will release the data at https://github.com/Nathan-Li123/LaMOT.\n","authors":["Yunhao Li","Xiaoqiong Liu","Luke Liu","Heng Fan","Libo Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09712v2","updated":"2024-06-12T15:20:36Z","published":"2024-02-15T05:07:54Z","title":"Diffusion Model with Cross Attention as an Inductive Bias for\n  Disentanglement","summary":"  Disentangled representation learning strives to extract the intrinsic factors\nwithin observed data. Factorizing these representations in an unsupervised\nmanner is notably challenging and usually requires tailored loss functions or\nspecific structural designs. In this paper, we introduce a new perspective and\nframework, demonstrating that diffusion models with cross-attention can serve\nas a powerful inductive bias to facilitate the learning of disentangled\nrepresentations. We propose to encode an image to a set of concept tokens and\ntreat them as the condition of the latent diffusion for image reconstruction,\nwhere cross-attention over the concept tokens is used to bridge the interaction\nbetween the encoder and diffusion. Without any additional regularization, this\nframework achieves superior disentanglement performance on the benchmark\ndatasets, surpassing all previous methods with intricate designs. We have\nconducted comprehensive ablation studies and visualization analysis, shedding\nlight on the functioning of this model. This is the first work to reveal the\npotent disentanglement capability of diffusion models with cross-attention,\nrequiring no complex designs. We anticipate that our findings will inspire more\ninvestigation on exploring diffusion for disentangled representation learning\ntowards more sophisticated data analysis and understanding.\n","authors":["Tao Yang","Cuiling Lan","Yan Lu","Nanning zheng"],"pdf_url":"https://arxiv.org/pdf/2402.09712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.05172v2","updated":"2024-06-12T15:06:10Z","published":"2022-04-11T15:05:06Z","title":"Event Transformer","summary":"  The event camera's low power consumption and ability to capture microsecond\nbrightness changes make it attractive for various computer vision tasks.\nExisting event representation methods typically convert events into frames,\nvoxel grids, or spikes for deep neural networks (DNNs). However, these\napproaches often sacrifice temporal granularity or require specialized devices\nfor processing. This work introduces a novel token-based event representation,\nwhere each event is considered a fundamental processing unit termed an\nevent-token. This approach preserves the sequence's intricate spatiotemporal\nattributes at the event level. Moreover, we propose a Three-way Attention\nmechanism in the Event Transformer Block (ETB) to collaboratively construct\ntemporal and spatial correlations between events. We compare our proposed\ntoken-based event representation extensively with other prevalent methods for\nobject classification and optical flow estimation. The experimental results\nshowcase its competitive performance while demanding minimal computational\nresources on standard devices. Our code is publicly accessible at\n\\url{https://github.com/NJUVISION/EventTransformer}.\n","authors":["Bin Jiang","Zhihao Li","M. Salman Asif","Xun Cao","Zhan Ma"],"pdf_url":"https://arxiv.org/pdf/2204.05172v2.pdf","comment":"Accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2406.08300v1","updated":"2024-06-12T15:00:16Z","published":"2024-06-12T15:00:16Z","title":"From Chaos to Clarity: 3DGS in the Dark","summary":"  Novel view synthesis from raw images provides superior high dynamic range\n(HDR) information compared to reconstructions from low dynamic range RGB\nimages. However, the inherent noise in unprocessed raw images compromises the\naccuracy of 3D scene representation. Our study reveals that 3D Gaussian\nSplatting (3DGS) is particularly susceptible to this noise, leading to numerous\nelongated Gaussian shapes that overfit the noise, thereby significantly\ndegrading reconstruction quality and reducing inference speed, especially in\nscenarios with limited views. To address these issues, we introduce a novel\nself-supervised learning framework designed to reconstruct HDR 3DGS from a\nlimited number of noisy raw images. This framework enhances 3DGS by integrating\na noise extractor and employing a noise-robust reconstruction loss that\nleverages a noise distribution prior. Experimental results show that our method\noutperforms LDR/HDR 3DGS and previous state-of-the-art (SOTA) self-supervised\nand supervised pre-trained models in both reconstruction quality and inference\nspeed on the RawNeRF dataset across a broad range of training views. Code can\nbe found in \\url{https://lizhihao6.github.io/Raw3DGS}.\n","authors":["Zhihao Li","Yufei Wang","Alex Kot","Bihan Wen"],"pdf_url":"https://arxiv.org/pdf/2406.08300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02219v2","updated":"2024-06-12T14:59:55Z","published":"2023-12-03T16:39:36Z","title":"Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large\n  Image-Language Models","summary":"  Large Vision and Language Models have enabled significant advances in fully\nsupervised and zero-shot visual tasks. These large architectures serve as the\nbaseline to what is currently known as Instruction Tuning Large Vision and\nLanguage models (IT-LVLMs). IT-LVLMs are general-purpose multi-modal assistants\nwhose responses are modulated by natural language instructions and visual data.\nDespite this versatility, IT-LVLM effectiveness in fundamental computer vision\nproblems remains unclear, primarily due to the absence of a standardized\nevaluation benchmark. This paper introduces a Multi-modal Evaluation Benchmark\nnamed MERLIM, a scalable test-bed to assess the capabilities of IT-LVLMs on\nfundamental computer vision tasks. MERLIM contains over 300K image-question\npairs and has a strong focus on detecting cross-modal \"hallucination\" events in\nIT-LVLMs. Our results bring important insights on the performance of\nstate-of-the-art IT-LVMLs including limitations at identifying fine-grained\nvisual concepts, object hallucinations across tasks, and biases towards the\nlanguage query. Our findings also suggest that these models have weak visual\ngrounding, but manage to make adequate guesses from global visual patterns or\nlanguage biases contained in the LLM component.\n","authors":["Andrés Villa","Juan Carlos León Alcázar","Alvaro Soto","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2312.02219v2.pdf","comment":"16 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.08298v1","updated":"2024-06-12T14:59:12Z","published":"2024-06-12T14:59:12Z","title":"AdaNCA: Neural Cellular Automata As Adaptors For More Robust Vision\n  Transformer","summary":"  Vision Transformers (ViTs) have demonstrated remarkable performance in image\nclassification tasks, particularly when equipped with local information via\nregion attention or convolutions. While such architectures improve the feature\naggregation from different granularities, they often fail to contribute to the\nrobustness of the networks. Neural Cellular Automata (NCA) enables the modeling\nof global cell representations through local interactions, with its training\nstrategies and architecture design conferring strong generalization ability and\nrobustness against noisy inputs. In this paper, we propose Adaptor Neural\nCellular Automata (AdaNCA) for Vision Transformer that uses NCA as plug-in-play\nadaptors between ViT layers, enhancing ViT's performance and robustness against\nadversarial samples as well as out-of-distribution inputs. To overcome the\nlarge computational overhead of standard NCAs, we propose Dynamic Interaction\nfor more efficient interaction learning. Furthermore, we develop an algorithm\nfor identifying the most effective insertion points for AdaNCA based on our\nanalysis of AdaNCA placement and robustness improvement. With less than a 3%\nincrease in parameters, AdaNCA contributes to more than 10% absolute\nimprovement in accuracy under adversarial attacks on the ImageNet1K benchmark.\nMoreover, we demonstrate with extensive evaluations across 8 robustness\nbenchmarks and 4 ViT architectures that AdaNCA, as a plug-in-play module,\nconsistently improves the robustness of ViTs.\n","authors":["Yitao Xu","Tong Zhang","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2406.08298v1.pdf","comment":"26 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.08294v1","updated":"2024-06-12T14:57:37Z","published":"2024-06-12T14:57:37Z","title":"Vessel Re-identification and Activity Detection in Thermal Domain for\n  Maritime Surveillance","summary":"  Maritime surveillance is vital to mitigate illegal activities such as drug\nsmuggling, illegal fishing, and human trafficking. Vision-based maritime\nsurveillance is challenging mainly due to visibility issues at night, which\nresults in failures in re-identifying vessels and detecting suspicious\nactivities. In this paper, we introduce a thermal, vision-based approach for\nmaritime surveillance with object tracking, vessel re-identification, and\nsuspicious activity detection capabilities. For vessel re-identification, we\npropose a novel viewpoint-independent algorithm which compares features of the\nsides of the vessel separately (separate side-spaces) leveraging shape\ninformation in the absence of color features. We propose techniques to adapt\ntracking and activity detection algorithms for the thermal domain and train\nthem using a thermal dataset we created. This dataset will be the first\npublicly available benchmark dataset for thermal maritime surveillance. Our\nsystem is capable of re-identifying vessels with an 81.8% Top1 score and\nidentifying suspicious activities with a 72.4\\% frame mAP score; a new\nbenchmark for each task in the thermal domain.\n","authors":["Yasod Ginige","Ransika Gunasekara","Darsha Hewavitharana","Manjula Ariyarathne","Ranga Rodrigo","Peshala Jayasekara"],"pdf_url":"https://arxiv.org/pdf/2406.08294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08292v1","updated":"2024-06-12T14:56:56Z","published":"2024-06-12T14:56:56Z","title":"Outdoor Scene Extrapolation with Hierarchical Generative Cellular\n  Automata","summary":"  We aim to generate fine-grained 3D geometry from large-scale sparse LiDAR\nscans, abundantly captured by autonomous vehicles (AV). Contrary to prior work\non AV scene completion, we aim to extrapolate fine geometry from unlabeled and\nbeyond spatial limits of LiDAR scans, taking a step towards generating\nrealistic, high-resolution simulation-ready 3D street environments. We propose\nhierarchical Generative Cellular Automata (hGCA), a spatially scalable\nconditional 3D generative model, which grows geometry recursively with local\nkernels following, in a coarse-to-fine manner, equipped with a light-weight\nplanner to induce global consistency. Experiments on synthetic scenes show that\nhGCA generates plausible scene geometry with higher fidelity and completeness\ncompared to state-of-the-art baselines. Our model generalizes strongly from\nsim-to-real, qualitatively outperforming baselines on the Waymo-open dataset.\nWe also show anecdotal evidence of the ability to create novel objects from\nreal-world geometric cues even when trained on limited synthetic content. More\nresults and details can be found on\nhttps://research.nvidia.com/labs/toronto-ai/hGCA/.\n","authors":["Dongsu Zhang","Francis Williams","Zan Gojcic","Karsten Kreis","Sanja Fidler","Young Min Kim","Amlan Kar"],"pdf_url":"https://arxiv.org/pdf/2406.08292v1.pdf","comment":"Accepted to CVPR 2024 as highlight"},{"id":"http://arxiv.org/abs/2406.08285v1","updated":"2024-06-12T14:50:40Z","published":"2024-06-12T14:50:40Z","title":"A New Class Biorthogonal Spline Wavelet for Image Edge Detection","summary":"  Spline wavelets have shown favorable characteristics for localizing in both\ntime and frequency. In this paper, we propose a new biorthogonal cubic special\nspline wavelet (BCSSW), based on the Cohen-Daubechies-Feauveau wavelet\nconstruction method and the cubic special spline algorithm. BCSSW has better\nproperties in compact support, symmetry, and frequency domain characteristics.\nHowever, current mainstream detection operators usually ignore the uncertain\nrepresentation of regional pixels and global structures. To solve these\nproblems, we propose a structural uncertainty-aware and multi-structure\noperator fusion detection algorithm (EDBSW) based on a new BCSSW spline\nwavelet. By constructing a spline wavelet that efficiently handles edge\neffects, we utilize structural uncertainty-aware modulus maxima to detect\nhighly uncertain edge samples. The proposed wavelet detection operator utilizes\nthe multi-structure morphological operator and fusion reconstruction strategy\nto effectively address anti-noise processing and edge information of different\nfrequencies. Numerous experiments have demonstrated its excellent performance\nin reducing noise and capturing edge structure details.\n","authors":["Dujuan Zhou","Zizhao Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.08285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13534v3","updated":"2024-06-12T14:49:15Z","published":"2023-12-21T02:28:41Z","title":"SE(3)-Equivariant and Noise-Invariant 3D Rigid Motion Tracking in Brain\n  MRI","summary":"  Rigid motion tracking is paramount in many medical imaging applications where\nmovements need to be detected, corrected, or accounted for. Modern strategies\nrely on convolutional neural networks (CNN) and pose this problem as rigid\nregistration. Yet, CNNs do not exploit natural symmetries in this task, as they\nare equivariant to translations (their outputs shift with their inputs) but not\nto rotations. Here we propose EquiTrack, the first method that uses recent\nsteerable SE(3)-equivariant CNNs (E-CNN) for motion tracking. While steerable\nE-CNNs can extract corresponding features across different poses, testing them\non noisy medical images reveals that they do not have enough learning capacity\nto learn noise invariance. Thus, we introduce a hybrid architecture that pairs\na denoiser with an E-CNN to decouple the processing of anatomically irrelevant\nintensity features from the extraction of equivariant spatial features. Rigid\ntransforms are then estimated in closed-form. EquiTrack outperforms\nstate-of-the-art learning and optimisation methods for motion tracking in adult\nbrain MRI and fetal MRI time series. Our code is available at\nhttps://github.com/BBillot/EquiTrack.\n","authors":["Benjamin Billot","Neel Dey","Daniel Moyer","Malte Hoffmann","Esra Abaci Turk","Borjan Gagoski","Ellen Grant","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2312.13534v3.pdf","comment":"Published at IEEE transactions on Medical Imaging"},{"id":"http://arxiv.org/abs/2406.08282v1","updated":"2024-06-12T14:47:51Z","published":"2024-06-12T14:47:51Z","title":"Interpretable Representation Learning of Cardiac MRI via Attribute\n  Regularization","summary":"  Interpretability is essential in medical imaging to ensure that clinicians\ncan comprehend and trust artificial intelligence models. Several approaches\nhave been recently considered to encode attributes in the latent space to\nenhance its interpretability. Notably, attribute regularization aims to encode\na set of attributes along the dimensions of a latent representation. However,\nthis approach is based on Variational AutoEncoder and suffers from blurry\nreconstruction. In this paper, we propose an Attributed-regularized Soft\nIntrospective Variational Autoencoder that combines attribute regularization of\nthe latent space within the framework of an adversarially trained variational\nautoencoder. We demonstrate on short-axis cardiac Magnetic Resonance images of\nthe UK Biobank the ability of the proposed method to address blurry\nreconstruction issues of variational autoencoder methods while preserving the\nlatent space interpretability.\n","authors":["Maxime Di Folco","Cosmin I. Bercea","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2406.08282v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2312.08915"},{"id":"http://arxiv.org/abs/2403.07621v2","updated":"2024-06-12T14:45:45Z","published":"2024-03-12T13:04:37Z","title":"Smartphone region-wise image indoor localization using deep learning for\n  indoor tourist attraction","summary":"  Smart indoor tourist attractions, such as smart museums and aquariums,\nusually require a significant investment in indoor localization devices. The\nsmartphone Global Positional Systems use is unsuitable for scenarios where\ndense materials such as concrete and metal block weaken the GPS signals, which\nis the most common scenario in an indoor tourist attraction. Deep learning\nmakes it possible to perform region-wise indoor localization using smartphone\nimages. This approach does not require any investment in infrastructure,\nreducing the cost and time to turn museums and aquariums into smart museums or\nsmart aquariums. This paper proposes using deep learning algorithms to classify\nlocations using smartphone camera images for indoor tourism attractions. We\nevaluate our proposal in a real-world scenario in Brazil. We extensively\ncollect images from ten different smartphones to classify biome-themed fish\ntanks inside the Pantanal Biopark, creating a new dataset of 3654 images. We\ntested seven state-of-the-art neural networks, three being transformer-based,\nachieving precision around 90% on average and recall and f-score around 89% on\naverage. The results indicate good feasibility of the proposal in a most indoor\ntourist attractions.\n","authors":["Gabriel Toshio Hirokawa Higa","Rodrigo Stuqui Monzani","Jorge Fernando da Silva Cecatto","Maria Fernanda Balestieri Mariano de Souza","Vanessa Aparecida de Moraes Weber","Hemerson Pistori","Edson Takashi Matsubara"],"pdf_url":"https://arxiv.org/pdf/2403.07621v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11473v3","updated":"2024-06-12T14:31:42Z","published":"2024-05-19T07:48:41Z","title":"FIFO-Diffusion: Generating Infinite Videos from Text without Training","summary":"  We propose a novel inference technique based on a pretrained diffusion model\nfor text-conditional video generation. Our approach, called FIFO-Diffusion, is\nconceptually capable of generating infinitely long videos without additional\ntraining. This is achieved by iteratively performing diagonal denoising, which\nconcurrently processes a series of consecutive frames with increasing noise\nlevels in a queue; our method dequeues a fully denoised frame at the head while\nenqueuing a new random noise frame at the tail. However, diagonal denoising is\na double-edged sword as the frames near the tail can take advantage of cleaner\nones by forward reference but such a strategy induces the discrepancy between\ntraining and inference. Hence, we introduce latent partitioning to reduce the\ntraining-inference gap and lookahead denoising to leverage the benefit of\nforward referencing. Practically, FIFO-Diffusion consumes a constant amount of\nmemory regardless of the target video length given a baseline model, while\nwell-suited for parallel inference on multiple GPUs. We have demonstrated the\npromising results and effectiveness of the proposed methods on existing\ntext-to-video generation baselines. Generated video samples and source codes\nare available at our project page.\n","authors":["Jihwan Kim","Junoh Kang","Jinyoung Choi","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2405.11473v3.pdf","comment":"Project Page: https://jjihwan.github.io/projects/FIFO-Diffusion"},{"id":"http://arxiv.org/abs/2406.08249v1","updated":"2024-06-12T14:18:07Z","published":"2024-06-12T14:18:07Z","title":"Dataset Enhancement with Instance-Level Augmentations","summary":"  We present a method for expanding a dataset by incorporating knowledge from\nthe wide distribution of pre-trained latent diffusion models. Data\naugmentations typically incorporate inductive biases about the image formation\nprocess into the training (e.g. translation, scaling, colour changes, etc.).\nHere, we go beyond simple pixel transformations and introduce the concept of\ninstance-level data augmentation by repainting parts of the image at the level\nof object instances. The method combines a conditional diffusion model with\ndepth and edge maps control conditioning to seamlessly repaint individual\nobjects inside the scene, being applicable to any segmentation or detection\ndataset. Used as a data augmentation method, it improves the performance and\ngeneralization of the state-of-the-art salient object detection, semantic\nsegmentation and object detection models. By redrawing all privacy-sensitive\ninstances (people, license plates, etc.), the method is also applicable for\ndata anonymization. We also release fully synthetic and anonymized expansions\nfor popular datasets: COCO, Pascal VOC and DUTS.\n","authors":["Orest Kupyn","Christian Rupprecht"],"pdf_url":"https://arxiv.org/pdf/2406.08249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08232v1","updated":"2024-06-12T14:00:07Z","published":"2024-06-12T14:00:07Z","title":"OpenCOLE: Towards Reproducible Automatic Graphic Design Generation","summary":"  Automatic generation of graphic designs has recently received considerable\nattention. However, the state-of-the-art approaches are complex and rely on\nproprietary datasets, which creates reproducibility barriers. In this paper, we\npropose an open framework for automatic graphic design called OpenCOLE, where\nwe build a modified version of the pioneering COLE and train our model\nexclusively on publicly available datasets. Based on GPT4V evaluations, our\nmodel shows promising performance comparable to the original COLE. We release\nthe pipeline and training results to encourage open development.\n","authors":["Naoto Inoue","Kento Masui","Wataru Shimoda","Kota Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2406.08232v1.pdf","comment":"To appear as an extended abstract (EA) in Workshop on Graphic Design\n  Understanding and Generation (in CVPR2024), code:\n  https://github.com/CyberAgentAILab/OpenCOLE"},{"id":"http://arxiv.org/abs/2406.08231v1","updated":"2024-06-12T13:59:45Z","published":"2024-06-12T13:59:45Z","title":"Using Deep Convolutional Neural Networks to Detect Rendered Glitches in\n  Video Games","summary":"  In this paper, we present a method using Deep Convolutional Neural Networks\n(DCNNs) to detect common glitches in video games. The problem setting consists\nof an image (800x800 RGB) as input to be classified into one of five defined\nclasses, normal image, or one of four different kinds of glitches (stretched,\nlow resolution, missing and placeholder textures). Using a supervised approach,\nwe train a ShuffleNetV2 using generated data. This work focuses on detecting\ntexture graphical anomalies achieving arguably good performance with an\naccuracy of 86.8\\%, detecting 88\\% of the glitches with a false positive rate\nof 8.7\\%, and with the models being able to generalize and detect glitches even\nin unseen objects. We apply a confidence measure as well to tackle the issue\nwith false positives as well as an effective way of aggregating images to\nachieve better detection in production. The main use of this work is the\npartial automatization of graphical testing in the final stages of video game\ndevelopment.\n","authors":["Carlos Garcia Ling","Konrad Tollmar","Linus Gisslen"],"pdf_url":"https://arxiv.org/pdf/2406.08231v1.pdf","comment":"8 pages, 6 figures, AAIDE conference"},{"id":"http://arxiv.org/abs/2406.08226v1","updated":"2024-06-12T13:55:12Z","published":"2024-06-12T13:55:12Z","title":"DistilDoc: Knowledge Distillation for Visually-Rich Document\n  Applications","summary":"  This work explores knowledge distillation (KD) for visually-rich document\n(VRD) applications such as document layout analysis (DLA) and document image\nclassification (DIC). While VRD research is dependent on increasingly\nsophisticated and cumbersome models, the field has neglected to study\nefficiency via model compression. Here, we design a KD experimentation\nmethodology for more lean, performant models on document understanding (DU)\ntasks that are integral within larger task pipelines. We carefully selected KD\nstrategies (response-based, feature-based) for distilling knowledge to and from\nbackbones with different architectures (ResNet, ViT, DiT) and capacities (base,\nsmall, tiny). We study what affects the teacher-student knowledge gap and find\nthat some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can\nconsistently outperform supervised student training. Furthermore, we design\ndownstream task setups to evaluate covariate shift and the robustness of\ndistilled DLA models on zero-shot layout-aware document visual question\nanswering (DocVQA). DLA-KD experiments result in a large mAP knowledge gap,\nwhich unpredictably translates to downstream robustness, accentuating the need\nto further explore how to efficiently obtain more semantic document layout\nawareness.\n","authors":["Jordy Van Landeghem","Subhajit Maity","Ayan Banerjee","Matthew Blaschko","Marie-Francine Moens","Josep Lladós","Sanket Biswas"],"pdf_url":"https://arxiv.org/pdf/2406.08226v1.pdf","comment":"Accepted to ICDAR 2024 (Athens, Greece)"},{"id":"http://arxiv.org/abs/2406.08222v1","updated":"2024-06-12T13:52:30Z","published":"2024-06-12T13:52:30Z","title":"A Sociotechnical Lens for Evaluating Computer Vision Models: A Case\n  Study on Detecting and Reasoning about Gender and Emotion","summary":"  In the evolving landscape of computer vision (CV) technologies, the automatic\ndetection and interpretation of gender and emotion in images is a critical area\nof study. This paper investigates social biases in CV models, emphasizing the\nlimitations of traditional evaluation metrics such as precision, recall, and\naccuracy. These metrics often fall short in capturing the complexities of\ngender and emotion, which are fluid and culturally nuanced constructs. Our\nstudy proposes a sociotechnical framework for evaluating CV models,\nincorporating both technical performance measures and considerations of social\nfairness. Using a dataset of 5,570 images related to vaccination and climate\nchange, we empirically compared the performance of various CV models, including\ntraditional models like DeepFace and FER, and generative models like GPT-4\nVision. Our analysis involved manually validating the gender and emotional\nexpressions in a subset of images to serve as benchmarks. Our findings reveal\nthat while GPT-4 Vision outperforms other models in technical accuracy for\ngender classification, it exhibits discriminatory biases, particularly in\nresponse to transgender and non-binary personas. Furthermore, the model's\nemotion detection skew heavily towards positive emotions, with a notable bias\ntowards associating female images with happiness, especially when prompted by\nmale personas. These findings underscore the necessity of developing more\ncomprehensive evaluation criteria that address both validity and discriminatory\nbiases in CV models. Our proposed framework provides guidelines for researchers\nto critically assess CV tools, ensuring their application in communication\nresearch is both ethical and effective. The significant contribution of this\nstudy lies in its emphasis on a sociotechnical approach, advocating for CV\ntechnologies that support social good and mitigate biases rather than\nperpetuate them.\n","authors":["Sha Luo","Sang Jung Kim","Zening Duan","Kaiping Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08217v1","updated":"2024-06-12T13:45:47Z","published":"2024-06-12T13:45:47Z","title":"Runtime Freezing: Dynamic Class Loss for Multi-Organ 3D Segmentation","summary":"  Segmentation has become a crucial pre-processing step to many refined\ndownstream tasks, and particularly so in the medical domain. Even with recent\nimprovements in segmentation models, many segmentation tasks remain difficult.\nWhen multiple organs are segmented simultaneously, difficulties are due not\nonly to the limited availability of labelled data, but also to class imbalance.\nIn this work we propose dynamic class-based loss strategies to mitigate the\neffects of highly imbalanced training data. We show how our approach improves\nsegmentation performance on a challenging Multi-Class 3D Abdominal Organ\ndataset.\n","authors":["James Willoughby","Irina Voiculescu"],"pdf_url":"https://arxiv.org/pdf/2406.08217v1.pdf","comment":"4 Pages. Accepted to ISBI 2024"},{"id":"http://arxiv.org/abs/2404.14027v3","updated":"2024-06-12T13:43:50Z","published":"2024-04-22T09:43:03Z","title":"OccFeat: Self-supervised Occupancy Feature Prediction for Pretraining\n  BEV Segmentation Networks","summary":"  We introduce a self-supervised pretraining method, called OccFeat, for\ncamera-only Bird's-Eye-View (BEV) segmentation networks. With OccFeat, we\npretrain a BEV network via occupancy prediction and feature distillation tasks.\nOccupancy prediction provides a 3D geometric understanding of the scene to the\nmodel. However, the geometry learned is class-agnostic. Hence, we add semantic\ninformation to the model in the 3D space through distillation from a\nself-supervised pretrained image foundation model. Models pretrained with our\nmethod exhibit improved BEV semantic segmentation performance, particularly in\nlow-data scenarios. Moreover, empirical results affirm the efficacy of\nintegrating feature distillation with 3D occupancy prediction in our\npretraining approach. Repository: https://github.com/valeoai/Occfeat\n","authors":["Sophia Sirko-Galouchenko","Alexandre Boulch","Spyros Gidaris","Andrei Bursuc","Antonin Vobecky","Patrick Pérez","Renaud Marlet"],"pdf_url":"https://arxiv.org/pdf/2404.14027v3.pdf","comment":"Accepted to CVPR 2024, Workshop on Autonomous Driving"},{"id":"http://arxiv.org/abs/2406.08204v1","updated":"2024-06-12T13:38:10Z","published":"2024-06-12T13:38:10Z","title":"Diffusion-Promoted HDR Video Reconstruction","summary":"  High dynamic range (HDR) video reconstruction aims to generate HDR videos\nfrom low dynamic range (LDR) frames captured with alternating exposures. Most\nexisting works solely rely on the regression-based paradigm, leading to adverse\neffects such as ghosting artifacts and missing details in saturated regions. In\nthis paper, we propose a diffusion-promoted method for HDR video\nreconstruction, termed HDR-V-Diff, which incorporates a diffusion model to\ncapture the HDR distribution. As such, HDR-V-Diff can reconstruct HDR videos\nwith realistic details while alleviating ghosting artifacts. However, the\ndirect introduction of video diffusion models would impose massive\ncomputational burden. Instead, to alleviate this burden, we first propose an\nHDR Latent Diffusion Model (HDR-LDM) to learn the distribution prior of single\nHDR frames. Specifically, HDR-LDM incorporates a tonemapping strategy to\ncompress HDR frames into the latent space and a novel exposure embedding to\naggregate the exposure information into the diffusion process. We then propose\na Temporal-Consistent Alignment Module (TCAM) to learn the temporal information\nas a complement for HDR-LDM, which conducts coarse-to-fine feature alignment at\ndifferent scales among video frames. Finally, we design a Zero-Init\nCross-Attention (ZiCA) mechanism to effectively integrate the learned\ndistribution prior and temporal information for generating HDR frames.\nExtensive experiments validate that HDR-V-Diff achieves state-of-the-art\nresults on several representative datasets.\n","authors":["Yuanshen Guan","Ruikang Xu","Mingde Yao","Ruisheng Gao","Lizhi Wang","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.08204v1.pdf","comment":"Arxiv Preprint"},{"id":"http://arxiv.org/abs/2406.08192v1","updated":"2024-06-12T13:21:33Z","published":"2024-06-12T13:21:33Z","title":"2nd Place Solution for MOSE Track in CVPR 2024 PVUW workshop: Complex\n  Video Object Segmentation","summary":"  Complex video object segmentation serves as a fundamental task for a wide\nrange of downstream applications such as video editing and automatic data\nannotation. Here we present the 2nd place solution in the MOSE track of PVUW\n2024. To mitigate problems caused by tiny objects, similar objects and fast\nmovements in MOSE. We use instance segmentation to generate extra pretraining\ndata from the valid and test set of MOSE. The segmented instances are combined\nwith objects extracted from COCO to augment the training data and enhance\nsemantic representation of the baseline model. Besides, motion blur is added\nduring training to increase robustness against image blur induced by motion.\nFinally, we apply test time augmentation (TTA) and memory strategy to the\ninference stage. Our method ranked 2nd in the MOSE track of PVUW 2024, with a\n$\\mathcal{J}$ of 0.8007, a $\\mathcal{F}$ of 0.8683 and a\n$\\mathcal{J}$\\&$\\mathcal{F}$ of 0.8345.\n","authors":["Zhensong Xu","Jiangtao Yao","Chengjing Wu","Ting Liu","Luoqi Liu"],"pdf_url":"https://arxiv.org/pdf/2406.08192v1.pdf","comment":"5pages, 4 figures, technique report for MOSE Track in CVPR 2024 PVUW\n  workshop: Complex Video Object Segmentation"},{"id":"http://arxiv.org/abs/2403.17633v3","updated":"2024-06-12T13:21:23Z","published":"2024-03-26T12:08:14Z","title":"UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object\n  Detection with Sparse LiDAR and Large Domain Gaps","summary":"  In this study, we address a gap in existing unsupervised domain adaptation\napproaches on LiDAR-based 3D object detection, which have predominantly\nconcentrated on adapting between established, high-density autonomous driving\ndatasets. We focus on sparser point clouds, capturing scenarios from different\nperspectives: not just from vehicles on the road but also from mobile robots on\nsidewalks, which encounter significantly different environmental conditions and\nsensor configurations. We introduce Unsupervised Adversarial Domain Adaptation\nfor 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source\nmodels or teacher-student architectures. Instead, it uses an adversarial\napproach to directly learn domain-invariant features. We demonstrate its\nefficacy in various adaptation scenarios, showing significant improvements in\nboth self-driving car and mobile robot domains. Our code is open-source and\nwill be available soon.\n","authors":["Maciej K Wozniak","Mattias Hansson","Marko Thiel","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.17633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17136v2","updated":"2024-06-12T13:13:10Z","published":"2024-05-27T12:54:05Z","title":"PanoTree: Autonomous Photo-Spot Explorer in Virtual Reality Scenes","summary":"  Social VR platforms enable social, economic, and creative activities by\nallowing users to create and share their own virtual spaces. In social VR,\nphotography within a VR scene is an important indicator of visitors'\nactivities. Although automatic identification of photo spots within a VR scene\ncan facilitate the process of creating a VR scene and enhance the visitor\nexperience, there are challenges in quantitatively evaluating photos taken in\nthe VR scene and efficiently exploring the large VR scene. We propose PanoTree,\nan automated photo-spot explorer in VR scenes. To assess the aesthetics of\nimages captured in VR scenes, a deep scoring network is trained on a large\ndataset of photos collected by a social VR platform to determine whether humans\nare likely to take similar photos. Furthermore, we propose a Hierarchical\nOptimistic Optimization (HOO)-based search algorithm to efficiently explore 3D\nVR spaces with the reward from the scoring network. Our user study shows that\nthe scoring network achieves human-level performance in distinguishing randomly\ntaken images from those taken by humans. In addition, we show applications\nusing the explored photo spots, such as automatic thumbnail generation, support\nfor VR world creation, and visitor flow planning within a VR scene.\n","authors":["Tomohiro Hayase","Sacha Braun","Hikari Yanagawa","Itsuki Orito","Yuichi Hiroi"],"pdf_url":"https://arxiv.org/pdf/2405.17136v2.pdf","comment":"12pages, 12 figures"},{"id":"http://arxiv.org/abs/2406.08177v1","updated":"2024-06-12T13:10:31Z","published":"2024-06-12T13:10:31Z","title":"One-Step Effective Diffusion Network for Real-World Image\n  Super-Resolution","summary":"  The pre-trained text-to-image diffusion models have been increasingly\nemployed to tackle the real-world image super-resolution (Real-ISR) problem due\nto their powerful generative image priors. Most of the existing methods start\nfrom random noise to reconstruct the high-quality (HQ) image under the guidance\nof the given low-quality (LQ) image. While promising results have been\nachieved, such Real- ISR methods require multiple diffusion steps to reproduce\nthe HQ image, increasing the computational cost. Meanwhile, the random noise\nintroduces uncertainty in the output, which is unfriendly to image restoration\ntasks. To address these issues, we propose a one-step effective diffusion\nnetwork, namely OSEDiff, for the Real- ISR problem. We argue that the LQ image\ncontains rich information to restore its HQ counterpart, and hence the given LQ\nimage can be directly taken as the starting point for diffusion, eliminating\nthe uncertainty introduced by random noise sampling. We finetune the\npre-trained diffusion network with trainable layers to adapt it to complex\nimage degradations. To ensure that the one-step diffusion model could yield HQ\nReal-ISR output, we apply variational score distillation in the latent space to\nconduct KL-divergence regularization. As a result, our OSEDiff model can\nefficiently and effectively generate HQ images in just one diffusion step. Our\nexperiments demonstrate that OSEDiff achieves comparable or even better\nReal-ISR results, in terms of both objective metrics and subjective\nevaluations, than previous diffusion model based Real-ISR methods that require\ndozens or hundreds of steps. The source codes will be released at\nhttps://github.com/cswry/OSEDiff.\n","authors":["Rongyuan Wu","Lingchen Sun","Zhiyuan Ma","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08176v1","updated":"2024-06-12T13:09:59Z","published":"2024-06-12T13:09:59Z","title":"Category-level Neural Field for Reconstruction of Partially Observed\n  Objects in Indoor Environment","summary":"  Neural implicit representation has attracted attention in 3D reconstruction\nthrough various success cases. For further applications such as scene\nunderstanding or editing, several works have shown progress towards object\ncompositional reconstruction. Despite their superior performance in observed\nregions, their performance is still limited in reconstructing objects that are\npartially observed. To better treat this problem, we introduce category-level\nneural fields that learn meaningful common 3D information among objects\nbelonging to the same category present in the scene. Our key idea is to\nsubcategorize objects based on their observed shape for better training of the\ncategory-level model. Then we take advantage of the neural field to conduct the\nchallenging task of registering partially observed objects by selecting and\naligning against representative objects selected by ray-based uncertainty.\nExperiments on both simulation and real-world datasets demonstrate that our\nmethod improves the reconstruction of unobserved parts for several categories.\n","authors":["Taekbeom Lee","Youngseok Jang","H. Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2406.08176v1.pdf","comment":"RA-L. 8 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.08171v1","updated":"2024-06-12T13:04:06Z","published":"2024-06-12T13:04:06Z","title":"Continuous fake media detection: adapting deepfake detectors to new\n  generative techniques","summary":"  Generative techniques continue to evolve at an impressively high rate, driven\nby the hype about these technologies. This rapid advancement severely limits\nthe application of deepfake detectors, which, despite numerous efforts by the\nscientific community, struggle to achieve sufficiently robust performance\nagainst the ever-changing content. To address these limitations, in this paper,\nwe propose an analysis of two continuous learning techniques on a Short and a\nLong sequence of fake media. Both sequences include a complex and heterogeneous\nrange of deepfakes generated from GANs, computer graphics techniques, and\nunknown sources. Our study shows that continual learning could be important in\nmitigating the need for generalizability. In fact, we show that, although with\nsome limitations, continual learning methods help to maintain good performance\nacross the entire training sequence. For these techniques to work in a\nsufficiently robust way, however, it is necessary that the tasks in the\nsequence share similarities. In fact, according to our experiments, the order\nand similarity of the tasks can affect the performance of the models over time.\nTo address this problem, we show that it is possible to group tasks based on\ntheir similarity. This small measure allows for a significant improvement even\nin longer sequences. This result suggests that continual techniques can be\ncombined with the most promising detection methods, allowing them to catch up\nwith the latest generative techniques. In addition to this, we propose an\noverview of how this learning approach can be integrated into a deepfake\ndetection pipeline for continuous integration and continuous deployment\n(CI/CD). This allows you to keep track of different funds, such as social\nnetworks, new generative tools, or third-party datasets, and through the\nintegration of continuous learning, allows constant maintenance of the\ndetectors.\n","authors":["Francesco Tassone","Luca Maiano","Irene Amerini"],"pdf_url":"https://arxiv.org/pdf/2406.08171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05338v2","updated":"2024-06-12T12:57:34Z","published":"2024-06-08T03:44:25Z","title":"MotionClone: Training-Free Motion Cloning for Controllable Video\n  Generation","summary":"  Motion-based controllable text-to-video generation involves motions to\ncontrol the video generation. Previous methods typically require the training\nof models to encode motion cues or the fine-tuning of video diffusion models.\nHowever, these approaches often result in suboptimal motion generation when\napplied outside the trained domain. In this work, we propose MotionClone, a\ntraining-free framework that enables motion cloning from a reference video to\ncontrol text-to-video generation. We employ temporal attention in video\ninversion to represent the motions in the reference video and introduce primary\ntemporal-attention guidance to mitigate the influence of noisy or very subtle\nmotions within the attention weights. Furthermore, to assist the generation\nmodel in synthesizing reasonable spatial relationships and enhance its\nprompt-following capability, we propose a location-aware semantic guidance\nmechanism that leverages the coarse location of the foreground from the\nreference video and original classifier-free guidance features to guide the\nvideo generation. Extensive experiments demonstrate that MotionClone exhibits\nproficiency in both global camera motion and local object motion, with notable\nsuperiority in terms of motion fidelity, textual alignment, and temporal\nconsistency.\n","authors":["Pengyang Ling","Jiazi Bu","Pan Zhang","Xiaoyi Dong","Yuhang Zang","Tong Wu","Huaian Chen","Jiaqi Wang","Yi Jin"],"pdf_url":"https://arxiv.org/pdf/2406.05338v2.pdf","comment":"17 pages, 12 figures,\n  https://bujiazi.github.io/motionclone.github.io/"},{"id":"http://arxiv.org/abs/2406.08164v1","updated":"2024-06-12T12:54:27Z","published":"2024-06-12T12:54:27Z","title":"ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs","summary":"  Compositional Reasoning (CR) entails grasping the significance of attributes,\nrelations, and word order. Recent Vision-Language Models (VLMs), comprising a\nvisual encoder and a Large Language Model (LLM) decoder, have demonstrated\nremarkable proficiency in such reasoning tasks. This prompts a crucial\nquestion: have VLMs effectively tackled the CR challenge? We conjecture that\nexisting CR benchmarks may not adequately push the boundaries of modern VLMs\ndue to the reliance on an LLM-only negative text generation pipeline.\nConsequently, the negatives produced either appear as outliers from the natural\nlanguage distribution learned by VLMs' LLM decoders or as improbable within the\ncorresponding image context. To address these limitations, we introduce ConMe\n-- a compositional reasoning benchmark and a novel data generation pipeline\nleveraging VLMs to produce `hard CR Q&A'. Through a new concept of VLMs\nconversing with each other to collaboratively expose their weaknesses, our\npipeline autonomously generates, evaluates, and selects challenging\ncompositional reasoning questions, establishing a robust CR benchmark, also\nsubsequently validated manually. Our benchmark provokes a noteworthy, up to\n33%, decrease in CR performance compared to preceding benchmarks, reinstating\nthe CR challenge even for state-of-the-art VLMs.\n","authors":["Irene Huang","Wei Lin","M. Jehanzeb Mirza","Jacob A. Hansen","Sivan Doveh","Victor Ion Butoi","Roei Herzig","Assaf Arbelle","Hilde Kuhene","Trevor Darrel","Chuang Gan","Aude Oliva","Rogerio Feris","Leonid Karlinsky"],"pdf_url":"https://arxiv.org/pdf/2406.08164v1.pdf","comment":"The first three authors contributed equally"},{"id":"http://arxiv.org/abs/2406.08152v1","updated":"2024-06-12T12:40:28Z","published":"2024-06-12T12:40:28Z","title":"CT3D++: Improving 3D Object Detection with Keypoint-induced Channel-wise\n  Transformer","summary":"  The field of 3D object detection from point clouds is rapidly advancing in\ncomputer vision, aiming to accurately and efficiently detect and localize\nobjects in three-dimensional space. Current 3D detectors commonly fall short in\nterms of flexibility and scalability, with ample room for advancements in\nperformance. In this paper, our objective is to address these limitations by\nintroducing two frameworks for 3D object detection with minimal hand-crafted\ndesign. Firstly, we propose CT3D, which sequentially performs raw-point-based\nembedding, a standard Transformer encoder, and a channel-wise decoder for point\nfeatures within each proposal. Secondly, we present an enhanced network called\nCT3D++, which incorporates geometric and semantic fusion-based embedding to\nextract more valuable and comprehensive proposal-aware information.\nAdditionally, CT3D ++ utilizes a point-to-key bidirectional encoder for more\nefficient feature encoding with reduced computational cost. By replacing the\ncorresponding components of CT3D with these novel modules, CT3D++ achieves\nstate-of-the-art performance on both the KITTI dataset and the large-scale\nWay\\-mo Open Dataset. The source code for our frameworks will be made\naccessible at https://github.com/hlsheng1/CT3D-plusplus.\n","authors":["Hualian Sheng","Sijia Cai","Na Zhao","Bing Deng","Qiao Liang","Min-Jian Zhao","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2406.08152v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.08149v1","updated":"2024-06-12T12:38:33Z","published":"2024-06-12T12:38:33Z","title":"Universal Scale Laws for Colors and Patterns in Imagery","summary":"  Distribution of colors and patterns in images is observed through cascades\nthat adjust spatial resolution and dynamics. Cascades of colors reveal the\nemergent universal property that Fully Colored Images (FCIs) of natural scenes\nadhere to the debated continuous linear log-scale law (slope $-2.00 \\pm 0.01$)\n(L1). Cascades of discrete $2 \\times 2$ patterns are derived from pixel squares\nreductions onto the seven unlabeled rotation-free textures (0000, 0001, 0011,\n0012, 0101, 0102, 0123). They exhibit an unparalleled universal entropy maximum\nof $1.74 \\pm 0.013$ at some dynamics regardless of spatial scale (L2). Patterns\nalso adhere to the Integral Fluctuation Theorem ($1.00 \\pm 0.01$) (L3), pivotal\nin studies of chaotic systems. Images with fewer colors exhibit quadratic shift\nand bias from L1 and L3 but adhere to L2. Randomized Hilbert fractals FCIs\nbetter match the laws than basic-to-AI-based simulations. Those results are of\ninterest in Neural Networks, out of equilibrium physics and spectral imagery.\n","authors":["Rémi Michel","Mohamed Tamaazousti"],"pdf_url":"https://arxiv.org/pdf/2406.08149v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2406.07500v2","updated":"2024-06-12T12:37:52Z","published":"2024-06-11T17:35:39Z","title":"SPIN: Spacecraft Imagery for Navigation","summary":"  Data acquired in space operational conditions is scarce due to the costs and\ncomplexity of space operations. This poses a challenge to learning-based\nvisual-based navigation algorithms employed in autonomous spacecraft\nnavigation. Existing datasets, which largely depend on computer-simulated data,\nhave partially filled this gap. However, the image generation tools they use\nare proprietary, which limits the evaluation of methods to unseen scenarios.\nFurthermore, these datasets provide limited ground-truth data, primarily\nfocusing on the spacecraft's translation and rotation relative to the camera.\nTo address these limitations, we present SPIN (SPacecraft Imagery for\nNavigation), an open-source realistic spacecraft image generation tool for\nrelative navigation between two spacecrafts. SPIN provides a wide variety of\nground-truth data and allows researchers to employ custom 3D models of\nsatellites, define specific camera-relative poses, and adjust various settings\nsuch as camera parameters and environmental illumination conditions. For the\ntask of spacecraft pose estimation, we compare the results of training with a\nSPIN-generated dataset against existing synthetic datasets. We show a %50\naverage error reduction in common testbed data (that simulates realistic space\nconditions). Both the SPIN tool (and source code) and our enhanced version of\nthe synthetic datasets will be publicly released upon paper acceptance on\nGitHub https://github.com/vpulab/SPIN.\n","authors":["Javier Montalvo","Juan Ignacio Bravo Pérez-Villar","Álvaro García-Martín","Pablo Carballeira","Jesús Besc'os"],"pdf_url":"https://arxiv.org/pdf/2406.07500v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04288v4","updated":"2024-06-12T12:37:43Z","published":"2022-10-09T15:42:36Z","title":"CoopHash: Cooperative Learning of Multipurpose Descriptor and\n  Contrastive Pair Generator via Variational MCMC Teaching for Supervised Image\n  Hashing","summary":"  Leveraging supervised information can lead to superior retrieval performance\nin the image hashing domain but the performance degrades significantly without\nenough labeled data. One effective solution to boost performance is to employ\ngenerative models, such as Generative Adversarial Networks (GANs), to generate\nsynthetic data in an image hashing model. However, GAN-based methods are\ndifficult to train, which prevents the hashing approaches from jointly training\nthe generative models and the hash functions. This limitation results in\nsub-optimal retrieval performance. To overcome this limitation, we propose a\nnovel framework, the generative cooperative hashing network, which is based on\nenergy-based cooperative learning. This framework jointly learns a powerful\ngenerative representation of the data and a robust hash function via two\ncomponents: a top-down contrastive pair generator that synthesizes contrastive\nimages and a bottom-up multipurpose descriptor that simultaneously represents\nthe images from multiple perspectives, including probability density, hash\ncode, latent code, and category. The two components are jointly learned via a\nnovel likelihood-based cooperative learning scheme. We conduct experiments on\nseveral real-world datasets and show that the proposed method outperforms the\ncompeting hashing supervised methods, achieving up to 10\\% relative improvement\nover the current state-of-the-art supervised hashing methods, and exhibits a\nsignificantly better performance in out-of-distribution retrieval.\n","authors":["Khoa D. Doan","Jianwen Xie","Yaxuan Zhu","Yang Zhao","Ping Li"],"pdf_url":"https://arxiv.org/pdf/2210.04288v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05022v3","updated":"2024-06-12T12:30:13Z","published":"2023-08-09T15:38:36Z","title":"Exploring Frequency-Inspired Optimization in Transformer for Efficient\n  Single Image Super-Resolution","summary":"  Transformer-based methods have exhibited remarkable potential in single image\nsuper-resolution (SISR) by effectively extracting long-range dependencies.\nHowever, most of the current research in this area has prioritized the design\nof transformer blocks to capture global information, while overlooking the\nimportance of incorporating high-frequency priors, which we believe could be\nbeneficial. In our study, we conducted a series of experiments and found that\ntransformer structures are more adept at capturing low-frequency information,\nbut have limited capacity in constructing high-frequency representations when\ncompared to their convolutional counterparts. Our proposed solution, the\ncross-refinement adaptive feature modulation transformer (CRAFT), integrates\nthe strengths of both convolutional and transformer structures. It comprises\nthree key components: the high-frequency enhancement residual block (HFERB) for\nextracting high-frequency information, the shift rectangle window attention\nblock (SRWAB) for capturing global information, and the hybrid fusion block\n(HFB) for refining the global representation. To tackle the inherent\nintricacies of transformer structures, we introduce a frequency-guided\npost-training quantization (PTQ) method aimed at enhancing CRAFT's efficiency.\nThese strategies incorporate adaptive dual clipping and boundary refinement. To\nfurther amplify the versatility of our proposed approach, we extend our PTQ\nstrategy to function as a general quantization method for transformer-based\nSISR techniques. Our experimental findings showcase CRAFT's superiority over\ncurrent state-of-the-art methods, both in full-precision and quantization\nscenarios. These results underscore the efficacy and universality of our PTQ\nstrategy.\n","authors":["Ao Li","Le Zhang","Yun Liu","Ce Zhu"],"pdf_url":"https://arxiv.org/pdf/2308.05022v3.pdf","comment":"Extended version of CRAFT"},{"id":"http://arxiv.org/abs/2406.08137v1","updated":"2024-06-12T12:26:26Z","published":"2024-06-12T12:26:26Z","title":"The impact of deep learning aid on the workload and interpretation\n  accuracy of radiologists on chest computed tomography: a cross-over reader\n  study","summary":"  Interpretation of chest computed tomography (CT) is time-consuming. Previous\nstudies have measured the time-saving effect of using a deep-learning-based aid\n(DLA) for CT interpretation. We evaluated the joint impact of a multi-pathology\nDLA on the time and accuracy of radiologists' reading.\n  40 radiologists were randomly split into three experimental arms: control\n(10), who interpret studies without assistance; informed group (10), who were\nbriefed about DLA pathologies, but performed readings without it; and the\nexperimental group (20), who interpreted half studies with DLA, and half\nwithout. Every arm used the same 200 CT studies retrospectively collected from\nBIMCV-COVID19 dataset; each radiologist provided readings for 20 CT studies. We\ncompared interpretation time, and accuracy of participants diagnostic report\nwith respect to 12 pathological findings.\n  Mean reading time per study was 15.6 minutes [SD 8.5] in the control arm,\n13.2 minutes [SD 8.7] in the informed arm, 14.4 [SD 10.3] in the experimental\narm without DLA, and 11.4 minutes [SD 7.8] in the experimental arm with DLA.\nMean sensitivity and specificity were 41.5 [SD 30.4], 86.8 [SD 28.3] in the\ncontrol arm; 53.5 [SD 22.7], 92.3 [SD 9.4] in the informed non-assisted arm;\n63.2 [SD 16.4], 92.3 [SD 8.2] in the experimental arm without DLA; and 91.6 [SD\n7.2], 89.9 [SD 6.0] in the experimental arm with DLA. DLA speed up\ninterpretation time per study by 2.9 minutes (CI95 [1.7, 4.3], p<0.0005),\nincreased sensitivity by 28.4 (CI95 [23.4, 33.4], p<0.0005), and decreased\nspecificity by 2.4 (CI95 [0.6, 4.3], p=0.13).\n  Of 20 radiologists in the experimental arm, 16 have improved reading time and\nsensitivity, two improved their time with a marginal drop in sensitivity, and\ntwo participants improved sensitivity with increased time. Overall, DLA\nintroduction decreased reading time by 20.6%.\n","authors":["Anvar Kurmukov","Valeria Chernina","Regina Gareeva","Maria Dugova","Ekaterina Petrash","Olga Aleshina","Maxim Pisov","Boris Shirokikh","Valentin Samokhin","Vladislav Proskurov","Stanislav Shimovolos","Maria Basova","Mikhail Goncahrov","Eugenia Soboleva","Maria Donskova","Farukh Yaushev","Alexey Shevtsov","Alexey Zakharov","Talgat Saparov","Victor Gombolevskiy","Mikhail Belyaev"],"pdf_url":"https://arxiv.org/pdf/2406.08137v1.pdf","comment":"17 pages, 6 figures, 8 tables"},{"id":"http://arxiv.org/abs/2406.00123v2","updated":"2024-06-12T12:21:52Z","published":"2024-05-31T18:25:23Z","title":"Correlation-aware Coarse-to-fine MLPs for Deformable Medical Image\n  Registration","summary":"  Deformable image registration is a fundamental step for medical image\nanalysis. Recently, transformers have been used for registration and\noutperformed Convolutional Neural Networks (CNNs). Transformers can capture\nlong-range dependence among image features, which have been shown beneficial\nfor registration. However, due to the high computation/memory loads of\nself-attention, transformers are typically used at downsampled feature\nresolutions and cannot capture fine-grained long-range dependence at the full\nimage resolution. This limits deformable registration as it necessitates\nprecise dense correspondence between each image pixel. Multi-layer Perceptrons\n(MLPs) without self-attention are efficient in computation/memory usage,\nenabling the feasibility of capturing fine-grained long-range dependence at\nfull resolution. Nevertheless, MLPs have not been extensively explored for\nimage registration and are lacking the consideration of inductive bias crucial\nfor medical registration tasks. In this study, we propose the first\ncorrelation-aware MLP-based registration network (CorrMLP) for deformable\nmedical image registration. Our CorrMLP introduces a correlation-aware\nmulti-window MLP block in a novel coarse-to-fine registration architecture,\nwhich captures fine-grained multi-range dependence to perform correlation-aware\ncoarse-to-fine registration. Extensive experiments with seven public medical\ndatasets show that our CorrMLP outperforms state-of-the-art deformable\nregistration methods.\n","authors":["Mingyuan Meng","Dagan Feng","Lei Bi","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2406.00123v2.pdf","comment":"Accepted at CVPR2024 as Oral Presentation && Best Paper Candidate"},{"id":"http://arxiv.org/abs/2402.03944v3","updated":"2024-06-12T12:06:01Z","published":"2024-02-03T14:27:18Z","title":"IMUSE: IMU-based Facial Expression Capture","summary":"  For facial motion capture and analysis, the dominated solutions are generally\nbased on visual cues, which cannot protect privacy and are vulnerable to\nocclusions. Inertial measurement units (IMUs) serve as potential rescues yet\nare mainly adopted for full-body motion capture. In this paper, we propose\nIMUSE to fill the gap, a novel path for facial expression capture using purely\nIMU signals, significantly distant from previous visual solutions.The key\ndesign in our IMUSE is a trilogy. We first design micro-IMUs to suit facial\ncapture, companion with an anatomy-driven IMU placement scheme. Then, we\ncontribute a novel IMU-ARKit dataset, which provides rich paired IMU/visual\nsignals for diverse facial expressions and performances. Such unique\nmulti-modality brings huge potential for future directions like IMU-based\nfacial behavior analysis. Moreover, utilizing IMU-ARKit, we introduce a strong\nbaseline approach to accurately predict facial blendshape parameters from\npurely IMU signals. The IMUSE framework empowers us to perform accurate facial\ncapture in scenarios where visual methods falter and simultaneously safeguard\nuser privacy. We conduct extensive experiments about both the IMU configuration\nand technical components to validate the effectiveness of our IMUSE approach.\nNotably, IMUSE enables various potential and novel applications, i.e., facial\ncapture against occlusions or in a moving performance. We will release our\ndataset and implementations to enrich more possibilities of facial capture and\nanalysis in our community.\n","authors":["Youjia Wang","Yiwen Wu","Hengan Zhou","Hongyang Lin","Xingyue Peng","Yingwenqi Jiang","Yingsheng Zhu","Guanpeng Long","Yatu Zhang","Jingya Wang","Lan Xu","Jingyi Yu"],"pdf_url":"https://arxiv.org/pdf/2402.03944v3.pdf","comment":"Go to IMUSE project page\n  https://sites.google.com/view/projectpage-imuse and watch our video\n  https://youtu.be/Rki9syHsvpc"},{"id":"http://arxiv.org/abs/2406.08113v1","updated":"2024-06-12T11:50:51Z","published":"2024-06-12T11:50:51Z","title":"Valeo4Cast: A Modular Approach to End-to-End Forecasting","summary":"  Motion forecasting is crucial in autonomous driving systems to anticipate the\nfuture trajectories of surrounding agents such as pedestrians, vehicles, and\ntraffic signals. In end-to-end forecasting, the model must jointly detect from\nsensor data (cameras or LiDARs) the position and past trajectories of the\ndifferent elements of the scene and predict their future location. We depart\nfrom the current trend of tackling this task via end-to-end training from\nperception to forecasting and we use a modular approach instead. Following a\nrecent study, we individually build and train detection, tracking, and\nforecasting modules. We then only use consecutive finetuning steps to integrate\nthe modules better and alleviate compounding errors. Our study reveals that\nthis simple yet effective approach significantly improves performance on the\nend-to-end forecasting benchmark. Consequently, our solution ranks first in the\nArgoverse 2 end-to-end Forecasting Challenge held at CVPR 2024 Workshop on\nAutonomous Driving (WAD), with 63.82 mAPf. We surpass forecasting results by\n+17.1 points over last year's winner and by +13.3 points over this year's\nrunner-up. This remarkable performance in forecasting can be explained by our\nmodular paradigm, which integrates finetuning strategies and significantly\noutperforms the end-to-end-trained counterparts.\n","authors":["Yihong Xu","Éloi Zablocki","Alexandre Boulch","Gilles Puy","Mickael Chen","Florent Bartoccioni","Nermin Samet","Oriane Siméoni","Spyros Gidaris","Tuan-Hung Vu","Andrei Bursuc","Eduardo Valle","Renaud Marlet","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2406.08113v1.pdf","comment":"Winning solution of the Argoverse 2 \"Unified Detection, Tracking, and\n  Forecasting\" challenge, held at CVPR 2024 WAD"},{"id":"http://arxiv.org/abs/2402.04168v2","updated":"2024-06-12T11:34:30Z","published":"2024-02-06T17:24:06Z","title":"Informed Reinforcement Learning for Situation-Aware Traffic Rule\n  Exceptions","summary":"  Reinforcement Learning is a highly active research field with promising\nadvancements. In the field of autonomous driving, however, often very simple\nscenarios are being examined. Common approaches use non-interpretable control\ncommands as the action space and unstructured reward designs which lack\nstructure. In this work, we introduce Informed Reinforcement Learning, where a\nstructured rulebook is integrated as a knowledge source. We learn trajectories\nand asses them with a situation-aware reward design, leading to a dynamic\nreward which allows the agent to learn situations which require controlled\ntraffic rule exceptions. Our method is applicable to arbitrary RL models. We\nsuccessfully demonstrate high completion rates of complex scenarios with recent\nmodel-based agents.\n","authors":["Daniel Bogdoll","Jing Qin","Moritz Nekolla","Ahmed Abouelazm","Tim Joseph","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2402.04168v2.pdf","comment":"Daniel Bogdoll and Jing Qin contributed equally. Accepted for\n  publication at ICRA 2024"},{"id":"http://arxiv.org/abs/2406.08102v1","updated":"2024-06-12T11:31:18Z","published":"2024-06-12T11:31:18Z","title":"Adversarial Patch for 3D Local Feature Extractor","summary":"  Local feature extractors are the cornerstone of many computer vision tasks.\nHowever, their vulnerability to adversarial attacks can significantly\ncompromise their effectiveness. This paper discusses approaches to attack\nsophisticated local feature extraction algorithms and models to achieve two\ndistinct goals: (1) forcing a match between originally non-matching image\nregions, and (2) preventing a match between originally matching regions. At the\nend of the paper, we discuss the performance and drawbacks of different patch\ngeneration methods.\n","authors":["Yu Wen Pao","Li Chang Lai","Hong-Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2406.08102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08096v1","updated":"2024-06-12T11:22:03Z","published":"2024-06-12T11:22:03Z","title":"Make Your Actor Talk: Generalizable and High-Fidelity Lip Sync with\n  Motion and Appearance Disentanglement","summary":"  We aim to edit the lip movements in talking video according to the given\nspeech while preserving the personal identity and visual details. The task can\nbe decomposed into two sub-problems: (1) speech-driven lip motion generation\nand (2) visual appearance synthesis. Current solutions handle the two\nsub-problems within a single generative model, resulting in a challenging\ntrade-off between lip-sync quality and visual details preservation. Instead, we\npropose to disentangle the motion and appearance, and then generate them one by\none with a speech-to-motion diffusion model and a motion-conditioned appearance\ngeneration model. However, there still remain challenges in each stage, such as\nmotion-aware identity preservation in (1) and visual details preservation in\n(2). Therefore, to preserve personal identity, we adopt landmarks to represent\nthe motion, and further employ a landmark-based identity loss. To capture\nmotion-agnostic visual details, we use separate encoders to encode the lip,\nnon-lip appearance and motion, and then integrate them with a learned fusion\nmodule. We train MyTalk on a large-scale and diverse dataset. Experiments show\nthat our method generalizes well to the unknown, even out-of-domain person, in\nterms of both lip sync and visual detail preservation. We encourage the readers\nto watch the videos on our project page (https://Ingrid789.github.io/MyTalk/).\n","authors":["Runyi Yu","Tianyu He","Ailing Zeng","Yuchi Wang","Junliang Guo","Xu Tan","Chang Liu","Jie Chen","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2406.08096v1.pdf","comment":"14 pages of main text, 23 pages in total, 9 figures"},{"id":"http://arxiv.org/abs/2406.08090v1","updated":"2024-06-12T11:15:59Z","published":"2024-06-12T11:15:59Z","title":"From Sim-to-Real: Toward General Event-based Low-light Frame\n  Interpolation with Per-scene Optimization","summary":"  Video Frame Interpolation (VFI) is important for video enhancement, frame\nrate up-conversion, and slow-motion generation. The introduction of event\ncameras, which capture per-pixel brightness changes asynchronously, has\nsignificantly enhanced VFI capabilities, particularly for high-speed, nonlinear\nmotions. However, these event-based methods encounter challenges in low-light\nconditions, notably trailing artifacts and signal latency, which hinder their\ndirect applicability and generalization. Addressing these issues, we propose a\nnovel per-scene optimization strategy tailored for low-light conditions. This\napproach utilizes the internal statistics of a sequence to handle degraded\nevent data under low-light conditions, improving the generalizability to\ndifferent lighting and camera settings. To evaluate its robustness in low-light\ncondition, we further introduce EVFI-LL, a unique RGB+Event dataset captured\nunder low-light conditions. Our results demonstrate state-of-the-art\nperformance in low-light environments. Both the dataset and the source code\nwill be made publicly available upon publication. Project page:\nhttps://naturezhanghn.github.io/sim2real.\n","authors":["Ziran Zhang","Yongrui Ma","Yueting Chen","Feng Zhang","Jinwei Gu","Tianfan Xue","Shi Guo"],"pdf_url":"https://arxiv.org/pdf/2406.08090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08089v1","updated":"2024-06-12T11:12:30Z","published":"2024-06-12T11:12:30Z","title":"Identification of Conversation Partners from Egocentric Video","summary":"  Communicating in noisy, multi-talker environments is challenging, especially\nfor people with hearing impairments. Egocentric video data can potentially be\nused to identify a user's conversation partners, which could be used to inform\nselective acoustic amplification of relevant speakers. Recent introduction of\ndatasets and tasks in computer vision enable progress towards analyzing social\ninteractions from an egocentric perspective. Building on this, we focus on the\ntask of identifying conversation partners from egocentric video and describe a\nsuitable dataset. Our dataset comprises 69 hours of egocentric video of diverse\nmulti-conversation scenarios where each individual was assigned one or more\nconversation partners, providing the labels for our computer vision task. This\ndataset enables the development and assessment of algorithms for identifying\nconversation partners and evaluating related approaches. Here, we describe the\ndataset alongside initial baseline results of this ongoing work, aiming to\ncontribute to the exciting advancements in egocentric video analysis for social\nsettings.\n","authors":["Tobias Dorszewski","Søren A. Fuglsang","Jens Hjortkjær"],"pdf_url":"https://arxiv.org/pdf/2406.08089v1.pdf","comment":"First Joint Egocentric Vision (EgoVis) Workshop at CVPR 2024"},{"id":"http://arxiv.org/abs/2406.05704v2","updated":"2024-06-12T11:11:07Z","published":"2024-06-09T09:15:54Z","title":"Hierarchical Features Matter: A Deep Exploration of GAN Priors for\n  Improved Dataset Distillation","summary":"  Dataset distillation is an emerging dataset reduction method, which condenses\nlarge-scale datasets while maintaining task accuracy. Current methods have\nintegrated parameterization techniques to boost synthetic dataset performance\nby shifting the optimization space from pixel to another informative feature\ndomain. However, they limit themselves to a fixed optimization space for\ndistillation, neglecting the diverse guidance across different informative\nlatent spaces. To overcome this limitation, we propose a novel parameterization\nmethod dubbed Hierarchical Generative Latent Distillation (H-GLaD), to\nsystematically explore hierarchical layers within the generative adversarial\nnetworks (GANs). This allows us to progressively span from the initial latent\nspace to the final pixel space. In addition, we introduce a novel\nclass-relevant feature distance metric to alleviate the computational burden\nassociated with synthetic dataset evaluation, bridging the gap between\nsynthetic and original datasets. Experimental results demonstrate that the\nproposed H-GLaD achieves a significant improvement in both same-architecture\nand cross-architecture performance with equivalent time consumption.\n","authors":["Xinhao Zhong","Hao Fang","Bin Chen","Xulin Gu","Tao Dai","Meikang Qiu","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2406.05704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07251v2","updated":"2024-06-12T11:08:59Z","published":"2024-06-11T13:33:33Z","title":"Is One GPU Enough? Pushing Image Generation at Higher-Resolutions with\n  Foundation Models","summary":"  In this work, we introduce Pixelsmith, a zero-shot text-to-image generative\nframework to sample images at higher resolutions with a single GPU. We are the\nfirst to show that it is possible to scale the output of a pre-trained\ndiffusion model by a factor of 1000, opening the road for gigapixel image\ngeneration at no additional cost. Our cascading method uses the image generated\nat the lowest resolution as a baseline to sample at higher resolutions. For the\nguidance, we introduce the Slider, a tunable mechanism that fuses the overall\nstructure contained in the first-generated image with enhanced fine details. At\neach inference step, we denoise patches rather than the entire latent space,\nminimizing memory demands such that a single GPU can handle the process,\nregardless of the image's resolution. Our experimental results show that\nPixelsmith not only achieves higher quality and diversity compared to existing\ntechniques, but also reduces sampling time and artifacts. The code for our work\nis available at https://github.com/Thanos-DB/Pixelsmith.\n","authors":["Athanasios Tragakis","Marco Aversa","Chaitanya Kaul","Roderick Murray-Smith","Daniele Faccio"],"pdf_url":"https://arxiv.org/pdf/2406.07251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08085v1","updated":"2024-06-12T11:07:55Z","published":"2024-06-12T11:07:55Z","title":"Flash-VStream: Memory-Based Real-Time Understanding for Long Video\n  Streams","summary":"  Benefiting from the advancements in large language models and cross-modal\nalignment, existing multi-modal video understanding methods have achieved\nprominent performance in offline scenario. However, online video streams, as\none of the most common media forms in the real world, have seldom received\nattention. Compared to offline videos, the 'dynamic' nature of online video\nstreams poses challenges for the direct application of existing models and\nintroduces new problems, such as the storage of extremely long-term\ninformation, interaction between continuous visual content and 'asynchronous'\nuser questions. Therefore, in this paper we present Flash-VStream, a\nvideo-language model that simulates the memory mechanism of human. Our model is\nable to process extremely long video streams in real-time and respond to user\nqueries simultaneously. Compared to existing models, Flash-VStream achieves\nsignificant reductions in inference latency and VRAM consumption, which is\nintimately related to performing understanding of online streaming video. In\naddition, given that existing video understanding benchmarks predominantly\nconcentrate on offline scenario, we propose VStream-QA, a novel question\nanswering benchmark specifically designed for online video streaming\nunderstanding. Comparisons with popular existing methods on the proposed\nbenchmark demonstrate the superiority of our method for such challenging\nsetting. To verify the generalizability of our approach, we further evaluate it\non existing video understanding benchmarks and achieves state-of-the-art\nperformance in offline scenarios as well. All code, models, and datasets are\navailable at the https://invinciblewyq.github.io/vstream-page/\n","authors":["Haoji Zhang","Yiqin Wang","Yansong Tang","Yong Liu","Jiashi Feng","Jifeng Dai","Xiaojie Jin"],"pdf_url":"https://arxiv.org/pdf/2406.08085v1.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.08079v1","updated":"2024-06-12T11:02:15Z","published":"2024-06-12T11:02:15Z","title":"A$^{2}$-MAE: A spatial-temporal-spectral unified remote sensing\n  pre-training method based on anchor-aware masked autoencoder","summary":"  Vast amounts of remote sensing (RS) data provide Earth observations across\nmultiple dimensions, encompassing critical spatial, temporal, and spectral\ninformation which is essential for addressing global-scale challenges such as\nland use monitoring, disaster prevention, and environmental change mitigation.\nDespite various pre-training methods tailored to the characteristics of RS\ndata, a key limitation persists: the inability to effectively integrate\nspatial, temporal, and spectral information within a single unified model. To\nunlock the potential of RS data, we construct a Spatial-Temporal-Spectral\nStructured Dataset (STSSD) characterized by the incorporation of multiple RS\nsources, diverse coverage, unified locations within image sets, and\nheterogeneity within images. Building upon this structured dataset, we propose\nan Anchor-Aware Masked AutoEncoder method (A$^{2}$-MAE), leveraging intrinsic\ncomplementary information from the different kinds of images and\ngeo-information to reconstruct the masked patches during the pre-training\nphase. A$^{2}$-MAE integrates an anchor-aware masking strategy and a geographic\nencoding module to comprehensively exploit the properties of RS images.\nSpecifically, the proposed anchor-aware masking strategy dynamically adapts the\nmasking process based on the meta-information of a pre-selected anchor image,\nthereby facilitating the training on images captured by diverse types of RS\nsources within one model. Furthermore, we propose a geographic encoding method\nto leverage accurate spatial patterns, enhancing the model generalization\ncapabilities for downstream applications that are generally location-related.\nExtensive experiments demonstrate our method achieves comprehensive\nimprovements across various downstream tasks compared with existing RS\npre-training methods, including image classification, semantic segmentation,\nand change detection tasks.\n","authors":["Lixian Zhang","Yi Zhao","Runmin Dong","Jinxiao Zhang","Shuai Yuan","Shilei Cao","Mengxuan Chen","Juepeng Zheng","Weijia Li","Wei Liu","Litong Feng","Haohuan Fu"],"pdf_url":"https://arxiv.org/pdf/2406.08079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08074v1","updated":"2024-06-12T10:48:53Z","published":"2024-06-12T10:48:53Z","title":"A Concept-Based Explainability Framework for Large Multimodal Models","summary":"  Large multimodal models (LMMs) combine unimodal encoders and large language\nmodels (LLMs) to perform multimodal tasks. Despite recent advancements towards\nthe interpretability of these models, understanding internal representations of\nLMMs remains largely a mystery. In this paper, we present a novel framework for\nthe interpretation of LMMs. We propose a dictionary learning based approach,\napplied to the representation of tokens. The elements of the learned dictionary\ncorrespond to our proposed concepts. We show that these concepts are well\nsemantically grounded in both vision and text. Thus we refer to these as\n\"multi-modal concepts\". We qualitatively and quantitatively evaluate the\nresults of the learnt concepts. We show that the extracted multimodal concepts\nare useful to interpret representations of test samples. Finally, we evaluate\nthe disentanglement between different concepts and the quality of grounding\nconcepts visually and textually. We will publicly release our code.\n","authors":["Jayneel Parekh","Pegah Khayatan","Mustafa Shukor","Alasdair Newson","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2406.08074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08070v1","updated":"2024-06-12T10:40:10Z","published":"2024-06-12T10:40:10Z","title":"CFG++: Manifold-constrained Classifier Free Guidance for Diffusion\n  Models","summary":"  Classifier-free guidance (CFG) is a fundamental tool in modern diffusion\nmodels for text-guided generation. Although effective, CFG has notable\ndrawbacks. For instance, DDIM with CFG lacks invertibility, complicating image\nediting; furthermore, high guidance scales, essential for high-quality outputs,\nfrequently result in issues like mode collapse. Contrary to the widespread\nbelief that these are inherent limitations of diffusion models, this paper\nreveals that the problems actually stem from the off-manifold phenomenon\nassociated with CFG, rather than the diffusion models themselves. More\nspecifically, inspired by the recent advancements of diffusion model-based\ninverse problem solvers (DIS), we reformulate text-guidance as an inverse\nproblem with a text-conditioned score matching loss, and develop CFG++, a novel\napproach that tackles the off-manifold challenges inherent in traditional CFG.\nCFG++ features a surprisingly simple fix to CFG, yet it offers significant\nimprovements, including better sample quality for text-to-image generation,\ninvertibility, smaller guidance scales, reduced mode collapse, etc.\nFurthermore, CFG++ enables seamless interpolation between unconditional and\nconditional sampling at lower guidance scales, consistently outperforming\ntraditional CFG at all scales. Experimental results confirm that our method\nsignificantly enhances performance in text-to-image generation, DDIM inversion,\nediting, and solving inverse problems, suggesting a wide-ranging impact and\npotential applications in various fields that utilize text guidance. Project\nPage: https://cfgpp-diffusion.github.io/.\n","authors":["Hyungjin Chung","Jeongsol Kim","Geon Yeong Park","Hyelin Nam","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2406.08070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08063v1","updated":"2024-06-12T10:26:52Z","published":"2024-06-12T10:26:52Z","title":"MWIRSTD: A MWIR Small Target Detection Dataset","summary":"  This paper presents a novel mid-wave infrared (MWIR) small target detection\ndataset (MWIRSTD) comprising 14 video sequences containing approximately 1053\nimages with annotated targets of three distinct classes of small objects.\nCaptured using cooled MWIR imagers, the dataset offers a unique opportunity for\nresearchers to develop and evaluate state-of-the-art methods for small object\ndetection in realistic MWIR scenes. Unlike existing datasets, which primarily\nconsist of uncooled thermal images or synthetic data with targets superimposed\nonto the background or vice versa, MWIRSTD provides authentic MWIR data with\ndiverse targets and environments. Extensive experiments on various traditional\nmethods and deep learning-based techniques for small target detection are\nperformed on the proposed dataset, providing valuable insights into their\nefficacy. The dataset and code are available at\nhttps://github.com/avinres/MWIRSTD.\n","authors":["Nikhil Kumar","Avinash Upadhyay","Shreya Sharma","Manoj Sharma","Pravendra Singh"],"pdf_url":"https://arxiv.org/pdf/2406.08063v1.pdf","comment":"Accepted in ICIP2024"},{"id":"http://arxiv.org/abs/2406.08048v1","updated":"2024-06-12T10:01:00Z","published":"2024-06-12T10:01:00Z","title":"3D CBCT Challenge 2024: Improved Cone Beam CT Reconstruction using\n  SwinIR-Based Sinogram and Image Enhancement","summary":"  In this paper, we present our approach to the 3D CBCT Challenge 2024, a part\nof ICASSP SP Grand Challenges 2024. Improvement in Cone Beam Computed\nTomography (CBCT) reconstruction has been achieved by integrating Swin Image\nRestoration (SwinIR) based sinogram and image enhancement modules. The proposed\nmethodology uses Nesterov Accelerated Gradient Descent (NAG) to solve the least\nsquares (NAG-LS) problem in CT image reconstruction. The integration of\nsinogram and image enhancement modules aims to enhance image clarity and\npreserve fine details, offering a promising solution for both low dose and\nclinical dose CBCT reconstruction. The averaged mean squared error (MSE) over\nthe validation dataset has decreased significantly, in the case of low dose by\none-fifth and clinical dose by one-tenth. Our solution is one of the top 5\napproaches in this challenge.\n","authors":["Sasidhar Alavala","Subrahmanyam Gorthi"],"pdf_url":"https://arxiv.org/pdf/2406.08048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08046v1","updated":"2024-06-12T09:58:42Z","published":"2024-06-12T09:58:42Z","title":"A Robust Pipeline for Classification and Detection of Bleeding Frames in\n  Wireless Capsule Endoscopy using Swin Transformer and RT-DETR","summary":"  In this paper, we present our approach to the Auto WCEBleedGen Challenge V2\n2024. Our solution combines the Swin Transformer for the initial classification\nof bleeding frames and RT-DETR for further detection of bleeding in Wireless\nCapsule Endoscopy (WCE), enhanced by a series of image preprocessing steps.\nThese steps include converting images to Lab colour space, applying Contrast\nLimited Adaptive Histogram Equalization (CLAHE) for better contrast, and using\nGaussian blur to suppress artefacts. The Swin Transformer utilizes a tiered\narchitecture with shifted windows to efficiently manage self-attention\ncalculations, focusing on local windows while enabling cross-window\ninteractions. RT-DETR features an efficient hybrid encoder for fast processing\nof multi-scale features and an uncertainty-minimal query selection for enhanced\naccuracy. The class activation maps by Ablation-CAM are plausible to the\nmodel's decisions. On the validation set, this approach achieves a\nclassification accuracy of 98.5% (best among the other state-of-the-art models)\ncompared to 91.7% without any pre-processing and an $\\text{AP}_{50}$ of 66.7%\ncompared to 65.0% with state-of-the-art YOLOv8. On the test set, this approach\nachieves a classification accuracy and F1 score of 87.0% and 89.0%\nrespectively.\n","authors":["Sasidhar Alavala","Anil Kumar Vadde","Aparnamala Kancheti","Subrahmanyam Gorthi"],"pdf_url":"https://arxiv.org/pdf/2406.08046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03229v3","updated":"2024-06-12T09:54:33Z","published":"2024-06-05T13:06:17Z","title":"Global Clipper: Enhancing Safety and Reliability of Transformer-based\n  Object Detection Models","summary":"  As transformer-based object detection models progress, their impact in\ncritical sectors like autonomous vehicles and aviation is expected to grow.\nSoft errors causing bit flips during inference have significantly impacted DNN\nperformance, altering predictions. Traditional range restriction solutions for\nCNNs fall short for transformers. This study introduces the Global Clipper and\nGlobal Hybrid Clipper, effective mitigation strategies specifically designed\nfor transformer-based models. It significantly enhances their resilience to\nsoft errors and reduces faulty inferences to ~ 0\\%. We also detail extensive\ntesting across over 64 scenarios involving two transformer models (DINO-DETR\nand Lite-DETR) and two CNN models (YOLOv3 and SSD) using three datasets,\ntotalling approximately 3.3 million inferences, to assess model robustness\ncomprehensively. Moreover, the paper explores unique aspects of attention\nblocks in transformers and their operational differences from CNNs.\n","authors":["Qutub Syed Sha","Michael Paulitsch","Karthik Pattabiraman","Korbinian Hagn","Fabian Oboril","Cornelius Buerkle","Kay-Ulrich Scholl","Gereon Hinz","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2406.03229v3.pdf","comment":"Accepted at IJCAI-AISafety'24 Workshop"},{"id":"http://arxiv.org/abs/2406.08037v1","updated":"2024-06-12T09:39:18Z","published":"2024-06-12T09:39:18Z","title":"Adaptively Bypassing Vision Transformer Blocks for Efficient Visual\n  Tracking","summary":"  Empowered by transformer-based models, visual tracking has advanced\nsignificantly. However, the slow speed of current trackers limits their\napplicability on devices with constrained computational resources. To address\nthis challenge, we introduce ABTrack, an adaptive computation framework that\nadaptively bypassing transformer blocks for efficient visual tracking. The\nrationale behind ABTrack is rooted in the observation that semantic features or\nrelations do not uniformly impact the tracking task across all abstraction\nlevels. Instead, this impact varies based on the characteristics of the target\nand the scene it occupies. Consequently, disregarding insignificant semantic\nfeatures or relations at certain abstraction levels may not significantly\naffect the tracking accuracy. We propose a Bypass Decision Module (BDM) to\ndetermine if a transformer block should be bypassed, which adaptively\nsimplifies the architecture of ViTs and thus speeds up the inference process.\nTo counteract the time cost incurred by the BDMs and further enhance the\nefficiency of ViTs, we innovatively adapt a pruning technique to reduce the\ndimension of the latent representation of tokens in each transformer block.\nExtensive experiments on multiple tracking benchmarks validate the\neffectiveness and generality of the proposed method and show that it achieves\nstate-of-the-art performance. Code is released at:\n\\href{https://github.com/1HykhqV3rU/ABTrack}\n","authors":["Xiangyang Yang","Dan Zeng","Xucheng Wang","You Wu","Hengzhou Ye","Shuiwang Li"],"pdf_url":"https://arxiv.org/pdf/2406.08037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08035v1","updated":"2024-06-12T09:36:52Z","published":"2024-06-12T09:36:52Z","title":"LVBench: An Extreme Long Video Understanding Benchmark","summary":"  Recent progress in multimodal large language models has markedly enhanced the\nunderstanding of short videos (typically under one minute), and several\nevaluation datasets have emerged accordingly. However, these advancements fall\nshort of meeting the demands of real-world applications such as embodied\nintelligence for long-term decision-making, in-depth movie reviews and\ndiscussions, and live sports commentary, all of which require comprehension of\nlong videos spanning several hours. To address this gap, we introduce LVBench,\na benchmark specifically designed for long video understanding. Our dataset\ncomprises publicly sourced videos and encompasses a diverse set of tasks aimed\nat long video comprehension and information extraction. LVBench is designed to\nchallenge multimodal models to demonstrate long-term memory and extended\ncomprehension capabilities. Our extensive evaluations reveal that current\nmultimodal models still underperform on these demanding long video\nunderstanding tasks. Through LVBench, we aim to spur the development of more\nadvanced models capable of tackling the complexities of long video\ncomprehension. Our data and code are publicly available at:\nhttps://lvbench.github.io.\n","authors":["Weihan Wang","Zehai He","Wenyi Hong","Yean Cheng","Xiaohan Zhang","Ji Qi","Shiyu Huang","Bin Xu","Yuxiao Dong","Ming Ding","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2406.08035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07471v2","updated":"2024-06-12T09:36:19Z","published":"2024-06-11T17:18:11Z","title":"OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow\n  Understanding","summary":"  Surgical scene perception via videos are critical for advancing robotic\nsurgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology.\nHowever, the scarcity of diverse and richly annotated video datasets has\nhindered the development of intelligent systems for surgical workflow analysis.\nExisting datasets for surgical workflow analysis, which typically face\nchallenges such as small scale, a lack of diversity in surgery and phase\ncategories, and the absence of time-localized annotations, limit the\nrequirements for action understanding and model generalization validation in\ncomplex and diverse real-world surgical scenarios. To address this gap, we\nintroduce OphNet, a large-scale, expert-annotated video benchmark for\nophthalmic surgical workflow understanding. OphNet features: 1) A diverse\ncollection of 2,278 surgical videos spanning 66 types of cataract, glaucoma,\nand corneal surgeries, with detailed annotations for 102 unique surgical phases\nand 150 granular operations; 2) It offers sequential and hierarchical\nannotations for each surgery, phase, and operation, enabling comprehensive\nunderstanding and improved interpretability; 3) Moreover, OphNet provides\ntime-localized annotations, facilitating temporal localization and prediction\ntasks within surgical workflows. With approximately 205 hours of surgical\nvideos, OphNet is about 20 times larger than the largest existing surgical\nworkflow analysis benchmark. Our dataset and code have been made available at:\n\\url{https://github.com/minghu0830/OphNet-benchmark}.\n","authors":["Ming Hu","Peng Xia","Lin Wang","Siyuan Yan","Feilong Tang","Zhongxing Xu","Yimin Luo","Kaimin Song","Jurgen Leitner","Xuelian Cheng","Jun Cheng","Chi Liu","Kaijing Zhou","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2406.07471v2.pdf","comment":"Version 1. arXiv admin note: text overlap with arXiv:2210.11566 by\n  other authors"},{"id":"http://arxiv.org/abs/2306.08658v2","updated":"2024-06-12T09:33:29Z","published":"2023-06-14T17:53:06Z","title":"Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language\n  Representations","summary":"  Vision-and-language (VL) models with separate encoders for each modality\n(e.g., CLIP) have become the go-to models for zero-shot image classification\nand image-text retrieval. They are, however, mostly evaluated in English as\nmultilingual benchmarks are limited in availability. We introduce\nBabel-ImageNet, a massively multilingual benchmark that offers (partial)\ntranslations of ImageNet labels to 100 languages, built without machine\ntranslation or manual annotation. We instead automatically obtain reliable\ntranslations by linking them -- via shared WordNet synsets -- to BabelNet, a\nmassively multilingual lexico-semantic network. We evaluate 11 public\nmultilingual CLIP models on zero-shot image classification (ZS-IC) on our\nbenchmark, demonstrating a significant gap between English ImageNet performance\nand that of high-resource languages (e.g., German or Chinese), and an even\nbigger gap for low-resource languages (e.g., Sinhala or Lao). Crucially, we\nshow that the models' ZS-IC performance highly correlates with their\nperformance in image-text retrieval, validating the use of Babel-ImageNet to\nevaluate multilingual models for the vast majority of languages without gold\nimage-text data. Finally, we show that the performance of multilingual CLIP can\nbe drastically improved for low-resource languages with parameter-efficient\nlanguage-specific training. We make our code and data publicly available:\n\\url{https://github.com/gregor-ge/Babel-ImageNet}\n","authors":["Gregor Geigle","Radu Timofte","Goran Glavaš"],"pdf_url":"https://arxiv.org/pdf/2306.08658v2.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.08031v1","updated":"2024-06-12T09:31:52Z","published":"2024-06-12T09:31:52Z","title":"Deep Learning for Slum Mapping in Remote Sensing Images: A Meta-analysis\n  and Review","summary":"  The major Sustainable Development Goals (SDG) 2030, set by the United Nations\nDevelopment Program (UNDP), include sustainable cities and communities, no\npoverty, and reduced inequalities. However, millions of people live in slums or\ninformal settlements with poor living conditions in many major cities around\nthe world, especially in less developed countries. To emancipate these\nsettlements and their inhabitants through government intervention, accurate\ndata about slum location and extent is required. While ground survey data is\nthe most reliable, such surveys are costly and time-consuming. An alternative\nis remotely sensed data obtained from very high-resolution (VHR) imagery. With\nthe advancement of new technology, remote sensing based mapping of slums has\nemerged as a prominent research area. The parallel rise of Artificial\nIntelligence, especially Deep Learning has added a new dimension to this field\nas it allows automated analysis of satellite imagery to identify complex\nspatial patterns associated with slums. This article offers a detailed review\nand meta-analysis of research on slum mapping using remote sensing imagery from\n2014 to 2024, with a special focus on deep learning approaches. Our analysis\nreveals a trend towards increasingly complex neural network architectures, with\nadvancements in data preprocessing and model training techniques significantly\nenhancing slum identification accuracy. We have attempted to identify key\nmethodologies that are effective across diverse geographic contexts. While\nacknowledging the transformative impact Convolutional Neural Networks (CNNs) in\nslum detection, our review underscores the absence of a universally optimal\nmodel, suggesting the need for context-specific adaptations. We also identify\nprevailing challenges in this field, such as data limitations and a lack of\nmodel explainability and suggest potential strategies for overcoming these.\n","authors":["Anjali Raj","Adway Mitra","Manjira Sinha"],"pdf_url":"https://arxiv.org/pdf/2406.08031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.01290v3","updated":"2024-06-12T09:22:48Z","published":"2022-06-02T20:23:33Z","title":"Points2NeRF: Generating Neural Radiance Fields from 3D point cloud","summary":"  Contemporary registration devices for 3D visual information, such as LIDARs\nand various depth cameras, capture data as 3D point clouds. In turn, such\nclouds are challenging to be processed due to their size and complexity.\nExisting methods address this problem by fitting a mesh to the point cloud and\nrendering it instead. This approach, however, leads to the reduced fidelity of\nthe resulting visualization and misses color information of the objects crucial\nin computer graphics applications. In this work, we propose to mitigate this\nchallenge by representing 3D objects as Neural Radiance Fields (NeRFs). We\nleverage a hypernetwork paradigm and train the model to take a 3D point cloud\nwith the associated color values and return a NeRF network's weights that\nreconstruct 3D objects from input 2D images. Our method provides efficient 3D\nobject representation and offers several advantages over the existing\napproaches, including the ability to condition NeRFs and improved\ngeneralization beyond objects seen in training. The latter we also confirmed in\nthe results of our empirical evaluation.\n","authors":["Dominik Zimny","Joanna Waczyńska","Tomasz Trzciński","Przemysław Spurek"],"pdf_url":"https://arxiv.org/pdf/2206.01290v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2003.08934 by other authors"},{"id":"http://arxiv.org/abs/2406.08024v1","updated":"2024-06-12T09:22:45Z","published":"2024-06-12T09:22:45Z","title":"Fewer Tokens and Fewer Videos: Extending Video Understanding Abilities\n  in Large Vision-Language Models","summary":"  Amidst the advancements in image-based Large Vision-Language Models\n(image-LVLM), the transition to video-based models (video-LVLM) is hindered by\nthe limited availability of quality video data. This paper addresses the\nchallenge by leveraging the visual commonalities between images and videos to\nefficiently evolve image-LVLMs into video-LVLMs. We present a cost-effective\nvideo-LVLM that enhances model architecture, introduces innovative training\nstrategies, and identifies the most effective types of video instruction data.\nOur innovative weighted token sampler significantly compresses the visual token\nnumbers of each video frame, effectively cutting computational expenses. We\nalso find that judiciously using just 10% of the video data, compared to prior\nvideo-LVLMs, yields impressive results during various training phases.\nMoreover, we delve into the influence of video instruction data in\nlimited-resource settings, highlighting the significance of incorporating video\ntraining data that emphasizes temporal understanding to enhance model\nperformance. The resulting Fewer Tokens and Fewer Videos LVLM (FTFV-LVLM)\nexhibits exceptional performance across video and image benchmarks, validating\nour model's design and training approaches.\n","authors":["Shimin Chen","Yitian Yuan","Shaoxiang Chen","Zequn Jie","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2406.08024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08020v1","updated":"2024-06-12T09:21:28Z","published":"2024-06-12T09:21:28Z","title":"Generalizable Disaster Damage Assessment via Change Detection with\n  Vision Foundation Model","summary":"  The increasing frequency and intensity of natural disasters demand more\nsophisticated approaches for rapid and precise damage assessment. To tackle\nthis issue, researchers have developed various methods on disaster benchmark\ndatasets from satellite imagery to aid in detecting disaster damage. However,\nthe diverse nature of geographical landscapes and disasters makes it\nchallenging to apply existing methods to regions unseen during training. We\npresent DAVI (Disaster Assessment with VIsion foundation model), which\novercomes domain disparities and detects structural damage (e.g., building)\nwithout requiring ground-truth labels of the target region. DAVI integrates\ntask-specific knowledge from a model trained on source regions with an image\nsegmentation foundation model to generate pseudo labels of possible damage in\nthe target region. It then employs a two-stage refinement process, targeting\nboth the pixel and overall image, to more accurately pinpoint changes in\ndisaster-struck areas based on before-and-after images. Comprehensive\nevaluations demonstrate that DAVI achieves exceptional performance across\ndiverse terrains (e.g., USA and Mexico) and disaster types (e.g., wildfires,\nhurricanes, and earthquakes). This confirms its robustness in assessing\ndisaster impact without dependence on ground-truth labels.\n","authors":["Kyeongjin Ahn","Sungwon Han","Sungwon Park","Jihee Kim","Sangyoon Park","Meeyoung Cha"],"pdf_url":"https://arxiv.org/pdf/2406.08020v1.pdf","comment":"9 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.13729v5","updated":"2024-06-12T09:06:02Z","published":"2023-12-21T10:52:59Z","title":"Gaussian Splatting with NeRF-based Color and Opacity","summary":"  Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of\nneural networks to capture the intricacies of 3D objects. By encoding the shape\nand color information within neural network weights, NeRFs excel at producing\nstrikingly sharp novel views of 3D objects. Recently, numerous generalizations\nof NeRFs utilizing generative models have emerged, expanding its versatility.\nIn contrast, Gaussian Splatting (GS) offers a similar render quality with\nfaster training and inference as it does not need neural networks to work. It\nencodes information about the 3D objects in the set of Gaussian distributions\nthat can be rendered in 3D similarly to classical meshes. Unfortunately, GS are\ndifficult to condition since they usually require circa hundred thousand\nGaussian components. To mitigate the caveats of both models, we propose a\nhybrid model Viewing Direction Gaussian Splatting (VDGS) that uses GS\nrepresentation of the 3D object's shape and NeRF-based encoding of color and\nopacity. Our model uses Gaussian distributions with trainable positions (i.e.\nmeans of Gaussian), shape (i.e. covariance of Gaussian), color and opacity, and\na neural network that takes Gaussian parameters and viewing direction to\nproduce changes in the said color and opacity. As a result, our model better\ndescribes shadows, light reflections, and the transparency of 3D objects\nwithout adding additional texture and light components.\n","authors":["Dawid Malarz","Weronika Smolak","Jacek Tabor","Sławomir Tadeja","Przemysław Spurek"],"pdf_url":"https://arxiv.org/pdf/2312.13729v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.04063v3","updated":"2024-06-12T09:02:47Z","published":"2023-05-06T14:22:33Z","title":"Exploring One-shot Semi-supervised Federated Learning with A Pre-trained\n  Diffusion Model","summary":"  Recently, semi-supervised federated learning (semi-FL) has been proposed to\nhandle the commonly seen real-world scenarios with labeled data on the server\nand unlabeled data on the clients. However, existing methods face several\nchallenges such as communication costs, data heterogeneity, and training\npressure on client devices. To address these challenges, we introduce the\npowerful diffusion models (DM) into semi-FL and propose FedDISC, a Federated\nDiffusion-Inspired Semi-supervised Co-training method. Specifically, we first\nextract prototypes of the labeled server data and use these prototypes to\npredict pseudo-labels of the client data. For each category, we compute the\ncluster centroids and domain-specific representations to signify the semantic\nand stylistic information of their distributions. After adding noise, these\nrepresentations are sent back to the server, which uses the pre-trained DM to\ngenerate synthetic datasets complying with the client distributions and train a\nglobal model on it. With the assistance of vast knowledge within DM, the\nsynthetic datasets have comparable quality and diversity to the client images,\nsubsequently enabling the training of global models that achieve performance\nequivalent to or even surpassing the ceiling of supervised centralized\ntraining. FedDISC works within one communication round, does not require any\nlocal training, and involves very minimal information uploading, greatly\nenhancing its practicality. Extensive experiments on three large-scale datasets\ndemonstrate that FedDISC effectively addresses the semi-FL problem on non-IID\nclients and outperforms the compared SOTA methods. Sufficient visualization\nexperiments also illustrate that the synthetic dataset generated by FedDISC\nexhibits comparable diversity and quality to the original client dataset, with\na neglectable possibility of leaking privacy-sensitive information of the\nclients.\n","authors":["Mingzhao Yang","Shangchao Su","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2305.04063v3.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2406.08009v1","updated":"2024-06-12T08:59:33Z","published":"2024-06-12T08:59:33Z","title":"OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with\n  Fine-Grained Understanding","summary":"  In recent years, there has been a surge of interest in open-vocabulary 3D\nscene reconstruction facilitated by visual language models (VLMs), which\nshowcase remarkable capabilities in open-set retrieval. However, existing\nmethods face some limitations: they either focus on learning point-wise\nfeatures, resulting in blurry semantic understanding, or solely tackle\nobject-level reconstruction, thereby overlooking the intricate details of the\nobject's interior. To address these challenges, we introduce OpenObj, an\ninnovative approach to build open-vocabulary object-level Neural Radiance\nFields (NeRF) with fine-grained understanding. In essence, OpenObj establishes\na robust framework for efficient and watertight scene modeling and\ncomprehension at the object-level. Moreover, we incorporate part-level features\ninto the neural fields, enabling a nuanced representation of object interiors.\nThis approach captures object-level instances while maintaining a fine-grained\nunderstanding. The results on multiple datasets demonstrate that OpenObj\nachieves superior performance in zero-shot semantic segmentation and retrieval\ntasks. Additionally, OpenObj supports real-world robotics tasks at multiple\nscales, including global movement and local manipulation.\n","authors":["Yinan Deng","Jiahui Wang","Jingyu Zhao","Jianyu Dou","Yi Yang","Yufeng Yue"],"pdf_url":"https://arxiv.org/pdf/2406.08009v1.pdf","comment":"8 pages, 7figures. Project Url: https://openobj.github.io/"},{"id":"http://arxiv.org/abs/2406.04998v2","updated":"2024-06-12T08:49:16Z","published":"2024-06-07T15:09:25Z","title":"ADBA:Approximation Decision Boundary Approach for Black-Box Adversarial\n  Attacks","summary":"  Many machine learning models are susceptible to adversarial attacks, with\ndecision-based black-box attacks representing the most critical threat in\nreal-world applications. These attacks are extremely stealthy, generating\nadversarial examples using hard labels obtained from the target machine\nlearning model. This is typically realized by optimizing perturbation\ndirections, guided by decision boundaries identified through query-intensive\nexact search, significantly limiting the attack success rate. This paper\nintroduces a novel approach using the Approximation Decision Boundary (ADB) to\nefficiently and accurately compare perturbation directions without precisely\ndetermining decision boundaries. The effectiveness of our ADB approach (ADBA)\nhinges on promptly identifying suitable ADB, ensuring reliable differentiation\nof all perturbation directions. For this purpose, we analyze the probability\ndistribution of decision boundaries, confirming that using the distribution's\nmedian value as ADB can effectively distinguish different perturbation\ndirections, giving rise to the development of the ADBA-md algorithm. ADBA-md\nonly requires four queries on average to differentiate any pair of perturbation\ndirections, which is highly query-efficient. Extensive experiments on six\nwell-known image classifiers clearly demonstrate the superiority of ADBA and\nADBA-md over multiple state-of-the-art black-box attacks. The source code is\navailable at https://github.com/BUPTAIOC/ADBA.\n","authors":["Feiyang Wang","Xingquan Zuo","Hai Huang","Gang Chen"],"pdf_url":"https://arxiv.org/pdf/2406.04998v2.pdf","comment":"10 pages, 5 figures, conference"},{"id":"http://arxiv.org/abs/2406.08001v1","updated":"2024-06-12T08:47:44Z","published":"2024-06-12T08:47:44Z","title":"Asymptotic Unbiased Sample Sampling to Speed Up Sharpness-Aware\n  Minimization","summary":"  Sharpness-Aware Minimization (SAM) has emerged as a promising approach for\neffectively reducing the generalization error. However, SAM incurs twice the\ncomputational cost compared to base optimizer (e.g., SGD). We propose\nAsymptotic Unbiased Sampling with respect to iterations to accelerate SAM\n(AUSAM), which maintains the model's generalization capacity while\nsignificantly enhancing computational efficiency. Concretely, we\nprobabilistically sample a subset of data points beneficial for SAM\noptimization based on a theoretically guaranteed criterion, i.e., the Gradient\nNorm of each Sample (GNS). We further approximate the GNS by the difference in\nloss values before and after perturbation in SAM. As a plug-and-play,\narchitecture-agnostic method, our approach consistently accelerates SAM across\na range of tasks and networks, i.e., classification, human pose estimation and\nnetwork quantization. On CIFAR10/100 and Tiny-ImageNet, AUSAM achieves results\ncomparable to SAM while providing a speedup of over 70%. Compared to recent\ndynamic data pruning methods, AUSAM is better suited for SAM and excels in\nmaintaining performance. Additionally, AUSAM accelerates optimization in human\npose estimation and model quantization without sacrificing performance,\ndemonstrating its broad practicality.\n","authors":["Jiaxin Deng","Junbiao Pang","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04224v2","updated":"2024-06-12T08:27:40Z","published":"2023-10-27T14:57:10Z","title":"MELEP: A Novel Predictive Measure of Transferability in Multi-Label ECG\n  Diagnosis","summary":"  In practical electrocardiography (ECG) interpretation, the scarcity of\nwell-annotated data is a common challenge. Transfer learning techniques are\nvaluable in such situations, yet the assessment of transferability has received\nlimited attention. To tackle this issue, we introduce MELEP, which stands for\nMuti-label Expected Log of Empirical Predictions, a measure designed to\nestimate the effectiveness of knowledge transfer from a pre-trained model to a\ndownstream multi-label ECG diagnosis task. MELEP is generic, working with new\ntarget data with different label sets, and computationally efficient, requiring\nonly a single forward pass through the pre-trained model. To the best of our\nknowledge, MELEP is the first transferability metric specifically designed for\nmulti-label ECG classification problems. Our experiments show that MELEP can\npredict the performance of pre-trained convolutional and recurrent deep neural\nnetworks, on small and imbalanced ECG data. Specifically, we observed strong\ncorrelation coefficients (with absolute values exceeding 0.6 in most cases)\nbetween MELEP and the actual average F1 scores of the fine-tuned models. Our\nwork highlights the potential of MELEP to expedite the selection of suitable\npre-trained models for ECG diagnosis tasks, saving time and effort that would\notherwise be spent on fine-tuning these models.\n","authors":["Cuong V. Nguyen","Hieu Minh Duong","Cuong D. Do"],"pdf_url":"https://arxiv.org/pdf/2311.04224v2.pdf","comment":"Accepted to the Journal of Healthcare Informatics Research"},{"id":"http://arxiv.org/abs/2311.18537v2","updated":"2024-06-12T08:20:59Z","published":"2023-11-30T13:20:09Z","title":"A Simple Video Segmenter by Tracking Objects Along Axial Trajectories","summary":"  Video segmentation requires consistently segmenting and tracking objects over\ntime. Due to the quadratic dependency on input size, directly applying\nself-attention to video segmentation with high-resolution input features poses\nsignificant challenges, often leading to insufficient GPU memory capacity.\nConsequently, modern video segmenters either extend an image segmenter without\nincorporating any temporal attention or resort to window space-time attention\nin a naive manner. In this work, we present Axial-VS, a general and simple\nframework that enhances video segmenters by tracking objects along axial\ntrajectories. The framework tackles video segmentation through two sub-tasks:\nshort-term within-clip segmentation and long-term cross-clip tracking. In the\nfirst step, Axial-VS augments an off-the-shelf clip-level video segmenter with\nthe proposed axial-trajectory attention, sequentially tracking objects along\nthe height- and width-trajectories within a clip, thereby enhancing temporal\nconsistency by capturing motion trajectories. The axial decomposition\nsignificantly reduces the computational complexity for dense features, and\noutperforms the window space-time attention in segmentation quality. In the\nsecond step, we further employ axial-trajectory attention to the object queries\nin clip-level segmenters, which are learned to encode object information,\nthereby aiding object tracking across different clips and achieving consistent\nsegmentation throughout the video. Without bells and whistles, Axial-VS\nshowcases state-of-the-art results on video segmentation benchmarks,\nemphasizing its effectiveness in addressing the limitations of modern\nclip-level video segmenters. Code and models are available at\nhttps://github.com/TACJu/Axial-VS.\n","authors":["Ju He","Qihang Yu","Inkyu Shin","Xueqing Deng","Alan Yuille","Xiaohui Shen","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2311.18537v2.pdf","comment":"The paper and model names have been updated to better reflect the\n  methodological contributions"},{"id":"http://arxiv.org/abs/2406.07986v1","updated":"2024-06-12T08:17:06Z","published":"2024-06-12T08:17:06Z","title":"SimSAM: Simple Siamese Representations Based Semantic Affinity Matrix\n  for Unsupervised Image Segmentation","summary":"  Recent developments in self-supervised learning (SSL) have made it possible\nto learn data representations without the need for annotations. Inspired by the\nnon-contrastive SSL approach (SimSiam), we introduce a novel framework SIMSAM\nto compute the Semantic Affinity Matrix, which is significant for unsupervised\nimage segmentation. Given an image, SIMSAM first extracts features using\npre-trained DINO-ViT, then projects the features to predict the correlations of\ndense features in a non-contrastive way. We show applications of the Semantic\nAffinity Matrix in object segmentation and semantic segmentation tasks. Our\ncode is available at https://github.com/chandagrover/SimSAM.\n","authors":["Chanda Grover Kamra","Indra Deep Mastan","Nitin Kumar","Debayan Gupta"],"pdf_url":"https://arxiv.org/pdf/2406.07986v1.pdf","comment":"6 Pages-Main Paper , 6 figures, 6Tables (Main Paper), ICIP 2024, 8\n  Pages: Supplementary"},{"id":"http://arxiv.org/abs/2405.05095v3","updated":"2024-06-12T08:16:32Z","published":"2024-05-08T14:44:34Z","title":"Approximation properties relative to continuous scale space for hybrid\n  discretizations of Gaussian derivative operators","summary":"  This paper presents an analysis of properties of two hybrid discretization\nmethods for Gaussian derivatives, based on convolutions with either the\nnormalized sampled Gaussian kernel or the integrated Gaussian kernel followed\nby central differences. The motivation for studying these discretization\nmethods is that in situations when multiple spatial derivatives of different\norder are needed at the same scale level, they can be computed significantly\nmore efficiently compared to more direct derivative approximations based on\nexplicit convolutions with either sampled Gaussian kernels or integrated\nGaussian kernels.\n  While these computational benefits do also hold for the genuinely discrete\napproach for computing discrete analogues of Gaussian derivatives, based on\nconvolution with the discrete analogue of the Gaussian kernel followed by\ncentral differences, the underlying mathematical primitives for the discrete\nanalogue of the Gaussian kernel, in terms of modified Bessel functions of\ninteger order, may not be available in certain frameworks for image processing,\nsuch as when performing deep learning based on scale-parameterized filters in\nterms of Gaussian derivatives, with learning of the scale levels.\n  In this paper, we present a characterization of the properties of these\nhybrid discretization methods, in terms of quantitative performance measures\nconcerning the amount of spatial smoothing that they imply, as well as the\nrelative consistency of scale estimates obtained from scale-invariant feature\ndetectors with automatic scale selection, with an emphasis on the behaviour for\nvery small values of the scale parameter, which may differ significantly from\ncorresponding results obtained from the fully continuous scale-space theory, as\nwell as between different types of discretization methods.\n","authors":["Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2405.05095v3.pdf","comment":"13 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:2311.11317"},{"id":"http://arxiv.org/abs/2303.03645v2","updated":"2024-06-12T08:05:18Z","published":"2023-03-07T04:26:44Z","title":"Filter Pruning based on Information Capacity and Independence","summary":"  Filter pruning has gained widespread adoption for the purpose of compressing\nand speeding up convolutional neural networks (CNNs). However, existing\napproaches are still far from practical applications due to biased filter\nselection and heavy computation cost. This paper introduces a new filter\npruning method that selects filters in an interpretable, multi-perspective, and\nlightweight manner. Specifically, we evaluate the contributions of filters from\nboth individual and overall perspectives. For the amount of information\ncontained in each filter, a new metric called information capacity is proposed.\nInspired by the information theory, we utilize the interpretable entropy to\nmeasure the information capacity, and develop a feature-guided approximation\nprocess. For correlations among filters, another metric called information\nindependence is designed. Since the aforementioned metrics are evaluated in a\nsimple but effective way, we can identify and prune the least important filters\nwith less computation cost. We conduct comprehensive experiments on benchmark\ndatasets employing various widely-used CNN architectures to evaluate the\nperformance of our method. For instance, on ILSVRC-2012, our method outperforms\nstate-of-the-art methods by reducing FLOPs by 77.4% and parameters by 69.3% for\nResNet-50 with only a minor decrease in accuracy of 2.64%.\n","authors":["Xiaolong Tang","Shuo Ye","Yufeng Shi","Tianheng Hu","Qinmu Peng","Xinge You"],"pdf_url":"https://arxiv.org/pdf/2303.03645v2.pdf","comment":"Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (IEEE TNNLS).The code will be available at https://github.com/txl-hub/ICI"},{"id":"http://arxiv.org/abs/2406.07966v1","updated":"2024-06-12T07:44:22Z","published":"2024-06-12T07:44:22Z","title":"Real-world Image Dehazing with Coherence-based Label Generator and\n  Cooperative Unfolding Network","summary":"  Real-world Image Dehazing (RID) aims to alleviate haze-induced degradation in\nreal-world settings. This task remains challenging due to the complexities in\naccurately modeling real haze distributions and the scarcity of paired\nreal-world data. To address these challenges, we first introduce a cooperative\nunfolding network that jointly models atmospheric scattering and image scenes,\neffectively integrating physical knowledge into deep networks to restore\nhaze-contaminated details. Additionally, we propose the first RID-oriented\niterative mean-teacher framework, termed the Coherence-based Label Generator,\nto generate high-quality pseudo labels for network training. Specifically, we\nprovide an optimal label pool to store the best pseudo-labels during network\ntraining, leveraging both global and local coherence to select high-quality\ncandidates and assign weights to prioritize haze-free regions. We verify the\neffectiveness of our method, with experiments demonstrating that it achieves\nstate-of-the-art performance on RID tasks. Code will be available at\n\\url{https://github.com/cnyvfang/CORUN-Colabator}.\n","authors":["Chengyu Fang","Chunming He","Fengyang Xiao","Yulun Zhang","Longxiang Tang","Yuelin Zhang","Kai Li","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2406.07966v1.pdf","comment":"10 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.07961v1","updated":"2024-06-12T07:41:00Z","published":"2024-06-12T07:41:00Z","title":"Accurate Explanation Model for Image Classifiers using Class Association\n  Embedding","summary":"  Image classification is a primary task in data analysis where explainable\nmodels are crucially demanded in various applications. Although amounts of\nmethods have been proposed to obtain explainable knowledge from the black-box\nclassifiers, these approaches lack the efficiency of extracting global\nknowledge regarding the classification task, thus is vulnerable to local traps\nand often leads to poor accuracy. In this study, we propose a generative\nexplanation model that combines the advantages of global and local knowledge\nfor explaining image classifiers. We develop a representation learning method\ncalled class association embedding (CAE), which encodes each sample into a pair\nof separated class-associated and individual codes. Recombining the individual\ncode of a given sample with altered class-associated code leads to a synthetic\nreal-looking sample with preserved individual characters but modified\nclass-associated features and possibly flipped class assignments. A\nbuilding-block coherency feature extraction algorithm is proposed that\nefficiently separates class-associated features from individual ones. The\nextracted feature space forms a low-dimensional manifold that visualizes the\nclassification decision patterns. Explanation on each individual sample can be\nthen achieved in a counter-factual generation manner which continuously\nmodifies the sample in one direction, by shifting its class-associated code\nalong a guided path, until its classification outcome is changed. We compare\nour method with state-of-the-art ones on explaining image classification tasks\nin the form of saliency maps, demonstrating that our method achieves higher\naccuracies. The code is available at https://github.com/xrt11/XAI-CODE.\n","authors":["Ruitao Xie","Jingbang Chen","Limai Jiang","Rui Xiao","Yi Pan","Yunpeng Cai"],"pdf_url":"https://arxiv.org/pdf/2406.07961v1.pdf","comment":"40th IEEE International Conference on Data Engineering"},{"id":"http://arxiv.org/abs/2401.02651v3","updated":"2024-06-12T07:22:24Z","published":"2024-01-05T05:58:22Z","title":"Benchmarking PathCLIP for Pathology Image Analysis","summary":"  Accurate image classification and retrieval are of importance for clinical\ndiagnosis and treatment decision-making. The recent contrastive language-image\npretraining (CLIP) model has shown remarkable proficiency in understanding\nnatural images. Drawing inspiration from CLIP, PathCLIP is specifically\ndesigned for pathology image analysis, utilizing over 200,000 image and text\npairs in training. While the performance the PathCLIP is impressive, its\nrobustness under a wide range of image corruptions remains unknown. Therefore,\nwe conduct an extensive evaluation to analyze the performance of PathCLIP on\nvarious corrupted images from the datasets of Osteosarcoma and WSSS4LUAD. In\nour experiments, we introduce seven corruption types including brightness,\ncontrast, Gaussian blur, resolution, saturation, hue, and markup at four\nseverity levels. Through experiments, we find that PathCLIP is relatively\nrobustness to image corruptions and surpasses OpenAI-CLIP and PLIP in zero-shot\nclassification. Among the seven corruptions, blur and resolution can cause\nserver performance degradation of the PathCLIP. This indicates that ensuring\nthe quality of images is crucial before conducting a clinical test.\nAdditionally, we assess the robustness of PathCLIP in the task of image-image\nretrieval, revealing that PathCLIP performs less effectively than PLIP on\nOsteosarcoma but performs better on WSSS4LUAD under diverse corruptions.\nOverall, PathCLIP presents impressive zero-shot classification and retrieval\nperformance for pathology images, but appropriate care needs to be taken when\nusing it. We hope this study provides a qualitative impression of PathCLIP and\nhelps understand its differences from other CLIP models.\n","authors":["Sunyi Zheng","Xiaonan Cui","Yuxuan Sun","Jingxiong Li","Honglin Li","Yunlong Zhang","Pingyi Chen","Xueping Jing","Zhaoxiang Ye","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.02651v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07952v1","updated":"2024-06-12T07:22:05Z","published":"2024-06-12T07:22:05Z","title":"Spatial-Frequency Dual Progressive Attention Network For Medical Image\n  Segmentation","summary":"  In medical images, various types of lesions often manifest significant\ndifferences in their shape and texture. Accurate medical image segmentation\ndemands deep learning models with robust capabilities in multi-scale and\nboundary feature learning. However, previous networks still have limitations in\naddressing the above issues. Firstly, previous networks simultaneously fuse\nmulti-level features or employ deep supervision to enhance multi-scale\nlearning. However, this may lead to feature redundancy and excessive\ncomputational overhead, which is not conducive to network training and clinical\ndeployment. Secondly, the majority of medical image segmentation networks\nexclusively learn features in the spatial domain, disregarding the abundant\nglobal information in the frequency domain. This results in a bias towards\nlow-frequency components, neglecting crucial high-frequency information. To\naddress these problems, we introduce SF-UNet, a spatial-frequency dual-domain\nattention network. It comprises two main components: the Multi-scale\nProgressive Channel Attention (MPCA) block, which progressively extract\nmulti-scale features across adjacent encoder layers, and the lightweight\nFrequency-Spatial Attention (FSA) block, with only 0.05M parameters, enabling\nconcurrent learning of texture and boundary features from both spatial and\nfrequency domains. We validate the effectiveness of the proposed SF-UNet on\nthree public datasets. Experimental results show that compared to previous\nstate-of-the-art (SOTA) medical image segmentation networks, SF-UNet achieves\nthe best performance, and achieves up to 9.4\\% and 10.78\\% improvement in DSC\nand IOU. Codes will be released at https://github.com/nkicsl/SF-UNet.\n","authors":["Zhenhuan Zhou","Along He","Yanlin Wu","Rui Yao","Xueshuo Xie","Tao Li"],"pdf_url":"https://arxiv.org/pdf/2406.07952v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2406.07951v1","updated":"2024-06-12T07:20:46Z","published":"2024-06-12T07:20:46Z","title":"DemosaicFormer: Coarse-to-Fine Demosaicing Network for HybridEVS Camera","summary":"  Hybrid Event-Based Vision Sensor (HybridEVS) is a novel sensor integrating\ntraditional frame-based and event-based sensors, offering substantial benefits\nfor applications requiring low-light, high dynamic range, and low-latency\nenvironments, such as smartphones and wearable devices. Despite its potential,\nthe lack of Image signal processing (ISP) pipeline specifically designed for\nHybridEVS poses a significant challenge. To address this challenge, in this\nstudy, we propose a coarse-to-fine framework named DemosaicFormer which\ncomprises coarse demosaicing and pixel correction. Coarse demosaicing network\nis designed to produce a preliminary high-quality estimate of the RGB image\nfrom the HybridEVS raw data while the pixel correction network enhances the\nperformance of image restoration and mitigates the impact of defective pixels.\nOur key innovation is the design of a Multi-Scale Gating Module (MSGM) applying\nthe integration of cross-scale features, which allows feature information to\nflow between different scales. Additionally, the adoption of progressive\ntraining and data augmentation strategies further improves model's robustness\nand effectiveness. Experimental results show superior performance against the\nexisting methods both qualitatively and visually, and our DemosaicFormer\nachieves the best performance in terms of all the evaluation metrics in the\nMIPI 2024 challenge on Demosaic for Hybridevs Camera. The code is available at\nhttps://github.com/QUEAHREN/DemosaicFormer.\n","authors":["Senyan Xu","Zhijing Sun","Jiaying Zhu","Yurui Zhu","Xueyang Fu","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2406.07951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16567v2","updated":"2024-06-12T07:16:21Z","published":"2023-11-28T07:14:41Z","title":"MobileDiffusion: Instant Text-to-Image Generation on Mobile Devices","summary":"  The deployment of large-scale text-to-image diffusion models on mobile\ndevices is impeded by their substantial model size and slow inference speed. In\nthis paper, we propose \\textbf{MobileDiffusion}, a highly efficient\ntext-to-image diffusion model obtained through extensive optimizations in both\narchitecture and sampling techniques. We conduct a comprehensive examination of\nmodel architecture design to reduce redundancy, enhance computational\nefficiency, and minimize model's parameter count, while preserving image\ngeneration quality. Additionally, we employ distillation and diffusion-GAN\nfinetuning techniques on MobileDiffusion to achieve 8-step and 1-step inference\nrespectively. Empirical studies, conducted both quantitatively and\nqualitatively, demonstrate the effectiveness of our proposed techniques.\nMobileDiffusion achieves a remarkable \\textbf{sub-second} inference speed for\ngenerating a $512\\times512$ image on mobile devices, establishing a new state\nof the art.\n","authors":["Yang Zhao","Yanwu Xu","Zhisheng Xiao","Haolin Jia","Tingbo Hou"],"pdf_url":"https://arxiv.org/pdf/2311.16567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07949v1","updated":"2024-06-12T07:13:31Z","published":"2024-06-12T07:13:31Z","title":"Multi-Teacher Multi-Objective Meta-Learning for Zero-Shot Hyperspectral\n  Band Selection","summary":"  Band selection plays a crucial role in hyperspectral image classification by\nremoving redundant and noisy bands and retaining discriminative ones. However,\nmost existing deep learning-based methods are aimed at dealing with a specific\nband selection dataset, and need to retrain parameters for new datasets, which\nsignificantly limits their generalizability.To address this issue, a novel\nmulti-teacher multi-objective meta-learning network (M$^3$BS) is proposed for\nzero-shot hyperspectral band selection. In M$^3$BS, a generalizable graph\nconvolution network (GCN) is constructed to generate dataset-agnostic base, and\nextract compatible meta-knowledge from multiple band selection tasks. To\nenhance the ability of meta-knowledge extraction, multiple band selection\nteachers are introduced to provide diverse high-quality experiences.strategy\nFinally, subsequent classification tasks are attached and jointly optimized\nwith multi-teacher band selection tasks through multi-objective meta-learning\nin an end-to-end trainable way. Multi-objective meta-learning guarantees to\ncoordinate diverse optimization objectives automatically and adapt to various\ndatasets simultaneously. Once the optimization is accomplished, the acquired\nmeta-knowledge can be directly transferred to unseen datasets without any\nretraining or fine-tuning. Experimental results demonstrate the effectiveness\nand efficiency of our proposed method on par with state-of-the-art baselines\nfor zero-shot hyperspectral band selection.\n","authors":["Jie Feng","Xiaojian Zhong","Di Li","Weisheng Dong","Ronghua Shang","Licheng Jiao"],"pdf_url":"https://arxiv.org/pdf/2406.07949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07937v1","updated":"2024-06-12T06:59:58Z","published":"2024-06-12T06:59:58Z","title":"IFTD: Image Feature Triangle Descriptor for Loop Detection in Driving\n  Scenes","summary":"  In this work, we propose a fast and robust Image Feature Triangle Descriptor\n(IFTD) based on the STD method, aimed at improving the efficiency and accuracy\nof place recognition in driving scenarios. We extract keypoints from BEV\nprojection image of point cloud and construct these keypoints into triangle\ndescriptors. By matching these feature triangles, we achieved precise place\nrecognition and calculated the 4-DOF pose estimation between two keyframes.\nFurthermore, we employ image similarity inspection to perform the final place\nrecognition. Experimental results on three public datasets demonstrate that our\nIFTD can achieve greater robustness and accuracy than state-of-the-art methods\nwith low computational overhead.\n","authors":["Fengtian Lang","Ruiye Ming","Zikang Yuan","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2406.07937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00137v2","updated":"2024-06-12T06:38:13Z","published":"2023-12-30T04:21:12Z","title":"SSL-OTA: Unveiling Backdoor Threats in Self-Supervised Learning for\n  Object Detection","summary":"  The extensive adoption of Self-supervised learning(SSL) has led to an\nincreased security threat from backdoor attacks. While existing research has\nmainly focused on backdoor attacks in image classification, there has been\nlimited exploration of their implications for object detection. Object\ndetection plays a critical role in security-sensitive applications, such as\nautonomous driving, where backdoor attacks seriously threaten human life and\nproperty. In this work, we propose the first backdoor attack designed for\nobject detection tasks in SSL scenarios, called Object Transform Attack\n(SSL-OTA). SSL-OTA employs a trigger capable of altering predictions of the\ntarget object to the desired category, encompassing two attacks: Naive\nAttack(NA) and Dual-Source Blending Attack (DSBA). NA conducts data poisoning\nduring downstream fine-tuning of the object detector, while DSBA additionally\ninjects backdoors into the pre-trained encoder. We establish appropriate\nmetrics and conduct extensive experiments on benchmark datasets, demonstrating\nthe effectiveness of our proposed attack and its resistance to potential\ndefenses. Notably, both NA and DSBA achieve high attack success rates (ASR) at\nextremely low poisoning rates (0.5%). The results underscore the importance of\nconsidering backdoor threats in SSL-based object detection and contribute a\nnovel perspective to the field.\n","authors":["Qiannan Wang","Changchun Yin","Lu Zhou","Liming Fang"],"pdf_url":"https://arxiv.org/pdf/2401.00137v2.pdf","comment":"10 pages, 4figures"},{"id":"http://arxiv.org/abs/2404.14955v3","updated":"2024-06-12T06:21:59Z","published":"2024-04-23T12:00:20Z","title":"A Comprehensive Survey for Hyperspectral Image Classification: The\n  Evolution from Conventional to Transformers","summary":"  Hyperspectral Image Classification (HSC) is a challenging task due to the\nhigh dimensionality and complex nature of Hyperspectral (HS) data. Traditional\nMachine Learning approaches while effective, face challenges in real-world data\ndue to varying optimal feature sets, subjectivity in human-driven design,\nbiases, and limitations. Traditional approaches encounter the curse of\ndimensionality, struggle with feature selection and extraction, lack spatial\ninformation consideration, exhibit limited robustness to noise, face\nscalability issues, and may not adapt well to complex data distributions. In\nrecent years, DL techniques have emerged as powerful tools for addressing these\nchallenges. This survey provides a comprehensive overview of the current trends\nand future prospects in HSC, focusing on the advancements from DL models to the\nemerging use of Transformers. We review the key concepts, methodologies, and\nstate-of-the-art approaches in DL for HSC. We explore the potential of\nTransformer-based models in HSC, outlining their benefits and challenges. We\nalso delve into emerging trends in HSC, as well as thorough discussions on\nExplainable AI and Interoperability concepts along with Diffusion Models (image\ndenoising, feature extraction, and image fusion). Additionally, we address\nseveral open challenges and research questions pertinent to HSC. Comprehensive\nexperimental results have been undertaken using three HS datasets to verify the\nefficacy of various conventional DL models and Transformers. Finally, we\noutline future research directions and potential applications that can further\nenhance the accuracy and efficiency of HSC. The Source code is available at\n\\url{https://github.com/mahmad00/Conventional-to-Transformer-for-Hyperspectral-Image-Classification-Survey-2024}.\n","authors":["Muhammad Ahmad","Salvatore Distifano","Adil Mehmood Khan","Manuel Mazzara","Chenyu Li","Jing Yao","Hao Li","Jagannath Aryal","Gemine Vivone","Danfeng Hong"],"pdf_url":"https://arxiv.org/pdf/2404.14955v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07895v1","updated":"2024-06-12T06:00:00Z","published":"2024-06-12T06:00:00Z","title":"Emotional Conversation: Empowering Talking Faces with Cohesive\n  Expression, Gaze and Pose Generation","summary":"  Vivid talking face generation holds immense potential applications across\ndiverse multimedia domains, such as film and game production. While existing\nmethods accurately synchronize lip movements with input audio, they typically\nignore crucial alignments between emotion and facial cues, which include\nexpression, gaze, and head pose. These alignments are indispensable for\nsynthesizing realistic videos. To address these issues, we propose a two-stage\naudio-driven talking face generation framework that employs 3D facial landmarks\nas intermediate variables. This framework achieves collaborative alignment of\nexpression, gaze, and pose with emotions through self-supervised learning.\nSpecifically, we decompose this task into two key steps, namely\nspeech-to-landmarks synthesis and landmarks-to-face generation. The first step\nfocuses on simultaneously synthesizing emotionally aligned facial cues,\nincluding normalized landmarks that represent expressions, gaze, and head pose.\nThese cues are subsequently reassembled into relocated facial landmarks. In the\nsecond step, these relocated landmarks are mapped to latent key points using\nself-supervised learning and then input into a pretrained model to create\nhigh-quality face images. Extensive experiments on the MEAD dataset demonstrate\nthat our model significantly advances the state-of-the-art performance in both\nvisual quality and emotional alignment.\n","authors":["Jiadong Liang","Feng Lu"],"pdf_url":"https://arxiv.org/pdf/2406.07895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07880v1","updated":"2024-06-12T05:19:55Z","published":"2024-06-12T05:19:55Z","title":"A Comprehensive Survey on Machine Learning Driven Material Defect\n  Detection: Challenges, Solutions, and Future Prospects","summary":"  Material defects (MD) represent a primary challenge affecting product\nperformance and giving rise to safety issues in related products. The rapid and\naccurate identification and localization of MD constitute crucial research\nendeavours in addressing contemporary challenges associated with MD. Although\nconventional non-destructive testing methods such as ultrasonic and X-ray\napproaches have mitigated issues related to low efficiency in manual\ninspections, they struggle to meet the diverse requirements of high precision,\nreal-time speed, automation, and intelligence. In recent years, propelled by\nthe swift advancement of machine learning (ML) technologies, particularly\nexemplified by deep learning, ML has swiftly emerged as the core technology and\na prominent research direction for material defect detection (MDD). Through a\ncomprehensive review of the latest literature, we systematically survey the ML\ntechniques applied in MDD into five categories: unsupervised learning,\nsupervised learning, semi-supervised learning, reinforcement learning, and\ngenerative learning. We provide a detailed analysis of the main principles and\ntechniques used, together with the advantages and potential challenges\nassociated with these techniques. Furthermore, the survey focuses on the\ntechniques for defect detection in composite materials, which are important\ntypes of materials enjoying increasingly wide application in various industries\nsuch as aerospace, automotive, construction, and renewable energy. Finally, the\nsurvey explores potential future directions in MDD utilizing ML technologies.\nThis comprehensive survey not only consolidates existing literature on ML-based\nMDD technologies but also serves as a foundational reference for future\nresearchers and industrial practitioners, providing valuable insights and\nguidance in developing advanced and efficient MDD systems.\n","authors":["Jun Bai","Di Wu","Tristan Shelley","Peter Schubel","David Twine","John Russell","Xuesen Zeng","Ji Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07879v1","updated":"2024-06-12T05:16:26Z","published":"2024-06-12T05:16:26Z","title":"KernelWarehouse: Rethinking the Design of Dynamic Convolution","summary":"  Dynamic convolution learns a linear mixture of n static kernels weighted with\ntheir input-dependent attentions, demonstrating superior performance than\nnormal convolution. However, it increases the number of convolutional\nparameters by n times, and thus is not parameter efficient. This leads to no\nresearch progress that can allow researchers to explore the setting n>100 (an\norder of magnitude larger than the typical setting n<10) for pushing forward\nthe performance boundary of dynamic convolution while enjoying parameter\nefficiency. To fill this gap, in this paper, we propose KernelWarehouse, a more\ngeneral form of dynamic convolution, which redefines the basic concepts of\n``kernels\", ``assembling kernels\" and ``attention function\" through the lens of\nexploiting convolutional parameter dependencies within the same layer and\nacross neighboring layers of a ConvNet. We testify the effectiveness of\nKernelWarehouse on ImageNet and MS-COCO datasets using various ConvNet\narchitectures. Intriguingly, KernelWarehouse is also applicable to Vision\nTransformers, and it can even reduce the model size of a backbone while\nimproving the model accuracy. For instance, KernelWarehouse (n=4) achieves\n5.61%|3.90%|4.38% absolute top-1 accuracy gain on the\nResNet18|MobileNetV2|DeiT-Tiny backbone, and KernelWarehouse (n=1/4) with\n65.10% model size reduction still achieves 2.29% gain on the ResNet18 backbone.\nThe code and models are available at https://github.com/OSVAI/KernelWarehouse.\n","authors":["Chao Li","Anbang Yao"],"pdf_url":"https://arxiv.org/pdf/2406.07879v1.pdf","comment":"This work is accepted to ICML 2024. The project page:\n  https://github.com/OSVAI/KernelWarehouse. arXiv admin note: substantial text\n  overlap with arXiv:2308.08361"},{"id":"http://arxiv.org/abs/2406.07876v1","updated":"2024-06-12T05:09:41Z","published":"2024-06-12T05:09:41Z","title":"Small Scale Data-Free Knowledge Distillation","summary":"  Data-free knowledge distillation is able to utilize the knowledge learned by\na large teacher network to augment the training of a smaller student network\nwithout accessing the original training data, avoiding privacy, security, and\nproprietary risks in real applications. In this line of research, existing\nmethods typically follow an inversion-and-distillation paradigm in which a\ngenerative adversarial network on-the-fly trained with the guidance of the\npre-trained teacher network is used to synthesize a large-scale sample set for\nknowledge distillation. In this paper, we reexamine this common data-free\nknowledge distillation paradigm, showing that there is considerable room to\nimprove the overall training efficiency through a lens of ``small-scale\ninverted data for knowledge distillation\". In light of three empirical\nobservations indicating the importance of how to balance class distributions in\nterms of synthetic sample diversity and difficulty during both data inversion\nand distillation processes, we propose Small Scale Data-free Knowledge\nDistillation SSD-KD. In formulation, SSD-KD introduces a modulating function to\nbalance synthetic samples and a priority sampling function to select proper\nsamples, facilitated by a dynamic replay buffer and a reinforcement learning\nstrategy. As a result, SSD-KD can perform distillation training conditioned on\nan extremely small scale of synthetic samples (e.g., 10X less than the original\ntraining data scale), making the overall training efficiency one or two orders\nof magnitude faster than many mainstream methods while retaining superior or\ncompetitive model performance, as demonstrated on popular image classification\nand semantic segmentation benchmarks. The code is available at\nhttps://github.com/OSVAI/SSD-KD.\n","authors":["He Liu","Yikai Wang","Huaping Liu","Fuchun Sun","Anbang Yao"],"pdf_url":"https://arxiv.org/pdf/2406.07876v1.pdf","comment":"This work is accepted to CVPR 2024. The project page:\n  https://github.com/OSVAI/SSD-KD"},{"id":"http://arxiv.org/abs/2406.07873v1","updated":"2024-06-12T05:02:16Z","published":"2024-06-12T05:02:16Z","title":"Robust 3D Face Alignment with Multi-Path Neural Architecture Search","summary":"  3D face alignment is a very challenging and fundamental problem in computer\nvision. Existing deep learning-based methods manually design different networks\nto regress either parameters of a 3D face model or 3D positions of face\nvertices. However, designing such networks relies on expert knowledge, and\nthese methods often struggle to produce consistent results across various face\nposes. To address this limitation, we employ Neural Architecture Search (NAS)\nto automatically discover the optimal architecture for 3D face alignment. We\npropose a novel Multi-path One-shot Neural Architecture Search (MONAS)\nframework that leverages multi-scale features and contextual information to\nenhance face alignment across various poses. The MONAS comprises two key\nalgorithms: Multi-path Networks Unbiased Sampling Based Training and Simulated\nAnnealing based Multi-path One-shot Search. Experimental results on three\npopular benchmarks demonstrate the superior performance of the MONAS for both\nsparse alignment and dense alignment.\n","authors":["Zhichao Jiang","Hongsong Wang","Xi Teng","Baopu Li"],"pdf_url":"https://arxiv.org/pdf/2406.07873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07871v1","updated":"2024-06-12T04:55:14Z","published":"2024-06-12T04:55:14Z","title":"Flexible Music-Conditioned Dance Generation with Style Description\n  Prompts","summary":"  Dance plays an important role as an artistic form and expression in human\nculture, yet the creation of dance remains a challenging task. Most dance\ngeneration methods primarily rely solely on music, seldom taking into\nconsideration intrinsic attributes such as music style or genre. In this work,\nwe introduce Flexible Dance Generation with Style Description Prompts (DGSDP),\na diffusion-based framework suitable for diversified tasks of dance generation\nby fully leveraging the semantics of music style. The core component of this\nframework is Music-Conditioned Style-Aware Diffusion (MCSAD), which comprises a\nTransformer-based network and a music Style Modulation module. The MCSAD seemly\nintegrates music conditions and style description prompts into the dance\ngeneration framework, ensuring that generated dances are consistent with the\nmusic content and style. To facilitate flexible dance generation and\naccommodate different tasks, a spatial-temporal masking strategy is effectively\napplied in the backward diffusion process. The proposed framework successfully\ngenerates realistic dance sequences that are accurately aligned with music for\na variety of tasks such as long-term generation, dance in-betweening, dance\ninpainting, and etc. We hope that this work has the potential to inspire dance\ngeneration and creation, with promising applications in entertainment, art, and\neducation.\n","authors":["Hongsong Wang","Yin Zhu","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2406.07871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07869v1","updated":"2024-06-12T04:52:40Z","published":"2024-06-12T04:52:40Z","title":"Unveiling the Power of Wavelets: A Wavelet-based Kolmogorov-Arnold\n  Network for Hyperspectral Image Classification","summary":"  Hyperspectral image classification is a crucial but challenging task due to\nthe high dimensionality and complex spatial-spectral correlations inherent in\nhyperspectral data. This paper employs Wavelet-based Kolmogorov-Arnold Network\n(wav-kan) architecture tailored for efficient modeling of these intricate\ndependencies. Inspired by the Kolmogorov-Arnold representation theorem, Wav-KAN\nincorporates wavelet functions as learnable activation functions, enabling\nnon-linear mapping of the input spectral signatures. The wavelet-based\nactivation allows Wav-KAN to effectively capture multi-scale spatial and\nspectral patterns through dilations and translations. Experimental evaluation\non three benchmark hyperspectral datasets (Salinas, Pavia, Indian Pines)\ndemonstrates the superior performance of Wav-KAN compared to traditional\nmultilayer perceptrons (MLPs) and the recently proposed Spline-based KAN\n(Spline-KAN) model. In this work we are: (1) conducting more experiments on\nadditional hyperspectral datasets (Pavia University, WHU-Hi, and Urban\nHyperspectral Image) to further validate the generalizability of Wav-KAN; (2)\ndeveloping a multiresolution Wav-KAN architecture to capture scale-invariant\nfeatures; (3) analyzing the effect of dimensional reduction techniques on\nclassification performance; (4) exploring optimization methods for tuning the\nhyperparameters of KAN models; and (5) comparing Wav-KAN with other\nstate-of-the-art models in hyperspectral image classification.\n","authors":["Seyd Teymoor Seydi"],"pdf_url":"https://arxiv.org/pdf/2406.07869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07867v1","updated":"2024-06-12T04:48:36Z","published":"2024-06-12T04:48:36Z","title":"Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation","summary":"  In this paper, we introduce a novel Face-to-Face spoken dialogue model. It\nprocesses audio-visual speech from user input and generates audio-visual speech\nas the response, marking the initial step towards creating an avatar chatbot\nsystem without relying on intermediate text. To this end, we newly introduce\nMultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken\ndialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded\nbased on the open domain dialogue dataset, TopicalChat. The MultiDialog\ncontains parallel audio-visual recordings of conversation partners acting\naccording to the given script with emotion annotations, which we expect to open\nup research opportunities in multimodal synthesis. Our Face-to-Face spoken\ndialogue model incorporates a textually pretrained large language model and\nadapts it into the audio-visual spoken dialogue domain by incorporating\nspeech-text joint pretraining. Through extensive experiments, we validate the\neffectiveness of our model in facilitating a face-to-face conversation. Demo\nand data are available at https://multidialog.github.io and\nhttps://huggingface.co/datasets/IVLLab/MultiDialog, respectively.\n","authors":["Se Jin Park","Chae Won Kim","Hyeongseop Rha","Minsu Kim","Joanna Hong","Jeong Hun Yeo","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2406.07867v1.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.07865v1","updated":"2024-06-12T04:45:33Z","published":"2024-06-12T04:45:33Z","title":"FaithFill: Faithful Inpainting for Object Completion Using a Single\n  Reference Image","summary":"  We present FaithFill, a diffusion-based inpainting object completion approach\nfor realistic generation of missing object parts. Typically, multiple reference\nimages are needed to achieve such realistic generation, otherwise the\ngeneration would not faithfully preserve shape, texture, color, and background.\nIn this work, we propose a pipeline that utilizes only a single input reference\nimage -having varying lighting, background, object pose, and/or viewpoint. The\nsingular reference image is used to generate multiple views of the object to be\ninpainted. We demonstrate that FaithFill produces faithful generation of the\nobject's missing parts, together with background/scene preservation, from a\nsingle reference image. This is demonstrated through standard similarity\nmetrics, human judgement, and GPT evaluation. Our results are presented on the\nDreamBooth dataset, and a novel proposed dataset.\n","authors":["Rupayan Mallick","Amr Abdalla","Sarah Adel Bargal"],"pdf_url":"https://arxiv.org/pdf/2406.07865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07862v1","updated":"2024-06-12T04:30:40Z","published":"2024-06-12T04:30:40Z","title":"Self-Distillation Learning Based on Temporal-Spatial Consistency for\n  Spiking Neural Networks","summary":"  Spiking neural networks (SNNs) have attracted considerable attention for\ntheir event-driven, low-power characteristics and high biological\ninterpretability. Inspired by knowledge distillation (KD), recent research has\nimproved the performance of the SNN model with a pre-trained teacher model.\nHowever, additional teacher models require significant computational resources,\nand it is tedious to manually define the appropriate teacher network\narchitecture. In this paper, we explore cost-effective self-distillation\nlearning of SNNs to circumvent these concerns. Without an explicit defined\nteacher, the SNN generates pseudo-labels and learns consistency during\ntraining. On the one hand, we extend the timestep of the SNN during training to\ncreate an implicit temporal ``teacher\" that guides the learning of the original\n``student\", i.e., the temporal self-distillation. On the other hand, we guide\nthe output of the weak classifier at the intermediate stage by the final output\nof the SNN, i.e., the spatial self-distillation. Our temporal-spatial\nself-distillation (TSSD) learning method does not introduce any inference\noverhead and has excellent generalization ability. Extensive experiments on the\nstatic image datasets CIFAR10/100 and ImageNet as well as the neuromorphic\ndatasets CIFAR10-DVS and DVS-Gesture validate the superior performance of the\nTSSD method. This paper presents a novel manner of fusing SNNs with KD,\nproviding insights into high-performance SNN learning methods.\n","authors":["Lin Zuo","Yongqi Ding","Mengmeng Jing","Kunshan Yang","Yunqian Yu"],"pdf_url":"https://arxiv.org/pdf/2406.07862v1.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.07852v1","updated":"2024-06-12T03:40:17Z","published":"2024-06-12T03:40:17Z","title":"DiffPop: Plausibility-Guided Object Placement Diffusion for Image\n  Composition","summary":"  In this paper, we address the problem of plausible object placement for the\nchallenging task of realistic image composition. We propose DiffPop, the first\nframework that utilizes plausibility-guided denoising diffusion probabilistic\nmodel to learn the scale and spatial relations among multiple objects and the\ncorresponding scene image. First, we train an unguided diffusion model to\ndirectly learn the object placement parameters in a self-supervised manner.\nThen, we develop a human-in-the-loop pipeline which exploits human labeling on\nthe diffusion-generated composite images to provide the weak supervision for\ntraining a structural plausibility classifier. The classifier is further used\nto guide the diffusion sampling process towards generating the plausible object\nplacement. Experimental results verify the superiority of our method for\nproducing plausible and diverse composite images on the new Cityscapes-OP\ndataset and the public OPA dataset, as well as demonstrate its potential in\napplications such as data augmentation and multi-object placement tasks. Our\ndataset and code will be released.\n","authors":["Jiacheng Liu","Hang Zhou","Shida Wei","Rui Ma"],"pdf_url":"https://arxiv.org/pdf/2406.07852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07851v1","updated":"2024-06-12T03:39:16Z","published":"2024-06-12T03:39:16Z","title":"A Labeled Array Distance Metric for Measuring Image Segmentation Quality","summary":"  This work introduces two new distance metrics for comparing labeled arrays,\nwhich are common outputs of image segmentation algorithms. Each pixel in an\nimage is assigned a label, with binary segmentation providing only two labels\n('foreground' and 'background'). These can be represented by a simple binary\nmatrix and compared using pixel differences. However, many segmentation\nalgorithms output multiple regions in a labeled array. We propose two distance\nmetrics, named LAD and MADLAD, that calculate the distance between two labeled\nimages. By doing so, the accuracy of different image segmentation algorithms\ncan be evaluated by measuring their outputs against a 'ground truth' labeling.\nBoth proposed metrics, operating with a complexity of $O(N)$ for images with\n$N$ pixels, are designed to quickly identify similar labeled arrays, even when\ndifferent labeling methods are used. Comparisons are made between images\nlabeled manually and those labeled by segmentation algorithms. This evaluation\nis crucial when searching through a space of segmentation algorithms and their\nhyperparameters via a genetic algorithm to identify the optimal solution for\nautomated segmentation, which is the goal in our lab, SEE-Insight. By measuring\nthe distance from the ground truth, these metrics help determine which\nalgorithm provides the most accurate segmentation.\n","authors":["Maryam Berijanian","Katrina Gensterblum","Doruk Alp Mutlu","Katelyn Reagan","Andrew Hart","Dirk Colbry"],"pdf_url":"https://arxiv.org/pdf/2406.07851v1.pdf","comment":"Submitted to: Electronic Letters on Computer Vision and Image\n  Analysis"},{"id":"http://arxiv.org/abs/2406.07844v1","updated":"2024-06-12T03:21:34Z","published":"2024-06-12T03:21:34Z","title":"Understanding and Mitigating Compositional Issues in Text-to-Image\n  Generative Models","summary":"  Recent text-to-image diffusion-based generative models have the stunning\nability to generate highly detailed and photo-realistic images and achieve\nstate-of-the-art low FID scores on challenging image generation benchmarks.\nHowever, one of the primary failure modes of these text-to-image generative\nmodels is in composing attributes, objects, and their associated relationships\naccurately into an image. In our paper, we investigate this\ncompositionality-based failure mode and highlight that imperfect text\nconditioning with CLIP text-encoder is one of the primary reasons behind the\ninability of these models to generate high-fidelity compositional scenes. In\nparticular, we show that (i) there exists an optimal text-embedding space that\ncan generate highly coherent compositional scenes which shows that the output\nspace of the CLIP text-encoder is sub-optimal, and (ii) we observe that the\nfinal token embeddings in CLIP are erroneous as they often include attention\ncontributions from unrelated tokens in compositional prompts. Our main finding\nshows that the best compositional improvements can be achieved (without harming\nthe model's FID scores) by fine-tuning {\\it only} a simple linear projection on\nCLIP's representation space in Stable-Diffusion variants using a small set of\ncompositional image-text pairs. This result demonstrates that the\nsub-optimality of the CLIP's output space is a major error source. We also show\nthat re-weighting the erroneous attention contributions in CLIP can also lead\nto improved compositional performances, however these improvements are often\nless significant than those achieved by solely learning a linear projection\nhead, highlighting erroneous attentions to be only a minor error source.\n","authors":["Arman Zarei","Keivan Rezaei","Samyadeep Basu","Mehrdad Saberi","Mazda Moayeri","Priyatham Kattakinda","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2406.07844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07843v1","updated":"2024-06-12T03:21:06Z","published":"2024-06-12T03:21:06Z","title":"Incremental Learning and Self-Attention Mechanisms Improve Neural System\n  Identification","summary":"  Convolutional neural networks (CNNs) have been shown to be the\nstate-of-the-art approach for modeling the transfer functions of visual\ncortical neurons. Cortical neurons in the primary visual cortex are are\nsensitive to contextual information mediated by extensive horizontal and\nfeedback connections. Standard CNNs can integrate global spatial image\ninformation to model such contextual modulation via two mechanisms: successive\nrounds of convolutions and a fully connected readout layer. In this paper, we\nfind that non-local networks or self-attention (SA) mechanisms, theoretically\nrelated to context-dependent flexible gating mechanisms observed in the primary\nvisual cortex, improve neural response predictions over parameter-matched CNNs\nin two key metrics: tuning curve correlation and tuning peak. We factorize\nnetworks to determine the relative contribution of each context mechanism. This\nreveals that information in the local receptive field is most important for\nmodeling the overall tuning curve, but surround information is critically\nnecessary for characterizing the tuning peak. We find that self-attention can\nreplace subsequent spatial-integration convolutions when learned in an\nincremental manner, and is further enhanced in the presence of a fully\nconnected readout layer, suggesting that the two context mechanisms are\ncomplementary. Finally, we find that learning a receptive-field-centric model\nwith self-attention, before incrementally learning a fully connected readout,\nyields a more biologically realistic model in terms of center-surround\ncontributions.\n","authors":["Isaac Lin","Tianye Wang","Shang Gao","Shiming Tang","Tai Sing Lee"],"pdf_url":"https://arxiv.org/pdf/2406.07843v1.pdf","comment":"Preprint NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.07841v1","updated":"2024-06-12T03:16:45Z","published":"2024-06-12T03:16:45Z","title":"Labeling Comic Mischief Content in Online Videos with a Multimodal\n  Hierarchical-Cross-Attention Model","summary":"  We address the challenge of detecting questionable content in online media,\nspecifically the subcategory of comic mischief. This type of content combines\nelements such as violence, adult content, or sarcasm with humor, making it\ndifficult to detect. Employing a multimodal approach is vital to capture the\nsubtle details inherent in comic mischief content. To tackle this problem, we\npropose a novel end-to-end multimodal system for the task of comic mischief\ndetection. As part of this contribution, we release a novel dataset for the\ntargeted task consisting of three modalities: video, text (video captions and\nsubtitles), and audio. We also design a HIerarchical Cross-attention model with\nCAPtions (HICCAP) to capture the intricate relationships among these\nmodalities. The results show that the proposed approach makes a significant\nimprovement over robust baselines and state-of-the-art models for comic\nmischief detection and its type classification. This emphasizes the potential\nof our system to empower users, to make informed decisions about the online\ncontent they choose to see. In addition, we conduct experiments on the UCF101,\nHMDB51, and XD-Violence datasets, comparing our model against other\nstate-of-the-art approaches showcasing the outstanding performance of our\nproposed model in various scenarios.\n","authors":["Elaheh Baharlouei","Mahsa Shafaei","Yigeng Zhang","Hugo Jair Escalante","Thamar Solorio"],"pdf_url":"https://arxiv.org/pdf/2406.07841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20764v3","updated":"2024-06-12T03:16:40Z","published":"2024-05-31T12:35:06Z","title":"CoMoFusion: Fast and High-quality Fusion of Infrared and Visible Image\n  with Consistency Model","summary":"  Generative models are widely utilized to model the distribution of fused\nimages in the field of infrared and visible image fusion. However, current\ngenerative models based fusion methods often suffer from unstable training and\nslow inference speed. To tackle this problem, a novel fusion method based on\nconsistency model is proposed, termed as CoMoFusion, which can generate the\nhigh-quality images and achieve fast image inference speed. In specific, the\nconsistency model is used to construct multi-modal joint features in the latent\nspace with the forward and reverse process. Then, the infrared and visible\nfeatures extracted by the trained consistency model are fed into fusion module\nto generate the final fused image. In order to enhance the texture and salient\ninformation of fused images, a novel loss based on pixel value selection is\nalso designed. Extensive experiments on public datasets illustrate that our\nmethod obtains the SOTA fusion performance compared with the existing fusion\nmethods.\n","authors":["Zhiming Meng","Hui Li","Zeyang Zhang","Zhongwei Shen","Yunlong Yu","Xiaoning Song","Xiaojun Wu"],"pdf_url":"https://arxiv.org/pdf/2405.20764v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07840v1","updated":"2024-06-12T03:15:15Z","published":"2024-06-12T03:15:15Z","title":"SynthForge: Synthesizing High-Quality Face Dataset with Controllable 3D\n  Generative Models","summary":"  Recent advancements in generative models have unlocked the capabilities to\nrender photo-realistic data in a controllable fashion. Trained on the real\ndata, these generative models are capable of producing realistic samples with\nminimal to no domain gap, as compared to the traditional graphics rendering.\nHowever, using the data generated using such models for training downstream\ntasks remains under-explored, mainly due to the lack of 3D consistent\nannotations. Moreover, controllable generative models are learned from massive\ndata and their latent space is often too vast to obtain meaningful sample\ndistributions for downstream task with limited generation. To overcome these\nchallenges, we extract 3D consistent annotations from an existing controllable\ngenerative model, making the data useful for downstream tasks. Our experiments\nshow competitive performance against state-of-the-art models using only\ngenerated synthetic data, demonstrating potential for solving downstream tasks.\nProject page: https://synth-forge.github.io\n","authors":["Abhay Rawat","Shubham Dokania","Astitva Srivastava","Shuaib Ahmed","Haiwen Feng","Rahul Tallamraju"],"pdf_url":"https://arxiv.org/pdf/2406.07840v1.pdf","comment":"11 pages, 4 figures, 3 tables. Under Review"},{"id":"http://arxiv.org/abs/2406.07833v1","updated":"2024-06-12T03:02:54Z","published":"2024-06-12T03:02:54Z","title":"Sense Less, Generate More: Pre-training LiDAR Perception with Masked\n  Autoencoders for Ultra-Efficient 3D Sensing","summary":"  In this work, we propose a disruptively frugal LiDAR perception dataflow that\ngenerates rather than senses parts of the environment that are either\npredictable based on the extensive training of the environment or have limited\nconsequence to the overall prediction accuracy. Therefore, the proposed\nmethodology trades off sensing energy with training data for low-power robotics\nand autonomous navigation to operate frugally with sensors, extending their\nlifetime on a single battery charge. Our proposed generative pre-training\nstrategy for this purpose, called as radially masked autoencoding (R-MAE), can\nalso be readily implemented in a typical LiDAR system by selectively activating\nand controlling the laser power for randomly generated angular regions during\non-field operations. Our extensive evaluations show that pre-training with\nR-MAE enables focusing on the radial segments of the data, thereby capturing\nspatial relationships and distances between objects more effectively than\nconventional procedures. Therefore, the proposed methodology not only reduces\nsensing energy but also improves prediction accuracy. For example, our\nextensive evaluations on Waymo, nuScenes, and KITTI datasets show that the\napproach achieves over a 5% average precision improvement in detection tasks\nacross datasets and over a 4% accuracy improvement in transferring domains from\nWaymo and nuScenes to KITTI. In 3D object detection, it enhances small object\ndetection by up to 4.37% in AP at moderate difficulty levels in the KITTI\ndataset. Even with 90% radial masking, it surpasses baseline models by up to\n5.59% in mAP/mAPH across all object classes in the Waymo dataset. Additionally,\nour method achieves up to 3.17% and 2.31% improvements in mAP and NDS,\nrespectively, on the nuScenes dataset, demonstrating its effectiveness with\nboth single and fused LiDAR-camera modalities.\nhttps://github.com/sinatayebati/Radial_MAE.\n","authors":["Sina Tayebati","Theja Tulabandhula","Amit R. Trivedi"],"pdf_url":"https://arxiv.org/pdf/2406.07833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05768v3","updated":"2024-06-12T02:57:00Z","published":"2024-06-09T12:55:50Z","title":"MLCM: Multistep Consistency Distillation of Latent Diffusion Model","summary":"  Distilling large latent diffusion models (LDMs) into ones that are fast to\nsample from is attracting growing research interest. However, the majority of\nexisting methods face a dilemma where they either (i) depend on multiple\nindividual distilled models for different sampling budgets, or (ii) sacrifice\ngeneration quality with limited (e.g., 2-4) and/or moderate (e.g., 5-8)\nsampling steps. To address these, we extend the recent multistep consistency\ndistillation (MCD) strategy to representative LDMs, establishing the Multistep\nLatent Consistency Models (MLCMs) approach for low-cost high-quality image\nsynthesis. MLCM serves as a unified model for various sampling steps due to the\npromise of MCD. We further augment MCD with a progressive training strategy to\nstrengthen inter-segment consistency to boost the quality of few-step\ngenerations. We take the states from the sampling trajectories of the teacher\nmodel as training data for MLCMs to lift the requirements for high-quality\ntraining datasets and to bridge the gap between the training and inference of\nthe distilled model. MLCM is compatible with preference learning strategies for\nfurther improvement of visual quality and aesthetic appeal. Empirically, MLCM\ncan generate high-quality, delightful images with only 2-8 sampling steps. On\nthe MSCOCO-2017 5K benchmark, MLCM distilled from SDXL gets a CLIP Score of\n33.30, Aesthetic Score of 6.19, and Image Reward of 1.20 with only 4 steps,\nsubstantially surpassing 4-step LCM [23], 8-step SDXL-Lightning [17], and\n8-step HyperSD [33]. We also demonstrate the versatility of MLCMs in\napplications including controllable generation, image style transfer, and\nChinese-to-image generation.\n","authors":["Qingsong Xie","Zhenyi Liao","Chen chen","Zhijie Deng","Shixiang Tang","Haonan Lu"],"pdf_url":"https://arxiv.org/pdf/2406.05768v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07828v1","updated":"2024-06-12T02:48:52Z","published":"2024-06-12T02:48:52Z","title":"Spatial Annealing Smoothing for Efficient Few-shot Neural Rendering","summary":"  Neural Radiance Fields (NeRF) with hybrid representations have shown\nimpressive capabilities in reconstructing scenes for view synthesis, delivering\nhigh efficiency. Nonetheless, their performance significantly drops with sparse\nview inputs, due to the issue of overfitting. While various regularization\nstrategies have been devised to address these challenges, they often depend on\ninefficient assumptions or are not compatible with hybrid models. There is a\nclear need for a method that maintains efficiency and improves resilience to\nsparse views within a hybrid framework. In this paper, we introduce an accurate\nand efficient few-shot neural rendering method named Spatial Annealing\nsmoothing regularized NeRF (SANeRF), which is specifically designed for a\npre-filtering-driven hybrid representation architecture. We implement an\nexponential reduction of the sample space size from an initially large value.\nThis methodology is crucial for stabilizing the early stages of the training\nphase and significantly contributes to the enhancement of the subsequent\nprocess of detail refinement. Our extensive experiments reveal that, by adding\nmerely one line of code, SANeRF delivers superior rendering quality and much\nfaster reconstruction speed compared to current few-shot NeRF methods. Notably,\nSANeRF outperforms FreeNeRF by 0.3 dB in PSNR on the Blender dataset, while\nachieving 700x faster reconstruction speed.\n","authors":["Yuru Xiao","Xianming Liu","Deming Zhai","Kui Jiang","Junjun Jiang","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2406.07828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07822v1","updated":"2024-06-12T02:43:19Z","published":"2024-06-12T02:43:19Z","title":"Tell Me What's Next: Textual Foresight for Generic UI Representations","summary":"  Mobile app user interfaces (UIs) are rich with action, text, structure, and\nimage content that can be utilized to learn generic UI representations for\ntasks like automating user commands, summarizing content, and evaluating the\naccessibility of user interfaces. Prior work has learned strong visual\nrepresentations with local or global captioning losses, but fails to retain\nboth granularities. To combat this, we propose Textual Foresight, a novel\npretraining objective for learning UI screen representations. Textual Foresight\ngenerates global text descriptions of future UI states given a current UI and\nlocal action taken. Our approach requires joint reasoning over elements and\nentire screens, resulting in improved UI features: on generation tasks, UI\nagents trained with Textual Foresight outperform state-of-the-art by 2% with\n28x fewer images. We train with our newly constructed mobile app dataset,\nOpenApp, which results in the first public dataset for app UI representation\nlearning. OpenApp enables new baselines, and we find Textual Foresight improves\naverage task performance over them by 5.7% while having access to 2x less data.\n","authors":["Andrea Burns","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2406.07822v1.pdf","comment":"Accepted to ACL 2024 Findings. Data and code to be released at\n  https://github.com/aburns4/textualforesight"},{"id":"http://arxiv.org/abs/2406.07820v1","updated":"2024-06-12T02:39:46Z","published":"2024-06-12T02:39:46Z","title":"Are Objective Explanatory Evaluation metrics Trustworthy? An Adversarial\n  Analysis","summary":"  Explainable AI (XAI) has revolutionized the field of deep learning by\nempowering users to have more trust in neural network models. The field of XAI\nallows users to probe the inner workings of these algorithms to elucidate their\ndecision-making processes. The rise in popularity of XAI has led to the advent\nof different strategies to produce explanations, all of which only occasionally\nagree. Thus several objective evaluation metrics have been devised to decide\nwhich of these modules give the best explanation for specific scenarios. The\ngoal of the paper is twofold: (i) we employ the notions of necessity and\nsufficiency from causal literature to come up with a novel explanatory\ntechnique called SHifted Adversaries using Pixel Elimination(SHAPE) which\nsatisfies all the theoretical and mathematical criteria of being a valid\nexplanation, (ii) we show that SHAPE is, infact, an adversarial explanation\nthat fools causal metrics that are employed to measure the robustness and\nreliability of popular importance based visual XAI methods. Our analysis shows\nthat SHAPE outperforms popular explanatory techniques like GradCAM and\nGradCAM++ in these tests and is comparable to RISE, raising questions about the\nsanity of these metrics and the need for human involvement for an overall\nbetter evaluation.\n","authors":["Prithwijit Chowdhury","Mohit Prabhushankar","Ghassan AlRegib","Mohamed Deriche"],"pdf_url":"https://arxiv.org/pdf/2406.07820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03035v2","updated":"2024-06-12T02:38:23Z","published":"2024-06-05T08:03:18Z","title":"Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation\n  for Stable Pose Control","summary":"  Pose-controllable character video generation is in high demand with extensive\napplications for fields such as automatic advertising and content creation on\nsocial media platforms. While existing character image animation methods using\npose sequences and reference images have shown promising performance, they tend\nto struggle with incoherent animation in complex scenarios, such as multiple\ncharacter animation and body occlusion. Additionally, current methods request\nlarge-scale high-quality videos with stable backgrounds and temporal\nconsistency as training datasets, otherwise, their performance will greatly\ndeteriorate. These two issues hinder the practical utilization of character\nimage animation tools. In this paper, we propose a practical and robust\nframework Follow-Your-Pose v2, which can be trained on noisy open-sourced\nvideos readily available on the internet. Multi-condition guiders are designed\nto address the challenges of background stability, body occlusion in\nmulti-character generation, and consistency of character appearance. Moreover,\nto fill the gap of fair evaluation of multi-character pose animation, we\npropose a new benchmark comprising approximately 4,000 frames. Extensive\nexperiments demonstrate that our approach outperforms state-of-the-art methods\nby a margin of over 35\\% across 2 datasets and on 7 metrics. Meanwhile,\nqualitative assessments reveal a significant improvement in the quality of\ngenerated video, particularly in scenarios involving complex backgrounds and\nbody occlusion of multi-character, suggesting the superiority of our approach.\n","authors":["Jingyun Xue","Hongfa Wang","Qi Tian","Yue Ma","Andong Wang","Zhiyuan Zhao","Shaobo Min","Wenzhe Zhao","Kaihao Zhang","Heung-Yeung Shum","Wei Liu","Mengyang Liu","Wenhan Luo"],"pdf_url":"https://arxiv.org/pdf/2406.03035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07023v2","updated":"2024-06-12T02:26:46Z","published":"2024-06-11T07:26:54Z","title":"LiSD: An Efficient Multi-Task Learning Framework for LiDAR Segmentation\n  and Detection","summary":"  With the rapid proliferation of autonomous driving, there has been a\nheightened focus on the research of lidar-based 3D semantic segmentation and\nobject detection methodologies, aiming to ensure the safety of traffic\nparticipants. In recent decades, learning-based approaches have emerged,\ndemonstrating remarkable performance gains in comparison to conventional\nalgorithms. However, the segmentation and detection tasks have traditionally\nbeen examined in isolation to achieve the best precision. To this end, we\npropose an efficient multi-task learning framework named LiSD which can address\nboth segmentation and detection tasks, aiming to optimize the overall\nperformance. Our proposed LiSD is a voxel-based encoder-decoder framework that\ncontains a hierarchical feature collaboration module and a holistic information\naggregation module. Different integration methods are adopted to keep sparsity\nin segmentation while densifying features for query initialization in\ndetection. Besides, cross-task information is utilized in an instance-aware\nrefinement module to obtain more accurate predictions. Experimental results on\nthe nuScenes dataset and Waymo Open Dataset demonstrate the effectiveness of\nour proposed model. It is worth noting that LiSD achieves the state-of-the-art\nperformance of 83.3% mIoU on the nuScenes segmentation benchmark for lidar-only\nmethods.\n","authors":["Jiahua Xu","Si Zuo","Chenfeng Wei","Wei Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.07023v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07813v1","updated":"2024-06-12T02:09:05Z","published":"2024-06-12T02:09:05Z","title":"Evaluating the Impact of Sequence Combinations on Breast Tumor\n  Segmentation in Multiparametric MRI","summary":"  Multiparametric magnetic resonance imaging (mpMRI) is a key tool for\nassessing breast cancer progression. Although deep learning has been applied to\nautomate tumor segmentation in breast MRI, the effect of sequence combinations\nin mpMRI remains under-investigated. This study explores the impact of\ndifferent combinations of T2-weighted (T2w), dynamic contrast-enhanced MRI\n(DCE-MRI) and diffusion-weighted imaging (DWI) with apparent diffusion\ncoefficient (ADC) map on breast tumor segmentation using nnU-Net. Evaluated on\na multicenter mpMRI dataset, the nnU-Net model using DCE sequences achieved a\nDice similarity coefficient (DSC) of 0.69 $\\pm$ 0.18 for functional tumor\nvolume (FTV) segmentation. For whole tumor mask (WTM) segmentation, adding the\npredicted FTV to DWI and ADC map improved the DSC from 0.57 $\\pm$ 0.24 to 0.60\n$\\pm$ 0.21. Adding T2w did not yield significant improvement, which still\nrequires further investigation under a more standardized imaging protocol. This\nstudy serves as a foundation for future work on predicting breast cancer\ntreatment response using mpMRI.\n","authors":["Hang Min","Gorane Santamaria Hormaechea","Prabhakar Ramachandran","Jason Dowling"],"pdf_url":"https://arxiv.org/pdf/2406.07813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07255v2","updated":"2024-06-12T02:04:20Z","published":"2024-06-11T13:34:57Z","title":"Towards Realistic Data Generation for Real-World Super-Resolution","summary":"  Existing image super-resolution (SR) techniques often fail to generalize\neffectively in complex real-world settings due to the significant divergence\nbetween training data and practical scenarios. To address this challenge,\nprevious efforts have either manually simulated intricate physical-based\ndegradations or utilized learning-based techniques, yet these approaches remain\ninadequate for producing large-scale, realistic, and diverse data\nsimultaneously. In this paper, we introduce a novel Realistic Decoupled Data\nGenerator (RealDGen), an unsupervised learning data generation framework\ndesigned for real-world super-resolution. We meticulously develop content and\ndegradation extraction strategies, which are integrated into a novel\ncontent-degradation decoupled diffusion model to create realistic\nlow-resolution images from unpaired real LR and HR images. Extensive\nexperiments demonstrate that RealDGen excels in generating large-scale,\nhigh-quality paired data that mirrors real-world degradations, significantly\nadvancing the performance of popular SR models on various real-world\nbenchmarks.\n","authors":["Long Peng","Wenbo Li","Renjing Pei","Jingjing Ren","Xueyang Fu","Yang Wang","Yang Cao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2406.07255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17042v2","updated":"2024-06-12T01:33:21Z","published":"2024-03-25T15:58:26Z","title":"Provably Robust Score-Based Diffusion Posterior Sampling for\n  Plug-and-Play Image Reconstruction","summary":"  In a great number of tasks in science and engineering, the goal is to infer\nan unknown image from a small number of measurements collected from a known\nforward model describing certain sensing or imaging modality. Due to resource\nconstraints, this task is often extremely ill-posed, which necessitates the\nadoption of expressive prior information to regularize the solution space.\nScore-based diffusion models, due to its impressive empirical success, have\nemerged as an appealing candidate of an expressive prior in image\nreconstruction. In order to accommodate diverse tasks at once, it is of great\ninterest to develop efficient, consistent and robust algorithms that\nincorporate unconditional score functions of an image prior distribution in\nconjunction with flexible choices of forward models.\n  This work develops an algorithmic framework for employing score-based\ndiffusion models as an expressive data prior in general nonlinear inverse\nproblems. Motivated by the plug-and-play framework in the imaging community, we\nintroduce a diffusion plug-and-play method (DPnP) that alternatively calls two\nsamplers, a proximal consistency sampler based solely on the likelihood\nfunction of the forward model, and a denoising diffusion sampler based solely\non the score functions of the image prior. The key insight is that denoising\nunder white Gaussian noise can be solved rigorously via both stochastic (i.e.,\nDDPM-type) and deterministic (i.e., DDIM-type) samplers using the unconditional\nscore functions. We establish both asymptotic and non-asymptotic performance\nguarantees of DPnP, and provide numerical experiments to illustrate its promise\nin solving both linear and nonlinear image reconstruction tasks. To the best of\nour knowledge, DPnP is the first provably-robust posterior sampling method for\nnonlinear inverse problems using unconditional diffusion priors.\n","authors":["Xingyu Xu","Yuejie Chi"],"pdf_url":"https://arxiv.org/pdf/2403.17042v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07792v1","updated":"2024-06-12T01:12:53Z","published":"2024-06-12T01:12:53Z","title":"Hierarchical Patch Diffusion Models for High-Resolution Video Generation","summary":"  Diffusion models have demonstrated remarkable performance in image and video\nsynthesis. However, scaling them to high-resolution inputs is challenging and\nrequires restructuring the diffusion pipeline into multiple independent\ncomponents, limiting scalability and complicating downstream applications. This\nmakes it very efficient during training and unlocks end-to-end optimization on\nhigh-resolution videos. We improve PDMs in two principled ways. First, to\nenforce consistency between patches, we develop deep context fusion -- an\narchitectural technique that propagates the context information from low-scale\nto high-scale patches in a hierarchical manner. Second, to accelerate training\nand inference, we propose adaptive computation, which allocates more network\ncapacity and computation towards coarse image details. The resulting model sets\na new state-of-the-art FVD score of 66.32 and Inception Score of 87.68 in\nclass-conditional video generation on UCF-101 $256^2$, surpassing recent\nmethods by more than 100%. Then, we show that it can be rapidly fine-tuned from\na base $36\\times 64$ low-resolution generator for high-resolution $64 \\times\n288 \\times 512$ text-to-video synthesis. To the best of our knowledge, our\nmodel is the first diffusion-based architecture which is trained on such high\nresolutions entirely end-to-end. Project webpage:\nhttps://snap-research.github.io/hpdm.\n","authors":["Ivan Skorokhodov","Willi Menapace","Aliaksandr Siarohin","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2406.07792v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2405.02782v2","updated":"2024-06-12T01:01:51Z","published":"2024-05-05T01:51:58Z","title":"A self-supervised text-vision framework for automated brain abnormality\n  detection","summary":"  Artificial neural networks trained on large, expert-labelled datasets are\nconsidered state-of-the-art for a range of medical image recognition tasks.\nHowever, categorically labelled datasets are time-consuming to generate and\nconstrain classification to a pre-defined, fixed set of classes. For\nneuroradiological applications in particular, this represents a barrier to\nclinical adoption. To address these challenges, we present a self-supervised\ntext-vision framework that learns to detect clinically relevant abnormalities\nin brain MRI scans by directly leveraging the rich information contained in\naccompanying free-text neuroradiology reports. Our training approach consisted\nof two-steps. First, a dedicated neuroradiological language model - NeuroBERT -\nwas trained to generate fixed-dimensional vector representations of\nneuroradiology reports (N = 50,523) via domain-specific self-supervised\nlearning tasks. Next, convolutional neural networks (one per MRI sequence)\nlearnt to map individual brain scans to their corresponding text vector\nrepresentations by optimising a mean square error loss. Once trained, our\ntext-vision framework can be used to detect abnormalities in unreported brain\nMRI examinations by scoring scans against suitable query sentences (e.g.,\n'there is an acute stroke', 'there is hydrocephalus' etc.), enabling a range of\nclassification-based applications including automated triage. Potentially, our\nframework could also serve as a clinical decision support tool, not only by\nsuggesting findings to radiologists and detecting errors in provisional\nreports, but also by retrieving and displaying examples of pathologies from\nhistorical examinations that could be relevant to the current case based on\ntextual descriptors.\n","authors":["David A. Wood","Emily Guilhem","Sina Kafiabadi","Ayisha Al Busaidi","Kishan Dissanayake","Ahmed Hammam","Nina Mansoor","Matthew Townend","Siddharth Agarwal","Yiran Wei","Asif Mazumder","Gareth J. Barker","Peter Sasieni","Sebastien Ourselin","James H. Cole","Thomas C. Booth"],"pdf_url":"https://arxiv.org/pdf/2405.02782v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.07785v1","updated":"2024-06-12T00:41:25Z","published":"2024-06-12T00:41:25Z","title":"From Variance to Veracity: Unbundling and Mitigating Gradient Variance\n  in Differentiable Bundle Adjustment Layers","summary":"  Various pose estimation and tracking problems in robotics can be decomposed\ninto a correspondence estimation problem (often computed using a deep network)\nfollowed by a weighted least squares optimization problem to solve for the\nposes. Recent work has shown that coupling the two problems by iteratively\nrefining one conditioned on the other's output yields SOTA results across\ndomains. However, training these models has proved challenging, requiring a\nlitany of tricks to stabilize and speed up training. In this work, we take the\nvisual odometry problem as an example and identify three plausible causes: (1)\nflow loss interference, (2) linearization errors in the bundle adjustment (BA)\nlayer, and (3) dependence of weight gradients on the BA residual. We show how\nthese issues result in noisy and higher variance gradients, potentially leading\nto a slow down in training and instabilities. We then propose a simple, yet\neffective solution to reduce the gradient variance by using the weights\npredicted by the network in the inner optimization loop to weight the\ncorrespondence objective in the training problem. This helps the training\nobjective `focus' on the more important points, thereby reducing the variance\nand mitigating the influence of outliers. We show that the resulting method\nleads to faster training and can be more flexibly trained in varying training\nsetups without sacrificing performance. In particular we show $2$--$2.5\\times$\ntraining speedups over a baseline visual odometry model we modify.\n","authors":["Swaminathan Gurumurthy","Karnik Ram","Bingqing Chen","Zachary Manchester","Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2406.07785v1.pdf","comment":"Accepted at CVPR 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.08477v1","updated":"2024-06-12T17:59:05Z","published":"2024-06-12T17:59:05Z","title":"Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens","summary":"  Characterizing users and items through vector representations is crucial for\nvarious tasks in recommender systems. Recent approaches attempt to apply Large\nLanguage Models (LLMs) in recommendation through a question and answer format,\nwhere real users and items (e.g., Item No.2024) are represented with\nin-vocabulary tokens (e.g., \"item\", \"20\", \"24\"). However, since LLMs are\ntypically pretrained on natural language tasks, these in-vocabulary tokens lack\nthe expressive power for distinctive users and items, thereby weakening the\nrecommendation ability even after fine-tuning on recommendation tasks. In this\npaper, we explore how to effectively tokenize users and items in LLM-based\nrecommender systems. We emphasize the role of out-of-vocabulary (OOV) tokens in\naddition to the in-vocabulary ones and claim the memorization of OOV tokens\nthat capture correlations of users/items as well as diversity of OOV tokens. By\nclustering the learned representations from historical user-item interactions,\nwe make the representations of user/item combinations share the same OOV tokens\nif they have similar properties. Furthermore, integrating these OOV tokens into\nthe LLM's vocabulary allows for better distinction between users and items and\nenhanced capture of user-item relationships during fine-tuning on downstream\ntasks. Our proposed framework outperforms existing state-of-the-art methods\nacross various downstream recommendation tasks.\n","authors":["Ting-Ji Huang","Jia-Qi Yang","Chunxu Shen","Kai-Qi Liu","De-Chuan Zhan","Han-Jia Ye"],"pdf_url":"https://arxiv.org/pdf/2406.08477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08461v1","updated":"2024-06-12T17:51:47Z","published":"2024-06-12T17:51:47Z","title":"Bridging the Gap: Unravelling Local Government Data Sharing Barriers in\n  Estonia and Beyond","summary":"  Estonia's digital government success has received global acclaim, yet its\nOpen Government Data (OGD) initiatives, especially at the local level,\nencounter persistent challenges. Despite significant progress of national OGD\ninitiative in OGD rankings, local governments lag in OGD provision. This study\naims to examine barriers hindering municipalities from openly sharing OGD.\nEmploying a qualitative approach through interviews with Estonian\nmunicipalities and drawing on the OGD-adapted Innovation Resistance Theory\nmodel, the study sheds light on barriers impeding OGD sharing. Practical\nrecommendations are proposed to bridge the gap between national policies and\nlocal implementation, including enhancing awareness, improving data governance\nframeworks, and fostering collaboration be-tween local and national\nauthorities. By addressing overlooked weaknesses in the Estonian open data\necosystem and providing actionable recommendations, this research contributes\nto a more resilient and sustainable open data ecosystem. Additionally, by\nvalidating the OGD-adapted Innovation Resistance Theory model and proposing a\nrevised version tailored for local government contexts, the study advances\ntheoretical frameworks for understanding data sharing resistance. Ultimately,\nthis study serves as a call to action for policymakers and practitioners to\nprioritize local OGD initiatives.\n","authors":["Katrin Rajamäe Soosaar","Anastasija Nikiforova"],"pdf_url":"https://arxiv.org/pdf/2406.08461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08435v1","updated":"2024-06-12T17:22:00Z","published":"2024-06-12T17:22:00Z","title":"Wiki Entity Summarization Benchmark","summary":"  Entity summarization aims to compute concise summaries for entities in\nknowledge graphs. Existing datasets and benchmarks are often limited to a few\nhundred entities and discard graph structure in source knowledge graphs. This\nlimitation is particularly pronounced when it comes to ground-truth summaries,\nwhere there exist only a few labeled summaries for evaluation and training. We\npropose WikES, a comprehensive benchmark comprising of entities, their\nsummaries, and their connections. Additionally, WikES features a dataset\ngenerator to test entity summarization algorithms in different areas of the\nknowledge graph. Importantly, our approach combines graph algorithms and NLP\nmodels as well as different data sources such that WikES does not require human\nannotation, rendering the approach cost-effective and generalizable to multiple\ndomains. Finally, WikES is scalable and capable of capturing the complexities\nof knowledge graphs in terms of topology and semantics. WikES features existing\ndatasets for comparison. Empirical studies of entity summarization methods\nconfirm the usefulness of our benchmark. Data, code, and models are available\nat: https://github.com/msorkhpar/wiki-entity-summarization.\n","authors":["Saeedeh Javadi","Atefeh Moradan","Mohammad Sorkhpar","Klim Zaporojets","Davide Mottin","Ira Assent"],"pdf_url":"https://arxiv.org/pdf/2406.08435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15053v3","updated":"2024-06-12T16:01:45Z","published":"2023-07-27T17:57:42Z","title":"On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation\n  Metric for Top-$n$ Recommendation","summary":"  Approaches to recommendation are typically evaluated in one of two ways: (1)\nvia a (simulated) online experiment, often seen as the gold standard, or (2)\nvia some offline evaluation procedure, where the goal is to approximate the\noutcome of an online experiment. Several offline evaluation metrics have been\nadopted in the literature, inspired by ranking metrics prevalent in the field\nof Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one\nsuch metric that has seen widespread adoption in empirical studies, and higher\n(n)DCG values have been used to present new methods as the state-of-the-art in\ntop-$n$ recommendation for many years.\n  Our work takes a critical look at this approach, and investigates when we can\nexpect such metrics to approximate the gold standard outcome of an online\nexperiment. We formally present the assumptions that are necessary to consider\nDCG an unbiased estimator of online reward and provide a derivation for this\nmetric from first principles, highlighting where we deviate from its\ntraditional uses in IR. Importantly, we show that normalising the metric\nrenders it inconsistent, in that even when DCG is unbiased, ranking competing\nmethods by their normalised DCG can invert their relative order. Through a\ncorrelation analysis between off- and on-line experiments conducted on a\nlarge-scale recommendation platform, we show that our unbiased DCG estimates\nstrongly correlate with online reward, even when some of the metric's inherent\nassumptions are violated. This statement no longer holds for its normalised\nvariant, suggesting that nDCG's practical utility may be limited.\n","authors":["Olivier Jeunen","Ivan Potapov","Aleksei Ustimenko"],"pdf_url":"https://arxiv.org/pdf/2307.15053v3.pdf","comment":"To appear in the research track at the ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining (KDD '24)"},{"id":"http://arxiv.org/abs/2310.14037v4","updated":"2024-06-12T15:08:09Z","published":"2023-10-21T15:21:39Z","title":"MARVEL: Unlocking the Multi-Modal Capability of Dense Retrieval via\n  Visual Module Plugin","summary":"  This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin\n(MARVEL), which learns an embedding space for queries and multi-modal documents\nto conduct retrieval. MARVEL encodes queries and multi-modal documents with a\nunified encoder model, which helps to alleviate the modality gap between images\nand texts. Specifically, we enable the image understanding ability of the\nwell-trained dense retriever, T5-ANCE, by incorporating the visual module's\nencoded image features as its inputs. To facilitate the multi-modal retrieval\ntasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which\nregards anchor texts as queries, and extracts the related text and image\ndocuments from anchor-linked web pages. Our experiments show that MARVEL\nsignificantly outperforms the state-of-the-art methods on the multi-modal\nretrieval dataset WebQA and ClueWeb22-MM. MARVEL provides an opportunity to\nbroaden the advantages of text retrieval to the multi-modal scenario. Besides,\nwe also illustrate that the language model has the ability to extract image\nsemantics and partly map the image features to the input word embedding space.\nAll codes are available at https://github.com/OpenMatch/MARVEL.\n","authors":["Tianshuo Zhou","Sen Mei","Xinze Li","Zhenghao Liu","Chenyan Xiong","Zhiyuan Liu","Yu Gu","Ge Yu"],"pdf_url":"https://arxiv.org/pdf/2310.14037v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08270v1","updated":"2024-06-12T14:35:43Z","published":"2024-06-12T14:35:43Z","title":"Boosting Multimedia Recommendation via Separate Generic and Unique\n  Awareness","summary":"  Multimedia recommendation, which incorporates various modalities (e.g.,\nimages, texts, etc.) into user or item representation to improve recommendation\nquality, has received widespread attention. Recent methods mainly focus on\ncross-modal alignment with self-supervised learning to obtain higher quality\nrepresentation. Despite remarkable performance, we argue that there is still a\nlimitation: completely aligning representation undermines modality-unique\ninformation. We consider that cross-modal alignment is right, but it should not\nbe the entirety, as different modalities contain generic information between\nthem, and each modality also contains unique information. Simply aligning each\nmodality may ignore modality-unique features, thus degrading the performance of\nmultimedia recommendation. To tackle the above limitation, we propose a\nSeparate Alignment aNd Distancing framework (SAND) for multimedia\nrecommendation, which concurrently learns both modal-unique and -generic\nrepresentation to achieve more comprehensive items representation. First, we\nsplit each modal feature into generic and unique part. Then, in the alignment\nmodule, for better integration of semantic information between different\nmodalities , we design a SoloSimLoss to align generic modalities. Furthermore,\nin the distancing module, we aim to distance the unique modalities from the\nmodal-generic so that each modality retains its unique and complementary\ninformation. In the light of the flexibility of our framework, we give two\ntechnical solutions, the more capable mutual information minimization and the\nsimple negative l2 distance. Finally, extensive experimental results on three\npopular datasets demonstrate the effectiveness and generalization of our\nproposed framework.\n","authors":["Zhuangzhuang He","Zihan Wang","Yonghui Yang","Haoyue Bai","Le Wu"],"pdf_url":"https://arxiv.org/pdf/2406.08270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08229v1","updated":"2024-06-12T13:59:31Z","published":"2024-06-12T13:59:31Z","title":"GPT4Rec: Graph Prompt Tuning for Streaming Recommendation","summary":"  In the realm of personalized recommender systems, the challenge of adapting\nto evolving user preferences and the continuous influx of new users and items\nis paramount. Conventional models, typically reliant on a static training-test\napproach, struggle to keep pace with these dynamic demands. Streaming\nrecommendation, particularly through continual graph learning, has emerged as a\nnovel solution. However, existing methods in this area either rely on\nhistorical data replay, which is increasingly impractical due to stringent data\nprivacy regulations; or are inability to effectively address the over-stability\nissue; or depend on model-isolation and expansion strategies. To tackle these\ndifficulties, we present GPT4Rec, a Graph Prompt Tuning method for streaming\nRecommendation. Given the evolving user-item interaction graph, GPT4Rec first\ndisentangles the graph patterns into multiple views. After isolating specific\ninteraction patterns and relationships in different views, GPT4Rec utilizes\nlightweight graph prompts to efficiently guide the model across varying\ninteraction patterns within the user-item graph. Firstly, node-level prompts\nare employed to instruct the model to adapt to changes in the attributes or\nproperties of individual nodes within the graph. Secondly, structure-level\nprompts guide the model in adapting to broader patterns of connectivity and\nrelationships within the graph. Finally, view-level prompts are innovatively\ndesigned to facilitate the aggregation of information from multiple\ndisentangled views. These prompt designs allow GPT4Rec to synthesize a\ncomprehensive understanding of the graph, ensuring that all vital aspects of\nthe user-item interactions are considered and effectively integrated.\nExperiments on four diverse real-world datasets demonstrate the effectiveness\nand efficiency of our proposal.\n","authors":["Peiyan Zhang","Yuchen Yan","Xi Zhang","Liying Kang","Chaozhuo Li","Feiran Huang","Senzhang Wang","Sunghun Kim"],"pdf_url":"https://arxiv.org/pdf/2406.08229v1.pdf","comment":"Accepted by SIGIR 2024. arXiv admin note: text overlap with\n  arXiv:2303.11700 by other authors"},{"id":"http://arxiv.org/abs/2406.08214v1","updated":"2024-06-12T13:44:22Z","published":"2024-06-12T13:44:22Z","title":"Graph Bottlenecked Social Recommendation","summary":"  With the emergence of social networks, social recommendation has become an\nessential technique for personalized services. Recently, graph-based social\nrecommendations have shown promising results by capturing the high-order social\ninfluence. Most empirical studies of graph-based social recommendations\ndirectly take the observed social networks into formulation, and produce user\npreferences based on social homogeneity. Despite the effectiveness, we argue\nthat social networks in the real-world are inevitably noisy~(existing redundant\nsocial relations), which may obstruct precise user preference characterization.\nNevertheless, identifying and removing redundant social relations is\nchallenging due to a lack of labels. In this paper, we focus on learning the\ndenoised social structure to facilitate recommendation tasks from an\ninformation bottleneck perspective. Specifically, we propose a novel Graph\nBottlenecked Social Recommendation (GBSR) framework to tackle the social noise\nissue.GBSR is a model-agnostic social denoising framework, that aims to\nmaximize the mutual information between the denoised social graph and\nrecommendation labels, meanwhile minimizing it between the denoised social\ngraph and the original one. This enables GBSR to learn the minimal yet\nsufficient social structure, effectively reducing redundant social relations\nand enhancing social recommendations. Technically, GBSR consists of two\nelaborate components, preference-guided social graph refinement, and HSIC-based\nbottleneck learning. Extensive experimental results demonstrate the superiority\nof the proposed GBSR, including high performances and good generality combined\nwith various backbones. Our code is available at:\nhttps://github.com/yimutianyang/KDD24-GBSR.\n","authors":["Yonghui Yang","Le Wu","Zihan Wang","Zhuangzhuang He","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08214v1.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2309.15408v2","updated":"2024-06-12T13:15:11Z","published":"2023-09-27T05:20:53Z","title":"A smoothed-Bayesian approach to frequency recovery from sketched data","summary":"  We provide a novel statistical perspective on a classical problem at the\nintersection of computer science and information theory: recovering the\nempirical frequency of a symbol in a large discrete dataset using only a\ncompressed representation, or sketch, obtained via random hashing. Departing\nfrom traditional algorithmic approaches, recent works have proposed Bayesian\nnonparametric (BNP) methods that can provide more informative frequency\nestimates by leveraging modeling assumptions about the distribution of the\nsketched data. In this paper, we propose a {\\em smoothed-Bayesian} method,\ninspired by existing BNP approaches but designed in a frequentist framework to\novercome the computational limitations of the BNP approaches when dealing with\nlarge-scale data from realistic distributions, including those with power-law\ntail behaviors. For sketches obtained with a single hash function, our approach\nis supported by rigorous frequentist properties, including unbiasedness and\noptimality under a squared error loss function within an intuitive class of\nlinear estimators. For sketches with multiple hash functions, we introduce an\napproach based on \\emph{multi-view} learning to construct computationally\nefficient frequency estimators. We validate our method on synthetic and real\ndata, comparing its performance to that of existing alternatives.\n","authors":["Mario Beraha","Stefano Favaro","Matteo Sesia"],"pdf_url":"https://arxiv.org/pdf/2309.15408v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04288v4","updated":"2024-06-12T12:37:43Z","published":"2022-10-09T15:42:36Z","title":"CoopHash: Cooperative Learning of Multipurpose Descriptor and\n  Contrastive Pair Generator via Variational MCMC Teaching for Supervised Image\n  Hashing","summary":"  Leveraging supervised information can lead to superior retrieval performance\nin the image hashing domain but the performance degrades significantly without\nenough labeled data. One effective solution to boost performance is to employ\ngenerative models, such as Generative Adversarial Networks (GANs), to generate\nsynthetic data in an image hashing model. However, GAN-based methods are\ndifficult to train, which prevents the hashing approaches from jointly training\nthe generative models and the hash functions. This limitation results in\nsub-optimal retrieval performance. To overcome this limitation, we propose a\nnovel framework, the generative cooperative hashing network, which is based on\nenergy-based cooperative learning. This framework jointly learns a powerful\ngenerative representation of the data and a robust hash function via two\ncomponents: a top-down contrastive pair generator that synthesizes contrastive\nimages and a bottom-up multipurpose descriptor that simultaneously represents\nthe images from multiple perspectives, including probability density, hash\ncode, latent code, and category. The two components are jointly learned via a\nnovel likelihood-based cooperative learning scheme. We conduct experiments on\nseveral real-world datasets and show that the proposed method outperforms the\ncompeting hashing supervised methods, achieving up to 10\\% relative improvement\nover the current state-of-the-art supervised hashing methods, and exhibits a\nsignificantly better performance in out-of-distribution retrieval.\n","authors":["Khoa D. Doan","Jianwen Xie","Yaxuan Zhu","Yang Zhao","Ping Li"],"pdf_url":"https://arxiv.org/pdf/2210.04288v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12732v3","updated":"2024-06-12T11:44:34Z","published":"2024-01-23T13:06:19Z","title":"CDRNP: Cross-Domain Recommendation to Cold-Start Users via Neural\n  Process","summary":"  Cross-domain recommendation (CDR) has been proven as a promising way to\ntackle the user cold-start problem, which aims to make recommendations for\nusers in the target domain by transferring the user preference derived from the\nsource domain. Traditional CDR studies follow the embedding and mapping (EMCDR)\nparadigm, which transfers user representations from the source to target domain\nby learning a user-shared mapping function, neglecting the user-specific\npreference. Recent CDR studies attempt to learn user-specific mapping functions\nin meta-learning paradigm, which regards each user's CDR as an individual task,\nbut neglects the preference correlations among users, limiting the beneficial\ninformation for user representations. Moreover, both of the paradigms neglect\nthe explicit user-item interactions from both domains during the mapping\nprocess. To address the above issues, this paper proposes a novel CDR framework\nwith neural process (NP), termed as CDRNP. Particularly, it develops the\nmeta-learning paradigm to leverage user-specific preference, and further\nintroduces a stochastic process by NP to capture the preference correlations\namong the overlapping and cold-start users, thus generating more powerful\nmapping functions by mapping the user-specific preference and common preference\ncorrelations to a predictive probability distribution. In addition, we also\nintroduce a preference remainer to enhance the common preference from the\noverlapping users, and finally devises an adaptive conditional decoder with\npreference modulation to make prediction for cold-start users with items in the\ntarget domain. Experimental results demonstrate that CDRNP outperforms previous\nSOTA methods in three real-world CDR scenarios.\n","authors":["Xiaodong Li","Jiawei Sheng","Jiangxia Cao","Wenyuan Zhang","Quangang Li","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2401.12732v3.pdf","comment":"WSDM'2024 Oral"},{"id":"http://arxiv.org/abs/2406.08105v1","updated":"2024-06-12T11:34:19Z","published":"2024-06-12T11:34:19Z","title":"Prediction of the Realisation of an Information Need: An EEG Study","summary":"  One of the foundational goals of Information Retrieval (IR) is to satisfy\nsearchers' Information Needs (IN). Understanding how INs physically manifest\nhas long been a complex and elusive process. However, recent studies utilising\nElectroencephalography (EEG) data have provided real-time insights into the\nneural processes associated with INs. Unfortunately, they have yet to\ndemonstrate how this insight can practically benefit the search experience. As\nsuch, within this study, we explore the ability to predict the realisation of\nIN within EEG data across 14 subjects whilst partaking in a Question-Answering\n(Q/A) task. Furthermore, we investigate the combinations of EEG features that\nyield optimal predictive performance, as well as identify regions within the\nQ/A queries where a subject's realisation of IN is more pronounced. The\nfindings from this work demonstrate that EEG data is sufficient for the\nreal-time prediction of the realisation of an IN across all subjects with an\naccuracy of 73.5\\% (SD 2.6\\%) and on a per-subject basis with an accuracy of\n90.1\\% (SD 22.1\\%). This work helps to close the gap by bridging theoretical\nneuroscientific advancements with tangible improvements in information\nretrieval practices, paving the way for real-time prediction of the realisation\nof IN.\n","authors":["Niall McGuire","Dr Yashar Moshfeghi"],"pdf_url":"https://arxiv.org/pdf/2406.08105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02587v2","updated":"2024-06-12T09:34:43Z","published":"2024-04-03T09:12:22Z","title":"The Surprising Effectiveness of Rankers Trained on Expanded Queries","summary":"  An important problem in text-ranking systems is handling the hard queries\nthat form the tail end of the query distribution. The difficulty may arise due\nto the presence of uncommon, underspecified, or incomplete queries. In this\nwork, we improve the ranking performance of hard or difficult queries without\ncompromising the performance of other queries. Firstly, we do LLM based query\nenrichment for training queries using relevant documents. Next, a specialized\nranker is fine-tuned only on the enriched hard queries instead of the original\nqueries. We combine the relevance scores from the specialized ranker and the\nbase ranker, along with a query performance score estimated for each query. Our\napproach departs from existing methods that usually employ a single ranker for\nall queries, which is biased towards easy queries, which form the majority of\nthe query distribution. In our extensive experiments on the DL-Hard dataset, we\nfind that a principled query performance based scoring method using base and\nspecialized ranker offers a significant improvement of up to 25% on the passage\nranking task and up to 48.4% on the document ranking task when compared to the\nbaseline performance of using original queries, even outperforming SOTA model.\n","authors":["Abhijit Anand","Venktesh V","Vinay Setty","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2404.02587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08010v1","updated":"2024-06-12T09:00:49Z","published":"2024-06-12T09:00:49Z","title":"A Self-boosted Framework for Calibrated Ranking","summary":"  Scale-calibrated ranking systems are ubiquitous in real-world applications\nnowadays, which pursue accurate ranking quality and calibrated probabilistic\npredictions simultaneously. For instance, in the advertising ranking system,\nthe predicted click-through rate (CTR) is utilized for ranking and required to\nbe calibrated for the downstream cost-per-click ads bidding. Recently,\nmulti-objective based methods have been wildly adopted as a standard approach\nfor Calibrated Ranking, which incorporates the combination of two loss\nfunctions: a pointwise loss that focuses on calibrated absolute values and a\nranking loss that emphasizes relative orderings. However, when applied to\nindustrial online applications, existing multi-objective CR approaches still\nsuffer from two crucial limitations. First, previous methods need to aggregate\nthe full candidate list within a single mini-batch to compute the ranking loss.\nSuch aggregation strategy violates extensive data shuffling which has long been\nproven beneficial for preventing overfitting, and thus degrades the training\neffectiveness. Second, existing multi-objective methods apply the two\ninherently conflicting loss functions on a single probabilistic prediction,\nwhich results in a sub-optimal trade-off between calibration and ranking. To\ntackle the two limitations, we propose a Self-Boosted framework for Calibrated\nRanking (SBCR).\n","authors":["Shunyu Zhang","Hu Liu","Wentian Bao","Enyun Yu","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2406.08010v1.pdf","comment":"KDD 2024"},{"id":"http://arxiv.org/abs/2406.07932v1","updated":"2024-06-12T06:55:35Z","published":"2024-06-12T06:55:35Z","title":"Counteracting Duration Bias in Video Recommendation via Counterfactual\n  Watch Time","summary":"  In video recommendation, an ongoing effort is to satisfy users' personalized\ninformation needs by leveraging their logged watch time. However, watch time\nprediction suffers from duration bias, hindering its ability to reflect users'\ninterests accurately. Existing label-correction approaches attempt to uncover\nuser interests through grouping and normalizing observed watch time according\nto video duration. Although effective to some extent, we found that these\napproaches regard completely played records (i.e., a user watches the entire\nvideo) as equally high interest, which deviates from what we observed on real\ndatasets: users have varied explicit feedback proportion when completely\nplaying videos. In this paper, we introduce the counterfactual watch time(CWT),\nthe potential watch time a user would spend on the video if its duration is\nsufficiently long. Analysis shows that the duration bias is caused by the\ntruncation of CWT due to the video duration limitation, which usually occurs on\nthose completely played records. Besides, a Counterfactual Watch Model (CWM) is\nproposed, revealing that CWT equals the time users get the maximum benefit from\nvideo recommender systems. Moreover, a cost-based transform function is defined\nto transform the CWT into the estimation of user interest, and the model can be\nlearned by optimizing a counterfactual likelihood function defined over\nobserved user watch times. Extensive experiments on three real video\nrecommendation datasets and online A/B testing demonstrated that CWM\neffectively enhanced video recommendation accuracy and counteracted the\nduration bias.\n","authors":["Haiyuan Zhao","Guohao Cai","Jieming Zhu","Zhenhua Dong","Jun Xu","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2406.07932v1.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2406.07913v1","updated":"2024-06-12T06:33:54Z","published":"2024-06-12T06:33:54Z","title":"DeTriever: Decoder-representation-based Retriever for Improving NL2SQL\n  In-Context Learning","summary":"  While in-context Learning (ICL) has proven to be an effective technique to\nimprove the performance of Large Language Models (LLMs) in a variety of complex\ntasks, notably in translating natural language questions into Structured Query\nLanguage (NL2SQL), the question of how to select the most beneficial\ndemonstration examples remains an open research problem. While prior works\noften adapted off-the-shelf encoders to retrieve examples dynamically, an\ninherent discrepancy exists in the representational capacities between the\nexternal retrievers and the LLMs. Further, optimizing the selection of examples\nis a non-trivial task, since there are no straightforward methods to assess the\nrelative benefits of examples without performing pairwise inference. To address\nthese shortcomings, we propose DeTriever, a novel demonstration retrieval\nframework that learns a weighted combination of LLM hidden states, where rich\nsemantic information is encoded. To train the model, we propose a proxy score\nthat estimates the relative benefits of examples based on the similarities\nbetween output queries. Experiments on two popular NL2SQL benchmarks\ndemonstrate that our method significantly outperforms the state-of-the-art\nbaselines on one-shot NL2SQL tasks.\n","authors":["Yuxi Feng","Raymond Li","Zhenan Fan","Giuseppe Carenini","Mohammadreza Pourreza","Weiwei Zhang","Yong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05666v2","updated":"2024-06-12T05:49:50Z","published":"2024-06-09T06:49:22Z","title":"General Distribution Learning: A theoretical framework for Deep Learning","summary":"  There remain numerous unanswered research questions on deep learning (DL)\nwithin the classical learning theory framework. These include the remarkable\ngeneralization capabilities of overparametrized neural networks (NNs), the\nefficient optimization performance despite non-convexity of objectives, the\nmechanism of flat minima for generalization, and the exceptional performance of\ndeep architectures in solving physical problems. This paper introduces General\nDistribution Learning (GD Learning), a novel theoretical learning framework\ndesigned to address a comprehensive range of machine learning and statistical\ntasks, including classification, regression and parameter estimation. Departing\nfrom traditional statistical machine learning, GD Learning focuses on the true\nunderlying distribution. In GD Learning, learning error, corresponding to the\nexpected error in classical statistical learning framework, is divided into\nfitting errors due to models and algorithms, as well as sampling errors\nintroduced by limited sampling data. The framework significantly incorporates\nprior knowledge, especially in scenarios characterized by data scarcity,\nthereby enhancing performance. Within the GD Learning framework, we demonstrate\nthat the global optimal solutions in non-convex optimization can be approached\nby minimizing the gradient norm and the non-uniformity of the eigenvalues of\nthe model's Jacobian matrix. This insight leads to the development of the\ngradient structure control algorithm. GD Learning also offers fresh insights\ninto the questions on deep learning, including overparameterization and\nnon-convex optimization, bias-variance trade-off, and the mechanism of flat\nminima.\n","authors":["Binchuan Qi","Li Li","Wei Gong"],"pdf_url":"https://arxiv.org/pdf/2406.05666v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors"},{"id":"http://arxiv.org/abs/2402.18150v2","updated":"2024-06-12T03:21:15Z","published":"2024-02-28T08:24:38Z","title":"Unsupervised Information Refinement Training of Large Language Models\n  for Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating additional information from retrieval. However, studies have\nshown that LLMs still face challenges in effectively using the retrieved\ninformation, even ignoring it or being misled by it. The key reason is that the\ntraining of LLMs does not clearly make LLMs learn how to utilize input\nretrieved texts with varied quality. In this paper, we propose a novel\nperspective that considers the role of LLMs in RAG as ``Information Refiner'',\nwhich means that regardless of correctness, completeness, or usefulness of\nretrieved texts, LLMs can consistently integrate knowledge within the retrieved\ntexts and model parameters to generate the texts that are more concise,\naccurate, and complete than the retrieved texts. To this end, we propose an\ninformation refinement training method named InFO-RAG that optimizes LLMs for\nRAG in an unsupervised manner. InFO-RAG is low-cost and general across various\ntasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse\ntasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,\nand Code Generation show that InFO-RAG improves the performance of LLaMA2 by an\naverage of 9.39\\% relative points. InFO-RAG also shows advantages in in-context\nlearning and robustness of RAG.\n","authors":["Shicheng Xu","Liang Pang","Mo Yu","Fandong Meng","Huawei Shen","Xueqi Cheng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.18150v2.pdf","comment":"ACL 2024 Main"},{"id":"http://arxiv.org/abs/2403.17780v3","updated":"2024-06-12T03:14:36Z","published":"2024-03-26T15:13:16Z","title":"CaseLink: Inductive Graph Learning for Legal Case Retrieval","summary":"  In case law, the precedents are the relevant cases that are used to support\nthe decisions made by the judges and the opinions of lawyers towards a given\ncase. This relevance is referred to as the case-to-case reference relation. To\nefficiently find relevant cases from a large case pool, retrieval tools are\nwidely used by legal practitioners. Existing legal case retrieval models mainly\nwork by comparing the text representations of individual cases. Although they\nobtain a decent retrieval accuracy, the intrinsic case connectivity\nrelationships among cases have not been well exploited for case encoding,\ntherefore limiting the further improvement of retrieval performance. In a case\npool, there are three types of case connectivity relationships: the case\nreference relationship, the case semantic relationship, and the case legal\ncharge relationship. Due to the inductive manner in the task of legal case\nretrieval, using case reference as input is not applicable for testing. Thus,\nin this paper, a CaseLink model based on inductive graph learning is proposed\nto utilise the intrinsic case connectivity for legal case retrieval, a novel\nGlobal Case Graph is incorporated to represent both the case semantic\nrelationship and the case legal charge relationship. A novel contrastive\nobjective with a regularisation on the degree of case nodes is proposed to\nleverage the information carried by the case reference relationship to optimise\nthe model. Extensive experiments have been conducted on two benchmark datasets,\nwhich demonstrate the state-of-the-art performance of CaseLink. The code has\nbeen released on https://github.com/yanran-tang/CaseLink.\n","authors":["Yanran Tang","Ruihong Qiu","Hongzhi Yin","Xue Li","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17780v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10835v2","updated":"2024-06-12T02:38:21Z","published":"2024-05-17T14:57:52Z","title":"A Unified Search and Recommendation Framework Based on Multi-Scenario\n  Learning for Ranking in E-commerce","summary":"  Search and recommendation (S&R) are the two most important scenarios in\ne-commerce. The majority of users typically interact with products in S&R\nscenarios, indicating the need and potential for joint modeling. Traditional\nmulti-scenario models use shared parameters to learn the similarity of multiple\ntasks, and task-specific parameters to learn the divergence of individual\ntasks. This coarse-grained modeling approach does not effectively capture the\ndifferences between S&R scenarios. Furthermore, this approach does not\nsufficiently exploit the information across the global label space. These\nissues can result in the suboptimal performance of multi-scenario models in\nhandling both S&R scenarios. To address these issues, we propose an effective\nand universal framework for Unified Search and Recommendation (USR), designed\nwith S&R Views User Interest Extractor Layer (IE) and S&R Views Feature\nGenerator Layer (FG) to separately generate user interests and\nscenario-agnostic feature representations for S&R. Next, we introduce a Global\nLabel Space Multi-Task Layer (GLMT) that uses global labels as supervised\nsignals of auxiliary tasks and jointly models the main task and auxiliary tasks\nusing conditional probability. Extensive experimental evaluations on real-world\nindustrial datasets show that USR can be applied to various multi-scenario\nmodels and significantly improve their performance. Online A/B testing also\nindicates substantial performance gains across multiple metrics. Currently, USR\nhas been successfully deployed in the 7Fresh App.\n","authors":["Jinhan Liu","Qiyu Chen","Junjie Xu","Junjie Li","Baoli Li","Sulong Xu"],"pdf_url":"https://arxiv.org/pdf/2405.10835v2.pdf","comment":"Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2310.07815v3","updated":"2024-06-12T20:36:22Z","published":"2023-10-11T18:56:15Z","title":"Language Models As Semantic Indexers","summary":"  Semantic identifier (ID) is an important concept in information retrieval\nthat aims to preserve the semantics of objects such as documents and items\ninside their IDs. Previous studies typically adopt a two-stage pipeline to\nlearn semantic IDs by first procuring embeddings using off-the-shelf text\nencoders and then deriving IDs based on the embeddings. However, each step\nintroduces potential information loss, and there is usually an inherent\nmismatch between the distribution of embeddings within the latent space\nproduced by text encoders and the anticipated distribution required for\nsemantic indexing. It is non-trivial to design a method that can learn the\ndocument's semantic representations and its hierarchical structure\nsimultaneously, given that semantic IDs are discrete and sequentially\nstructured, and the semantic supervision is deficient. In this paper, we\nintroduce LMIndexer, a self-supervised framework to learn semantic IDs with a\ngenerative language model. We tackle the challenge of sequential discrete ID by\nintroducing a semantic indexer capable of generating neural sequential discrete\nrepresentations with progressive training and contrastive learning. In response\nto the semantic supervision deficiency, we propose to train the model with a\nself-supervised document reconstruction objective. We show the high quality of\nthe learned IDs and demonstrate their effectiveness on three tasks including\nrecommendation, product search, and document retrieval on five datasets from\nvarious domains. Code is available at\nhttps://github.com/PeterGriffinJin/LMIndexer.\n","authors":["Bowen Jin","Hansi Zeng","Guoyin Wang","Xiusi Chen","Tianxin Wei","Ruirui Li","Zhengyang Wang","Zheng Li","Yang Li","Hanqing Lu","Suhang Wang","Jiawei Han","Xianfeng Tang"],"pdf_url":"https://arxiv.org/pdf/2310.07815v3.pdf","comment":"10 pages, 5 appendix pages"},{"id":"http://arxiv.org/abs/2406.08633v1","updated":"2024-06-12T20:30:34Z","published":"2024-06-12T20:30:34Z","title":"Unraveling Code-Mixing Patterns in Migration Discourse: Automated\n  Detection and Analysis of Online Conversations on Reddit","summary":"  The surge in global migration patterns underscores the imperative of\nintegrating migrants seamlessly into host communities, necessitating inclusive\nand trustworthy public services. Despite the Nordic countries' robust public\nsector infrastructure, recent immigrants often encounter barriers to accessing\nthese services, exacerbating social disparities and eroding trust. Addressing\ndigital inequalities and linguistic diversity is paramount in this endeavor.\nThis paper explores the utilization of code-mixing, a communication strategy\nprevalent among multilingual speakers, in migration-related discourse on social\nmedia platforms such as Reddit. We present Ensemble Learning for Multilingual\nIdentification of Code-mixed Texts (ELMICT), a novel approach designed to\nautomatically detect code-mixed messages in migration-related discussions.\nLeveraging ensemble learning techniques for combining multiple tokenizers'\noutputs and pre-trained language models, ELMICT demonstrates high performance\n(with F1 more than 0.95) in identifying code-mixing across various languages\nand contexts, particularly in cross-lingual zero-shot conditions (with avg. F1\nmore than 0.70). Moreover, the utilization of ELMICT helps to analyze the\nprevalence of code-mixing in migration-related threads compared to other\nthematic categories on Reddit, shedding light on the topics of concern to\nmigrant communities. Our findings reveal insights into the communicative\nstrategies employed by migrants on social media platforms, offering\nimplications for the development of inclusive digital public services and\nconversational systems. By addressing the research questions posed in this\nstudy, we contribute to the understanding of linguistic diversity in migration\ndiscourse and pave the way for more effective tools for building trust in\nmulticultural societies.\n","authors":["Fedor Vitiugin","Sunok Lee","Henna Paakki","Anastasiia Chizhikova","Nitin Sawhney"],"pdf_url":"https://arxiv.org/pdf/2406.08633v1.pdf","comment":"10 pages, 3 figures, Workshop Proceedings of the 18th International\n  AAAI Conference on Web and Social Media"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2406.05132v2","updated":"2024-06-12T17:59:58Z","published":"2024-06-07T17:59:59Z","title":"3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and\n  Less Hallucination","summary":"  The integration of language and 3D perception is crucial for developing\nembodied agents and robots that comprehend and interact with the physical\nworld. While large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, their adaptation to 3D environments\n(3D-LLMs) remains in its early stages. A primary challenge is the absence of\nlarge-scale datasets that provide dense grounding between language and 3D\nscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset\ncomprising 40,087 household scenes paired with 6.2 million densely-grounded\nscene-language instructions. Our results show that instruction tuning with\n3D-GRAND significantly enhances grounding capabilities and reduces\nhallucinations in 3D-LLMs. As part of our contributions, we propose a\ncomprehensive benchmark 3D-POPE to systematically evaluate hallucination in\n3D-LLMs, enabling fair comparisons among future models. Our experiments\nhighlight a scaling effect between dataset size and 3D-LLM performance,\nemphasizing the critical role of large-scale 3D-text datasets in advancing\nembodied AI research. Notably, our results demonstrate early signals for\neffective sim-to-real transfer, indicating that models trained on large\nsynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and\n3D-POPE, we aim to equip the embodied AI community with essential resources and\ninsights, setting the stage for more reliable and better-grounded 3D-LLMs.\nProject website: https://3d-grand.github.io\n","authors":["Jianing Yang","Xuweiyi Chen","Nikhil Madaan","Madhavan Iyengar","Shengyi Qian","David F. Fouhey","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2406.05132v2.pdf","comment":"Project website: https://3d-grand.github.io"},{"id":"http://arxiv.org/abs/2402.13254v4","updated":"2024-06-12T17:59:55Z","published":"2024-02-20T18:59:55Z","title":"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples","summary":"  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V. To facilitate future research, we release our code, dataset, benchmark,\nand checkpoints at https://countercurate.github.io.\n","authors":["Jianrui Zhang","Mu Cai","Tengyang Xie","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2402.13254v4.pdf","comment":"15 pages, 6 figures, 12 tables, Project Page:\n  https://countercurate.github.io/"},{"id":"http://arxiv.org/abs/2406.08488v1","updated":"2024-06-12T17:59:52Z","published":"2024-06-12T17:59:52Z","title":"ICE-G: Image Conditional Editing of 3D Gaussian Splats","summary":"  Recently many techniques have emerged to create high quality 3D assets and\nscenes. When it comes to editing of these objects, however, existing approaches\nare either slow, compromise on quality, or do not provide enough customization.\nWe introduce a novel approach to quickly edit a 3D model from a single\nreference view. Our technique first segments the edit image, and then matches\nsemantically corresponding regions across chosen segmented dataset views using\nDINO features. A color or texture change from a particular region of the edit\nimage can then be applied to other views automatically in a semantically\nsensible manner. These edited views act as an updated dataset to further train\nand re-style the 3D scene. The end-result is therefore an edited 3D model. Our\nframework enables a wide variety of editing tasks such as manual local edits,\ncorrespondence based style transfer from any example image, and a combination\nof different styles from multiple example images. We use Gaussian Splats as our\nprimary 3D representation due to their speed and ease of local editing, but our\ntechnique works for other methods such as NeRFs as well. We show through\nmultiple examples that our method produces higher quality results while\noffering fine-grained control of editing. Project page: ice-gaussian.github.io\n","authors":["Vishnu Jaganathan","Hannah Hanyun Huang","Muhammad Zubair Irshad","Varun Jampani","Amit Raj","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2406.08488v1.pdf","comment":"Accepted to CVPR AI4CC Workshop 2024. Project page:\n  https://ice-gaussian.github.io"},{"id":"http://arxiv.org/abs/2406.08474v1","updated":"2024-06-12T17:57:06Z","published":"2024-06-12T17:57:06Z","title":"Real2Code: Reconstruct Articulated Objects via Code Generation","summary":"  We present Real2Code, a novel approach to reconstructing articulated objects\nvia code generation. Given visual observations of an object, we first\nreconstruct its part geometry using an image segmentation model and a shape\ncompletion model. We then represent the object parts with oriented bounding\nboxes, which are input to a fine-tuned large language model (LLM) to predict\njoint articulation as code. By leveraging pre-trained vision and language\nmodels, our approach scales elegantly with the number of articulated parts, and\ngeneralizes from synthetic training data to real world objects in unstructured\nenvironments. Experimental results demonstrate that Real2Code significantly\noutperforms previous state-of-the-art in reconstruction accuracy, and is the\nfirst approach to extrapolate beyond objects' structural complexity in the\ntraining set, and reconstructs objects with up to 10 articulated parts. When\nincorporated with a stereo reconstruction model, Real2Code also generalizes to\nreal world objects from a handful of multi-view RGB images, without the need\nfor depth or camera information.\n","authors":["Zhao Mandi","Yijia Weng","Dominik Bauer","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2406.08474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08473v1","updated":"2024-06-12T17:56:46Z","published":"2024-06-12T17:56:46Z","title":"Strategies for Pretraining Neural Operators","summary":"  Pretraining for partial differential equation (PDE) modeling has recently\nshown promise in scaling neural operators across datasets to improve\ngeneralizability and performance. Despite these advances, our understanding of\nhow pretraining affects neural operators is still limited; studies generally\npropose tailored architectures and datasets that make it challenging to compare\nor examine different pretraining frameworks. To address this, we compare\nvarious pretraining methods without optimizing architecture choices to\ncharacterize pretraining dynamics on different models and datasets as well as\nto understand its scaling and generalization behavior. We find that pretraining\nis highly dependent on model and dataset choices, but in general transfer\nlearning or physics-based pretraining strategies work best. In addition,\npretraining performance can be further improved by using data augmentations.\nLastly, pretraining is additionally beneficial when fine-tuning in scarce data\nregimes or when generalizing to downstream data similar to the pretraining\ndistribution. Through providing insights into pretraining neural operators for\nphysics prediction, we hope to motivate future work in developing and\nevaluating pretraining methods for PDEs.\n","authors":["Anthony Zhou","Cooper Lorsung","AmirPouya Hemmasian","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2406.08473v1.pdf","comment":"25 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.08472v1","updated":"2024-06-12T17:56:31Z","published":"2024-06-12T17:56:31Z","title":"RILe: Reinforced Imitation Learning","summary":"  Reinforcement Learning has achieved significant success in generating complex\nbehavior but often requires extensive reward function engineering. Adversarial\nvariants of Imitation Learning and Inverse Reinforcement Learning offer an\nalternative by learning policies from expert demonstrations via a\ndiscriminator. Employing discriminators increases their data- and computational\nefficiency over the standard approaches; however, results in sensitivity to\nimperfections in expert data. We propose RILe, a teacher-student system that\nachieves both robustness to imperfect data and efficiency. In RILe, the student\nlearns an action policy while the teacher dynamically adjusts a reward function\nbased on the student's performance and its alignment with expert\ndemonstrations. By tailoring the reward function to both performance of the\nstudent and expert similarity, our system reduces dependence on the\ndiscriminator and, hence, increases robustness against data imperfections.\nExperiments show that RILe outperforms existing methods by 2x in settings with\nlimited or noisy expert data.\n","authors":["Mert Albaba","Sammy Christen","Christoph Gebhardt","Thomas Langarek","Michael J. Black","Otmar Hilliges"],"pdf_url":"https://arxiv.org/pdf/2406.08472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08469v1","updated":"2024-06-12T17:54:54Z","published":"2024-06-12T17:54:54Z","title":"PAL: Pluralistic Alignment Framework for Learning from Heterogeneous\n  Preferences","summary":"  Large foundation models pretrained on raw web-scale data are not readily\ndeployable without additional step of extensive alignment to human preferences.\nSuch alignment is typically done by collecting large amounts of pairwise\ncomparisons from humans (\"Do you prefer output A or B?\") and learning a reward\nmodel or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for a\nhuman's underlying implicit preferences. These methods generally suffer from\nassuming a universal preference shared by all humans, which lacks the\nflexibility of adapting to plurality of opinions and preferences. In this work,\nwe propose PAL, a framework to model human preference complementary to existing\npretraining strategies, which incorporates plurality from the ground up. We\npropose using the ideal point model as a lens to view alignment using\npreference comparisons. Together with our novel reformulation and using mixture\nmodeling, our framework captures the plurality of population preferences while\nsimultaneously learning a common preference latent space across different\npreferences, which can few-shot generalize to new, unseen users. Our approach\nenables us to use the penultimate-layer representation of large foundation\nmodels and simple MLP layers to learn reward functions that are on-par with the\nexisting large state-of-the-art reward models, thereby enhancing efficiency of\nreward modeling significantly. We show that PAL achieves competitive reward\nmodel accuracy compared to strong baselines on 1) Language models with Summary\ndataset ; 2) Image Generative models with Pick-a-Pic dataset ; 3) A new\nsemisynthetic heterogeneous dataset generated using Anthropic Personas.\nFinally, our experiments also highlight the shortcoming of current preference\ndatasets that are created using rigid rubrics which wash away heterogeneity,\nand call for more nuanced data collection approaches.\n","authors":["Daiwei Chen","Yi Chen","Aniket Rege","Ramya Korlakai Vinayak"],"pdf_url":"https://arxiv.org/pdf/2406.08469v1.pdf","comment":"22 pages, 14 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.08467v1","updated":"2024-06-12T17:53:31Z","published":"2024-06-12T17:53:31Z","title":"DafnyBench: A Benchmark for Formal Software Verification","summary":"  We introduce DafnyBench, the largest benchmark of its kind for training and\nevaluating machine learning systems for formal software verification. We test\nthe ability of LLMs such as GPT-4 and Claude 3 to auto-generate enough hints\nfor the Dafny formal verification engine to successfully verify over 750\nprograms with about 53,000 lines of code. The best model and prompting scheme\nachieved 68% success rate, and we quantify how this rate improves when retrying\nwith error message feedback and how it deteriorates with the amount of required\ncode and hints. We hope that DafnyBench will enable rapid improvements from\nthis baseline as LLMs and verification techniques grow in quality.\n","authors":["Chloe Loughridge","Qinyi Sun","Seth Ahrenbach","Federico Cassano","Chuyue Sun","Ying Sheng","Anish Mudide","Md Rakib Hossain Misu","Nada Amin","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2406.08467v1.pdf","comment":"Code & dataset available at: https://github.com/sun-wendy/DafnyBench"},{"id":"http://arxiv.org/abs/2406.08466v1","updated":"2024-06-12T17:53:29Z","published":"2024-06-12T17:53:29Z","title":"Scaling Laws in Linear Regression: Compute, Parameters, and Data","summary":"  Empirically, large-scale deep learning models often satisfy a neural scaling\nlaw: the test error of the trained model improves polynomially as the model\nsize and data size grow. However, conventional wisdom suggests the test error\nconsists of approximation, bias, and variance errors, where the variance error\nincreases with model size. This disagrees with the general form of neural\nscaling laws, which predict that increasing model size monotonically improves\nperformance.\n  We study the theory of scaling laws in an infinite dimensional linear\nregression setup. Specifically, we consider a model with $M$ parameters as a\nlinear function of sketched covariates. The model is trained by one-pass\nstochastic gradient descent (SGD) using $N$ data. Assuming the optimal\nparameter satisfies a Gaussian prior and the data covariance matrix has a\npower-law spectrum of degree $a>1$, we show that the reducible part of the test\nerror is $\\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which\nincreases with $M$, is dominated by the other errors due to the implicit\nregularization of SGD, thus disappearing from the bound. Our theory is\nconsistent with the empirical neural scaling laws and verified by numerical\nsimulation.\n","authors":["Licong Lin","Jingfeng Wu","Sham M. Kakade","Peter L. Bartlett","Jason D. Lee"],"pdf_url":"https://arxiv.org/pdf/2406.08466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08465v1","updated":"2024-06-12T17:53:28Z","published":"2024-06-12T17:53:28Z","title":"Nonconvex Federated Learning on Compact Smooth Submanifolds With\n  Heterogeneous Data","summary":"  Many machine learning tasks, such as principal component analysis and\nlow-rank matrix completion, give rise to manifold optimization problems.\nAlthough there is a large body of work studying the design and analysis of\nalgorithms for manifold optimization in the centralized setting, there are\ncurrently very few works addressing the federated setting. In this paper, we\nconsider nonconvex federated learning over a compact smooth submanifold in the\nsetting of heterogeneous client data. We propose an algorithm that leverages\nstochastic Riemannian gradients and a manifold projection operator to improve\ncomputational efficiency, uses local updates to improve communication\nefficiency, and avoids client drift. Theoretically, we show that our proposed\nalgorithm converges sub-linearly to a neighborhood of a first-order optimal\nsolution by using a novel analysis that jointly exploits the manifold structure\nand properties of the loss functions. Numerical experiments demonstrate that\nour algorithm has significantly smaller computational and communication\noverhead than existing methods.\n","authors":["Jiaojiao Zhang","Jiang Hu","Anthony Man-Cho So","Mikael Johansson"],"pdf_url":"https://arxiv.org/pdf/2406.08465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14552v2","updated":"2024-06-12T17:44:44Z","published":"2023-10-23T04:15:39Z","title":"Knowledge-Induced Medicine Prescribing Network for Medication\n  Recommendation","summary":"  Extensive adoption of electronic health records (EHRs) offers opportunities\nfor their use in various downstream clinical analyses. To accomplish this\npurpose, enriching an EHR cohort with external knowledge (e.g., standardized\nmedical ontology and wealthy semantics) could help us reveal more comprehensive\ninsights via a spectrum of informative relations among medical codes.\nNevertheless, harnessing those beneficial interconnections was scarcely\nexercised, especially in the medication recommendation task. This study\nproposes a novel Knowledge-Induced Medicine Prescribing Network (KindMed) to\nrecommend medicines by inducing knowledge from myriad medical-related external\nsources upon the EHR cohort and rendering interconnected medical codes as\nmedical knowledge graphs (KGs). On top of relation-aware graph representation\nlearning to obtain an adequate embedding over such KGs, we leverage\nhierarchical sequence learning to discover and fuse temporal dynamics of\nclinical (i.e., diagnosis and procedures) and medicine streams across patients'\nhistorical admissions to foster personalized recommendations. Eventually, we\nemploy attentive prescribing that accounts for three essential patient\nrepresentations, i.e., a summary of joint historical medical records, clinical\nprogression, and the current clinical state of patients. We validated the\neffectiveness of our KindMed on the augmented real-world EHR cohorts, achieving\nimproved recommendation performances against a handful of graph-driven\nbaselines.\n","authors":["Ahmad Wisnu Mulyadi","Heung-Il Suk"],"pdf_url":"https://arxiv.org/pdf/2310.14552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08447v1","updated":"2024-06-12T17:38:20Z","published":"2024-06-12T17:38:20Z","title":"The Impact of Initialization on LoRA Finetuning Dynamics","summary":"  In this paper, we study the role of initialization in Low Rank Adaptation\n(LoRA) as originally introduced in Hu et al. (2021). Essentially, to start from\nthe pretrained model as initialization for finetuning, one can either\ninitialize B to zero and A to random (default initialization in PEFT package),\nor vice-versa. In both cases, the product BA is equal to zero at\ninitialization, which makes finetuning starts from the pretrained model. These\ntwo initialization schemes are seemingly similar. They should in-principle\nyield the same performance and share the same optimal learning rate. We\ndemonstrate that this is an incorrect intuition and that the first scheme\n(initializing B to zero and A to random) on average yields better performance\ncompared to the other scheme. Our theoretical analysis shows that the reason\nbehind this might be that the first initialization allows the use of larger\nlearning rates (without causing output instability) compared to the second\ninitialization, resulting in more efficient learning of the first scheme. We\nvalidate our results with extensive experiments on LLMs.\n","authors":["Soufiane Hayou","Nikhil Ghosh","Bin Yu"],"pdf_url":"https://arxiv.org/pdf/2406.08447v1.pdf","comment":"TDLR: Different Initializations lead to completely different\n  finetuning dynamics. One initialization (set A random and B zero) is\n  generally better than the natural opposite initialization. arXiv admin note:\n  text overlap with arXiv:2402.12354"},{"id":"http://arxiv.org/abs/2406.08445v1","updated":"2024-06-12T17:37:09Z","published":"2024-06-12T17:37:09Z","title":"SVSNet+: Enhancing Speaker Voice Similarity Assessment Models with\n  Representations from Speech Foundation Models","summary":"  Representations from pre-trained speech foundation models (SFMs) have shown\nimpressive performance in many downstream tasks. However, the potential\nbenefits of incorporating pre-trained SFM representations into speaker voice\nsimilarity assessment have not been thoroughly investigated. In this paper, we\npropose SVSNet+, a model that integrates pre-trained SFM representations to\nimprove performance in assessing speaker voice similarity. Experimental results\non the Voice Conversion Challenge 2018 and 2020 datasets show that SVSNet+\nincorporating WavLM representations shows significant improvements compared to\nbaseline models. In addition, while fine-tuning WavLM with a small dataset of\nthe downstream task does not improve performance, using the same dataset to\nlearn a weighted-sum representation of WavLM can substantially improve\nperformance. Furthermore, when WavLM is replaced by other SFMs, SVSNet+ still\noutperforms the baseline models and exhibits strong generalization ability.\n","authors":["Chun Yin","Tai-Shih Chi","Yu Tsao","Hsin-Min Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08445v1.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.08443v1","updated":"2024-06-12T17:31:36Z","published":"2024-06-12T17:31:36Z","title":"Transformation-Dependent Adversarial Attacks","summary":"  We introduce transformation-dependent adversarial attacks, a new class of\nthreats where a single additive perturbation can trigger diverse, controllable\nmis-predictions by systematically transforming the input (e.g., scaling,\nblurring, compression). Unlike traditional attacks with static effects, our\nperturbations embed metamorphic properties to enable different adversarial\nattacks as a function of the transformation parameters. We demonstrate the\ntransformation-dependent vulnerability across models (e.g., convolutional\nnetworks and vision transformers) and vision tasks (e.g., image classification\nand object detection). Our proposed geometric and photometric transformations\nenable a range of targeted errors from one crafted input (e.g., higher than 90%\nattack success rate for classifiers). We analyze effects of model architecture\nand type/variety of transformations on attack effectiveness. This work forces a\nparadigm shift by redefining adversarial inputs as dynamic, controllable\nthreats. We highlight the need for robust defenses against such multifaceted,\nchameleon-like perturbations that current techniques are ill-prepared for.\n","authors":["Yaoteng Tan","Zikui Cai","M. Salman Asif"],"pdf_url":"https://arxiv.org/pdf/2406.08443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06374v2","updated":"2024-06-12T17:30:24Z","published":"2023-11-10T20:02:58Z","title":"Higher-Order Newton Methods with Polynomial Work per Iteration","summary":"  We present generalizations of Newton's method that incorporate derivatives of\nan arbitrary order $d$ but maintain a polynomial dependence on dimension in\ntheir cost per iteration. At each step, our $d^{\\text{th}}$-order method uses\nsemidefinite programming to construct and minimize a sum of squares-convex\napproximation to the $d^{\\text{th}}$-order Taylor expansion of the function we\nwish to minimize. We prove that our $d^{\\text{th}}$-order method has local\nconvergence of order $d$. This results in lower oracle complexity compared to\nthe classical Newton method. We show on numerical examples that basins of\nattraction around local minima can get larger as $d$ increases. Under\nadditional assumptions, we present a modified algorithm, again with polynomial\ncost per iteration, which is globally convergent and has local convergence of\norder $d$.\n","authors":["Amir Ali Ahmadi","Abraar Chaudhry","Jeffrey Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.06374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08440v1","updated":"2024-06-12T17:26:54Z","published":"2024-06-12T17:26:54Z","title":"Adaptive Swarm Mesh Refinement using Deep Reinforcement Learning with\n  Local Rewards","summary":"  Simulating physical systems is essential in engineering, but analytical\nsolutions are limited to straightforward problems. Consequently, numerical\nmethods like the Finite Element Method (FEM) are widely used. However, the FEM\nbecomes computationally expensive as problem complexity and accuracy demands\nincrease. Adaptive Mesh Refinement (AMR) improves the FEM by dynamically\nallocating mesh elements on the domain, balancing computational speed and\naccuracy. Classical AMR depends on heuristics or expensive error estimators,\nlimiting its use in complex simulations. While learning-based AMR methods are\npromising, they currently only scale to simple problems. In this work, we\nformulate AMR as a system of collaborating, homogeneous agents that iteratively\nsplit into multiple new agents. This agent-wise perspective enables a spatial\nreward formulation focused on reducing the maximum mesh element error. Our\napproach, Adaptive Swarm Mesh Refinement (ASMR), offers efficient, stable\noptimization and generates highly adaptive meshes at user-defined resolution\nduring inference. Extensive experiments, including volumetric meshes and\nNeumann boundary conditions, demonstrate that ASMR exceeds heuristic approaches\nand learned baselines, matching the performance of expensive error-based oracle\nAMR strategies. ASMR additionally generalizes to different domains during\ninference, and produces meshes that simulate up to 2 orders of magnitude faster\nthan uniform refinements in more demanding settings.\n","authors":["Niklas Freymuth","Philipp Dahlinger","Tobias Würth","Simon Reisch","Luise Kärger","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2406.08440v1.pdf","comment":"Submitted to Journal of Machine Learning Research (JMLR)"},{"id":"http://arxiv.org/abs/2405.19277v3","updated":"2024-06-12T17:24:00Z","published":"2024-05-29T17:07:33Z","title":"Deep Latent Variable Modeling of Physiological Signals","summary":"  A deep latent variable model is a powerful method for capturing complex\ndistributions. These models assume that underlying structures, but unobserved,\nare present within the data. In this dissertation, we explore high-dimensional\nproblems related to physiological monitoring using latent variable models.\nFirst, we present a novel deep state-space model to generate electrical\nwaveforms of the heart using optically obtained signals as inputs. This can\nbring about clinical diagnoses of heart disease via simple assessment through\nwearable devices. Second, we present a brain signal modeling scheme that\ncombines the strengths of probabilistic graphical models and deep adversarial\nlearning. The structured representations can provide interpretability and\nencode inductive biases to reduce the data complexity of neural oscillations.\nThe efficacy of the learned representations is further studied in epilepsy\nseizure detection formulated as an unsupervised learning problem. Third, we\npropose a framework for the joint modeling of physiological measures and\nbehavior. Existing methods to combine multiple sources of brain data provided\nare limited. Direct analysis of the relationship between different types of\nphysiological measures usually does not involve behavioral data. Our method can\nidentify the unique and shared contributions of brain regions to behavior and\ncan be used to discover new functions of brain regions. The success of these\ninnovative computational methods would allow the translation of biomarker\nfindings across species and provide insight into neurocognitive analysis in\nnumerous biological studies and clinical diagnoses, as well as emerging\nconsumer applications.\n","authors":["Khuong Vo"],"pdf_url":"https://arxiv.org/pdf/2405.19277v3.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2304.07647v4","updated":"2024-06-12T17:16:39Z","published":"2023-04-15T22:24:05Z","title":"LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene\n  Graphs with Weak Supervision","summary":"  We propose LASER, a neuro-symbolic approach to learn semantic video\nrepresentations that capture rich spatial and temporal properties in video data\nby leveraging high-level logic specifications. In particular, we formulate the\nproblem in terms of alignment between raw videos and spatio-temporal logic\nspecifications. The alignment algorithm leverages a differentiable symbolic\nreasoner and a combination of contrastive, temporal, and semantics losses. It\neffectively and efficiently trains low-level perception models to extract a\nfine-grained video representation in the form of a spatio-temporal scene graph\nthat conforms to the desired high-level specification. To practically reduce\nthe manual effort of obtaining ground truth labels, we derive logic\nspecifications from captions by employing a large language model with a generic\nprompting template. In doing so, we explore a novel methodology that weakly\nsupervises the learning of spatio-temporal scene graphs with widely accessible\nvideo-caption data. We evaluate our method on three datasets with rich spatial\nand temporal specifications: 20BN-Something-Something, MUGEN, and OpenPVSG. We\ndemonstrate that our method learns better fine-grained video semantics than\nexisting baselines.\n","authors":["Jiani Huang","Ziyang Li","Mayur Naik","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2304.07647v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08431v1","updated":"2024-06-12T17:16:16Z","published":"2024-06-12T17:16:16Z","title":"Diffusion Soup: Model Merging for Text-to-Image Diffusion Models","summary":"  We present Diffusion Soup, a compartmentalization method for Text-to-Image\nGeneration that averages the weights of diffusion models trained on sharded\ndata. By construction, our approach enables training-free continual learning\nand unlearning with no additional memory or inference costs, since models\ncorresponding to data shards can be added or removed by re-averaging. We show\nthat Diffusion Soup samples from a point in weight space that approximates the\ngeometric mean of the distributions of constituent datasets, which offers\nanti-memorization guarantees and enables zero-shot style mixing. Empirically,\nDiffusion Soup outperforms a paragon model trained on the union of all data\nshards and achieves a 30% improvement in Image Reward (.34 $\\to$ .44) on domain\nsharded data, and a 59% improvement in IR (.37 $\\to$ .59) on aesthetic data. In\nboth cases, souping also prevails in TIFA score (respectively, 85.5 $\\to$ 86.5\nand 85.6 $\\to$ 86.8). We demonstrate robust unlearning -- removing any\nindividual domain shard only lowers performance by 1% in IR (.45 $\\to$ .44) --\nand validate our theoretical insights on anti-memorization using real data.\nFinally, we showcase Diffusion Soup's ability to blend the distinct styles of\nmodels finetuned on different shards, resulting in the zero-shot generation of\nhybrid styles.\n","authors":["Benjamin Biggs","Arjun Seshadri","Yang Zou","Achin Jain","Aditya Golatkar","Yusheng Xie","Alessandro Achille","Ashwin Swaminathan","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2406.08431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08428v1","updated":"2024-06-12T17:14:44Z","published":"2024-06-12T17:14:44Z","title":"Improving Noise Robustness through Abstractions and its Impact on\n  Machine Learning","summary":"  Noise is a fundamental problem in learning theory with huge effects in the\napplication of Machine Learning (ML) methods, due to real world data tendency\nto be noisy. Additionally, introduction of malicious noise can make ML methods\nfail critically, as is the case with adversarial attacks. Thus, finding and\ndeveloping alternatives to improve robustness to noise is a fundamental problem\nin ML. In this paper, we propose a method to deal with noise: mitigating its\neffect through the use of data abstractions. The goal is to reduce the effect\nof noise over the model's performance through the loss of information produced\nby the abstraction. However, this information loss comes with a cost: it can\nresult in an accuracy reduction due to the missing information. First, we\nexplored multiple methodologies to create abstractions, using the training\ndataset, for the specific case of numerical data and binary classification\ntasks. We also tested how these abstractions can affect robustness to noise\nwith several experiments that explore the robustness of an Artificial Neural\nNetwork to noise when trained using raw data \\emph{vs} when trained using\nabstracted data. The results clearly show that using abstractions is a viable\napproach for developing noise robust ML methods.\n","authors":["Alfredo Ibias","Karol Capala","Varun Ravi Varma","Anna Drozdz","Jose Sousa"],"pdf_url":"https://arxiv.org/pdf/2406.08428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06858v2","updated":"2024-06-12T17:12:23Z","published":"2024-06-11T00:17:39Z","title":"FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel\n  Fusion","summary":"  Large deep learning models have demonstrated strong ability to solve many\ntasks across a wide range of applications. Those large models typically require\ntraining and inference to be distributed. Tensor parallelism is a common\ntechnique partitioning computation of an operation or layer across devices to\novercome the memory capacity limitation of a single processor, and/or to\naccelerate computation to meet a certain latency requirement. However, this\nkind of parallelism introduces additional communication that might contribute a\nsignificant portion of overall runtime. Thus limits scalability of this\ntechnique within a group of devices with high speed interconnects, such as GPUs\nwith NVLinks in a node. This paper proposes a novel method, Flux, to\nsignificantly hide communication latencies with dependent computations for\nGPUs. Flux over-decomposes communication and computation operations into much\nfiner-grained operations and further fuses them into a larger kernel to\neffectively hide communication without compromising kernel efficiency. Flux can\npotentially overlap up to 96% of communication given a fused kernel. Overall,\nit can achieve up to 1.24x speedups for training over Megatron-LM on a cluster\nof 128 GPUs with various GPU generations and interconnects, and up to 1.66x and\n1.30x speedups for prefill and decoding inference over vLLM on a cluster with 8\nGPUs with various GPU generations and interconnects.\n","authors":["Liwen Chang","Wenlei Bao","Qi Hou","Chengquan Jiang","Ningxin Zheng","Yinmin Zhong","Xuanrun Zhang","Zuquan Song","Ziheng Jiang","Haibin Lin","Xin Jin","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2406.06858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08423v1","updated":"2024-06-12T17:06:07Z","published":"2024-06-12T17:06:07Z","title":"State Soup: In-Context Skill Learning, Retrieval and Mixing","summary":"  A new breed of gated-linear recurrent neural networks has reached\nstate-of-the-art performance on a range of sequence modeling problems. Such\nmodels naturally handle long sequences efficiently, as the cost of processing a\nnew input is independent of sequence length. Here, we explore another advantage\nof these stateful sequence models, inspired by the success of model merging\nthrough parameter interpolation. Building on parallels between fine-tuning and\nin-context learning, we investigate whether we can treat internal states as\ntask vectors that can be stored, retrieved, and then linearly combined,\nexploiting the linearity of recurrence. We study this form of fast model\nmerging on Mamba-2.8b, a pretrained recurrent model, and present preliminary\nevidence that simple linear state interpolation methods suffice to improve\nnext-token perplexity as well as downstream in-context learning task\nperformance.\n","authors":["Maciej Pióro","Maciej Wołczyk","Razvan Pascanu","Johannes von Oswald","João Sacramento"],"pdf_url":"https://arxiv.org/pdf/2406.08423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08414v1","updated":"2024-06-12T16:58:41Z","published":"2024-06-12T16:58:41Z","title":"Discovering Preference Optimization Algorithms with and for Large\n  Language Models","summary":"  Offline preference optimization is a key method for enhancing and controlling\nthe quality of Large Language Model (LLM) outputs. Typically, preference\noptimization is approached as an offline supervised learning task using\nmanually-crafted convex loss functions. While these methods are based on\ntheoretical insights, they are inherently constrained by human creativity, so\nthe large search space of possible loss functions remains under explored. We\naddress this by performing LLM-driven objective discovery to automatically\ndiscover new state-of-the-art preference optimization algorithms without\n(expert) human intervention. Specifically, we iteratively prompt an LLM to\npropose and implement new preference optimization loss functions based on\npreviously-evaluated performance metrics. This process leads to the discovery\nof previously-unknown and performant preference optimization algorithms. The\nbest performing of these we call Discovered Preference Optimization (DiscoPOP),\na novel algorithm that adaptively blends logistic and exponential losses.\nExperiments demonstrate the state-of-the-art performance of DiscoPOP and its\nsuccessful transfer to held-out tasks.\n","authors":["Chris Lu","Samuel Holt","Claudio Fanconi","Alex J. Chan","Jakob Foerster","Mihaela van der Schaar","Robert Tjarko Lange"],"pdf_url":"https://arxiv.org/pdf/2406.08414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08413v1","updated":"2024-06-12T16:57:58Z","published":"2024-06-12T16:57:58Z","title":"Memory Is All You Need: An Overview of Compute-in-Memory Architectures\n  for Accelerating Large Language Model Inference","summary":"  Large language models (LLMs) have recently transformed natural language\nprocessing, enabling machines to generate human-like text and engage in\nmeaningful conversations. This development necessitates speed, efficiency, and\naccessibility in LLM inference as the computational and memory requirements of\nthese systems grow exponentially. Meanwhile, advancements in computing and\nmemory capabilities are lagging behind, exacerbated by the discontinuation of\nMoore's law. With LLMs exceeding the capacity of single GPUs, they require\ncomplex, expert-level configurations for parallel processing. Memory accesses\nbecome significantly more expensive than computation, posing a challenge for\nefficient scaling, known as the memory wall. Here, compute-in-memory (CIM)\ntechnologies offer a promising solution for accelerating AI inference by\ndirectly performing analog computations in memory, potentially reducing latency\nand power consumption. By closely integrating memory and compute elements, CIM\neliminates the von Neumann bottleneck, reducing data movement and improving\nenergy efficiency. This survey paper provides an overview and analysis of\ntransformer-based models, reviewing various CIM architectures and exploring how\nthey can address the imminent challenges of modern AI computing systems. We\ndiscuss transformer-related operators and their hardware acceleration schemes\nand highlight challenges, trends, and insights in corresponding CIM designs.\n","authors":["Christopher Wolters","Xiaoxuan Yang","Ulf Schlichtmann","Toyotaro Suzumura"],"pdf_url":"https://arxiv.org/pdf/2406.08413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08406v1","updated":"2024-06-12T16:53:51Z","published":"2024-06-12T16:53:51Z","title":"RRLS : Robust Reinforcement Learning Suite","summary":"  Robust reinforcement learning is the problem of learning control policies\nthat provide optimal worst-case performance against a span of adversarial\nenvironments. It is a crucial ingredient for deploying algorithms in real-world\nscenarios with prevalent environmental uncertainties and has been a\nlong-standing object of attention in the community, without a standardized set\nof benchmarks. This contribution endeavors to fill this gap. We introduce the\nRobust Reinforcement Learning Suite (RRLS), a benchmark suite based on Mujoco\nenvironments. RRLS provides six continuous control tasks with two types of\nuncertainty sets for training and evaluation. Our benchmark aims to standardize\nrobust reinforcement learning tasks, facilitating reproducible and comparable\nexperiments, in particular those from recent state-of-the-art contributions,\nfor which we demonstrate the use of RRLS. It is also designed to be easily\nexpandable to new environments. The source code is available at\n\\href{https://github.com/SuReLI/RRLS}{https://github.com/SuReLI/RRLS}.\n","authors":["Adil Zouitine","David Bertoin","Pierre Clavier","Matthieu Geist","Emmanuel Rachelson"],"pdf_url":"https://arxiv.org/pdf/2406.08406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14758v2","updated":"2024-06-12T16:53:22Z","published":"2024-02-22T18:20:22Z","title":"Batch and match: black-box variational inference with a score-based\n  divergence","summary":"  Most leading implementations of black-box variational inference (BBVI) are\nbased on optimizing a stochastic evidence lower bound (ELBO). But such\napproaches to BBVI often converge slowly due to the high variance of their\ngradient estimates and their sensitivity to hyperparameters. In this work, we\npropose batch and match (BaM), an alternative approach to BBVI based on a\nscore-based divergence. Notably, this score-based divergence can be optimized\nby a closed-form proximal update for Gaussian variational families with full\ncovariance matrices. We analyze the convergence of BaM when the target\ndistribution is Gaussian, and we prove that in the limit of infinite batch size\nthe variational parameter updates converge exponentially quickly to the target\nmean and covariance. We also evaluate the performance of BaM on Gaussian and\nnon-Gaussian target distributions that arise from posterior inference in\nhierarchical and deep generative models. In these experiments, we find that BaM\ntypically converges in fewer (and sometimes significantly fewer) gradient\nevaluations than leading implementations of BBVI based on ELBO maximization.\n","authors":["Diana Cai","Chirag Modi","Loucas Pillaud-Vivien","Charles C. Margossian","Robert M. Gower","David M. Blei","Lawrence K. Saul"],"pdf_url":"https://arxiv.org/pdf/2402.14758v2.pdf","comment":"49 pages, 14 figures. To appear in the Proceedings of the 41st\n  International Conference on Machine Learning (ICML), 2024"},{"id":"http://arxiv.org/abs/2406.08404v1","updated":"2024-06-12T16:52:54Z","published":"2024-06-12T16:52:54Z","title":"Scaling Value Iteration Networks to 5000 Layers for Extreme Long-Term\n  Planning","summary":"  The Value Iteration Network (VIN) is an end-to-end differentiable\narchitecture that performs value iteration on a latent MDP for planning in\nreinforcement learning (RL). However, VINs struggle to scale to long-term and\nlarge-scale planning tasks, such as navigating a $100\\times 100$ maze -- a task\nwhich typically requires thousands of planning steps to solve. We observe that\nthis deficiency is due to two issues: the representation capacity of the latent\nMDP and the planning module's depth. We address these by augmenting the latent\nMDP with a dynamic transition kernel, dramatically improving its\nrepresentational capacity, and, to mitigate the vanishing gradient problem,\nintroducing an \"adaptive highway loss\" that constructs skip connections to\nimprove gradient flow. We evaluate our method on both 2D maze navigation\nenvironments and the ViZDoom 3D navigation benchmark. We find that our new\nmethod, named Dynamic Transition VIN (DT-VIN), easily scales to 5000 layers and\ncasually solves challenging versions of the above tasks. Altogether, we believe\nthat DT-VIN represents a concrete step forward in performing long-term\nlarge-scale planning in RL environments.\n","authors":["Yuhui Wang","Qingyuan Wu","Weida Li","Dylan R. Ashley","Francesco Faccio","Chao Huang","Jürgen Schmidhuber"],"pdf_url":"https://arxiv.org/pdf/2406.08404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08402v1","updated":"2024-06-12T16:51:54Z","published":"2024-06-12T16:51:54Z","title":"Understanding Sounds, Missing the Questions: The Challenge of Object\n  Hallucination in Large Audio-Language Models","summary":"  Large audio-language models (LALMs) enhance traditional large language models\nby integrating audio perception capabilities, allowing them to tackle\naudio-related tasks. Previous research has primarily focused on assessing the\nperformance of LALMs across various tasks, yet overlooking their reliability,\nparticularly concerning issues like object hallucination. In our study, we\nintroduce methods to assess the extent of object hallucination of publicly\navailable LALMs. Our findings reveal that LALMs are comparable to specialized\naudio captioning models in their understanding of audio content, but struggle\nto answer discriminative questions, specifically those requiring the\nidentification of the presence of particular object sounds within an audio\nclip. This limitation highlights a critical weakness in current LALMs: their\ninadequate understanding of discriminative queries. Moreover, we explore the\npotential of prompt engineering to enhance LALMs' performance on discriminative\nquestions.\n","authors":["Chun-Yi Kuan","Wei-Ping Huang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2406.08402v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.08401v1","updated":"2024-06-12T16:50:12Z","published":"2024-06-12T16:50:12Z","title":"Nyström Kernel Stein Discrepancy","summary":"  Kernel methods underpin many of the most successful approaches in data\nscience and statistics, and they allow representing probability measures as\nelements of a reproducing kernel Hilbert space without loss of information.\nRecently, the kernel Stein discrepancy (KSD), which combines Stein's method\nwith kernel techniques, gained considerable attention. Through the Stein\noperator, KSD allows the construction of powerful goodness-of-fit tests where\nit is sufficient to know the target distribution up to a multiplicative\nconstant. However, the typical U- and V-statistic-based KSD estimators suffer\nfrom a quadratic runtime complexity, which hinders their application in\nlarge-scale settings. In this work, we propose a Nystr\\\"om-based KSD\nacceleration -- with runtime $\\mathcal O\\!\\left(mn+m^3\\right)$ for $n$ samples\nand $m\\ll n$ Nystr\\\"om points -- , show its $\\sqrt{n}$-consistency under the\nnull with a classical sub-Gaussian assumption, and demonstrate its\napplicability for goodness-of-fit testing on a suite of benchmarks.\n","authors":["Florian Kalinke","Zoltan Szabo","Bharath K. Sriperumbudur"],"pdf_url":"https://arxiv.org/pdf/2406.08401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08399v1","updated":"2024-06-12T16:47:54Z","published":"2024-06-12T16:47:54Z","title":"Differentiable Cost-Parameterized Monge Map Estimators","summary":"  Within the field of optimal transport (OT), the choice of ground cost is\ncrucial to ensuring that the optimality of a transport map corresponds to\nusefulness in real-world applications. It is therefore desirable to use known\ninformation to tailor cost functions and hence learn OT maps which are adapted\nto the problem at hand. By considering a class of neural ground costs whose\nMonge maps have a known form, we construct a differentiable Monge map estimator\nwhich can be optimized to be consistent with known information about an OT map.\nIn doing so, we simultaneously learn both an OT map estimator and a\ncorresponding adapted cost function. Through suitable choices of loss function,\nour method provides a general approach for incorporating prior information\nabout the Monge map itself when learning adapted OT maps and cost functions.\n","authors":["Samuel Howard","George Deligiannidis","Patrick Rebeschini","James Thornton"],"pdf_url":"https://arxiv.org/pdf/2406.08399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08398v1","updated":"2024-06-12T16:46:12Z","published":"2024-06-12T16:46:12Z","title":"cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations\n  in Scientific Papers","summary":"  An emerging area of research in situated and multimodal interactive\nconversations (SIMMC) includes interactions in scientific papers. Since\nscientific papers are primarily composed of text, equations, figures, and\ntables, SIMMC methods must be developed specifically for each component to\nsupport the depth of inquiry and interactions required by research scientists.\nThis work introduces Conversational Papers (cPAPERS), a dataset of\nconversational question-answer pairs from reviews of academic papers grounded\nin these paper components and their associated references from scientific\ndocuments available on arXiv. We present a data collection strategy to collect\nthese question-answer pairs from OpenReview and associate them with contextual\ninformation from LaTeX source files. Additionally, we present a series of\nbaseline approaches utilizing Large Language Models (LLMs) in both zero-shot\nand fine-tuned configurations to address the cPAPERS dataset.\n","authors":["Anirudh Sundar","Jin Xu","William Gay","Christopher Richardson","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2406.08398v1.pdf","comment":"14 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.08395v1","updated":"2024-06-12T16:45:09Z","published":"2024-06-12T16:45:09Z","title":"Time-Constrained Robust MDPs","summary":"  Robust reinforcement learning is essential for deploying reinforcement\nlearning algorithms in real-world scenarios where environmental uncertainty\npredominates. Traditional robust reinforcement learning often depends on\nrectangularity assumptions, where adverse probability measures of outcome\nstates are assumed to be independent across different states and actions. This\nassumption, rarely fulfilled in practice, leads to overly conservative\npolicies. To address this problem, we introduce a new time-constrained robust\nMDP (TC-RMDP) formulation that considers multifactorial, correlated, and\ntime-dependent disturbances, thus more accurately reflecting real-world\ndynamics. This formulation goes beyond the conventional rectangularity\nparadigm, offering new perspectives and expanding the analytical framework for\nrobust RL. We propose three distinct algorithms, each using varying levels of\nenvironmental information, and evaluate them extensively on continuous control\nbenchmarks. Our results demonstrate that these algorithms yield an efficient\ntradeoff between performance and robustness, outperforming traditional deep\nrobust RL methods in time-constrained environments while preserving robustness\nin classical benchmarks. This study revisits the prevailing assumptions in\nrobust RL and opens new avenues for developing more practical and realistic RL\napplications.\n","authors":["Adil Zouitine","David Bertoin","Pierre Clavier","Matthieu Geist","Emmanuel Rachelson"],"pdf_url":"https://arxiv.org/pdf/2406.08395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08391v1","updated":"2024-06-12T16:41:31Z","published":"2024-06-12T16:41:31Z","title":"Large Language Models Must Be Taught to Know What They Don't Know","summary":"  When using large language models (LLMs) in high-stakes applications, we need\nto know when we can trust their predictions. Some works argue that prompting\nhigh-performance LLMs is sufficient to produce calibrated uncertainties, while\nothers introduce sampling methods that can be prohibitively expensive. In this\nwork, we first argue that prompting on its own is insufficient to achieve good\ncalibration and then show that fine-tuning on a small dataset of correct and\nincorrect answers can create an uncertainty estimate with good generalization\nand small computational overhead. We show that a thousand graded examples are\nsufficient to outperform baseline methods and that training through the\nfeatures of a model is necessary for good performance and tractable for large\nopen-source models when using LoRA. We also investigate the mechanisms that\nenable reliable LLM uncertainty estimation, finding that many models can be\nused as general-purpose uncertainty estimators, applicable not just to their\nown uncertainties but also the uncertainty of other models. Lastly, we show\nthat uncertainty estimates inform human use of LLMs in human-AI collaborative\nsettings through a user study.\n","authors":["Sanyam Kapoor","Nate Gruver","Manley Roberts","Katherine Collins","Arka Pal","Umang Bhatt","Adrian Weller","Samuel Dooley","Micah Goldblum","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2406.08391v1.pdf","comment":"Code available at:\n  https://github.com/activatedgeek/calibration-tuning"},{"id":"http://arxiv.org/abs/2309.15375v4","updated":"2024-06-12T16:41:16Z","published":"2023-09-27T03:07:46Z","title":"PPG-to-ECG Signal Translation for Continuous Atrial Fibrillation\n  Detection via Attention-based Deep State-Space Modeling","summary":"  Photoplethysmography (PPG) is a cost-effective and non-invasive technique\nthat utilizes optical methods to measure cardiac physiology. PPG has become\nincreasingly popular in health monitoring and is used in various commercial and\nclinical wearable devices. Compared to electrocardiography (ECG), PPG does not\nprovide substantial clinical diagnostic value, despite the strong correlation\nbetween the two. Here, we propose a subject-independent attention-based deep\nstate-space model (ADSSM) to translate PPG signals to corresponding ECG\nwaveforms. The model is not only robust to noise but also data-efficient by\nincorporating probabilistic prior knowledge. To evaluate our approach, 55\nsubjects' data from the MIMIC-III database were used in their original form,\nand then modified with noise, mimicking real-world scenarios. Our approach was\nproven effective as evidenced by the PR-AUC of 0.986 achieved when inputting\nthe translated ECG signals into an existing atrial fibrillation (AFib)\ndetector. ADSSM enables the integration of ECG's extensive knowledge base and\nPPG's continuous measurement for early diagnosis of cardiovascular disease.\n","authors":["Khuong Vo","Mostafa El-Khamy","Yoojin Choi"],"pdf_url":"https://arxiv.org/pdf/2309.15375v4.pdf","comment":"Accepted to 46th IEEE EMBC"},{"id":"http://arxiv.org/abs/2406.08373v1","updated":"2024-06-12T16:21:11Z","published":"2024-06-12T16:21:11Z","title":"Deep Learning Based Joint Multi-User MISO Power Allocation and\n  Beamforming Design","summary":"  The evolution of fifth generation (5G) wireless communication networks has\nled to an increased need for wireless resource management solutions that\nprovide higher data rates, wide coverage, low latency, and power efficiency.\nYet, many of existing traditional approaches remain non-practical due to\ncomputational limitations, and unrealistic presumptions of static network\nconditions and algorithm initialization dependencies. This creates an important\ngap between theoretical analysis and real-time processing of algorithms. To\nbridge this gap, deep learning based techniques offer promising solutions with\ntheir representational capabilities for universal function approximation. We\npropose a novel unsupervised deep learning based joint power allocation and\nbeamforming design for multi-user multiple-input single-output (MU-MISO)\nsystem. The objective is to enhance the spectral efficiency by maximizing the\nsum-rate with the proposed joint design framework, NNBF-P while also offering\ncomputationally efficient solution in contrast to conventional approaches. We\nconduct experiments for diverse settings to compare the performance of NNBF-P\nwith zero-forcing beamforming (ZFBF), minimum mean square error (MMSE)\nbeamforming, and NNBF, which is also our deep learning based beamforming design\nwithout joint power allocation scheme. Experiment results demonstrate the\nsuperiority of NNBF-P compared to ZFBF, and MMSE while NNBF can have lower\nperformances than MMSE and ZFBF in some experiment settings. It can also\ndemonstrate the effectiveness of joint design framework with respect to NNBF.\n","authors":["Cemil Vahapoglu","Timothy J. O'Shea","Tamoghna Roy","Sennur Ulukus"],"pdf_url":"https://arxiv.org/pdf/2406.08373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15301v2","updated":"2024-06-12T16:14:41Z","published":"2024-05-24T07:40:55Z","title":"Rankability-enhanced Revenue Uplift Modeling Framework for Online\n  Marketing","summary":"  Uplift modeling has been widely employed in online marketing by predicting\nthe response difference between the treatment and control groups, so as to\nidentify the sensitive individuals toward interventions like coupons or\ndiscounts. Compared with traditional \\textit{conversion uplift modeling},\n\\textit{revenue uplift modeling} exhibits higher potential due to its direct\nconnection with the corporate income. However, previous works can hardly handle\nthe continuous long-tail response distribution in revenue uplift modeling.\nMoreover, they have neglected to optimize the uplift ranking among different\nindividuals, which is actually the core of uplift modeling. To address such\nissues, in this paper, we first utilize the zero-inflated lognormal (ZILN) loss\nto regress the responses and customize the corresponding modeling network,\nwhich can be adapted to different existing uplift models. Then, we study the\nranking-related uplift modeling error from the theoretical perspective and\npropose two tighter error bounds as the additional loss terms to the\nconventional response regression loss. Finally, we directly model the uplift\nranking error for the entire population with a listwise uplift ranking loss.\nThe experiment results on offline public and industrial datasets validate the\neffectiveness of our method for revenue uplift modeling. Furthermore, we\nconduct large-scale experiments on a prominent online fintech marketing\nplatform, Tencent FiT, which further demonstrates the superiority of our method\nin real-world applications.\n","authors":["Bowei He","Yunpeng Weng","Xing Tang","Ziqiang Cui","Zexu Sun","Liang Chen","Xiuqiang He","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2405.15301v2.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2406.02313v2","updated":"2024-06-12T16:08:29Z","published":"2024-06-04T13:42:42Z","title":"Neural Thermodynamic Integration: Free Energies from Energy-based\n  Diffusion Models","summary":"  Thermodynamic integration (TI) offers a rigorous method for estimating\nfree-energy differences by integrating over a sequence of interpolating\nconformational ensembles. However, TI calculations are computationally\nexpensive and typically limited to coupling a small number of degrees of\nfreedom due to the need to sample numerous intermediate ensembles with\nsufficient conformational-space overlap. In this work, we propose to perform TI\nalong an alchemical pathway represented by a trainable neural network, which we\nterm Neural TI. Critically, we parametrize a time-dependent Hamiltonian\ninterpolating between the interacting and non-interacting systems, and optimize\nits gradient using a denoising-diffusion objective. The ability of the\nresulting energy-based diffusion model to sample all intermediate ensembles\nallows us to perform TI from a single reference calculation. We apply our\nmethod to Lennard-Jones fluids, where we report accurate calculations of the\nexcess chemical potential, demonstrating that Neural TI is capable of coupling\nhundreds of degrees of freedom at once.\n","authors":["Bálint Máté","François Fleuret","Tristan Bereau"],"pdf_url":"https://arxiv.org/pdf/2406.02313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09151v2","updated":"2024-06-12T16:03:31Z","published":"2024-02-14T13:08:25Z","title":"Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for\n  Chinese Mental Health Text Analysis","summary":"  In the current environment, psychological issues are prevalent and\nwidespread, with social media serving as a key outlet for individuals to share\ntheir feelings. This results in the generation of vast quantities of data\ndaily, where negative emotions have the potential to precipitate crisis\nsituations. There is a recognized need for models capable of efficient\nanalysis. While pre-trained language models have demonstrated their\neffectiveness broadly, there's a noticeable gap in pre-trained models tailored\nfor specialized domains like psychology. To address this, we have collected a\nhuge dataset from Chinese social media platforms and enriched it with publicly\navailable datasets to create a comprehensive database encompassing 3.36 million\ntext entries. To enhance the model's applicability to psychological text\nanalysis, we integrated psychological lexicons into the pre-training masking\nmechanism. Building on an existing Chinese language model, we performed\nadaptive training to develop a model specialized for the psychological domain.\nWe evaluated our model's performance across six public datasets, where it\ndemonstrated improvements compared to eight other models. Additionally, in the\nqualitative comparison experiment, our model provided psychologically relevant\npredictions given the masked sentences. Due to concerns regarding data privacy,\nthe dataset will not be made publicly available. However, we have made the\npre-trained models and codes publicly accessible to the community via:\nhttps://github.com/zwzzzQAQ/Chinese-MentalBERT.\n","authors":["Wei Zhai","Hongzhi Qi","Qing Zhao","Jianqiang Li","Ziqi Wang","Han Wang","Bing Xiang Yang","Guanghui Fu"],"pdf_url":"https://arxiv.org/pdf/2402.09151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15053v3","updated":"2024-06-12T16:01:45Z","published":"2023-07-27T17:57:42Z","title":"On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation\n  Metric for Top-$n$ Recommendation","summary":"  Approaches to recommendation are typically evaluated in one of two ways: (1)\nvia a (simulated) online experiment, often seen as the gold standard, or (2)\nvia some offline evaluation procedure, where the goal is to approximate the\noutcome of an online experiment. Several offline evaluation metrics have been\nadopted in the literature, inspired by ranking metrics prevalent in the field\nof Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one\nsuch metric that has seen widespread adoption in empirical studies, and higher\n(n)DCG values have been used to present new methods as the state-of-the-art in\ntop-$n$ recommendation for many years.\n  Our work takes a critical look at this approach, and investigates when we can\nexpect such metrics to approximate the gold standard outcome of an online\nexperiment. We formally present the assumptions that are necessary to consider\nDCG an unbiased estimator of online reward and provide a derivation for this\nmetric from first principles, highlighting where we deviate from its\ntraditional uses in IR. Importantly, we show that normalising the metric\nrenders it inconsistent, in that even when DCG is unbiased, ranking competing\nmethods by their normalised DCG can invert their relative order. Through a\ncorrelation analysis between off- and on-line experiments conducted on a\nlarge-scale recommendation platform, we show that our unbiased DCG estimates\nstrongly correlate with online reward, even when some of the metric's inherent\nassumptions are violated. This statement no longer holds for its normalised\nvariant, suggesting that nDCG's practical utility may be limited.\n","authors":["Olivier Jeunen","Ivan Potapov","Aleksei Ustimenko"],"pdf_url":"https://arxiv.org/pdf/2307.15053v3.pdf","comment":"To appear in the research track at the ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining (KDD '24)"},{"id":"http://arxiv.org/abs/2406.08354v1","updated":"2024-06-12T16:00:16Z","published":"2024-06-12T16:00:16Z","title":"DocSynthv2: A Practical Autoregressive Modeling for Document Generation","summary":"  While the generation of document layouts has been extensively explored,\ncomprehensive document generation encompassing both layout and content presents\na more complex challenge. This paper delves into this advanced domain,\nproposing a novel approach called DocSynthv2 through the development of a\nsimple yet effective autoregressive structured model. Our model, distinct in\nits integration of both layout and textual cues, marks a step beyond existing\nlayout-generation approaches. By focusing on the relationship between the\nstructural elements and the textual content within documents, we aim to\ngenerate cohesive and contextually relevant documents without any reliance on\nvisual components. Through experimental studies on our curated benchmark for\nthe new task, we demonstrate the ability of our model combining layout and\ntextual information in enhancing the generation quality and relevance of\ndocuments, opening new pathways for research in document creation and automated\ndesign. Our findings emphasize the effectiveness of autoregressive models in\nhandling complex document generation tasks.\n","authors":["Sanket Biswas","Rajiv Jain","Vlad I. Morariu","Jiuxiang Gu","Puneet Mathur","Curtis Wigington","Tong Sun","Josep Lladós"],"pdf_url":"https://arxiv.org/pdf/2406.08354v1.pdf","comment":"Spotlight (Oral) Acceptance to CVPR 2024 Workshop for Graphic Design\n  Understanding and Generation (GDUG)"},{"id":"http://arxiv.org/abs/2406.03390v2","updated":"2024-06-12T15:52:42Z","published":"2024-06-05T15:41:02Z","title":"What Drives Online Popularity: Author, Content or Sharers? Estimating\n  Spread Dynamics with Bayesian Mixture Hawkes","summary":"  The spread of content on social media is shaped by intertwining factors on\nthree levels: the source, the content itself, and the pathways of content\nspread. At the lowest level, the popularity of the sharing user determines its\neventual reach. However, higher-level factors such as the nature of the online\nitem and the credibility of its source also play crucial roles in determining\nhow widely and rapidly the online item spreads. In this work, we propose the\nBayesian Mixture Hawkes (BMH) model to jointly learn the influence of source,\ncontent and spread. We formulate the BMH model as a hierarchical mixture model\nof separable Hawkes processes, accommodating different classes of Hawkes\ndynamics and the influence of feature sets on these classes. We test the BMH\nmodel on two learning tasks, cold-start popularity prediction and temporal\nprofile generalization performance, applying to two real-world retweet cascade\ndatasets referencing articles from controversial and traditional media\npublishers. The BMH model outperforms the state-of-the-art models and\npredictive baselines on both datasets and utilizes cascade- and item-level\ninformation better than the alternatives. Lastly, we perform a counter-factual\nanalysis where we apply the trained publisher-level BMH models to a set of\narticle headlines and show that effectiveness of headline writing style\n(neutral, clickbait, inflammatory) varies across publishers. The BMH model\nunveils differences in style effectiveness between controversial and reputable\npublishers, where we find clickbait to be notably more effective for reputable\npublishers as opposed to controversial ones, which links to the latter's\noveruse of clickbait.\n","authors":["Pio Calderon","Marian-Andrei Rizoiu"],"pdf_url":"https://arxiv.org/pdf/2406.03390v2.pdf","comment":"accepted as a full paper in the Research Track at the European\n  Conference on Machine Learning and Principles and Practice of Knowledge\n  Discovery in Databases (ECML-PKDD) 2024"},{"id":"http://arxiv.org/abs/2406.08335v1","updated":"2024-06-12T15:41:06Z","published":"2024-06-12T15:41:06Z","title":"A Survey of Pipeline Tools for Data Engineering","summary":"  Currently, a variety of pipeline tools are available for use in data\nengineering. Data scientists can use these tools to resolve data wrangling\nissues associated with data and accomplish some data engineering tasks from\ndata ingestion through data preparation to utilization as input for machine\nlearning (ML). Some of these tools have essential built-in components or can be\ncombined with other tools to perform desired data engineering operations. While\nsome tools are wholly or partly commercial, several open-source tools are\navailable to perform expert-level data engineering tasks. This survey examines\nthe broad categories and examples of pipeline tools based on their design and\ndata engineering intentions. These categories are Extract Transform\nLoad/Extract Load Transform (ETL/ELT), pipelines for Data Integration,\nIngestion, and Transformation, Data Pipeline Orchestration and Workflow\nManagement, and Machine Learning Pipelines. The survey also provides a broad\noutline of the utilization with examples within these broad groups and finally,\na discussion is presented with case studies indicating the usage of pipeline\ntools for data engineering. The studies present some first-user application\nexperiences with sample data, some complexities of the applied pipeline, and a\nsummary note of approaches to using these tools to prepare data for machine\nlearning.\n","authors":["Anthony Mbata","Yaji Sripada","Mingjun Zhong"],"pdf_url":"https://arxiv.org/pdf/2406.08335v1.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.08334v1","updated":"2024-06-12T15:40:06Z","published":"2024-06-12T15:40:06Z","title":"ProTrain: Efficient LLM Training via Memory-Aware Techniques","summary":"  It is extremely memory-hungry to train Large Language Models (LLM). To solve\nthis problem, existing work exploits the combination of CPU and GPU for the\ntraining process, such as ZeRO-Offload. Such a technique largely democratizes\nbillion-scale model training, making it possible to train with few consumer\ngraphics cards. However, based on our observation, existing frameworks often\nprovide coarse-grained memory management and require experienced experts in\nconfiguration tuning, leading to suboptimal hardware utilization and\nperformance. This paper proposes ProTrain, a novel training system that\nintelligently balances memory usage and performance by coordinating memory,\ncomputation, and IO. ProTrain achieves adaptive memory management through\nChunk-Based Model State Management and Block-Wise Activation Management, guided\nby a Memory-Aware Runtime Profiler without user intervention. ProTrain does not\nchange the training algorithm and thus does not compromise accuracy.\nExperiments show that ProTrain improves training throughput by 1.43$\\times$ to\n2.71$\\times$ compared to the SOTA training systems.\n","authors":["Hanmei Yang","Jin Zhou","Yao Fu","Xiaoqun Wang","Ramine Roane","Hui Guan","Tongping Liu"],"pdf_url":"https://arxiv.org/pdf/2406.08334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08331v1","updated":"2024-06-12T15:34:42Z","published":"2024-06-12T15:34:42Z","title":"Genetic Column Generation for Computing Lower Bounds for Adversarial\n  Classification","summary":"  Recent theoretical results on adversarial multi-class classification showed a\nsimilarity to the multi-marginal formulation of Wasserstein-barycenter in\noptimal transport. Unfortunately, both problems suffer from the curse of\ndimension, making it hard to exploit the nice linear program structure of the\nproblems for numerical calculations. We investigate how ideas from Genetic\nColumn Generation for multi-marginal optimal transport can be used to overcome\nthe curse of dimension in computing the minimal adversarial risk in multi-class\nclassification.\n","authors":["Maximilian Penka"],"pdf_url":"https://arxiv.org/pdf/2406.08331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08330v1","updated":"2024-06-12T15:34:28Z","published":"2024-06-12T15:34:28Z","title":"It's all about PR -- Smart Benchmarking AI Accelerators using\n  Performance Representatives","summary":"  Statistical models are widely used to estimate the performance of commercial\noff-the-shelf (COTS) AI hardware accelerators. However, training of statistical\nperformance models often requires vast amounts of data, leading to a\nsignificant time investment and can be difficult in case of limited hardware\navailability. To alleviate this problem, we propose a novel performance\nmodeling methodology that significantly reduces the number of training samples\nwhile maintaining good accuracy. Our approach leverages knowledge of the target\nhardware architecture and initial parameter sweeps to identify a set of\nPerformance Representatives (PR) for deep neural network (DNN) layers. These\nPRs are then used for benchmarking, building a statistical performance model,\nand making estimations. This targeted approach drastically reduces the number\nof training samples needed, opposed to random sampling, to achieve a better\nestimation accuracy. We achieve a Mean Absolute Percentage Error (MAPE) of as\nlow as 0.02% for single-layer estimations and 0.68% for whole DNN estimations\nwith less than 10000 training samples. The results demonstrate the superiority\nof our method for single-layer estimations compared to models trained with\nrandomly sampled datasets of the same size.\n","authors":["Alexander Louis-Ferdinand Jung","Jannik Steinmetz","Jonathan Gietz","Konstantin Lübeck","Oliver Bringmann"],"pdf_url":"https://arxiv.org/pdf/2406.08330v1.pdf","comment":"Accepted version for: SAMOS'24"},{"id":"http://arxiv.org/abs/2402.10036v2","updated":"2024-06-12T15:27:20Z","published":"2024-02-15T15:59:59Z","title":"Predictive Linear Online Tracking for Unknown Targets","summary":"  In this paper, we study the problem of online tracking in linear control\nsystems, where the objective is to follow a moving target. Unlike classical\ntracking control, the target is unknown, non-stationary, and its state is\nrevealed sequentially, thus, fitting the framework of online non-stochastic\ncontrol. We consider the case of quadratic costs and propose a new algorithm,\ncalled predictive linear online tracking (PLOT). The algorithm uses recursive\nleast squares with exponential forgetting to learn a time-varying dynamic model\nof the target. The learned model is used in the optimal policy under the\nframework of receding horizon control. We show the dynamic regret of PLOT\nscales with $\\mathcal{O}(\\sqrt{TV_T})$, where $V_T$ is the total variation of\nthe target dynamics and $T$ is the time horizon. Unlike prior work, our\ntheoretical results hold for non-stationary targets. We implement PLOT on a\nreal quadrotor and provide open-source software, thus, showcasing one of the\nfirst successful applications of online control methods on real hardware.\n","authors":["Anastasios Tsiamis","Aren Karapetyan","Yueshan Li","Efe C. Balta","John Lygeros"],"pdf_url":"https://arxiv.org/pdf/2402.10036v2.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2405.14250v3","updated":"2024-06-12T15:26:18Z","published":"2024-05-23T07:28:56Z","title":"Diffusion models for Gaussian distributions: Exact solutions and\n  Wasserstein errors","summary":"  Diffusion or score-based models recently showed high performance in image\ngeneration. They rely on a forward and a backward stochastic differential\nequations (SDE). The sampling of a data distribution is achieved by solving\nnumerically the backward SDE or its associated flow ODE. Studying the\nconvergence of these models necessitates to control four different types of\nerror: the initialization error, the truncation error, the discretization and\nthe score approximation. In this paper, we study theoretically the behavior of\ndiffusion models and their numerical implementation when the data distribution\nis Gaussian. In this restricted framework where the score function is a linear\noperator, we can derive the analytical solutions of the forward and backward\nSDEs as well as the associated flow ODE. This provides exact expressions for\nvarious Wasserstein errors which enable us to compare the influence of each\nerror type for any sampling scheme, thus allowing to monitor convergence\ndirectly in the data space instead of relying on Inception features. Our\nexperiments show that the recommended numerical schemes from the diffusion\nmodels literature are also the best sampling schemes for Gaussian\ndistributions.\n","authors":["Emile Pierret","Bruno Galerne"],"pdf_url":"https://arxiv.org/pdf/2405.14250v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08322v1","updated":"2024-06-12T15:22:56Z","published":"2024-06-12T15:22:56Z","title":"MMIL: A novel algorithm for disease associated cell type discovery","summary":"  Single-cell datasets often lack individual cell labels, making it challenging\nto identify cells associated with disease. To address this, we introduce\nMixture Modeling for Multiple Instance Learning (MMIL), an expectation\nmaximization method that enables the training and calibration of cell-level\nclassifiers using patient-level labels. Our approach can be used to train e.g.\nlasso logistic regression models, gradient boosted trees, and neural networks.\nWhen applied to clinically-annotated, primary patient samples in Acute Myeloid\nLeukemia (AML) and Acute Lymphoblastic Leukemia (ALL), our method accurately\nidentifies cancer cells, generalizes across tissues and treatment timepoints,\nand selects biologically relevant features. In addition, MMIL is capable of\nincorporating cell labels into model training when they are known, providing a\npowerful framework for leveraging both labeled and unlabeled data\nsimultaneously. Mixture Modeling for MIL offers a novel approach for cell\nclassification, with significant potential to advance disease understanding and\nmanagement, especially in scenarios with unknown gold-standard labels and high\ndimensionality.\n","authors":["Erin Craig","Timothy Keyes","Jolanda Sarno","Maxim Zaslavsky","Garry Nolan","Kara Davis","Trevor Hastie","Robert Tibshirani"],"pdf_url":"https://arxiv.org/pdf/2406.08322v1.pdf","comment":"Erin Craig and Timothy Keyes contributed equally to this work"},{"id":"http://arxiv.org/abs/2406.08321v1","updated":"2024-06-12T15:21:51Z","published":"2024-06-12T15:21:51Z","title":"Deep learning from strongly mixing observations: Sparse-penalized\n  regularization and minimax optimality","summary":"  The explicit regularization and optimality of deep neural networks estimators\nfrom independent data have made considerable progress recently. The study of\nsuch properties on dependent data is still a challenge. In this paper, we carry\nout deep learning from strongly mixing observations, and deal with the squared\nand a broad class of loss functions. We consider sparse-penalized\nregularization for deep neural network predictor. For a general framework that\nincludes, regression estimation, classification, time series\nprediction,$\\cdots$, oracle inequality for the expected excess risk is\nestablished and a bound on the class of H\\\"older smooth functions is provided.\nFor nonparametric regression from strong mixing data and sub-exponentially\nerror, we provide an oracle inequality for the $L_2$ error and investigate an\nupper bound of this error on a class of H\\\"older composition functions. For the\nspecific case of nonparametric autoregression with Gaussian and Laplace errors,\na lower bound of the $L_2$ error on this H\\\"older composition class is\nestablished. Up to logarithmic factor, this bound matches its upper bound; so,\nthe deep neural network estimator attains the minimax optimal rate.\n","authors":["William Kengne","Modou Wade"],"pdf_url":"https://arxiv.org/pdf/2406.08321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06591v2","updated":"2024-06-12T15:19:46Z","published":"2024-06-05T16:11:55Z","title":"Exploring Multilingual Large Language Models for Enhanced TNM\n  classification of Radiology Report in lung cancer staging","summary":"  Background: Structured radiology reports remains underdeveloped due to\nlabor-intensive structuring and narrative-style reporting. Deep learning,\nparticularly large language models (LLMs) like GPT-3.5, offers promise in\nautomating the structuring of radiology reports in natural languages. However,\nalthough it has been reported that LLMs are less effective in languages other\nthan English, their radiological performance has not been extensively studied.\nPurpose: This study aimed to investigate the accuracy of TNM classification\nbased on radiology reports using GPT3.5-turbo (GPT3.5) and the utility of\nmultilingual LLMs in both Japanese and English. Material and Methods: Utilizing\nGPT3.5, we developed a system to automatically generate TNM classifications\nfrom chest CT reports for lung cancer and evaluate its performance. We\nstatistically analyzed the impact of providing full or partial TNM definitions\nin both languages using a Generalized Linear Mixed Model. Results: Highest\naccuracy was attained with full TNM definitions and radiology reports in\nEnglish (M = 94%, N = 80%, T = 47%, and ALL = 36%). Providing definitions for\neach of the T, N, and M factors statistically improved their respective\naccuracies (T: odds ratio (OR) = 2.35, p < 0.001; N: OR = 1.94, p < 0.01; M: OR\n= 2.50, p < 0.001). Japanese reports exhibited decreased N and M accuracies (N\naccuracy: OR = 0.74 and M accuracy: OR = 0.21). Conclusion: This study\nunderscores the potential of multilingual LLMs for automatic TNM classification\nin radiology reports. Even without additional model training, performance\nimprovements were evident with the provided TNM definitions, indicating LLMs'\nrelevance in radiology contexts.\n","authors":["Hidetoshi Matsuo","Mizuho Nishio","Takaaki Matsunaga","Koji Fujimoto","Takamichi Murakami"],"pdf_url":"https://arxiv.org/pdf/2406.06591v2.pdf","comment":"16 pages, 3figures"},{"id":"http://arxiv.org/abs/2406.08318v1","updated":"2024-06-12T15:19:25Z","published":"2024-06-12T15:19:25Z","title":"Invariant multiscale neural networks for data-scarce scientific\n  applications","summary":"  Success of machine learning (ML) in the modern world is largely determined by\nabundance of data. However at many industrial and scientific problems, amount\nof data is limited. Application of ML methods to data-scarce scientific\nproblems can be made more effective via several routes, one of them is\nequivariant neural networks possessing knowledge of symmetries. Here we suggest\nthat combination of symmetry-aware invariant architectures and stacks of\ndilated convolutions is a very effective and easy to implement receipt allowing\nsizable improvements in accuracy over standard approaches. We apply it to\nrepresentative physical problems from different realms: prediction of bandgaps\nof photonic crystals, and network approximations of magnetic ground states. The\nsuggested invariant multiscale architectures increase expressibility of\nnetworks, which allow them to perform better in all considered cases.\n","authors":["I. Schurov","D. Alforov","M. Katsnelson","A. Bagrov","A. Itin"],"pdf_url":"https://arxiv.org/pdf/2406.08318v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.08316v1","updated":"2024-06-12T15:16:40Z","published":"2024-06-12T15:16:40Z","title":"Is Programming by Example solved by LLMs?","summary":"  Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n`solved' PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short.\n","authors":["Wen-Ding Li","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2406.08316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08315v1","updated":"2024-06-12T15:16:26Z","published":"2024-06-12T15:16:26Z","title":"Improving Policy Optimization via $\\varepsilon$-Retrain","summary":"  We present $\\varepsilon$-retrain, an exploration strategy designed to\nencourage a behavioral preference while optimizing policies with monotonic\nimprovement guarantees. To this end, we introduce an iterative procedure for\ncollecting retrain areas -- parts of the state space where an agent did not\nfollow the behavioral preference. Our method then switches between the typical\nuniform restart state distribution and the retrain areas using a decaying\nfactor $\\varepsilon$, allowing agents to retrain on situations where they\nviolated the preference. Experiments over hundreds of seeds across locomotion,\nnavigation, and power network tasks show that our method yields agents that\nexhibit significant performance and sample efficiency improvements. Moreover,\nwe employ formal verification of neural networks to provably quantify the\ndegree to which agents adhere to behavioral preferences.\n","authors":["Luca Marzari","Changliu Liu","Priya L. Donti","Enrico Marchesini"],"pdf_url":"https://arxiv.org/pdf/2406.08315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.02001v4","updated":"2024-06-12T15:15:37Z","published":"2022-06-04T14:54:05Z","title":"A PDE-based Explanation of Extreme Numerical Sensitivities and Edge of\n  Stability in Training Neural Networks","summary":"  We discover restrained numerical instabilities in current training practices\nof deep networks with stochastic gradient descent (SGD), and its variants. We\nshow numerical error (on the order of the smallest floating point bit and thus\nthe most extreme or limiting numerical perturbations induced from floating\npoint arithmetic in training deep nets can be amplified significantly and\nresult in significant test accuracy variance (sensitivities), comparable to the\ntest accuracy variance due to stochasticity in SGD. We show how this is likely\ntraced to instabilities of the optimization dynamics that are restrained, i.e.,\nlocalized over iterations and regions of the weight tensor space. We do this by\npresenting a theoretical framework using numerical analysis of partial\ndifferential equations (PDE), and analyzing the gradient descent PDE of\nconvolutional neural networks (CNNs). We show that it is stable only under\ncertain conditions on the learning rate and weight decay. We show that rather\nthan blowing up when the conditions are violated, the instability can be\nrestrained. We show this is a consequence of the non-linear PDE associated with\nthe gradient descent of the CNN, whose local linearization changes when\nover-driving the step size of the discretization, resulting in a stabilizing\neffect. We link restrained instabilities to the recently discovered Edge of\nStability (EoS) phenomena, in which the stable step size predicted by classical\ntheory is exceeded while continuing to optimize the loss and still converging.\nBecause restrained instabilities occur at the EoS, our theory provides new\ninsights and predictions about the EoS, in particular, the role of\nregularization and the dependence on the network complexity.\n","authors":["Yuxin Sun","Dong Lao","Ganesh Sundaramoorthi","Anthony Yezzi"],"pdf_url":"https://arxiv.org/pdf/2206.02001v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08311v1","updated":"2024-06-12T15:12:49Z","published":"2024-06-12T15:12:49Z","title":"Causality for Tabular Data Synthesis: A High-Order Structure Causal\n  Benchmark Framework","summary":"  Tabular synthesis models remain ineffective at capturing complex\ndependencies, and the quality of synthetic data is still insufficient for\ncomprehensive downstream tasks, such as prediction under distribution shifts,\nautomated decision-making, and cross-table understanding. A major challenge is\nthe lack of prior knowledge about underlying structures and high-order\nrelationships in tabular data. We argue that a systematic evaluation on\nhigh-order structural information for tabular data synthesis is the first step\ntowards solving the problem. In this paper, we introduce high-order structural\ncausal information as natural prior knowledge and provide a benchmark framework\nfor the evaluation of tabular synthesis models. The framework allows us to\ngenerate benchmark datasets with a flexible range of data generation processes\nand to train tabular synthesis models using these datasets for further\nevaluation. We propose multiple benchmark tasks, high-order metrics, and causal\ninference tasks as downstream tasks for evaluating the quality of synthetic\ndata generated by the trained models. Our experiments demonstrate to leverage\nthe benchmark framework for evaluating the model capability of capturing\nhigh-order structural causal information. Furthermore, our benchmarking results\nprovide an initial assessment of state-of-the-art tabular synthesis models.\nThey have clearly revealed significant gaps between ideal and actual\nperformance and how baseline methods differ. Our benchmark framework is\navailable at URL https://github.com/TURuibo/CauTabBench.\n","authors":["Ruibo Tu","Zineb Senane","Lele Cao","Cheng Zhang","Hedvig Kjellström","Gustav Eje Henter"],"pdf_url":"https://arxiv.org/pdf/2406.08311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08310v1","updated":"2024-06-12T15:10:44Z","published":"2024-06-12T15:10:44Z","title":"GraphFM: A Comprehensive Benchmark for Graph Foundation Model","summary":"  Foundation Models (FMs) serve as a general class for the development of\nartificial intelligence systems, offering broad potential for generalization\nacross a spectrum of downstream tasks. Despite extensive research into\nself-supervised learning as the cornerstone of FMs, several outstanding issues\npersist in Graph Foundation Models that rely on graph self-supervised learning,\nnamely: 1) Homogenization. The extent of generalization capability on\ndownstream tasks remains unclear. 2) Scalability. It is unknown how effectively\nthese models can scale to large datasets. 3) Efficiency. The training time and\nmemory usage of these models require evaluation. 4) Training Stop Criteria.\nDetermining the optimal stopping strategy for pre-training across multiple\ntasks to maximize performance on downstream tasks. To address these questions,\nwe have constructed a rigorous benchmark that thoroughly analyzes and studies\nthe generalization and scalability of self-supervised Graph Neural Network\n(GNN) models. Regarding generalization, we have implemented and compared the\nperformance of various self-supervised GNN models, trained to generate node\nrepresentations, across tasks such as node classification, link prediction, and\nnode clustering. For scalability, we have compared the performance of various\nmodels after training using full-batch and mini-batch strategies. Additionally,\nwe have assessed the training efficiency of these models by conducting\nexperiments to test their GPU memory usage and throughput. Through these\nexperiments, we aim to provide insights to motivate future research. The code\nfor this benchmark is publicly available at https://github.com/NYUSHCS/GraphFM.\n","authors":["Yuhao Xu","Xinqi Liu","Keyu Duan","Yi Fang","Yu-Neng Chuang","Daochen Zha","Qiaoyu Tan"],"pdf_url":"https://arxiv.org/pdf/2406.08310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08307v1","updated":"2024-06-12T15:08:15Z","published":"2024-06-12T15:08:15Z","title":"Measuring model variability using robust non-parametric testing","summary":"  Training a deep neural network often involves stochastic optimization,\nmeaning each run will produce a different model. The seed used to initialize\nrandom elements of the optimization procedure heavily influences the quality of\na trained model, which may be obscure from many commonly reported summary\nstatistics, like accuracy. However, random seed is often not included in\nhyper-parameter optimization, perhaps because the relationship between seed and\nmodel quality is hard to describe. This work attempts to describe the\nrelationship between deep net models trained with different random seeds and\nthe behavior of the expected model. We adopt robust hypothesis testing to\npropose a novel summary statistic for network similarity, referred to as the\n$\\alpha$-trimming level. We use the $\\alpha$-trimming level to show that the\nempirical cumulative distribution function of an ensemble model created from a\ncollection of trained models with different random seeds approximates the\naverage of these functions as the number of models in the collection grows\nlarge. This insight provides guidance for how many random seeds should be\nsampled to ensure that an ensemble of these trained models is a reliable\nrepresentative. We also show that the $\\alpha$-trimming level is more\nexpressive than different performance metrics like validation accuracy, churn,\nor expected calibration error when taken alone and may help with random seed\nselection in a more principled fashion. We demonstrate the value of the\nproposed statistic in real experiments and illustrate the advantage of\nfine-tuning over random seed with an experiment in transfer learning.\n","authors":["Sinjini Banerjee","Tim Marrinan","Reilly Cannon","Tony Chiang","Anand D. Sarwate"],"pdf_url":"https://arxiv.org/pdf/2406.08307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07765v2","updated":"2024-06-12T14:57:56Z","published":"2023-10-11T18:00:02Z","title":"Feature Learning and Generalization in Deep Networks with Orthogonal\n  Weights","summary":"  Fully-connected deep neural networks with weights initialized from\nindependent Gaussian distributions can be tuned to criticality, which prevents\nthe exponential growth or decay of signals propagating through the network.\nHowever, such networks still exhibit fluctuations that grow linearly with the\ndepth of the network, which may impair the training of networks with width\ncomparable to depth. We show analytically that rectangular networks with tanh\nactivations and weights initialized from the ensemble of orthogonal matrices\nhave corresponding preactivation fluctuations which are independent of depth,\nto leading order in inverse width. Moreover, we demonstrate numerically that,\nat initialization, all correlators involving the neural tangent kernel (NTK)\nand its descendants at leading order in inverse width -- which govern the\nevolution of observables during training -- saturate at a depth of $\\sim 20$,\nrather than growing without bound as in the case of Gaussian initializations.\nWe speculate that this structure preserves finite-width feature learning while\nreducing overall noise, thus improving both generalization and training speed\nin deep networks with depth comparable to width. We provide some experimental\njustification by relating empirical measurements of the NTK to the superior\nperformance of deep nonlinear orthogonal networks trained under full-batch\ngradient descent on the MNIST and CIFAR-10 classification tasks.\n","authors":["Hannah Day","Yonatan Kahn","Daniel A. Roberts"],"pdf_url":"https://arxiv.org/pdf/2310.07765v2.pdf","comment":"v2: numerical experiments updated with more data, plots updated to\n  match, conclusions unchanged. 30+12 pages, 20 figures"},{"id":"http://arxiv.org/abs/2406.08294v1","updated":"2024-06-12T14:57:37Z","published":"2024-06-12T14:57:37Z","title":"Vessel Re-identification and Activity Detection in Thermal Domain for\n  Maritime Surveillance","summary":"  Maritime surveillance is vital to mitigate illegal activities such as drug\nsmuggling, illegal fishing, and human trafficking. Vision-based maritime\nsurveillance is challenging mainly due to visibility issues at night, which\nresults in failures in re-identifying vessels and detecting suspicious\nactivities. In this paper, we introduce a thermal, vision-based approach for\nmaritime surveillance with object tracking, vessel re-identification, and\nsuspicious activity detection capabilities. For vessel re-identification, we\npropose a novel viewpoint-independent algorithm which compares features of the\nsides of the vessel separately (separate side-spaces) leveraging shape\ninformation in the absence of color features. We propose techniques to adapt\ntracking and activity detection algorithms for the thermal domain and train\nthem using a thermal dataset we created. This dataset will be the first\npublicly available benchmark dataset for thermal maritime surveillance. Our\nsystem is capable of re-identifying vessels with an 81.8% Top1 score and\nidentifying suspicious activities with a 72.4\\% frame mAP score; a new\nbenchmark for each task in the thermal domain.\n","authors":["Yasod Ginige","Ransika Gunasekara","Darsha Hewavitharana","Manjula Ariyarathne","Ranga Rodrigo","Peshala Jayasekara"],"pdf_url":"https://arxiv.org/pdf/2406.08294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08337v2","updated":"2024-06-12T14:53:58Z","published":"2024-03-13T08:41:55Z","title":"LLM-Assisted Light: Leveraging Large Language Model Capabilities for\n  Human-Mimetic Traffic Signal Control in Complex Urban Environments","summary":"  Traffic congestion in metropolitan areas presents a formidable challenge with\nfar-reaching economic, environmental, and societal ramifications. Therefore,\neffective congestion management is imperative, with traffic signal control\n(TSC) systems being pivotal in this endeavor. Conventional TSC systems,\ndesigned upon rule-based algorithms or reinforcement learning (RL), frequently\nexhibit deficiencies in managing the complexities and variabilities of urban\ntraffic flows, constrained by their limited capacity for adaptation to\nunfamiliar scenarios. In response to these limitations, this work introduces an\ninnovative approach that integrates Large Language Models (LLMs) into TSC,\nharnessing their advanced reasoning and decision-making faculties.\nSpecifically, a hybrid framework that augments LLMs with a suite of perception\nand decision-making tools is proposed, facilitating the interrogation of both\nthe static and dynamic traffic information. This design places the LLM at the\ncenter of the decision-making process, combining external traffic data with\nestablished TSC methods. Moreover, a simulation platform is developed to\ncorroborate the efficacy of the proposed framework. The findings from our\nsimulations attest to the system's adeptness in adjusting to a multiplicity of\ntraffic environments without the need for additional training. Notably, in\ncases of Sensor Outage (SO), our approach surpasses conventional RL-based\nsystems by reducing the average waiting time by $20.4\\%$. This research\nsignifies a notable advance in TSC strategies and paves the way for the\nintegration of LLMs into real-world, dynamic scenarios, highlighting their\npotential to revolutionize traffic management. The related code is available at\nhttps://github.com/Traffic-Alpha/LLM-Assisted-Light.\n","authors":["Maonan Wang","Aoyu Pang","Yuheng Kan","Man-On Pun","Chung Shue Chen","Bo Huang"],"pdf_url":"https://arxiv.org/pdf/2403.08337v2.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.08288v1","updated":"2024-06-12T14:53:30Z","published":"2024-06-12T14:53:30Z","title":"Decoupling the Class Label and the Target Concept in Machine Unlearning","summary":"  Machine unlearning as an emerging research topic for data regulations, aims\nto adjust a trained model to approximate a retrained one that excludes a\nportion of training data. Previous studies showed that class-wise unlearning is\nsuccessful in forgetting the knowledge of a target class, through gradient\nascent on the forgetting data or fine-tuning with the remaining data. However,\nwhile these methods are useful, they are insufficient as the class label and\nthe target concept are often considered to coincide. In this work, we decouple\nthem by considering the label domain mismatch and investigate three problems\nbeyond the conventional all matched forgetting, e.g., target mismatch, model\nmismatch, and data mismatch forgetting. We systematically analyze the new\nchallenges in restrictively forgetting the target concept and also reveal\ncrucial forgetting dynamics in the representation level to realize these tasks.\nBased on that, we propose a general framework, namely, TARget-aware Forgetting\n(TARF). It enables the additional tasks to actively forget the target concept\nwhile maintaining the rest part, by simultaneously conducting annealed gradient\nascent on the forgetting data and selected gradient descent on the\nhard-to-affect remaining data. Empirically, various experiments under the newly\nintroduced settings are conducted to demonstrate the effectiveness of our TARF.\n","authors":["Jianing Zhu","Bo Han","Jiangchao Yao","Jianliang Xu","Gang Niu","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2406.08288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08287v1","updated":"2024-06-12T14:53:23Z","published":"2024-06-12T14:53:23Z","title":"Pre-Training Identification of Graph Winning Tickets in Adaptive\n  Spatial-Temporal Graph Neural Networks","summary":"  In this paper, we present a novel method to significantly enhance the\ncomputational efficiency of Adaptive Spatial-Temporal Graph Neural Networks\n(ASTGNNs) by introducing the concept of the Graph Winning Ticket (GWT), derived\nfrom the Lottery Ticket Hypothesis (LTH). By adopting a pre-determined star\ntopology as a GWT prior to training, we balance edge reduction with efficient\ninformation propagation, reducing computational demands while maintaining high\nmodel performance. Both the time and memory computational complexity of\ngenerating adaptive spatial-temporal graphs is significantly reduced from\n$\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$. Our approach streamlines the ASTGNN\ndeployment by eliminating the need for exhaustive training, pruning, and\nretraining cycles, and demonstrates empirically across various datasets that it\nis possible to achieve comparable performance to full models with substantially\nlower computational costs. Specifically, our approach enables training ASTGNNs\non the largest scale spatial-temporal dataset using a single A6000 equipped\nwith 48 GB of memory, overcoming the out-of-memory issue encountered during\noriginal training and even achieving state-of-the-art performance.\n{Furthermore, we delve into the effectiveness of the GWT from the perspective\nof spectral graph theory, providing substantial theoretical support.} This\nadvancement not only proves the existence of efficient sub-networks within\nASTGNNs but also broadens the applicability of the LTH in resource-constrained\nsettings, marking a significant step forward in the field of graph neural\nnetworks. Code is available at https://anonymous.4open.science/r/paper-1430.\n","authors":["Wenying Duan","Tianxiang Fang","Hong Rao","Xiaoxi He"],"pdf_url":"https://arxiv.org/pdf/2406.08287v1.pdf","comment":"Conference paper, accepted by KDD' 24"},{"id":"http://arxiv.org/abs/2308.12013v3","updated":"2024-06-12T14:52:10Z","published":"2023-08-23T09:09:32Z","title":"Quantum-Noise-Driven Generative Diffusion Models","summary":"  Generative models realized with machine learning techniques are powerful\ntools to infer complex and unknown data distributions from a finite number of\ntraining samples in order to produce new synthetic data. Diffusion models are\nan emerging framework that have recently overcome the performance of the\ngenerative adversarial networks in creating synthetic text and high-quality\nimages. Here, we propose and discuss the quantum generalization of diffusion\nmodels, i.e., three quantum-noise-driven generative diffusion models that could\nbe experimentally tested on real quantum systems. The idea is to harness unique\nquantum features, in particular the non-trivial interplay among coherence,\nentanglement and noise that the currently available noisy quantum processors do\nunavoidably suffer from, in order to overcome the main computational burdens of\nclassical diffusion models during inference. Hence, we suggest to exploit\nquantum noise not as an issue to be detected and solved but instead as a very\nremarkably beneficial key ingredient to generate much more complex probability\ndistributions that would be difficult or even impossible to express\nclassically, and from which a quantum processor might sample more efficiently\nthan a classical one. An example of numerical simulations for an hybrid\nclassical-quantum generative diffusion model is also included. Therefore, our\nresults are expected to pave the way for new quantum-inspired or quantum-based\ngenerative diffusion algorithms addressing more powerfully classical tasks as\ndata generation/prediction with widespread real-world applications ranging from\nclimate forecasting to neuroscience, from traffic flow analysis to financial\nforecasting.\n","authors":["Marco Parigi","Stefano Martina","Filippo Caruso"],"pdf_url":"https://arxiv.org/pdf/2308.12013v3.pdf","comment":"27 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.08281v1","updated":"2024-06-12T14:47:27Z","published":"2024-06-12T14:47:27Z","title":"Conformal Load Prediction with Transductive Graph Autoencoders","summary":"  Predicting edge weights on graphs has various applications, from\ntransportation systems to social networks. This paper describes a Graph Neural\nNetwork (GNN) approach for edge weight prediction with guaranteed coverage. We\nleverage conformal prediction to calibrate the GNN outputs and produce valid\nprediction intervals. We handle data heteroscedasticity through error\nreweighting and Conformalized Quantile Regression (CQR). We compare the\nperformance of our method against baseline techniques on real-world\ntransportation datasets. Our approach has better coverage and efficiency than\nall baselines and showcases robustness and adaptability.\n","authors":["Rui Luo","Nicolo Colombo"],"pdf_url":"https://arxiv.org/pdf/2406.08281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19557v3","updated":"2024-06-12T14:38:48Z","published":"2024-04-30T13:39:26Z","title":"Neural Dynamic Data Valuation","summary":"  Data constitute the foundational component of the data economy and its\nmarketplaces. Efficient and fair data valuation has emerged as a topic of\nsignificant interest.\\ Many approaches based on marginal contribution have\nshown promising results in various downstream tasks. However, they are well\nknown to be computationally expensive as they require training a large number\nof utility functions, which are used to evaluate the usefulness or value of a\ngiven dataset for a specific purpose. As a result, it has been recognized as\ninfeasible to apply these methods to a data marketplace involving large-scale\ndatasets. Consequently, a critical issue arises: how can the re-training of the\nutility function be avoided? To address this issue, we propose a novel data\nvaluation method from the perspective of optimal control, named the neural\ndynamic data valuation (NDDV). Our method has solid theoretical interpretations\nto accurately identify the data valuation via the sensitivity of the data\noptimal control state. In addition, we implement a data re-weighting strategy\nto capture the unique features of data points, ensuring fairness through the\ninteraction between data points and the mean-field states. Notably, our method\nrequires only training once to estimate the value of all data points,\nsignificantly improving the computational efficiency. We conduct comprehensive\nexperiments using different datasets and tasks. The results demonstrate that\nthe proposed NDDV method outperforms the existing state-of-the-art data\nvaluation methods in accurately identifying data points with either high or low\nvalues and is more computationally efficient.\n","authors":["Zhangyong Liang","Huanhuan Gao","Ji Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.19557v3.pdf","comment":"43 pages, 19 figures"},{"id":"http://arxiv.org/abs/2406.08272v1","updated":"2024-06-12T14:37:29Z","published":"2024-06-12T14:37:29Z","title":"The Importance of Positional Encoding Initialization in Transformers for\n  Relational Reasoning","summary":"  Relational reasoning refers to the ability to infer and understand the\nrelations between multiple entities. In humans, this ability underpins many\nhigher cognitive functions, such as problem solving and decision-making, and\nhas been reliably linked to fluid intelligence. Despite machine learning models\nmaking impressive advances across various domains, such as natural language\nprocessing and vision, the extent to which such models can perform relational\nreasoning tasks remains unclear. Here we study the importance of positional\nencoding (PE) for relational reasoning in the Transformer, and find that a\nlearnable PE outperforms all other commonly-used PEs (e.g., absolute, relative,\nrotary, etc.). Moreover, we find that when using a PE with a learnable\nparameter, the choice of initialization greatly influences the learned\nrepresentations and its downstream generalization performance. Specifically, we\nfind that a learned PE initialized from a small-norm distribution can 1)\nuncover ground-truth position information, 2) generalize in the presence of\nnoisy inputs, and 3) produce behavioral patterns that are consistent with human\nperformance. Our results shed light on the importance of learning\nhigh-performing and robust PEs during relational reasoning tasks, which will\nprove useful for tasks in which ground truth positions are not provided or not\nknown.\n","authors":["Takuya Ito","Luca Cocchi","Tim Klinger","Parikshit Ram","Murray Campbell","Luke Hearne"],"pdf_url":"https://arxiv.org/pdf/2406.08272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08269v1","updated":"2024-06-12T14:35:19Z","published":"2024-06-12T14:35:19Z","title":"Analyzing constrained LLM through PDFA-learning","summary":"  We define a congruence that copes with null next-symbol probabilities that\narise when the output of a language model is constrained by some means during\ntext generation. We develop an algorithm for efficiently learning the quotient\nwith respect to this congruence and evaluate it on case studies for analyzing\nstatistical properties of LLM.\n","authors":["Matías Carrasco","Franz Mayr","Sergio Yovine","Johny Kidd","Martín Iturbide","Juan Pedro da Silva","Alejo Garat"],"pdf_url":"https://arxiv.org/pdf/2406.08269v1.pdf","comment":"Workshop Paper"},{"id":"http://arxiv.org/abs/2406.08267v1","updated":"2024-06-12T14:35:13Z","published":"2024-06-12T14:35:13Z","title":"A deep cut into Split Federated Self-supervised Learning","summary":"  Collaborative self-supervised learning has recently become feasible in highly\ndistributed environments by dividing the network layers between client devices\nand a central server. However, state-of-the-art methods, such as MocoSFL, are\noptimized for network division at the initial layers, which decreases the\nprotection of the client data and increases communication overhead. In this\npaper, we demonstrate that splitting depth is crucial for maintaining privacy\nand communication efficiency in distributed training. We also show that MocoSFL\nsuffers from a catastrophic quality deterioration for the minimal communication\noverhead. As a remedy, we introduce Momentum-Aligned contrastive Split\nFederated Learning (MonAcoSFL), which aligns online and momentum client models\nduring training procedure. Consequently, we achieve state-of-the-art accuracy\nwhile significantly reducing the communication overhead, making MonAcoSFL more\npractical in real-world scenarios.\n","authors":["Marcin Przewięźlikowski","Marcin Osial","Bartosz Zieliński","Marek Śmieja"],"pdf_url":"https://arxiv.org/pdf/2406.08267v1.pdf","comment":"Accepted to European Conference on Machine Learning (ECML) 2024"},{"id":"http://arxiv.org/abs/2406.08249v1","updated":"2024-06-12T14:18:07Z","published":"2024-06-12T14:18:07Z","title":"Dataset Enhancement with Instance-Level Augmentations","summary":"  We present a method for expanding a dataset by incorporating knowledge from\nthe wide distribution of pre-trained latent diffusion models. Data\naugmentations typically incorporate inductive biases about the image formation\nprocess into the training (e.g. translation, scaling, colour changes, etc.).\nHere, we go beyond simple pixel transformations and introduce the concept of\ninstance-level data augmentation by repainting parts of the image at the level\nof object instances. The method combines a conditional diffusion model with\ndepth and edge maps control conditioning to seamlessly repaint individual\nobjects inside the scene, being applicable to any segmentation or detection\ndataset. Used as a data augmentation method, it improves the performance and\ngeneralization of the state-of-the-art salient object detection, semantic\nsegmentation and object detection models. By redrawing all privacy-sensitive\ninstances (people, license plates, etc.), the method is also applicable for\ndata anonymization. We also release fully synthetic and anonymized expansions\nfor popular datasets: COCO, Pascal VOC and DUTS.\n","authors":["Orest Kupyn","Christian Rupprecht"],"pdf_url":"https://arxiv.org/pdf/2406.08249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08246v1","updated":"2024-06-12T14:15:15Z","published":"2024-06-12T14:15:15Z","title":"Leveraging Large Language Models for Web Scraping","summary":"  Large Language Models (LLMs) demonstrate remarkable capabilities in\nreplicating human tasks and boosting productivity. However, their direct\napplication for data extraction presents limitations due to a prioritisation of\nfluency over factual accuracy and a restricted ability to manipulate specific\ninformation. Therefore to overcome these limitations, this research leverages\nthe knowledge representation power of pre-trained LLMs and the targeted\ninformation access enabled by RAG models, this research investigates a\ngeneral-purpose accurate data scraping recipe for RAG models designed for\nlanguage generation. To capture knowledge in a more modular and interpretable\nway, we use pre trained language models with a latent knowledge retriever,\nwhich allows the model to retrieve and attend over documents from a large\ncorpus. We utilised RAG model architecture and did an in-depth analysis of\ntheir capabilities under three tasks: (i) Semantic Classification of HTML\nelements, (ii) Chunking HTML text for effective understanding, and (iii)\ncomparing results from different LLMs and ranking algorithms. While previous\nwork has developed dedicated architectures and training procedures for HTML\nunderstanding and extraction, we show that LLMs pre-trained on standard natural\nlanguage with an addition of effective chunking, searching and ranking\nalgorithms, can prove to be efficient data scraping tool to extract complex\ndata from unstructured text. Future research directions include addressing the\nchallenges of provenance tracking and dynamic knowledge updates within the\nproposed RAG-based data extraction framework. By overcoming these limitations,\nthis approach holds the potential to revolutionise data extraction from vast\nrepositories of textual information.\n","authors":["Aman Ahluwalia","Suhrud Wani"],"pdf_url":"https://arxiv.org/pdf/2406.08246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17123v2","updated":"2024-06-12T14:12:17Z","published":"2024-04-26T02:40:03Z","title":"Text Sentiment Analysis and Classification Based on Bidirectional Gated\n  Recurrent Units (GRUs) Model","summary":"  This paper explores the importance of text sentiment analysis and\nclassification in the field of natural language processing, and proposes a new\napproach to sentiment analysis and classification based on the bidirectional\ngated recurrent units (GRUs) model. The study firstly analyses the word cloud\nmodel of the text with six sentiment labels, and then carries out data\npreprocessing, including the steps of removing special symbols, punctuation\nmarks, numbers, stop words and non-alphabetic parts. Subsequently, the data set\nis divided into training set and test set, and through model training and\ntesting, it is found that the accuracy of the validation set is increased from\n85% to 93% with training, which is an increase of 8%; at the same time, the\nloss value of the validation set decreases from 0.7 to 0.1 and tends to be\nstable, and the model is gradually close to the actual value, which can\neffectively classify the text emotions. The confusion matrix shows that the\naccuracy of the model on the test set reaches 94.8%, the precision is 95.9%,\nthe recall is 99.1%, and the F1 score is 97.4%, which proves that the model has\ngood generalisation ability and classification effect. Overall, the study\ndemonstrated an effective method for text sentiment analysis and classification\nwith satisfactory results.\n","authors":["Wei Xu","Jianlong Chen","Zhicheng Ding","Jinyin Wang"],"pdf_url":"https://arxiv.org/pdf/2404.17123v2.pdf","comment":"accepted by the 2nd International Conference on Software Engineering\n  and Machine Learning (CONF-SEML 2024)"},{"id":"http://arxiv.org/abs/2406.08238v1","updated":"2024-06-12T14:04:26Z","published":"2024-06-12T14:04:26Z","title":"Residual Learning and Context Encoding for Adaptive Offline-to-Online\n  Reinforcement Learning","summary":"  Offline reinforcement learning (RL) allows learning sequential behavior from\nfixed datasets. Since offline datasets do not cover all possible situations,\nmany methods collect additional data during online fine-tuning to improve\nperformance. In general, these methods assume that the transition dynamics\nremain the same during both the offline and online phases of training. However,\nin many real-world applications, such as outdoor construction and navigation\nover rough terrain, it is common for the transition dynamics to vary between\nthe offline and online phases. Moreover, the dynamics may vary during the\nonline fine-tuning. To address this problem of changing dynamics from offline\nto online RL we propose a residual learning approach that infers dynamics\nchanges to correct the outputs of the offline solution. At the online\nfine-tuning phase, we train a context encoder to learn a representation that is\nconsistent inside the current online learning environment while being able to\npredict dynamic transitions. Experiments in D4RL MuJoCo environments, modified\nto support dynamics' changes upon environment resets, show that our approach\ncan adapt to these dynamic changes and generalize to unseen perturbations in a\nsample-efficient way, whilst comparison methods cannot.\n","authors":["Mohammadreza Nakhaei","Aidan Scannell","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2406.08238v1.pdf","comment":"10 pages, 5 figures, 1 table. Accepted at L4DC 2024"},{"id":"http://arxiv.org/abs/2406.08234v1","updated":"2024-06-12T14:01:12Z","published":"2024-06-12T14:01:12Z","title":"MaIL: Improving Imitation Learning with Mamba","summary":"  This work introduces Mamba Imitation Learning (MaIL), a novel imitation\nlearning (IL) architecture that offers a computationally efficient alternative\nto state-of-the-art (SoTA) Transformer policies. Transformer-based policies\nhave achieved remarkable results due to their ability in handling\nhuman-recorded data with inherently non-Markovian behavior. However, their high\nperformance comes with the drawback of large models that complicate effective\ntraining. While state space models (SSMs) have been known for their efficiency,\nthey were not able to match the performance of Transformers. Mamba\nsignificantly improves the performance of SSMs and rivals against Transformers,\npositioning it as an appealing alternative for IL policies. MaIL leverages\nMamba as a backbone and introduces a formalism that allows using Mamba in the\nencoder-decoder structure. This formalism makes it a versatile architecture\nthat can be used as a standalone policy or as part of a more advanced\narchitecture, such as a diffuser in the diffusion process. Extensive\nevaluations on the LIBERO IL benchmark and three real robot experiments show\nthat MaIL: i) outperforms Transformers in all LIBERO tasks, ii) achieves good\nperformance even with small datasets, iii) is able to effectively process\nmulti-modal sensory inputs, iv) is more robust to input noise compared to\nTransformers.\n","authors":["Xiaogang Jia","Qian Wang","Atalay Donat","Bowen Xing","Ge Li","Hongyi Zhou","Onur Celik","Denis Blessing","Rudolf Lioutikov","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2406.08234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08229v1","updated":"2024-06-12T13:59:31Z","published":"2024-06-12T13:59:31Z","title":"GPT4Rec: Graph Prompt Tuning for Streaming Recommendation","summary":"  In the realm of personalized recommender systems, the challenge of adapting\nto evolving user preferences and the continuous influx of new users and items\nis paramount. Conventional models, typically reliant on a static training-test\napproach, struggle to keep pace with these dynamic demands. Streaming\nrecommendation, particularly through continual graph learning, has emerged as a\nnovel solution. However, existing methods in this area either rely on\nhistorical data replay, which is increasingly impractical due to stringent data\nprivacy regulations; or are inability to effectively address the over-stability\nissue; or depend on model-isolation and expansion strategies. To tackle these\ndifficulties, we present GPT4Rec, a Graph Prompt Tuning method for streaming\nRecommendation. Given the evolving user-item interaction graph, GPT4Rec first\ndisentangles the graph patterns into multiple views. After isolating specific\ninteraction patterns and relationships in different views, GPT4Rec utilizes\nlightweight graph prompts to efficiently guide the model across varying\ninteraction patterns within the user-item graph. Firstly, node-level prompts\nare employed to instruct the model to adapt to changes in the attributes or\nproperties of individual nodes within the graph. Secondly, structure-level\nprompts guide the model in adapting to broader patterns of connectivity and\nrelationships within the graph. Finally, view-level prompts are innovatively\ndesigned to facilitate the aggregation of information from multiple\ndisentangled views. These prompt designs allow GPT4Rec to synthesize a\ncomprehensive understanding of the graph, ensuring that all vital aspects of\nthe user-item interactions are considered and effectively integrated.\nExperiments on four diverse real-world datasets demonstrate the effectiveness\nand efficiency of our proposal.\n","authors":["Peiyan Zhang","Yuchen Yan","Xi Zhang","Liying Kang","Chaozhuo Li","Feiran Huang","Senzhang Wang","Sunghun Kim"],"pdf_url":"https://arxiv.org/pdf/2406.08229v1.pdf","comment":"Accepted by SIGIR 2024. arXiv admin note: text overlap with\n  arXiv:2303.11700 by other authors"},{"id":"http://arxiv.org/abs/2406.06581v2","updated":"2024-06-12T13:59:13Z","published":"2024-06-04T16:09:13Z","title":"Set-Based Prompting: Provably Solving the Language Model Order\n  Dependency Problem","summary":"  The development of generative language models that can create long and\ncoherent textual outputs via autoregression has lead to a proliferation of uses\nand a corresponding sweep of analyses as researches work to determine the\nlimitations of this new paradigm. Unlike humans, these 'Large Language Models'\n(LLMs) are highly sensitive to small changes in their inputs, leading to\nunwanted inconsistency in their behavior. One problematic inconsistency when\nLLMs are used to answer multiple-choice questions or analyze multiple inputs is\norder dependency: the output of an LLM can (and often does) change\nsignificantly when sub-sequences are swapped, despite both orderings being\nsemantically identical. In this paper we present Set-Based Prompting, a\ntechnique that guarantees the output of an LLM will not have order dependence\non a specified set of sub-sequences. We show that this method provably\neliminates order dependency, and that it can be applied to any\ntransformer-based LLM to enable text generation that is unaffected by\nre-orderings. Delving into the implications of our method, we show that,\ndespite our inputs being out of distribution, the impact on expected accuracy\nis small, where the expectation is over the order of uniformly chosen shuffling\nof the candidate responses, and usually significantly less in practice. Thus,\nSet-Based Prompting can be used as a 'dropped-in' method on fully trained\nmodels. Finally, we discuss how our method's success suggests that other strong\nguarantees can be obtained on LLM performance via modifying the input\nrepresentations.\n","authors":["Reid McIlroy-Young","Katrina Brown","Conlan Olson","Linjun Zhang","Cynthia Dwork"],"pdf_url":"https://arxiv.org/pdf/2406.06581v2.pdf","comment":"29 pages, 27 figures, code\n  https://github.com/reidmcy/set-based-prompting"},{"id":"http://arxiv.org/abs/2306.11488v3","updated":"2024-06-12T13:58:19Z","published":"2023-06-20T12:20:23Z","title":"Informed POMDP: Leveraging Additional Information in Model-Based RL","summary":"  In this work, we generalize the problem of learning through interaction in a\nPOMDP by accounting for eventual additional information available at training\ntime. First, we introduce the informed POMDP, a new learning paradigm offering\na clear distinction between the information at training and the observation at\nexecution. Next, we propose an objective that leverages this information for\nlearning a sufficient statistic of the history for the optimal control. We then\nadapt this informed objective to learn a world model able to sample latent\ntrajectories. Finally, we empirically show a learning speed improvement in\nseveral environments using this informed world model in the Dreamer algorithm.\nThese results and the simplicity of the proposed adaptation advocate for a\nsystematic consideration of eventual additional information when learning in a\nPOMDP using model-based RL.\n","authors":["Gaspard Lambrechts","Adrien Bolland","Damien Ernst"],"pdf_url":"https://arxiv.org/pdf/2306.11488v3.pdf","comment":"In Reinforcement Learning Conference, 2024. 10 pages, 22 pages total,\n  10 figures"},{"id":"http://arxiv.org/abs/2405.17003v2","updated":"2024-06-12T13:57:58Z","published":"2024-05-27T09:47:09Z","title":"Graph Condensation for Open-World Graph Learning","summary":"  The burgeoning volume of graph data presents significant computational\nchallenges in training graph neural networks (GNNs), critically impeding their\nefficiency in various applications. To tackle this challenge, graph\ncondensation (GC) has emerged as a promising acceleration solution, focusing on\nthe synthesis of a compact yet representative graph for efficiently training\nGNNs while retaining performance. Despite the potential to promote scalable use\nof GNNs, existing GC methods are limited to aligning the condensed graph with\nmerely the observed static graph distribution. This limitation significantly\nrestricts the generalization capacity of condensed graphs, particularly in\nadapting to dynamic distribution changes. In real-world scenarios, however,\ngraphs are dynamic and constantly evolving, with new nodes and edges being\ncontinually integrated. Consequently, due to the limited generalization\ncapacity of condensed graphs, applications that employ GC for efficient GNN\ntraining end up with sub-optimal GNNs when confronted with evolving graph\nstructures and distributions in dynamic real-world situations. To overcome this\nissue, we propose open-world graph condensation (OpenGC), a robust GC framework\nthat integrates structure-aware distribution shift to simulate evolving graph\npatterns and exploit the temporal environments for invariance condensation.\nThis approach is designed to extract temporal invariant patterns from the\noriginal graph, thereby enhancing the generalization capabilities of the\ncondensed graph and, subsequently, the GNNs trained on it. Extensive\nexperiments on both real-world and synthetic evolving graphs demonstrate that\nOpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic\nchanges in open-world graph environments.\n","authors":["Xinyi Gao","Tong Chen","Wentao Zhang","Yayong Li","Xiangguo Sun","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2405.17003v2.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2406.07295v2","updated":"2024-06-12T13:55:30Z","published":"2024-06-11T14:24:00Z","title":"Multi-objective Reinforcement learning from AI Feedback","summary":"  This paper presents Multi-Objective Reinforcement Learning from AI Feedback\n(MORLAIF), a novel approach to improving the alignment and performance of\nlanguage models trained using reinforcement learning from AI feedback (RLAIF).\nIn contrast to standard approaches that train a single preference model to\nrepresent all human preferences, MORLAIF decomposes this task into multiple\nsimpler principles, such as toxicity, factuality, and sycophancy. Separate\npreference models are trained for each principle using feedback from\nGPT-3.5-Turbo. These preference model scores are then combined using different\nscalarization functions to provide a reward signal for Proximal Policy\nOptimization (PPO) training of the target language model. Our experiments\nindicate that MORLAIF outperforms the standard RLAIF baselines and that MORLAIF\ncan be used to align larger language models using smaller ones. Surprisingly,\nthe choice of scalarization function does not appear to significantly impact\nthe results.\n","authors":["Marcus Williams"],"pdf_url":"https://arxiv.org/pdf/2406.07295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08226v1","updated":"2024-06-12T13:55:12Z","published":"2024-06-12T13:55:12Z","title":"DistilDoc: Knowledge Distillation for Visually-Rich Document\n  Applications","summary":"  This work explores knowledge distillation (KD) for visually-rich document\n(VRD) applications such as document layout analysis (DLA) and document image\nclassification (DIC). While VRD research is dependent on increasingly\nsophisticated and cumbersome models, the field has neglected to study\nefficiency via model compression. Here, we design a KD experimentation\nmethodology for more lean, performant models on document understanding (DU)\ntasks that are integral within larger task pipelines. We carefully selected KD\nstrategies (response-based, feature-based) for distilling knowledge to and from\nbackbones with different architectures (ResNet, ViT, DiT) and capacities (base,\nsmall, tiny). We study what affects the teacher-student knowledge gap and find\nthat some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can\nconsistently outperform supervised student training. Furthermore, we design\ndownstream task setups to evaluate covariate shift and the robustness of\ndistilled DLA models on zero-shot layout-aware document visual question\nanswering (DocVQA). DLA-KD experiments result in a large mAP knowledge gap,\nwhich unpredictably translates to downstream robustness, accentuating the need\nto further explore how to efficiently obtain more semantic document layout\nawareness.\n","authors":["Jordy Van Landeghem","Subhajit Maity","Ayan Banerjee","Matthew Blaschko","Marie-Francine Moens","Josep Lladós","Sanket Biswas"],"pdf_url":"https://arxiv.org/pdf/2406.08226v1.pdf","comment":"Accepted to ICDAR 2024 (Athens, Greece)"},{"id":"http://arxiv.org/abs/2403.11827v2","updated":"2024-06-12T13:54:11Z","published":"2024-03-18T14:34:16Z","title":"Sound Event Detection and Localization with Distance Estimation","summary":"  Sound Event Detection and Localization (SELD) is a combined task of\nidentifying sound events and their corresponding direction-of-arrival (DOA).\nWhile this task has numerous applications and has been extensively researched\nin recent years, it fails to provide full information about the sound source\nposition. In this paper, we overcome this problem by extending the task to\nSound Event Detection, Localization with Distance Estimation (3D SELD). We\nstudy two ways of integrating distance estimation within the SELD core - a\nmulti-task approach, in which the problem is tackled by a separate model\noutput, and a single-task approach obtained by extending the multi-ACCDOA\nmethod to include distance information. We investigate both methods for the\nAmbisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial\nSoundscapes 2023. Moreover, our study involves experiments on the loss function\nrelated to the distance estimation part. Our results show that it is possible\nto perform 3D SELD without any degradation of performance in sound event\ndetection and DOA estimation.\n","authors":["Daniel Aleksander Krause","Archontis Politis","Annamaria Mesaros"],"pdf_url":"https://arxiv.org/pdf/2403.11827v2.pdf","comment":"This paper has been accepted for the 32nd European Signal Processing\n  Conference EUSIPCO 2024 in Lyon"},{"id":"http://arxiv.org/abs/2310.09605v3","updated":"2024-06-12T13:52:36Z","published":"2023-10-14T15:48:15Z","title":"Penetrative AI: Making LLMs Comprehend the Physical World","summary":"  Recent developments in Large Language Models (LLMs) have demonstrated their\nremarkable capabilities across a range of tasks. Questions, however, persist\nabout the nature of LLMs and their potential to integrate common-sense human\nknowledge when performing tasks involving information about the real physical\nworld. This paper delves into these questions by exploring how LLMs can be\nextended to interact with and reason about the physical world through IoT\nsensors and actuators, a concept that we term \"Penetrative AI\". The paper\nexplores such an extension at two levels of LLMs' ability to penetrate into the\nphysical world via the processing of sensory signals. Our preliminary findings\nindicate that LLMs, with ChatGPT being the representative example in our\nexploration, have considerable and unique proficiency in employing the embedded\nworld knowledge for interpreting IoT sensor data and reasoning over them about\ntasks in the physical realm. Not only this opens up new applications for LLMs\nbeyond traditional text-based tasks, but also enables new ways of incorporating\nhuman knowledge in cyber-physical systems.\n","authors":["Huatao Xu","Liying Han","Qirui Yang","Mo Li","Mani Srivastava"],"pdf_url":"https://arxiv.org/pdf/2310.09605v3.pdf","comment":"ACL Findings 2024"},{"id":"http://arxiv.org/abs/2307.03690v4","updated":"2024-06-12T13:47:38Z","published":"2023-06-19T20:20:10Z","title":"Suppressing unknown disturbances to dynamical systems using machine\n  learning","summary":"  Identifying and suppressing unknown disturbances to dynamical systems is a\nproblem with applications in many different fields. Here we present a\nmodel-free method to identify and suppress an unknown disturbance to an unknown\nsystem based only on previous observations of the system under the influence of\na known forcing function. We find that, under very mild restrictions on the\ntraining function, our method is able to robustly identify and suppress a large\nclass of unknown disturbances. We illustrate our scheme with the identification\nof both deterministic and stochastic unknown disturbances to an analog electric\nchaotic circuit and with a numerical example where a chaotic disturbance to the\nLorenz system is identified and suppressed.\n","authors":["Juan G. Restrepo","Clayton P. Byers","Per Sebastian Skardal"],"pdf_url":"https://arxiv.org/pdf/2307.03690v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08217v1","updated":"2024-06-12T13:45:47Z","published":"2024-06-12T13:45:47Z","title":"Runtime Freezing: Dynamic Class Loss for Multi-Organ 3D Segmentation","summary":"  Segmentation has become a crucial pre-processing step to many refined\ndownstream tasks, and particularly so in the medical domain. Even with recent\nimprovements in segmentation models, many segmentation tasks remain difficult.\nWhen multiple organs are segmented simultaneously, difficulties are due not\nonly to the limited availability of labelled data, but also to class imbalance.\nIn this work we propose dynamic class-based loss strategies to mitigate the\neffects of highly imbalanced training data. We show how our approach improves\nsegmentation performance on a challenging Multi-Class 3D Abdominal Organ\ndataset.\n","authors":["James Willoughby","Irina Voiculescu"],"pdf_url":"https://arxiv.org/pdf/2406.08217v1.pdf","comment":"4 Pages. Accepted to ISBI 2024"},{"id":"http://arxiv.org/abs/2404.14027v3","updated":"2024-06-12T13:43:50Z","published":"2024-04-22T09:43:03Z","title":"OccFeat: Self-supervised Occupancy Feature Prediction for Pretraining\n  BEV Segmentation Networks","summary":"  We introduce a self-supervised pretraining method, called OccFeat, for\ncamera-only Bird's-Eye-View (BEV) segmentation networks. With OccFeat, we\npretrain a BEV network via occupancy prediction and feature distillation tasks.\nOccupancy prediction provides a 3D geometric understanding of the scene to the\nmodel. However, the geometry learned is class-agnostic. Hence, we add semantic\ninformation to the model in the 3D space through distillation from a\nself-supervised pretrained image foundation model. Models pretrained with our\nmethod exhibit improved BEV semantic segmentation performance, particularly in\nlow-data scenarios. Moreover, empirical results affirm the efficacy of\nintegrating feature distillation with 3D occupancy prediction in our\npretraining approach. Repository: https://github.com/valeoai/Occfeat\n","authors":["Sophia Sirko-Galouchenko","Alexandre Boulch","Spyros Gidaris","Andrei Bursuc","Antonin Vobecky","Patrick Pérez","Renaud Marlet"],"pdf_url":"https://arxiv.org/pdf/2404.14027v3.pdf","comment":"Accepted to CVPR 2024, Workshop on Autonomous Driving"},{"id":"http://arxiv.org/abs/2406.08210v1","updated":"2024-06-12T13:41:07Z","published":"2024-06-12T13:41:07Z","title":"Expressivity and Generalization: Fragment-Biases for Molecular GNNs","summary":"  Although recent advances in higher-order Graph Neural Networks (GNNs) improve\nthe theoretical expressiveness and molecular property predictive performance,\nthey often fall short of the empirical performance of models that explicitly\nuse fragment information as inductive bias. However, for these approaches,\nthere exists no theoretic expressivity study. In this work, we propose the\nFragment-WL test, an extension to the well-known Weisfeiler & Leman (WL) test,\nwhich enables the theoretic analysis of these fragment-biased GNNs. Building on\nthe insights gained from the Fragment-WL test, we develop a new GNN\narchitecture and a fragmentation with infinite vocabulary that significantly\nboosts expressiveness. We show the effectiveness of our model on synthetic and\nreal-world data where we outperform all GNNs on Peptides and have 12% lower\nerror than all GNNs on ZINC and 34% lower error than other fragment-biased\nmodels. Furthermore, we show that our model exhibits superior generalization\ncapabilities compared to the latest transformer-based architectures,\npositioning it as a robust solution for a range of molecular modeling tasks.\n","authors":["Tom Wollschläger","Niklas Kemper","Leon Hetzel","Johanna Sommer","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2406.08210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08209v1","updated":"2024-06-12T13:40:47Z","published":"2024-06-12T13:40:47Z","title":"Forward-Euler time-discretization for Wasserstein gradient flows can be\n  wrong","summary":"  In this note, we examine the forward-Euler discretization for simulating\nWasserstein gradient flows. We provide two counter-examples showcasing the\nfailure of this discretization even for a simple case where the energy\nfunctional is defined as the KL divergence against some nicely structured\nprobability densities. A simple explanation of this failure is also discussed.\n","authors":["Yewei Xu","Qin Li"],"pdf_url":"https://arxiv.org/pdf/2406.08209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08207v1","updated":"2024-06-12T13:39:44Z","published":"2024-06-12T13:39:44Z","title":"Transformer-based Model for ASR N-Best Rescoring and Rewriting","summary":"  Voice assistants increasingly use on-device Automatic Speech Recognition\n(ASR) to ensure speed and privacy. However, due to resource constraints on the\ndevice, queries pertaining to complex information domains often require further\nprocessing by a search engine. For such applications, we propose a novel\nTransformer based model capable of rescoring and rewriting, by exploring full\ncontext of the N-best hypotheses in parallel. We also propose a new\ndiscriminative sequence training objective that can work well for both rescore\nand rewrite tasks. We show that our Rescore+Rewrite model outperforms the\nRescore-only baseline, and achieves up to an average 8.6% relative Word Error\nRate (WER) reduction over the ASR system by itself.\n","authors":["Iwen E. Kang","Christophe Van Gysel","Man-Hung Siu"],"pdf_url":"https://arxiv.org/pdf/2406.08207v1.pdf","comment":"Interspeech '24"},{"id":"http://arxiv.org/abs/2406.08206v1","updated":"2024-06-12T13:39:32Z","published":"2024-06-12T13:39:32Z","title":"Sources of Gain: Decomposing Performance in Conditional Average Dose\n  Response Estimation","summary":"  Estimating conditional average dose responses (CADR) is an important but\nchallenging problem. Estimators must correctly model the potentially complex\nrelationships between covariates, interventions, doses, and outcomes. In recent\nyears, the machine learning community has shown great interest in developing\ntailored CADR estimators that target specific challenges. Their performance is\ntypically evaluated against other methods on (semi-) synthetic benchmark\ndatasets. Our paper analyses this practice and shows that using popular\nbenchmark datasets without further analysis is insufficient to judge model\nperformance. Established benchmarks entail multiple challenges, whose impacts\nmust be disentangled. Therefore, we propose a novel decomposition scheme that\nallows the evaluation of the impact of five distinct components contributing to\nCADR estimator performance. We apply this scheme to eight popular CADR\nestimators on four widely-used benchmark datasets, running nearly 1,500\nindividual experiments. Our results reveal that most established benchmarks are\nchallenging for reasons different from their creators' claims. Notably,\nconfounding, the key challenge tackled by most estimators, is not an issue in\nany of the considered datasets. We discuss the major implications of our\nfindings and present directions for future research.\n","authors":["Christopher Bockel-Rickermann","Toon Vanderschueren","Tim Verdonck","Wouter Verbeke"],"pdf_url":"https://arxiv.org/pdf/2406.08206v1.pdf","comment":"25 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.08205v1","updated":"2024-06-12T13:38:48Z","published":"2024-06-12T13:38:48Z","title":"What do we know about Hugging Face? A systematic literature review and\n  quantitative validation of qualitative claims","summary":"  Background: Collaborative Software Package Registries (SPRs) are an integral\npart of the software supply chain. Much engineering work synthesizes SPR\npackage into applications. Prior research has examined SPRs for traditional\nsoftware, such as NPM (JavaScript) and PyPI (Python). Pre-Trained Model (PTM)\nRegistries are an emerging class of SPR of increasing importance, because they\nsupport the deep learning supply chain.\n  Aims: Recent empirical research has examined PTM registries in ways such as\nvulnerabilities, reuse processes, and evolution. However, no existing research\nsynthesizes them to provide a systematic understanding of the current\nknowledge. Some of the existing research includes qualitative claims lacking\nquantitative analysis. Our research fills these gaps by providing a knowledge\nsynthesis and quantitative analyses.\n  Methods: We first conduct a systematic literature review (SLR). We then\nobserve that some of the claims are qualitative. We identify quantifiable\nmetrics associated with those claims, and measure in order to substantiate\nthese claims.\n  Results: From our SLR, we identify 12 claims about PTM reuse on the\nHuggingFace platform, 4 of which lack quantitative validation. We successfully\ntest 3 of these claims through a quantitative analysis, and directly compare\none with traditional software. Our findings corroborate qualitative claims with\nquantitative measurements. Our findings are: (1) PTMs have a much higher\nturnover rate than traditional software, indicating a dynamic and rapidly\nevolving reuse environment within the PTM ecosystem; and (2) There is a strong\ncorrelation between documentation quality and PTM popularity.\n  Conclusions: We confirm qualitative research claims with concrete metrics,\nsupporting prior qualitative and case study research. Our measures show further\ndynamics of PTM reuse, inspiring research infrastructure and new measures.\n","authors":["Jason Jones","Wenxin Jiang","Nicholas Synovic","George K. Thiruvathukal","James C. Davis"],"pdf_url":"https://arxiv.org/pdf/2406.08205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08193v1","updated":"2024-06-12T13:22:26Z","published":"2024-06-12T13:22:26Z","title":"Minimal Communication-Cost Statistical Learning","summary":"  A client device which has access to $n$ training data samples needs to obtain\na statistical hypothesis or model $W$ and then to send it to a remote server.\nThe client and the server devices share some common randomness sequence as well\nas a prior on the hypothesis space. In this problem a suitable hypothesis or\nmodel $W$ should meet two distinct design criteria simultaneously: (i) small\n(population) risk during the inference phase and (ii) small 'complexity' for it\nto be conveyed to the server with minimum communication cost. In this paper, we\npropose a joint training and source coding scheme with provable in-expectation\nguarantees, where the expectation is over the encoder's output message.\nSpecifically, we show that by imposing a constraint on a suitable\nKullback-Leibler divergence between the conditional distribution induced by a\ncompressed learning model $\\widehat{W}$ given $W$ and the prior, one guarantees\nsimultaneously small average empirical risk (aka training loss), small average\ngeneralization error and small average communication cost. We also consider a\none-shot scenario in which the guarantees on the empirical risk and\ngeneralization error are obtained for every encoder's output message.\n","authors":["Milad Sefidgaran","Abdellatif Zaidi","Piotr Krasnowski"],"pdf_url":"https://arxiv.org/pdf/2406.08193v1.pdf","comment":"Accepted at ISIT 2024"},{"id":"http://arxiv.org/abs/2406.08188v1","updated":"2024-06-12T13:19:42Z","published":"2024-06-12T13:19:42Z","title":"Attention-Based Learning for Fluid State Interpolation and Editing in a\n  Time-Continuous Framework","summary":"  In this work, we introduce FluidsFormer: a transformer-based approach for\nfluid interpolation within a continuous-time framework. By combining the\ncapabilities of PITT and a residual neural network (RNN), we analytically\npredict the physical properties of the fluid state. This enables us to\ninterpolate substep frames between simulated keyframes, enhancing the temporal\nsmoothness and sharpness of animations. We demonstrate promising results for\nsmoke interpolation and conduct initial experiments on liquids.\n","authors":["Bruno Roy"],"pdf_url":"https://arxiv.org/pdf/2406.08188v1.pdf","comment":"5 pages, 3 figures, submitted and accepted to SIGGRAPH"},{"id":"http://arxiv.org/abs/2303.03660v2","updated":"2024-06-12T13:16:40Z","published":"2023-03-07T05:48:28Z","title":"ECG Classification System for Arrhythmia Detection Using Convolutional\n  Neural Networks","summary":"  Arrhythmia is just one of the many cardiovascular illnesses that have been\nextensively studied throughout the years. Using multi-lead ECG data, this\nresearch describes a deep learning (DL) pipeline technique based on\nconvolutional neural network (CNN) algorithms to detect cardiovascular lar\narrhythmia in patients. The suggested model architecture has hidden layers with\na residual block in addition to the input and output layers. In this study, the\nclassification of the ECG signals into five main groups, namely: Left Bundle\nBranch Block (LBBB), Right Bundle Branch Block (RBBB), Atrial Premature\nContraction (APC), Premature Ventricular Contraction (PVC), and Normal Beat\n(N), are performed. Using the MIT-BIH arrhythmia dataset, we assessed the\nsuggested technique. The findings show that our suggested strategy classified\n15,000 cases with a high accuracy of 98.2%\n","authors":["Aryan Odugoudar","Jaskaran Singh Walia"],"pdf_url":"https://arxiv.org/pdf/2303.03660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.07888v2","updated":"2024-06-12T13:14:07Z","published":"2023-05-13T10:21:53Z","title":"Consistency Regularization for Domain Generalization with Logit\n  Attribution Matching","summary":"  Domain generalization (DG) is about training models that generalize well\nunder domain shift. Previous research on DG has been conducted mostly in\nsingle-source or multi-source settings. In this paper, we consider a third,\nlesser-known setting where a training domain is endowed with a collection of\npairs of examples that share the same semantic information. Such semantic\nsharing (SS) pairs can be created via data augmentation and then utilized for\nconsistency regularization (CR). We present a theory showing CR is conducive to\nDG and propose a novel CR method called Logit Attribution Matching (LAM). We\nconduct experiments on five DG benchmarks and four pretrained models with SS\npairs created by both generic and targeted data augmentation methods. LAM\noutperforms representative single/multi-source DG methods and various CR\nmethods that leverage SS pairs. The code and data of this project are available\nat https://github.com/Gaohan123/LAM\n","authors":["Han Gao","Kaican Li","Weiyan Xie","Zhi Lin","Yongxiang Huang","Luning Wang","Caleb Chen Cao","Nevin L. Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.07888v2.pdf","comment":"19 pages, 12 figures. Accepted by Uncertainty in Artificial\n  Intelligence (UAI) 2024"},{"id":"http://arxiv.org/abs/2201.01680v4","updated":"2024-06-12T13:11:28Z","published":"2022-01-05T16:19:16Z","title":"Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems","summary":"  TWe establish regret lower bounds for adaptively controlling an unknown\nlinear Gaussian system with quadratic costs. We combine ideas from experiment\ndesign, estimation theory and a perturbation bound of certain information\nmatrices to derive regret lower bounds exhibiting scaling on the order of\nmagnitude $\\sqrt{T}$ in the time horizon $T$. Our bounds accurately capture the\nrole of control-theoretic parameters and we are able to show that systems that\nare hard to control are also hard to learn to control; when instantiated to\nstate feedback systems we recover the dimensional dependency of earlier work\nbut with improved scaling with system-theoretic constants such as system costs\nand Gramians. Furthermore, we extend our results to a class of partially\nobserved systems and demonstrate that systems with poor observability structure\nalso are hard to learn to control.\n","authors":["Ingvar Ziemann","Henrik Sandberg"],"pdf_url":"https://arxiv.org/pdf/2201.01680v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08155v1","updated":"2024-06-12T12:44:48Z","published":"2024-06-12T12:44:48Z","title":"Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark","summary":"  Large Language Models~(LLMs) have become foundational in the realm of natural\nlanguage processing, demonstrating performance improvements as model sizes\nincrease. The Mixture-of-Experts~(MoE) approach offers a promising way to scale\nLLMs more efficiently by using fewer computational FLOPs through sparse\nactivation. However, it suffers from significant memory overheads,\nnecessitating model compression techniques. Post-training quantization, a\npopular method for model compression, proves less effective when directly\napplied to MoE models due to MoE's overlooked inherent sparsity. This paper\nexplores several MoE structure-aware quantization heuristics, ranging from\ncoarse to fine granularity, from MoE block to individual linear weight. Our\ninvestigations reveal critical principles: different MoE structures (i.e.,\nblocks, experts, linear layers) require varying numbers of weight bits for\neffective and efficient quantization. Conclusions are supported by extensive\nbenchmarking across two representative MoE models and six tasks. We further\nintroduce novel enhancements to more accurately identify the most critical\nweights in MoE quantization that necessitate higher bit allocations, including\nthe linear weight outlier scorer and MoE block scorer. Additionally, subsequent\nexperiments validate our findings in the context of both weight and activation\nquantization.\n","authors":["Pingzhi Li","Xiaolong Jin","Yu Cheng","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08155v1.pdf","comment":"Our code for reproducing all our experiments is provided at\n  https://github.com/UNITES-Lab/moe-quantization"},{"id":"http://arxiv.org/abs/2406.08148v1","updated":"2024-06-12T12:37:53Z","published":"2024-06-12T12:37:53Z","title":"Probing Implicit Bias in Semi-gradient Q-learning: Visualizing the\n  Effective Loss Landscapes via the Fokker--Planck Equation","summary":"  Semi-gradient Q-learning is applied in many fields, but due to the absence of\nan explicit loss function, studying its dynamics and implicit bias in the\nparameter space is challenging. This paper introduces the Fokker--Planck\nequation and employs partial data obtained through sampling to construct and\nvisualize the effective loss landscape within a two-dimensional parameter\nspace. This visualization reveals how the global minima in the loss landscape\ncan transform into saddle points in the effective loss landscape, as well as\nthe implicit bias of the semi-gradient method. Additionally, we demonstrate\nthat saddle points, originating from the global minima in loss landscape, still\nexist in the effective loss landscape under high-dimensional parameter spaces\nand neural network settings. This paper develop a novel approach for probing\nimplicit bias in semi-gradient Q-learning.\n","authors":["Shuyu Yin","Fei Wen","Peilin Liu","Tao Luo"],"pdf_url":"https://arxiv.org/pdf/2406.08148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04288v4","updated":"2024-06-12T12:37:43Z","published":"2022-10-09T15:42:36Z","title":"CoopHash: Cooperative Learning of Multipurpose Descriptor and\n  Contrastive Pair Generator via Variational MCMC Teaching for Supervised Image\n  Hashing","summary":"  Leveraging supervised information can lead to superior retrieval performance\nin the image hashing domain but the performance degrades significantly without\nenough labeled data. One effective solution to boost performance is to employ\ngenerative models, such as Generative Adversarial Networks (GANs), to generate\nsynthetic data in an image hashing model. However, GAN-based methods are\ndifficult to train, which prevents the hashing approaches from jointly training\nthe generative models and the hash functions. This limitation results in\nsub-optimal retrieval performance. To overcome this limitation, we propose a\nnovel framework, the generative cooperative hashing network, which is based on\nenergy-based cooperative learning. This framework jointly learns a powerful\ngenerative representation of the data and a robust hash function via two\ncomponents: a top-down contrastive pair generator that synthesizes contrastive\nimages and a bottom-up multipurpose descriptor that simultaneously represents\nthe images from multiple perspectives, including probability density, hash\ncode, latent code, and category. The two components are jointly learned via a\nnovel likelihood-based cooperative learning scheme. We conduct experiments on\nseveral real-world datasets and show that the proposed method outperforms the\ncompeting hashing supervised methods, achieving up to 10\\% relative improvement\nover the current state-of-the-art supervised hashing methods, and exhibits a\nsignificantly better performance in out-of-distribution retrieval.\n","authors":["Khoa D. Doan","Jianwen Xie","Yaxuan Zhu","Yang Zhao","Ping Li"],"pdf_url":"https://arxiv.org/pdf/2210.04288v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04906v4","updated":"2024-06-12T12:35:14Z","published":"2024-02-07T14:35:25Z","title":"Conformal Convolution and Monte Carlo Meta-learners for Predictive\n  Inference of Individual Treatment Effects","summary":"  Knowledge of the effect of interventions, known as the treatment effect, is\nparamount for decision-making. Approaches to estimating this treatment effect\nusing conditional average treatment effect (CATE) meta-learners often provide\nonly a point estimate of this treatment effect, while additional uncertainty\nquantification is frequently desired to enhance decision-making confidence. To\naddress this, we introduce two novel approaches: the conformal convolution\nT-learner (CCT-learner) and conformal Monte Carlo (CMC) meta-learners. The\napproaches leverage weighted conformal predictive systems (WCPS), Monte Carlo\nsampling, and CATE meta-learners to generate predictive distributions of\nindividual treatment effect (ITE) that could enhance individualized\ndecision-making. Although we show how assumptions about the noise distribution\nof the outcome influence the uncertainty predictions, our experiments\ndemonstrate that the CCT- and CMC meta-learners achieve strong coverage while\nmaintaining narrow interval widths. They also generate probabilistically\ncalibrated predictive distributions, providing reliable ranges of ITEs across\nvarious synthetic and semi-synthetic datasets.\n  Code: https://github.com/predict-idlab/cct-cmc\n","authors":["Jef Jonkers","Jarne Verhaeghe","Glenn Van Wallendael","Luc Duchateau","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2402.04906v4.pdf","comment":"25 pages, 14 figures"},{"id":"http://arxiv.org/abs/2402.00258v3","updated":"2024-06-12T12:23:37Z","published":"2024-02-01T01:06:32Z","title":"Multi-group Learning for Hierarchical Groups","summary":"  The multi-group learning model formalizes the learning scenario in which a\nsingle predictor must generalize well on multiple, possibly overlapping\nsubgroups of interest. We extend the study of multi-group learning to the\nnatural case where the groups are hierarchically structured. We design an\nalgorithm for this setting that outputs an interpretable and deterministic\ndecision tree predictor with near-optimal sample complexity. We then conduct an\nempirical evaluation of our algorithm and find that it achieves attractive\ngeneralization properties on real datasets with hierarchical group structure.\n","authors":["Samuel Deng","Daniel Hsu"],"pdf_url":"https://arxiv.org/pdf/2402.00258v3.pdf","comment":"Accepted in International Conference on Machine Learning 2024 (ICML\n  2024). Fixed reference description in \"Related Work\" for multi-task learning"},{"id":"http://arxiv.org/abs/2406.08128v1","updated":"2024-06-12T12:12:38Z","published":"2024-06-12T12:12:38Z","title":"Short-Long Convolutions Help Hardware-Efficient Linear Attention to\n  Focus on Long Sequences","summary":"  To mitigate the computational complexity in the self-attention mechanism on\nlong sequences, linear attention utilizes computation tricks to achieve linear\ncomplexity, while state space models (SSMs) popularize a favorable practice of\nusing non-data-dependent memory pattern, i.e., emphasize the near and neglect\nthe distant, to processing sequences. Recent studies have shown the priorities\nby combining them as one. However, the efficiency of linear attention remains\nonly at the theoretical level in a causal setting, and SSMs require various\ndesigned constraints to operate effectively on specific data. Therefore, in\norder to unveil the true power of the hybrid design, the following two issues\nneed to be addressed: (1) hardware-efficient implementation for linear\nattention and (2) stabilization of SSMs. To achieve this, we leverage the\nthought of tiling and hierarchy to propose CHELA (short-long Convolutions with\nHardware-Efficient Linear Attention), which replaces SSMs with short-long\nconvolutions and implements linear attention in a divide-and-conquer manner.\nThis approach enjoys global abstraction and data-dependent selection from\nstable SSM and linear attention while maintaining real linear complexity. Our\ncomprehensive experiments on the Long Range Arena benchmark and language\nmodeling tasks demonstrate the effectiveness of the proposed method.\n","authors":["Zicheng Liu","Siyuan Li","Li Wang","Zedong Wang","Yunfan Liu","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2406.08128v1.pdf","comment":"ICML 2024. arXiv admin note: text overlap with arXiv:2404.11163; text\n  overlap with arXiv:2212.08136 by other authors"},{"id":"http://arxiv.org/abs/2303.15244v2","updated":"2024-06-12T12:09:47Z","published":"2023-03-27T14:29:04Z","title":"Manifold Learning by Mixture Models of VAEs for Inverse Problems","summary":"  Representing a manifold of very high-dimensional data with generative models\nhas been shown to be computationally efficient in practice. However, this\nrequires that the data manifold admits a global parameterization. In order to\nrepresent manifolds of arbitrary topology, we propose to learn a mixture model\nof variational autoencoders. Here, every encoder-decoder pair represents one\nchart of a manifold. We propose a loss function for maximum likelihood\nestimation of the model weights and choose an architecture that provides us the\nanalytical expression of the charts and of their inverses. Once the manifold is\nlearned, we use it for solving inverse problems by minimizing a data fidelity\nterm restricted to the learned manifold. To solve the arising minimization\nproblem we propose a Riemannian gradient descent algorithm on the learned\nmanifold. We demonstrate the performance of our method for low-dimensional toy\nexamples as well as for deblurring and electrical impedance tomography on\ncertain image manifolds.\n","authors":["Giovanni S. Alberti","Johannes Hertrich","Matteo Santacesaria","Silvia Sciutto"],"pdf_url":"https://arxiv.org/pdf/2303.15244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12858v2","updated":"2024-06-12T11:56:25Z","published":"2023-10-19T16:09:44Z","title":"Audio Editing with Non-Rigid Text Prompts","summary":"  In this paper, we explore audio-editing with non-rigid text edits. We show\nthat the proposed editing pipeline is able to create audio edits that remain\nfaithful to the input audio. We explore text prompts that perform addition,\nstyle transfer, and in-painting. We quantitatively and qualitatively show that\nthe edits are able to obtain results which outperform Audio-LDM, a recently\nreleased text-prompted audio generation model. Qualitative inspection of the\nresults points out that the edits given by our approach remain more faithful to\nthe input audio in terms of keeping the original onsets and offsets of the\naudio events.\n","authors":["Francesco Paissan","Luca Della Libera","Zhepei Wang","Mirco Ravanelli","Paris Smaragdis","Cem Subakan"],"pdf_url":"https://arxiv.org/pdf/2310.12858v2.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2311.14517v2","updated":"2024-06-12T11:41:31Z","published":"2023-11-24T14:45:53Z","title":"tinyCLAP: Distilling Constrastive Language-Audio Pretrained Models","summary":"  Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in\nthe field of audio and speech processing. Its employment ranges from sound\nevent detection to text-to-audio generation. However, one of the main\nlimitations is the considerable amount of data required in the training process\nand the overall computational complexity during inference. This paper\ninvestigates how we can reduce the complexity of contrastive language-audio\npre-trained models, yielding an efficient model that we call tinyCLAP. We\nderive an unimodal distillation loss from first principles and explore how the\ndimensionality of the shared, multimodal latent space can be reduced via\npruning. TinyCLAP uses only 6% of the original Microsoft CLAP parameters with a\nminimal reduction (less than 5%) in zero-shot classification performance across\nthe three sound event detection datasets on which it was tested\n","authors":["Francesco Paissan","Elisabetta Farella"],"pdf_url":"https://arxiv.org/pdf/2311.14517v2.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.08106v1","updated":"2024-06-12T11:38:13Z","published":"2024-06-12T11:38:13Z","title":"Counterfactual-based Root Cause Analysis for Dynamical Systems","summary":"  Identifying the underlying reason for a failing dynamic process or otherwise\nanomalous observation is a fundamental challenge, yet has numerous industrial\napplications. Identifying the failure-causing sub-system using causal\ninference, one can ask the question: \"Would the observed failure also occur, if\nwe had replaced the behaviour of a sub-system at a certain point in time with\nits normal behaviour?\" To this end, a formal description of behaviour of the\nfull system is needed in which such counterfactual questions can be answered.\nHowever, existing causal methods for root cause identification are typically\nlimited to static settings and focusing on additive external influences causing\nfailures rather than structural influences. In this paper, we address these\nproblems by modelling the dynamic causal system using a Residual Neural Network\nand deriving corresponding counterfactual distributions over trajectories. We\nshow quantitatively that more root causes are identified when an intervention\nis performed on the structural equation and the external influence, compared to\nan intervention on the external influence only. By employing an efficient\napproximation to a corresponding Shapley value, we also obtain a ranking\nbetween the different subsystems at different points in time being responsible\nfor an observed failure, which is applicable in settings with large number of\nvariables. We illustrate the effectiveness of the proposed method on a\nbenchmark dynamic system as well as on a real world river dataset.\n","authors":["Juliane Weilbach","Sebastian Gerwinn","Karim Barsim","Martin Fränzle"],"pdf_url":"https://arxiv.org/pdf/2406.08106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04168v2","updated":"2024-06-12T11:34:30Z","published":"2024-02-06T17:24:06Z","title":"Informed Reinforcement Learning for Situation-Aware Traffic Rule\n  Exceptions","summary":"  Reinforcement Learning is a highly active research field with promising\nadvancements. In the field of autonomous driving, however, often very simple\nscenarios are being examined. Common approaches use non-interpretable control\ncommands as the action space and unstructured reward designs which lack\nstructure. In this work, we introduce Informed Reinforcement Learning, where a\nstructured rulebook is integrated as a knowledge source. We learn trajectories\nand asses them with a situation-aware reward design, leading to a dynamic\nreward which allows the agent to learn situations which require controlled\ntraffic rule exceptions. Our method is applicable to arbitrary RL models. We\nsuccessfully demonstrate high completion rates of complex scenarios with recent\nmodel-based agents.\n","authors":["Daniel Bogdoll","Jing Qin","Moritz Nekolla","Ahmed Abouelazm","Tim Joseph","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2402.04168v2.pdf","comment":"Daniel Bogdoll and Jing Qin contributed equally. Accepted for\n  publication at ICRA 2024"},{"id":"http://arxiv.org/abs/2312.12275v3","updated":"2024-06-12T11:30:19Z","published":"2023-12-19T15:56:30Z","title":"Emergence of In-Context Reinforcement Learning from Noise Distillation","summary":"  Recently, extensive studies in Reinforcement Learning have been carried out\non the ability of transformers to adapt in-context to various environments and\ntasks. Current in-context RL methods are limited by their strict requirements\nfor data, which needs to be generated by RL agents or labeled with actions from\nan optimal policy. In order to address this prevalent problem, we propose\nAD$^\\varepsilon$, a new data acquisition approach that enables in-context\nReinforcement Learning from noise-induced curriculum. We show that it is viable\nto construct a synthetic noise injection curriculum which helps to obtain\nlearning histories. Moreover, we experimentally demonstrate that it is possible\nto alleviate the need for generation using optimal policies, with in-context RL\nstill able to outperform the best suboptimal policy in a learning dataset by a\n2x margin.\n","authors":["Ilya Zisman","Vladislav Kurenkov","Alexander Nikulin","Viacheslav Sinii","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2312.12275v3.pdf","comment":"Proceedings of the 41-st International Conference on Machine Learning\n  (ICML 2024); code: https://github.com/corl-team/ad-eps"},{"id":"http://arxiv.org/abs/2406.08099v1","updated":"2024-06-12T11:26:29Z","published":"2024-06-12T11:26:29Z","title":"Confidence Interval Estimation of Predictive Performance in the Context\n  of AutoML","summary":"  Any supervised machine learning analysis is required to provide an estimate\nof the out-of-sample predictive performance. However, it is imperative to also\nprovide a quantification of the uncertainty of this performance in the form of\na confidence or credible interval (CI) and not just a point estimate. In an\nAutoML setting, estimating the CI is challenging due to the ``winner's curse\",\ni.e., the bias of estimation due to cross-validating several machine learning\npipelines and selecting the winning one. In this work, we perform a comparative\nevaluation of 9 state-of-the-art methods and variants in CI estimation in an\nAutoML setting on a corpus of real and simulated datasets. The methods are\ncompared in terms of inclusion percentage (does a 95\\% CI include the true\nperformance at least 95\\% of the time), CI tightness (tighter CIs are\npreferable as being more informative), and execution time. The evaluation is\nthe first one that covers most, if not all, such methods and extends previous\nwork to imbalanced and small-sample tasks. In addition, we present a variant,\ncalled BBC-F, of an existing method (the Bootstrap Bias Correction, or BBC)\nthat maintains the statistical properties of the BBC but is more\ncomputationally efficient. The results support that BBC-F and BBC dominate the\nother methods in all metrics measured.\n","authors":["Konstantinos Paraschakis","Andrea Castellani","Giorgos Borboudakis","Ioannis Tsamardinos"],"pdf_url":"https://arxiv.org/pdf/2406.08099v1.pdf","comment":"Accepted at AutoML 2024 conference"},{"id":"http://arxiv.org/abs/2406.08097v1","updated":"2024-06-12T11:22:27Z","published":"2024-06-12T11:22:27Z","title":"Inductive Global and Local Manifold Approximation and Projection","summary":"  Nonlinear dimensional reduction with the manifold assumption, often called\nmanifold learning, has proven its usefulness in a wide range of\nhigh-dimensional data analysis. The significant impact of t-SNE and UMAP has\ncatalyzed intense research interest, seeking further innovations toward\nvisualizing not only the local but also the global structure information of the\ndata. Moreover, there have been consistent efforts toward generalizable\ndimensional reduction that handles unseen data. In this paper, we first propose\nGLoMAP, a novel manifold learning method for dimensional reduction and\nhigh-dimensional data visualization. GLoMAP preserves locally and globally\nmeaningful distance estimates and displays a progression from global to local\nformation during the course of optimization. Furthermore, we extend GLoMAP to\nits inductive version, iGLoMAP, which utilizes a deep neural network to map\ndata to its lower-dimensional representation. This allows iGLoMAP to provide\nlower-dimensional embeddings for unseen points without needing to re-train the\nalgorithm. iGLoMAP is also well-suited for mini-batch learning, enabling\nlarge-scale, accelerated gradient calculations. We have successfully applied\nboth GLoMAP and iGLoMAP to the simulated and real-data settings, with\ncompetitive experiments against the state-of-the-art methods.\n","authors":["Jungeum Kim","Xiao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08093v1","updated":"2024-06-12T11:17:11Z","published":"2024-06-12T11:17:11Z","title":"Learnable & Interpretable Model Combination in Dynamic Systems Modeling","summary":"  One of the core concepts in science, and something that happens intuitively\nin every-day dynamic systems modeling, is the combination of models or methods.\nEspecially in dynamical systems modeling, often two or more structures are\ncombined to obtain a more powerful or efficient architecture regarding a\nspecific application (area). Further, even physical simulations are combined\nwith machine learning architectures, to increase prediction accuracy or\noptimize the computational performance. In this work, we shortly discuss, which\ntypes of models are usually combined and propose a model interface that is\ncapable of expressing a width variety of mixed algebraic, discrete and\ndifferential equation based models. Further, we examine different established,\nas well as new ways of combining these models from a system theoretical point\nof view and highlight two challenges - algebraic loops and local event affect\nfunctions in discontinuous models - that require a special approach. Finally,\nwe propose a new wildcard topology, that is capable of describing the generic\nconnection between two combined models in an easy to interpret fashion that can\nbe learned as part of a gradient based optimization procedure. The\ncontributions of this paper are highlighted at a proof of concept: Different\nconnection topologies between two models are learned, interpreted and compared\napplying the proposed methodology and software implementation.\n","authors":["Tobias Thummerer","Lars Mikelsons"],"pdf_url":"https://arxiv.org/pdf/2406.08093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06674v2","updated":"2024-06-12T10:51:40Z","published":"2024-02-07T14:23:01Z","title":"On the Impact of Dataset Properties on Membership Privacy of Deep\n  Learning","summary":"  We apply a state-of-the-art membership inference attack (MIA) to\nsystematically test the practical privacy vulnerability of fine-tuning large\nimage classification models. We focus on understanding the properties of data\nsets and samples that make them vulnerable to membership inference. In terms of\ndata set properties, we find a strong power law dependence between the number\nof examples per class in the data and the MIA vulnerability, as measured by\ntrue positive rate of the attack at a low false positive rate. We train a\nlinear model to predict true positive rate based on data set properties and\nobserve good fit for MIA vulnerability on unseen data. To analyse the\nphenomenon theoretically, we reproduce the result on a simplified model of\nmembership inference that behaves similarly to our experimental data. We prove\nthat in this model, the logarithm of the difference of true and false positive\nrates depends linearly on the logarithm of the number of examples per class.For\nan individual sample, the gradient norm is predictive of its vulnerability.\n","authors":["Marlon Tobaben","Joonas Jälkö","Gauri Pradhan","Yuan He","Antti Honkela"],"pdf_url":"https://arxiv.org/pdf/2402.06674v2.pdf","comment":"29 pages, 17 figures"},{"id":"http://arxiv.org/abs/2406.08075v1","updated":"2024-06-12T10:51:00Z","published":"2024-06-12T10:51:00Z","title":"Balancing Molecular Information and Empirical Data in the Prediction of\n  Physico-Chemical Properties","summary":"  Predicting the physico-chemical properties of pure substances and mixtures is\na central task in thermodynamics. Established prediction methods range from\nfully physics-based ab-initio calculations, which are only feasible for very\nsimple systems, over descriptor-based methods that use some information on the\nmolecules to be modeled together with fitted model parameters (e.g.,\nquantitative-structure-property relationship methods or classical group\ncontribution methods), to representation-learning methods, which may, in\nextreme cases, completely ignore molecular descriptors and extrapolate only\nfrom existing data on the property to be modeled (e.g., matrix completion\nmethods). In this work, we propose a general method for combining molecular\ndescriptors with representation learning using the so-called expectation\nmaximization algorithm from the probabilistic machine learning literature,\nwhich uses uncertainty estimates to trade off between the two approaches. The\nproposed hybrid model exploits chemical structure information using graph\nneural networks, but it automatically detects cases where structure-based\npredictions are unreliable, in which case it corrects them by\nrepresentation-learning based predictions that can better specialize to unusual\ncases. The effectiveness of the proposed method is demonstrated using the\nprediction of activity coefficients in binary mixtures as an example. The\nresults are compelling, as the method significantly improves predictive\naccuracy over the current state of the art, showcasing its potential to advance\nthe prediction of physico-chemical properties in general.\n","authors":["Johannes Zenn","Dominik Gond","Fabian Jirasek","Robert Bamler"],"pdf_url":"https://arxiv.org/pdf/2406.08075v1.pdf","comment":"14 pages, including 10 pages of main text and 2 pages of appendix"},{"id":"http://arxiv.org/abs/2406.08074v1","updated":"2024-06-12T10:48:53Z","published":"2024-06-12T10:48:53Z","title":"A Concept-Based Explainability Framework for Large Multimodal Models","summary":"  Large multimodal models (LMMs) combine unimodal encoders and large language\nmodels (LLMs) to perform multimodal tasks. Despite recent advancements towards\nthe interpretability of these models, understanding internal representations of\nLMMs remains largely a mystery. In this paper, we present a novel framework for\nthe interpretation of LMMs. We propose a dictionary learning based approach,\napplied to the representation of tokens. The elements of the learned dictionary\ncorrespond to our proposed concepts. We show that these concepts are well\nsemantically grounded in both vision and text. Thus we refer to these as\n\"multi-modal concepts\". We qualitatively and quantitatively evaluate the\nresults of the learnt concepts. We show that the extracted multimodal concepts\nare useful to interpret representations of test samples. Finally, we evaluate\nthe disentanglement between different concepts and the quality of grounding\nconcepts visually and textually. We will publicly release our code.\n","authors":["Jayneel Parekh","Pegah Khayatan","Mustafa Shukor","Alasdair Newson","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2406.08074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08070v1","updated":"2024-06-12T10:40:10Z","published":"2024-06-12T10:40:10Z","title":"CFG++: Manifold-constrained Classifier Free Guidance for Diffusion\n  Models","summary":"  Classifier-free guidance (CFG) is a fundamental tool in modern diffusion\nmodels for text-guided generation. Although effective, CFG has notable\ndrawbacks. For instance, DDIM with CFG lacks invertibility, complicating image\nediting; furthermore, high guidance scales, essential for high-quality outputs,\nfrequently result in issues like mode collapse. Contrary to the widespread\nbelief that these are inherent limitations of diffusion models, this paper\nreveals that the problems actually stem from the off-manifold phenomenon\nassociated with CFG, rather than the diffusion models themselves. More\nspecifically, inspired by the recent advancements of diffusion model-based\ninverse problem solvers (DIS), we reformulate text-guidance as an inverse\nproblem with a text-conditioned score matching loss, and develop CFG++, a novel\napproach that tackles the off-manifold challenges inherent in traditional CFG.\nCFG++ features a surprisingly simple fix to CFG, yet it offers significant\nimprovements, including better sample quality for text-to-image generation,\ninvertibility, smaller guidance scales, reduced mode collapse, etc.\nFurthermore, CFG++ enables seamless interpolation between unconditional and\nconditional sampling at lower guidance scales, consistently outperforming\ntraditional CFG at all scales. Experimental results confirm that our method\nsignificantly enhances performance in text-to-image generation, DDIM inversion,\nediting, and solving inverse problems, suggesting a wide-ranging impact and\npotential applications in various fields that utilize text guidance. Project\nPage: https://cfgpp-diffusion.github.io/.\n","authors":["Hyungjin Chung","Jeongsol Kim","Geon Yeong Park","Hyelin Nam","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2406.08070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08069v1","updated":"2024-06-12T10:39:31Z","published":"2024-06-12T10:39:31Z","title":"Explore-Go: Leveraging Exploration for Generalisation in Deep\n  Reinforcement Learning","summary":"  One of the remaining challenges in reinforcement learning is to develop\nagents that can generalise to novel scenarios they might encounter once\ndeployed. This challenge is often framed in a multi-task setting where agents\ntrain on a fixed set of tasks and have to generalise to new tasks. Recent work\nhas shown that in this setting increased exploration during training can be\nleveraged to increase the generalisation performance of the agent. This makes\nsense when the states encountered during testing can actually be explored\nduring training. In this paper, we provide intuition why exploration can also\nbenefit generalisation to states that cannot be explicitly encountered during\ntraining. Additionally, we propose a novel method Explore-Go that exploits this\nintuition by increasing the number of states on which the agent trains.\nExplore-Go effectively increases the starting state distribution of the agent\nand as a result can be used in conjunction with most existing on-policy or\noff-policy reinforcement learning algorithms. We show empirically that our\nmethod can increase generalisation performance in an illustrative environment\nand on the Procgen benchmark.\n","authors":["Max Weltevrede","Felix Kaubek","Matthijs T. J. Spaan","Wendelin Böhmer"],"pdf_url":"https://arxiv.org/pdf/2406.08069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07837v2","updated":"2024-06-12T10:21:17Z","published":"2023-12-13T02:04:41Z","title":"The Real Deal Behind the Artificial Appeal: Inferential Utility of\n  Tabular Synthetic Data","summary":"  Recent advances in generative models facilitate the creation of synthetic\ndata to be made available for research in privacy-sensitive contexts. However,\nthe analysis of synthetic data raises a unique set of methodological\nchallenges. In this work, we highlight the importance of inferential utility\nand provide empirical evidence against naive inference from synthetic data,\nwhereby synthetic data are treated as if they were actually observed. Before\npublishing synthetic data, it is essential to develop statistical inference\ntools for such data. By means of a simulation study, we show that the rate of\nfalse-positive findings (type 1 error) will be unacceptably high, even when the\nestimates are unbiased. Despite the use of a previously proposed correction\nfactor, this problem persists for deep generative models, in part due to slower\nconvergence of estimators and resulting underestimation of the true standard\nerror. We further demonstrate our findings through a case study.\n","authors":["Alexander Decruyenaere","Heidelinde Dehaene","Paloma Rabaey","Christiaan Polet","Johan Decruyenaere","Stijn Vansteelandt","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2312.07837v2.pdf","comment":"Accepted for the 40th Conference on Uncertainty in Artificial\n  Intelligence (UAI 2024), *joint first authors"},{"id":"http://arxiv.org/abs/2401.08260v2","updated":"2024-06-12T10:20:38Z","published":"2024-01-16T10:31:27Z","title":"Fast Kernel Summation in High Dimensions via Slicing and Fourier\n  Transforms","summary":"  Kernel-based methods are heavily used in machine learning. However, they\nsuffer from $O(N^2)$ complexity in the number $N$ of considered data points. In\nthis paper, we propose an approximation procedure, which reduces this\ncomplexity to $O(N)$. Our approach is based on two ideas. First, we prove that\nany radial kernel with analytic basis function can be represented as sliced\nversion of some one-dimensional kernel and derive an analytic formula for the\none-dimensional counterpart. It turns out that the relation between one- and\n$d$-dimensional kernels is given by a generalized Riemann-Liouville fractional\nintegral. Hence, we can reduce the $d$-dimensional kernel summation to a\none-dimensional setting. Second, for solving these one-dimensional problems\nefficiently, we apply fast Fourier summations on non-equispaced data, a sorting\nalgorithm or a combination of both. Due to its practical importance we pay\nspecial attention to the Gaussian kernel, where we show a dimension-independent\nerror bound and represent its one-dimensional counterpart via a closed-form\nFourier transform. We provide a run time comparison and error estimate of our\nfast kernel summations.\n","authors":["Johannes Hertrich"],"pdf_url":"https://arxiv.org/pdf/2401.08260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03996v4","updated":"2024-06-12T10:19:23Z","published":"2023-04-08T12:25:08Z","title":"A Unified Characterization of Private Learnability via Graph Theory","summary":"  We provide a unified framework for characterizing pure and approximate\ndifferentially private (DP) learnability. The framework uses the language of\ngraph theory: for a concept class $\\mathcal{H}$, we define the contradiction\ngraph $G$ of $\\mathcal{H}$. Its vertices are realizable datasets, and two\ndatasets $S,S'$ are connected by an edge if they contradict each other (i.e.,\nthere is a point $x$ that is labeled differently in $S$ and $S'$). Our main\nfinding is that the combinatorial structure of $G$ is deeply related to\nlearning $\\mathcal{H}$ under DP. Learning $\\mathcal{H}$ under pure DP is\ncaptured by the fractional clique number of $G$. Learning $\\mathcal{H}$ under\napproximate DP is captured by the clique number of $G$. Consequently, we\nidentify graph-theoretic dimensions that characterize DP learnability: the\nclique dimension and fractional clique dimension. Along the way, we reveal\nproperties of the contradiction graph which may be of independent interest. We\nalso suggest several open questions and directions for future research.\n","authors":["Noga Alon","Shay Moran","Hilla Schefler","Amir Yehudayoff"],"pdf_url":"https://arxiv.org/pdf/2304.03996v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04865v3","updated":"2024-06-12T10:05:11Z","published":"2024-05-08T07:43:43Z","title":"Regime Learning for Differentiable Particle Filters","summary":"  Differentiable particle filters are an emerging class of models that combine\nsequential Monte Carlo techniques with the flexibility of neural networks to\nperform state space inference. This paper concerns the case where the system\nmay switch between a finite set of state-space models, i.e. regimes. No prior\napproaches effectively learn both the individual regimes and the switching\nprocess simultaneously. In this paper, we propose the neural network based\nregime learning differentiable particle filter (RLPF) to address this problem.\nWe further design a training procedure for the RLPF and other related\nalgorithms. We demonstrate competitive performance compared to the previous\nstate-of-the-art algorithms on a pair of numerical experiments.\n","authors":["John-Joseph Brady","Yuhui Luo","Wenwu Wang","Victor Elvira","Yunpeng Li"],"pdf_url":"https://arxiv.org/pdf/2405.04865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08050v1","updated":"2024-06-12T10:02:27Z","published":"2024-06-12T10:02:27Z","title":"Adversarial Evasion Attack Efficiency against Large Language Models","summary":"  Large Language Models (LLMs) are valuable for text classification, but their\nvulnerabilities must not be disregarded. They lack robustness against\nadversarial examples, so it is pertinent to understand the impacts of different\ntypes of perturbations, and assess if those attacks could be replicated by\ncommon users with a small amount of perturbations and a small number of queries\nto a deployed LLM. This work presents an analysis of the effectiveness,\nefficiency, and practicality of three different types of adversarial attacks\nagainst five different LLMs in a sentiment classification task. The obtained\nresults demonstrated the very distinct impacts of the word-level and\ncharacter-level attacks. The word attacks were more effective, but the\ncharacter and more constrained attacks were more practical and required a\nreduced number of perturbations and queries. These differences need to be\nconsidered during the development of adversarial defense strategies to train\nmore robust LLMs for intelligent text classification applications.\n","authors":["João Vitorino","Eva Maia","Isabel Praça"],"pdf_url":"https://arxiv.org/pdf/2406.08050v1.pdf","comment":"9 pages, 1 table, 2 figures, DCAI 2024 conference"},{"id":"http://arxiv.org/abs/2406.03229v3","updated":"2024-06-12T09:54:33Z","published":"2024-06-05T13:06:17Z","title":"Global Clipper: Enhancing Safety and Reliability of Transformer-based\n  Object Detection Models","summary":"  As transformer-based object detection models progress, their impact in\ncritical sectors like autonomous vehicles and aviation is expected to grow.\nSoft errors causing bit flips during inference have significantly impacted DNN\nperformance, altering predictions. Traditional range restriction solutions for\nCNNs fall short for transformers. This study introduces the Global Clipper and\nGlobal Hybrid Clipper, effective mitigation strategies specifically designed\nfor transformer-based models. It significantly enhances their resilience to\nsoft errors and reduces faulty inferences to ~ 0\\%. We also detail extensive\ntesting across over 64 scenarios involving two transformer models (DINO-DETR\nand Lite-DETR) and two CNN models (YOLOv3 and SSD) using three datasets,\ntotalling approximately 3.3 million inferences, to assess model robustness\ncomprehensively. Moreover, the paper explores unique aspects of attention\nblocks in transformers and their operational differences from CNNs.\n","authors":["Qutub Syed Sha","Michael Paulitsch","Karthik Pattabiraman","Korbinian Hagn","Fabian Oboril","Cornelius Buerkle","Kay-Ulrich Scholl","Gereon Hinz","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2406.03229v3.pdf","comment":"Accepted at IJCAI-AISafety'24 Workshop"},{"id":"http://arxiv.org/abs/2406.08045v1","updated":"2024-06-12T09:53:14Z","published":"2024-06-12T09:53:14Z","title":"A novel approach to graph distinction through GENEOs and permutants","summary":"  The theory of Group Equivariant Non-Expansive Operators (GENEOs) was\ninitially developed in Topological Data Analysis for the geometric\napproximation of data observers, including their invariances and symmetries.\nThis paper departs from that line of research and explores the use of GENEOs\nfor distinguishing $r$-regular graphs up to isomorphisms. In doing so, we aim\nto test the capabilities and flexibility of these operators. Our experiments\nshow that GENEOs offer a good compromise between efficiency and computational\ncost in comparing $r$-regular graphs, while their actions on data are easily\ninterpretable. This supports the idea that GENEOs could be a general-purpose\napproach to discriminative problems in Machine Learning when some structural\ninformation about data and observers is explicitly given.\n","authors":["Giovanni Bocchi","Massimo Ferri","Patrizio Frosini"],"pdf_url":"https://arxiv.org/pdf/2406.08045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08042v1","updated":"2024-06-12T09:51:29Z","published":"2024-06-12T09:51:29Z","title":"Efficient Network Traffic Feature Sets for IoT Intrusion Detection","summary":"  The use of Machine Learning (ML) models in cybersecurity solutions requires\nhigh-quality data that is stripped of redundant, missing, and noisy\ninformation. By selecting the most relevant features, data integrity and model\nefficiency can be significantly improved. This work evaluates the feature sets\nprovided by a combination of different feature selection methods, namely\nInformation Gain, Chi-Squared Test, Recursive Feature Elimination, Mean\nAbsolute Deviation, and Dispersion Ratio, in multiple IoT network datasets. The\ninfluence of the smaller feature sets on both the classification performance\nand the training time of ML models is compared, with the aim of increasing the\ncomputational efficiency of IoT intrusion detection. Overall, the most\nimpactful features of each dataset were identified, and the ML models obtained\nhigher computational efficiency while preserving a good generalization, showing\nlittle to no difference between the sets.\n","authors":["Miguel Silva","João Vitorino","Eva Maia","Isabel Praça"],"pdf_url":"https://arxiv.org/pdf/2406.08042v1.pdf","comment":"10 pages, 9 tables, DCAI 2024 conference"},{"id":"http://arxiv.org/abs/2406.08039v1","updated":"2024-06-12T09:41:12Z","published":"2024-06-12T09:41:12Z","title":"Beyond the Mean: Differentially Private Prototypes for Private Transfer\n  Learning","summary":"  Machine learning (ML) models have been shown to leak private information from\ntheir training datasets. Differential Privacy (DP), typically implemented\nthrough the differential private stochastic gradient descent algorithm\n(DP-SGD), has become the standard solution to bound leakage from the models.\nDespite recent improvements, DP-SGD-based approaches for private learning still\nusually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes,\nand when the private training datasets are imbalanced. To overcome these\nlimitations, we propose Differentially Private Prototype Learning (DPPL) as a\nnew paradigm for private transfer learning. DPPL leverages publicly pre-trained\nencoders to extract features from private data and generates DP prototypes that\nrepresent each private class in the embedding space and can be publicly\nreleased for inference. Since our DP prototypes can be obtained from only a few\nprivate training data points and without iterative noise addition, they offer\nhigh-utility predictions and strong privacy guarantees even under the notion of\npure DP. We additionally show that privacy-utility trade-offs can be further\nimproved when leveraging the public data beyond pre-training of the encoder: in\nparticular, we can privately sample our DP prototypes from the publicly\navailable data points used to train the encoder. Our experimental evaluation\nwith four state-of-the-art encoders, four vision datasets, and under different\ndata and imbalancedness regimes demonstrate DPPL's high performance under\nstrong privacy guarantees in challenging private learning setups.\n","authors":["Dariush Wahdany","Matthew Jagielski","Adam Dziedzic","Franziska Boenisch"],"pdf_url":"https://arxiv.org/pdf/2406.08039v1.pdf","comment":"Submitted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.06282v2","updated":"2024-06-12T09:39:41Z","published":"2024-06-10T14:01:21Z","title":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone","summary":"  This paper introduces PowerInfer-2, a framework designed for high-speed\ninference of Large Language Models (LLMs) on smartphones, particularly\neffective for models whose sizes exceed the device's memory capacity. The key\ninsight of PowerInfer-2 is to utilize the heterogeneous computation, memory,\nand I/O resources in smartphones by decomposing traditional matrix computations\ninto fine-grained neuron cluster computations. Specifically, PowerInfer-2\nfeatures a polymorphic neuron engine that adapts computational strategies for\nvarious stages of LLM inference. Additionally, it introduces segmented neuron\ncaching and fine-grained neuron-cluster-level pipelining, which effectively\nminimize and conceal the overhead caused by I/O operations. The implementation\nand evaluation of PowerInfer-2 demonstrate its capability to support a wide\narray of LLM models on two smartphones, achieving up to a 29.2x speed increase\ncompared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first\nsystem to serve the TurboSparse-Mixtral-47B model with a generation rate of\n11.68 tokens per second on a smartphone. For models that fit entirely within\nthe memory, PowerInfer-2 can achieve approximately a 40% reduction in memory\nusage while maintaining inference speeds comparable to llama.cpp and MLC-LLM.\nFor more details, including a demonstration video, please visit the project\nsite at www.powerinfer.ai/v2.\n","authors":["Zhenliang Xue","Yixin Song","Zeyu Mi","Le Chen","Yubin Xia","Haibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.06282v2.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.08034v1","updated":"2024-06-12T09:36:20Z","published":"2024-06-12T09:36:20Z","title":"Strong and Weak Random Walks on Signed Networks","summary":"  Random walks play an important role in probing the structure of complex\nnetworks. On traditional networks, they can be used to extract community\nstructure, understand node centrality, perform link prediction, or capture the\nsimilarity between nodes. On signed networks, where the edge weights can be\neither positive or negative, it is non-trivial to design a random walk which\ncan be used to extract information about the signed structure of the network,\nin particular the ability to partition the graph into communities with positive\nedges inside and negative edges in between. Prior works on signed network\nrandom walks focus on the case where there are only two such communities\n(strong balance), which is rarely the case in empirical networks. In this\npaper, we propose a signed network random walk which can capture the structure\nof a network with more than two such communities (weak balance). The walk\nresults in a similarity matrix which can be used to cluster the nodes into\nantagonistic communities. We compare the characteristics of the so-called\nstrong and weak random walks, in terms of walk length and stationarity. We show\nthrough a series of experiments on synthetic and empirical networks that the\nsimilarity matrix based on weak walks can be used for both unsupervised and\nsemi-supervised clustering, outperforming the same similarity matrix based on\nstrong walks when the graph has more than two communities, or exhibits\nasymmetry in the density of links. These results suggest that other random-walk\nbased algorithms for signed networks could be improved simply by running them\nwith weak walks instead of strong walks.\n","authors":["Shazia'Ayn Babul","Yu Tian","Renaud Lambiotte"],"pdf_url":"https://arxiv.org/pdf/2406.08034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08030v1","updated":"2024-06-12T09:31:03Z","published":"2024-06-12T09:31:03Z","title":"Fault detection in propulsion motors in the presence of concept drift","summary":"  Machine learning and statistical methods can be used to enhance monitoring\nand fault prediction in marine systems. These methods rely on a dataset with\nrecords of historical system behaviour, potentially containing periods of both\nfault-free and faulty operation. An unexpected change in the underlying system,\ncalled a concept drift, may impact the performance of these methods, triggering\nthe need for model retraining or other adaptations. In this article, we present\nan approach for detecting overheating in stator windings of marine propulsion\nmotors that is able to successfully operate during concept drift without the\nneed for full model retraining. Two distinct approaches are presented and\ntested. All models are trained and verified using a dataset from operational\npropulsion motors, with known, sudden concept drifts.\n","authors":["Martin Tveten","Morten Stakkeland"],"pdf_url":"https://arxiv.org/pdf/2406.08030v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2210.04470v5","updated":"2024-06-12T09:14:21Z","published":"2022-10-10T07:47:56Z","title":"Actor-Critic or Critic-Actor? A Tale of Two Time Scales","summary":"  We revisit the standard formulation of tabular actor-critic algorithm as a\ntwo time-scale stochastic approximation with value function computed on a\nfaster time-scale and policy computed on a slower time-scale. This emulates\npolicy iteration. We observe that reversal of the time scales will in fact\nemulate value iteration and is a legitimate algorithm. We provide a proof of\nconvergence and compare the two empirically with and without function\napproximation (with both linear and nonlinear function approximators) and\nobserve that our proposed critic-actor algorithm performs on par with\nactor-critic in terms of both accuracy and computational effort.\n","authors":["Shalabh Bhatnagar","Vivek S. Borkar","Soumyajit Guin"],"pdf_url":"https://arxiv.org/pdf/2210.04470v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07358v2","updated":"2024-06-12T09:06:13Z","published":"2024-06-11T15:26:57Z","title":"AI Sandbagging: Language Models can Strategically Underperform on\n  Evaluations","summary":"  Trustworthy capability evaluations are crucial for ensuring the safety of AI\nsystems, and are becoming a key component of AI regulation. However, the\ndevelopers of an AI system, or the AI system itself, may have incentives for\nevaluations to understate the AI's actual capability. These conflicting\ninterests lead to the problem of sandbagging $\\unicode{x2013}$ which we define\nas \"strategic underperformance on an evaluation\". In this paper we assess\nsandbagging capabilities in contemporary language models (LMs). We prompt\nfrontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on\ndangerous capability evaluations, while maintaining performance on general\n(harmless) capability evaluations. Moreover, we find that models can be\nfine-tuned, on a synthetic dataset, to hide specific capabilities unless given\na password. This behaviour generalizes to high-quality, held-out benchmarks\nsuch as WMDP. In addition, we show that both frontier and smaller models can be\nprompted, or password-locked, to target specific scores on a capability\nevaluation. Even more, we found that a capable password-locked model (Llama 3\n70b) is reasonably able to emulate a less capable model (Llama 2 7b). Overall,\nour results suggest that capability evaluations are vulnerable to sandbagging.\nThis vulnerability decreases the trustworthiness of evaluations, and thereby\nundermines important safety decisions regarding the development and deployment\nof advanced AI systems.\n","authors":["Teun van der Weij","Felix Hofstätter","Ollie Jaffe","Samuel F. Brown","Francis Rhys Ward"],"pdf_url":"https://arxiv.org/pdf/2406.07358v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02997v2","updated":"2024-06-12T09:06:10Z","published":"2024-06-05T06:53:16Z","title":"Residual Connections and Normalization Can Provably Prevent\n  Oversmoothing in GNNs","summary":"  Residual connections and normalization layers have become standard design\nchoices for graph neural networks (GNNs), and were proposed as solutions to the\nmitigate the oversmoothing problem in GNNs. However, how exactly these methods\nhelp alleviate the oversmoothing problem from a theoretical perspective is not\nwell understood. In this work, we provide a formal and precise characterization\nof (linearized) GNNs with residual connections and normalization layers. We\nestablish that (a) for residual connections, the incorporation of the initial\nfeatures at each layer can prevent the signal from becoming too smooth, and\ndetermines the subspace of possible node representations; (b) batch\nnormalization prevents a complete collapse of the output embedding space to a\none-dimensional subspace through the individual rescaling of each column of the\nfeature matrix. This results in the convergence of node representations to the\ntop-$k$ eigenspace of the message-passing operator; (c) moreover, we show that\nthe centering step of a normalization layer -- which can be understood as a\nprojection -- alters the graph signal in message-passing in such a way that\nrelevant information can become harder to extract. We therefore introduce a\nnovel, principled normalization layer called GraphNormv2 in which the centering\nstep is learned such that it does not distort the original graph signal in an\nundesirable way. Experimental results confirm the effectiveness of our method.\n","authors":["Michael Scholkemper","Xinyi Wu","Ali Jadbabaie","Michael T. Schaub"],"pdf_url":"https://arxiv.org/pdf/2406.02997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03262v3","updated":"2024-06-12T09:02:47Z","published":"2023-12-06T03:18:49Z","title":"Low-Cost High-Power Membership Inference Attacks","summary":"  Membership inference attacks aim to detect if a particular data point was\nused in training a model. We design a novel statistical test to perform robust\nmembership inference attacks (RMIA) with low computational overhead. We achieve\nthis by a fine-grained modeling of the null hypothesis in our likelihood ratio\ntests, and effectively leveraging both reference models and reference\npopulation data samples. RMIA has superior test power compared with prior\nmethods, throughout the TPR-FPR curve (even at extremely low FPR, as low as 0).\nUnder computational constraints, where only a limited number of pre-trained\nreference models (as few as 1) are available, and also when we vary other\nelements of the attack (e.g., data distribution), our method performs\nexceptionally well, unlike prior attacks that approach random guessing. RMIA\nlays the groundwork for practical yet accurate data privacy risk assessment in\nmachine learning.\n","authors":["Sajjad Zarifzadeh","Philippe Liu","Reza Shokri"],"pdf_url":"https://arxiv.org/pdf/2312.03262v3.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2404.14197v2","updated":"2024-06-12T09:01:19Z","published":"2024-04-22T14:06:35Z","title":"SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core\n  Fusion","summary":"  Multivariate time series forecasting plays a crucial role in various fields\nsuch as finance, traffic management, energy, and healthcare. Recent studies\nhave highlighted the advantages of channel independence to resist distribution\ndrift but neglect channel correlations, limiting further enhancements. Several\nmethods utilize mechanisms like attention or mixer to address this by capturing\nchannel correlations, but they either introduce excessive complexity or rely\ntoo heavily on the correlation to achieve satisfactory results under\ndistribution drifts, particularly with a large number of channels. Addressing\nthis gap, this paper presents an efficient MLP-based model, the Series-cOre\nFused Time Series forecaster (SOFTS), which incorporates a novel STar\nAggregate-Redistribute (STAR) module. Unlike traditional approaches that manage\nchannel interactions through distributed structures, \\textit{e.g.}, attention,\nSTAR employs a centralized strategy to improve efficiency and reduce reliance\non the quality of each channel. It aggregates all series to form a global core\nrepresentation, which is then dispatched and fused with individual series\nrepresentations to facilitate channel interactions effectively.SOFTS achieves\nsuperior performance over existing state-of-the-art methods with only linear\ncomplexity. The broad applicability of the STAR module across different\nforecasting models is also demonstrated empirically. For further research and\ndevelopment, we have made our code publicly available at\nhttps://github.com/Secilia-Cxy/SOFTS.\n","authors":["Lu Han","Xu-Yang Chen","Han-Jia Ye","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2404.14197v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08010v1","updated":"2024-06-12T09:00:49Z","published":"2024-06-12T09:00:49Z","title":"A Self-boosted Framework for Calibrated Ranking","summary":"  Scale-calibrated ranking systems are ubiquitous in real-world applications\nnowadays, which pursue accurate ranking quality and calibrated probabilistic\npredictions simultaneously. For instance, in the advertising ranking system,\nthe predicted click-through rate (CTR) is utilized for ranking and required to\nbe calibrated for the downstream cost-per-click ads bidding. Recently,\nmulti-objective based methods have been wildly adopted as a standard approach\nfor Calibrated Ranking, which incorporates the combination of two loss\nfunctions: a pointwise loss that focuses on calibrated absolute values and a\nranking loss that emphasizes relative orderings. However, when applied to\nindustrial online applications, existing multi-objective CR approaches still\nsuffer from two crucial limitations. First, previous methods need to aggregate\nthe full candidate list within a single mini-batch to compute the ranking loss.\nSuch aggregation strategy violates extensive data shuffling which has long been\nproven beneficial for preventing overfitting, and thus degrades the training\neffectiveness. Second, existing multi-objective methods apply the two\ninherently conflicting loss functions on a single probabilistic prediction,\nwhich results in a sub-optimal trade-off between calibration and ranking. To\ntackle the two limitations, we propose a Self-Boosted framework for Calibrated\nRanking (SBCR).\n","authors":["Shunyu Zhang","Hu Liu","Wentian Bao","Enyun Yu","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2406.08010v1.pdf","comment":"KDD 2024"},{"id":"http://arxiv.org/abs/2401.09074v4","updated":"2024-06-12T08:55:13Z","published":"2024-01-17T09:23:59Z","title":"Code Simulation Challenges for Large Language Models","summary":"  Many reasoning, planning, and problem-solving tasks share an intrinsic\nalgorithmic nature: correctly simulating each step is a sufficient condition to\nsolve them correctly. This work studies to what extent Large Language Models\n(LLMs) can simulate coding and algorithmic tasks to provide insights into\ngeneral capabilities in such algorithmic reasoning tasks. We introduce\nbenchmarks for straight-line programs, code that contains critical paths, and\napproximate and redundant instructions. We further assess the simulation\ncapabilities of LLMs with sorting algorithms and nested loops and show that a\nroutine's computational complexity directly affects an LLM's ability to\nsimulate its execution. While the most powerful LLMs exhibit relatively strong\nsimulation capabilities, the process is fragile, seems to rely heavily on\npattern recognition, and is affected by memorisation. We propose a novel\noff-the-shelf prompting method, Chain of Simulation (CoSm), which instructs\nLLMs to simulate code execution line by line/follow the computation pattern of\ncompilers. CoSm efficiently helps LLMs reduce memorisation and shallow pattern\nrecognition while improving simulation performance. We consider the success of\nCoSm in code simulation to be inspirational for other general routine\nsimulation reasoning tasks.\n","authors":["Emanuele La Malfa","Christoph Weinhuber","Orazio Torre","Fangru Lin","Samuele Marro","Anthony Cohn","Nigel Shadbolt","Michael Wooldridge"],"pdf_url":"https://arxiv.org/pdf/2401.09074v4.pdf","comment":"Code: https://github.com/EmanueleLM/CodeSimulation"},{"id":"http://arxiv.org/abs/2402.10487v4","updated":"2024-06-12T08:49:48Z","published":"2024-02-16T07:28:59Z","title":"RPMixer: Shaking Up Time Series Forecasting with Random Projections for\n  Large Spatial-Temporal Data","summary":"  Spatial-temporal forecasting systems play a crucial role in addressing\nnumerous real-world challenges. In this paper, we investigate the potential of\naddressing spatial-temporal forecasting problems using general time series\nforecasting models, i.e., models that do not leverage the spatial relationships\namong the nodes. We propose a all-Multi-Layer Perceptron (all-MLP) time series\nforecasting architecture called RPMixer. The all-MLP architecture was chosen\ndue to its recent success in time series forecasting benchmarks. Furthermore,\nour method capitalizes on the ensemble-like behavior of deep neural networks,\nwhere each individual block within the network behaves like a base learner in\nan ensemble model, particularly when identity mapping residual connections are\nincorporated. By integrating random projection layers into our model, we\nincrease the diversity among the blocks' outputs, thereby improving the overall\nperformance of the network. Extensive experiments conducted on the largest\nspatial-temporal forecasting benchmark datasets demonstrate that the proposed\nmethod outperforms alternative methods, including both spatial-temporal graph\nmodels and general forecasting models.\n","authors":["Chin-Chia Michael Yeh","Yujie Fan","Xin Dai","Uday Singh Saini","Vivian Lai","Prince Osei Aboagye","Junpeng Wang","Huiyuan Chen","Yan Zheng","Zhongfang Zhuang","Liang Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.10487v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04998v2","updated":"2024-06-12T08:49:16Z","published":"2024-06-07T15:09:25Z","title":"ADBA:Approximation Decision Boundary Approach for Black-Box Adversarial\n  Attacks","summary":"  Many machine learning models are susceptible to adversarial attacks, with\ndecision-based black-box attacks representing the most critical threat in\nreal-world applications. These attacks are extremely stealthy, generating\nadversarial examples using hard labels obtained from the target machine\nlearning model. This is typically realized by optimizing perturbation\ndirections, guided by decision boundaries identified through query-intensive\nexact search, significantly limiting the attack success rate. This paper\nintroduces a novel approach using the Approximation Decision Boundary (ADB) to\nefficiently and accurately compare perturbation directions without precisely\ndetermining decision boundaries. The effectiveness of our ADB approach (ADBA)\nhinges on promptly identifying suitable ADB, ensuring reliable differentiation\nof all perturbation directions. For this purpose, we analyze the probability\ndistribution of decision boundaries, confirming that using the distribution's\nmedian value as ADB can effectively distinguish different perturbation\ndirections, giving rise to the development of the ADBA-md algorithm. ADBA-md\nonly requires four queries on average to differentiate any pair of perturbation\ndirections, which is highly query-efficient. Extensive experiments on six\nwell-known image classifiers clearly demonstrate the superiority of ADBA and\nADBA-md over multiple state-of-the-art black-box attacks. The source code is\navailable at https://github.com/BUPTAIOC/ADBA.\n","authors":["Feiyang Wang","Xingquan Zuo","Hai Huang","Gang Chen"],"pdf_url":"https://arxiv.org/pdf/2406.04998v2.pdf","comment":"10 pages, 5 figures, conference"},{"id":"http://arxiv.org/abs/2406.08001v1","updated":"2024-06-12T08:47:44Z","published":"2024-06-12T08:47:44Z","title":"Asymptotic Unbiased Sample Sampling to Speed Up Sharpness-Aware\n  Minimization","summary":"  Sharpness-Aware Minimization (SAM) has emerged as a promising approach for\neffectively reducing the generalization error. However, SAM incurs twice the\ncomputational cost compared to base optimizer (e.g., SGD). We propose\nAsymptotic Unbiased Sampling with respect to iterations to accelerate SAM\n(AUSAM), which maintains the model's generalization capacity while\nsignificantly enhancing computational efficiency. Concretely, we\nprobabilistically sample a subset of data points beneficial for SAM\noptimization based on a theoretically guaranteed criterion, i.e., the Gradient\nNorm of each Sample (GNS). We further approximate the GNS by the difference in\nloss values before and after perturbation in SAM. As a plug-and-play,\narchitecture-agnostic method, our approach consistently accelerates SAM across\na range of tasks and networks, i.e., classification, human pose estimation and\nnetwork quantization. On CIFAR10/100 and Tiny-ImageNet, AUSAM achieves results\ncomparable to SAM while providing a speedup of over 70%. Compared to recent\ndynamic data pruning methods, AUSAM is better suited for SAM and excels in\nmaintaining performance. Additionally, AUSAM accelerates optimization in human\npose estimation and model quantization without sacrificing performance,\ndemonstrating its broad practicality.\n","authors":["Jiaxin Deng","Junbiao Pang","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09350v3","updated":"2024-06-12T08:40:35Z","published":"2023-03-16T14:31:50Z","title":"Unsupervised domain adaptation by learning using privileged information","summary":"  Successful unsupervised domain adaptation is guaranteed only under strong\nassumptions such as covariate shift and overlap between input domains. The\nlatter is often violated in high-dimensional applications like image\nclassification which, despite this limitation, continues to serve as\ninspiration and benchmark for algorithm development. In this work, we show that\ntraining-time access to side information in the form of auxiliary variables can\nhelp relax restrictions on input variables and increase the sample efficiency\nof learning at the cost of collecting a richer variable set. As this\ninformation is assumed available only during training, not in deployment, we\ncall this problem unsupervised domain adaptation by learning using privileged\ninformation (DALUPI). To solve this problem, we propose a simple two-stage\nlearning algorithm, inspired by our analysis of the expected error in the\ntarget domain, and a practical end-to-end variant for image classification. We\npropose three evaluation tasks based on classification of entities in photos\nand anomalies in medical images with different types of available privileged\ninformation (binary attributes and single or multiple regions of interest). We\ndemonstrate across these tasks that using privileged information in learning\ncan reduce errors in domain transfer compared to baselines, be robust to\nspurious correlations in the source domain, and increase sample efficiency.\n","authors":["Adam Breitholtz","Anton Matsson","Fredrik D. Johansson"],"pdf_url":"https://arxiv.org/pdf/2303.09350v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07992v1","updated":"2024-06-12T08:34:53Z","published":"2024-06-12T08:34:53Z","title":"A Federated Online Restless Bandit Framework for Cooperative Resource\n  Allocation","summary":"  Restless multi-armed bandits (RMABs) have been widely utilized to address\nresource allocation problems with Markov reward processes (MRPs). Existing\nworks often assume that the dynamics of MRPs are known prior, which makes the\nRMAB problem solvable from an optimization perspective. Nevertheless, an\nefficient learning-based solution for RMABs with unknown system dynamics\nremains an open problem. In this paper, we study the cooperative resource\nallocation problem with unknown system dynamics of MRPs. This problem can be\nmodeled as a multi-agent online RMAB problem, where multiple agents\ncollaboratively learn the system dynamics while maximizing their accumulated\nrewards. We devise a federated online RMAB framework to mitigate the\ncommunication overhead and data privacy issue by adopting the federated\nlearning paradigm. Based on this framework, we put forth a Federated Thompson\nSampling-enabled Whittle Index (FedTSWI) algorithm to solve this multi-agent\nonline RMAB problem. The FedTSWI algorithm enjoys a high communication and\ncomputation efficiency, and a privacy guarantee. Moreover, we derive a regret\nupper bound for the FedTSWI algorithm. Finally, we demonstrate the\neffectiveness of the proposed algorithm on the case of online multi-user\nmulti-channel access. Numerical results show that the proposed algorithm\nachieves a fast convergence rate of $\\mathcal{O}(\\sqrt{T\\log(T)})$ and better\nperformance compared with baselines. More importantly, its sample complexity\ndecreases with the number of agents.\n","authors":["Jingwen Tong","Xinran Li","Liqun Fu","Jun Zhang","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2406.07992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07991v1","updated":"2024-06-12T08:30:16Z","published":"2024-06-12T08:30:16Z","title":"Interpetable Target-Feature Aggregation for Multi-Task Learning based on\n  Bias-Variance Analysis","summary":"  Multi-task learning (MTL) is a powerful machine learning paradigm designed to\nleverage shared knowledge across tasks to improve generalization and\nperformance. Previous works have proposed approaches to MTL that can be divided\ninto feature learning, focused on the identification of a common feature\nrepresentation, and task clustering, where similar tasks are grouped together.\nIn this paper, we propose an MTL approach at the intersection between task\nclustering and feature transformation based on a two-phase iterative\naggregation of targets and features. First, we propose a bias-variance analysis\nfor regression models with additive Gaussian noise, where we provide a general\nexpression of the asymptotic bias and variance of a task, considering a linear\nregression trained on aggregated input features and an aggregated target. Then,\nwe exploit this analysis to provide a two-phase MTL algorithm (NonLinCTFA).\nFirstly, this method partitions the tasks into clusters and aggregates each\nobtained group of targets with their mean. Then, for each aggregated task, it\naggregates subsets of features with their mean in a dimensionality reduction\nfashion. In both phases, a key aspect is to preserve the interpretability of\nthe reduced targets and features through the aggregation with the mean, which\nis further motivated by applications to Earth science. Finally, we validate the\nalgorithms on synthetic data, showing the effect of different parameters and\nreal-world datasets, exploring the validity of the proposed methodology on\nclassical datasets, recent baselines, and Earth science applications.\n","authors":["Paolo Bonetti","Alberto Maria Metelli","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2406.07991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04224v2","updated":"2024-06-12T08:27:40Z","published":"2023-10-27T14:57:10Z","title":"MELEP: A Novel Predictive Measure of Transferability in Multi-Label ECG\n  Diagnosis","summary":"  In practical electrocardiography (ECG) interpretation, the scarcity of\nwell-annotated data is a common challenge. Transfer learning techniques are\nvaluable in such situations, yet the assessment of transferability has received\nlimited attention. To tackle this issue, we introduce MELEP, which stands for\nMuti-label Expected Log of Empirical Predictions, a measure designed to\nestimate the effectiveness of knowledge transfer from a pre-trained model to a\ndownstream multi-label ECG diagnosis task. MELEP is generic, working with new\ntarget data with different label sets, and computationally efficient, requiring\nonly a single forward pass through the pre-trained model. To the best of our\nknowledge, MELEP is the first transferability metric specifically designed for\nmulti-label ECG classification problems. Our experiments show that MELEP can\npredict the performance of pre-trained convolutional and recurrent deep neural\nnetworks, on small and imbalanced ECG data. Specifically, we observed strong\ncorrelation coefficients (with absolute values exceeding 0.6 in most cases)\nbetween MELEP and the actual average F1 scores of the fine-tuned models. Our\nwork highlights the potential of MELEP to expedite the selection of suitable\npre-trained models for ECG diagnosis tasks, saving time and effort that would\notherwise be spent on fine-tuning these models.\n","authors":["Cuong V. Nguyen","Hieu Minh Duong","Cuong D. Do"],"pdf_url":"https://arxiv.org/pdf/2311.04224v2.pdf","comment":"Accepted to the Journal of Healthcare Informatics Research"},{"id":"http://arxiv.org/abs/2406.07990v1","updated":"2024-06-12T08:26:30Z","published":"2024-06-12T08:26:30Z","title":"Blowfish: Topological and statistical signatures for quantifying\n  ambiguity in semantic search","summary":"  This works reports evidence for the topological signatures of ambiguity in\nsentence embeddings that could be leveraged for ranking and/or explanation\npurposes in the context of vector search and Retrieval Augmented Generation\n(RAG) systems. We proposed a working definition of ambiguity and designed an\nexperiment where we have broken down a proprietary dataset into collections of\nchunks of varying size - 3, 5, and 10 lines and used the different collections\nsuccessively as queries and answers sets. It allowed us to test the signatures\nof ambiguity with removal of confounding factors. Our results show that proxy\nambiguous queries (size 10 queries against size 3 documents) display different\ndistributions of homologies 0 and 1 based features than proxy clear queries\n(size 5 queries against size 10 documents). We then discuss those results in\nterms increased manifold complexity and/or approximately discontinuous\nembedding submanifolds. Finally we propose a strategy to leverage those\nfindings as a new scoring strategy of semantic similarities.\n","authors":["Thomas Roland Barillot","Alex De Castro"],"pdf_url":"https://arxiv.org/pdf/2406.07990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18382v2","updated":"2024-06-12T08:25:39Z","published":"2023-05-28T06:57:27Z","title":"Adaptive Sparsity Level during Training for Efficient Time Series\n  Forecasting with Transformers","summary":"  Efficient time series forecasting has become critical for real-world\napplications, particularly with deep neural networks (DNNs). Efficiency in DNNs\ncan be achieved through sparse connectivity and reducing the model size.\nHowever, finding the sparsity level automatically during training remains\nchallenging due to the heterogeneity in the loss-sparsity tradeoffs across the\ndatasets. In this paper, we propose \\enquote{\\textbf{P}runing with\n\\textbf{A}daptive \\textbf{S}parsity \\textbf{L}evel} (\\textbf{PALS}), to\nautomatically seek a decent balance between loss and sparsity, all without the\nneed for a predefined sparsity level. PALS draws inspiration from sparse\ntraining and during-training methods. It introduces the novel \"expand\"\nmechanism in training sparse neural networks, allowing the model to dynamically\nshrink, expand, or remain stable to find a proper sparsity level. In this\npaper, we focus on achieving efficiency in transformers known for their\nexcellent time series forecasting performance but high computational cost.\nNevertheless, PALS can be applied directly to any DNN. To this aim, we\ndemonstrate its effectiveness also on the DLinear model. Experimental results\non six benchmark datasets and five state-of-the-art (SOTA) transformer variants\nshow that PALS substantially reduces model size while maintaining comparable\nperformance to the dense model. More interestingly, PALS even outperforms the\ndense model, in \\textcolor{blue}{12} and \\textcolor{blue}{14} cases out of 30\ncases in terms of MSE and MAE loss, respectively, while reducing\n\\textcolor{blue}{65\\%} parameter count and \\textcolor{blue}{63\\%} FLOPs on\naverage. Our code and supplementary material are available on\nGithub\\footnote{\\tiny \\url{https://github.com/zahraatashgahi/PALS}}.\n","authors":["Zahra Atashgahi","Mykola Pechenizkiy","Raymond Veldhuis","Decebal Constantin Mocanu"],"pdf_url":"https://arxiv.org/pdf/2305.18382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03990v2","updated":"2024-06-12T08:23:38Z","published":"2024-02-06T13:43:22Z","title":"Subsampling is not Magic: Why Large Batch Sizes Work for Differentially\n  Private Stochastic Optimisation","summary":"  We study how the batch size affects the total gradient variance in\ndifferentially private stochastic gradient descent (DP-SGD), seeking a\ntheoretical explanation for the usefulness of large batch sizes. As DP-SGD is\nthe basis of modern DP deep learning, its properties have been widely studied,\nand recent works have empirically found large batch sizes to be beneficial.\nHowever, theoretical explanations of this benefit are currently heuristic at\nbest. We first observe that the total gradient variance in DP-SGD can be\ndecomposed into subsampling-induced and noise-induced variances. We then prove\nthat in the limit of an infinite number of iterations, the effective\nnoise-induced variance is invariant to the batch size. The remaining\nsubsampling-induced variance decreases with larger batch sizes, so large\nbatches reduce the effective total gradient variance. We confirm numerically\nthat the asymptotic regime is relevant in practical settings when the batch\nsize is not small, and find that outside the asymptotic regime, the total\ngradient variance decreases even more with large batch sizes. We also find a\nsufficient condition that implies that large batch sizes similarly reduce\neffective DP noise variance for one iteration of DP-SGD.\n","authors":["Ossi Räisä","Joonas Jälkö","Antti Honkela"],"pdf_url":"https://arxiv.org/pdf/2402.03990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13089v3","updated":"2024-06-12T08:21:53Z","published":"2024-05-21T11:42:20Z","title":"SEGAN: semi-supervised learning approach for missing data imputation","summary":"  In many practical real-world applications, data missing is a very common\nphenomenon, making the development of data-driven artificial intelligence\ntheory and technology increasingly difficult. Data completion is an important\nmethod for missing data preprocessing. Most existing miss-ing data completion\nmodels directly use the known information in the missing data set but ignore\nthe impact of the data label information contained in the data set on the\nmissing data completion model. To this end, this paper proposes a missing data\ncompletion model SEGAN based on semi-supervised learning, which mainly includes\nthree important modules: generator, discriminator and classifier. In the SEGAN\nmodel, the classifier enables the generator to make more full use of known data\nand its label information when predicting missing data values. In addition, the\nSE-GAN model introduces a missing hint matrix to allow the discriminator to\nmore effectively distinguish between known data and data filled by the\ngenerator. This paper theoretically proves that the SEGAN model that introduces\na classifier and a missing hint matrix can learn the real known data\ndistribution characteristics when reaching Nash equilibrium. Finally, a large\nnumber of experiments were conducted in this article, and the experimental\nresults show that com-pared with the current state-of-the-art multivariate data\ncompletion method, the performance of the SEGAN model is improved by more than\n3%.\n","authors":["Xiaohua Pan","Weifeng Wu","Peiran Liu","Zhen Li","Peng Lu","Peijian Cao","Jianfeng Zhang","Xianfei Qiu","YangYang Wu"],"pdf_url":"https://arxiv.org/pdf/2405.13089v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10095v2","updated":"2024-06-12T08:18:27Z","published":"2024-02-15T16:49:42Z","title":"Classification Diffusion Models: Revitalizing Density Ratio Estimation","summary":"  A prominent family of methods for learning data distributions relies on\ndensity ratio estimation (DRE), where a model is trained to $\\textit{classify}$\nbetween data samples and samples from some reference distribution. DRE-based\nmodels can directly output the likelihood for any given input, a highly desired\nproperty that is lacking in most generative techniques. Nevertheless, to date,\nDRE methods have struggled to accurately capture the distributions of complex\nhigh-dimensional data like images, which led to reduced research attention over\nthe years. In this work we present $\\textit{classification diffusion models}$\n(CDMs), a DRE-based generative method that adopts the formalism of denoising\ndiffusion models (DDMs) while making use of a classifier that predicts the\nlevel of noise added to a clean signal. Our method is based on an analytical\nconnection that we derive between an MSE-optimal denoiser for white Gaussian\nnoise and a cross-entropy-optimal classifier for predicting the noise level. To\nthe best of our knowledge, our method is the first DRE-based technique that can\nsuccessfully generate images. Furthermore, it can output the likelihood of any\ninput in a single forward pass, achieving state-of-the-art negative log\nlikelihood (NLL) among methods with this property. Code is available on the\nproject's webpage in https://shaharYadin.github.io/CDM/ .\n","authors":["Shahar Yadin","Noam Elata","Tomer Michaeli"],"pdf_url":"https://arxiv.org/pdf/2402.10095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07983v1","updated":"2024-06-12T08:09:29Z","published":"2024-06-12T08:09:29Z","title":"Meta-Learning Neural Procedural Biases","summary":"  The goal of few-shot learning is to generalize and achieve high performance\non new unseen learning tasks, where each task has only a limited number of\nexamples available. Gradient-based meta-learning attempts to address this\nchallenging task by learning how to learn new tasks by embedding inductive\nbiases informed by prior learning experiences into the components of the\nlearning algorithm. In this work, we build upon prior research and propose\nNeural Procedural Bias Meta-Learning (NPBML), a novel framework designed to\nmeta-learn task-adaptive procedural biases. Our approach aims to consolidate\nrecent advancements in meta-learned initializations, optimizers, and loss\nfunctions by learning them simultaneously and making them adapt to each\nindividual task to maximize the strength of the learned inductive biases. This\nimbues each learning task with a unique set of procedural biases which is\nspecifically designed and selected to attain strong learning performance in\nonly a few gradient steps. The experimental results show that by meta-learning\nthe procedural biases of a neural network, we can induce strong inductive\nbiases towards a distribution of learning tasks, enabling robust learning\nperformance across many well-established few-shot learning benchmarks.\n","authors":["Christian Raymond","Qi Chen","Bing Xue","Mengjie Zhan"],"pdf_url":"https://arxiv.org/pdf/2406.07983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07980v1","updated":"2024-06-12T08:06:31Z","published":"2024-06-12T08:06:31Z","title":"Reinforcement Learning for High-Level Strategic Control in Tower Defense\n  Games","summary":"  In strategy games, one of the most important aspects of game design is\nmaintaining a sense of challenge for players. Many mobile titles feature quick\ngameplay loops that allow players to progress steadily, requiring an abundance\nof levels and puzzles to prevent them from reaching the end too quickly. As\nwith any content creation, testing and validation are essential to ensure\nengaging gameplay mechanics, enjoyable game assets, and playable levels. In\nthis paper, we propose an automated approach that can be leveraged for gameplay\ntesting and validation that combines traditional scripted methods with\nreinforcement learning, reaping the benefits of both approaches while adapting\nto new situations similarly to how a human player would. We test our solution\non a popular tower defense game, Plants vs. Zombies. The results show that\ncombining a learned approach, such as reinforcement learning, with a scripted\nAI produces a higher-performing and more robust agent than using only heuristic\nAI, achieving a 57.12% success rate compared to 47.95% in a set of 40 levels.\nMoreover, the results demonstrate the difficulty of training a general agent\nfor this type of puzzle-like game.\n","authors":["Joakim Bergdahl","Alessandro Sestini","Linus Gisslén"],"pdf_url":"https://arxiv.org/pdf/2406.07980v1.pdf","comment":"Published at CoG 2024"},{"id":"http://arxiv.org/abs/2406.07979v1","updated":"2024-06-12T08:05:45Z","published":"2024-06-12T08:05:45Z","title":"Heuristic Learning with Graph Neural Networks: A Unified Framework for\n  Link Prediction","summary":"  Link prediction is a fundamental task in graph learning, inherently shaped by\nthe topology of the graph. While traditional heuristics are grounded in graph\ntopology, they encounter challenges in generalizing across diverse graphs.\nRecent research efforts have aimed to leverage the potential of heuristics, yet\na unified formulation accommodating both local and global heuristics remains\nundiscovered. Drawing insights from the fact that both local and global\nheuristics can be represented by adjacency matrix multiplications, we propose a\nunified matrix formulation to accommodate and generalize various heuristics. We\nfurther propose the Heuristic Learning Graph Neural Network (HL-GNN) to\nefficiently implement the formulation. HL-GNN adopts intra-layer propagation\nand inter-layer connections, allowing it to reach a depth of around 20 layers\nwith lower time complexity than GCN. HL-GNN is proven to be more expressive\nthan heuristics and conventional GNNs, and it can adaptively trade-off between\nnode features and topological information. Extensive experiments on the\nPlanetoid, Amazon, and OGB datasets underscore the effectiveness and efficiency\nof HL-GNN. It outperforms existing methods by a large margin in prediction\nperformance. Additionally, HL-GNN is several orders of magnitude faster than\nheuristic-inspired methods while requiring only a few trainable parameters. The\ncase study further demonstrates that the generalized heuristics and learned\nweights are highly interpretable.\n","authors":["Juzhen Zhang","Lanning Wei","Zhen Xu","Quanming Yao"],"pdf_url":"https://arxiv.org/pdf/2406.07979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11577v2","updated":"2024-06-12T08:04:58Z","published":"2024-04-17T17:20:27Z","title":"Towards Reliable Empirical Machine Unlearning Evaluation: A\n  Game-Theoretic View","summary":"  Machine unlearning is the process of updating machine learning models to\nremove the information of specific training data samples, in order to comply\nwith data protection regulations that allow individuals to request the removal\nof their personal data. Despite the recent development of numerous unlearning\nalgorithms, reliable evaluation of these algorithms remains an open research\nquestion. In this work, we focus on membership inference attack (MIA) based\nevaluation, one of the most common approaches for evaluating unlearning\nalgorithms, and address various pitfalls of existing evaluation metrics that\nlack reliability. Specifically, we propose a game-theoretic framework that\nformalizes the evaluation process as a game between unlearning algorithms and\nMIA adversaries, measuring the data removal efficacy of unlearning algorithms\nby the capability of the MIA adversaries. Through careful design of the game,\nwe demonstrate that the natural evaluation metric induced from the game enjoys\nprovable guarantees that the existing evaluation metrics fail to satisfy.\nFurthermore, we propose a practical and efficient algorithm to estimate the\nevaluation metric induced from the game, and demonstrate its effectiveness\nthrough both theoretical analysis and empirical experiments. This work presents\na novel and reliable approach to empirically evaluating unlearning algorithms,\npaving the way for the development of more effective unlearning techniques.\n","authors":["Yiwen Tu","Pingbang Hu","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2404.11577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07971v1","updated":"2024-06-12T07:52:17Z","published":"2024-06-12T07:52:17Z","title":"It Takes Two: On the Seamlessness between Reward and Policy Model in\n  RLHF","summary":"  Reinforcement Learning from Human Feedback (RLHF) involves training policy\nmodels (PMs) and reward models (RMs) to align language models with human\npreferences. Instead of focusing solely on PMs and RMs independently, we\npropose to examine their interactions during fine-tuning, introducing the\nconcept of seamlessness. Our study starts with observing the saturation\nphenomenon, where continual improvements in RM and PM do not translate into\nRLHF progress. Our analysis shows that RMs fail to assign proper scores to PM\nresponses, resulting in a 35% mismatch rate with human preferences,\nhighlighting a significant discrepancy between PM and RM. To measure\nseamlessness between PM and RM without human effort, we propose an automatic\nmetric, SEAM. SEAM quantifies the discrepancies between PM and RM judgments\ninduced by data samples. We validate the effectiveness of SEAM in data\nselection and model augmentation. Our experiments demonstrate that (1) using\nSEAM-filtered data for RL training improves RLHF performance by 4.5%, and (2)\nSEAM-guided model augmentation results in a 4% performance improvement over\nstandard augmentation methods.\n","authors":["Taiming Lu","Lingfeng Shen","Xinyu Yang","Weiting Tan","Beidi Chen","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2406.07971v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2406.08353v1","updated":"2024-06-12T15:59:25Z","published":"2024-06-12T15:59:25Z","title":"Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study\n  on Word Error Rate and Fusion Techniques","summary":"  Text data is commonly utilized as a primary input to enhance Speech Emotion\nRecognition (SER) performance and reliability. However, the reliance on\nhuman-transcribed text in most studies impedes the development of practical SER\nsystems, creating a gap between in-lab research and real-world scenarios where\nAutomatic Speech Recognition (ASR) serves as the text source. Hence, this study\nbenchmarks SER performance using ASR transcripts with varying Word Error Rates\n(WERs) on well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our\nevaluation includes text-only and bimodal SER with diverse fusion techniques,\naiming for a comprehensive analysis that uncovers novel findings and challenges\nfaced by current SER research. Additionally, we propose a unified ASR\nerror-robust framework integrating ASR error correction and modality-gated\nfusion, achieving lower WER and higher SER results compared to the\nbest-performing ASR transcript. This research is expected to provide insights\ninto SER with ASR assistance, especially for real-world applications.\n","authors":["Yuanchao Li","Peter Bell","Catherine Lai"],"pdf_url":"https://arxiv.org/pdf/2406.08353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.12478v3","updated":"2024-06-12T06:54:19Z","published":"2022-02-25T03:27:37Z","title":"GAME-ON: Graph Attention Network based Multimodal Fusion for Fake News\n  Detection","summary":"  Social media in present times has a significant and growing influence. Fake\nnews being spread on these platforms have a disruptive and damaging impact on\nour lives. Furthermore, as multimedia content improves the visibility of posts\nmore than text data, it has been observed that often multimedia is being used\nfor creating fake content. A plethora of previous multimodal-based work has\ntried to address the problem of modeling heterogeneous modalities in\nidentifying fake content. However, these works have the following limitations:\n(1) inefficient encoding of inter-modal relations by utilizing a simple\nconcatenation operator on the modalities at a later stage in a model, which\nmight result in information loss; (2) training very deep neural networks with a\ndisproportionate number of parameters on small but complex real-life multimodal\ndatasets result in higher chances of overfitting. To address these limitations,\nwe propose GAME-ON, a Graph Neural Network based end-to-end trainable framework\nthat allows granular interactions within and across different modalities to\nlearn more robust data representations for multimodal fake news detection. We\nuse two publicly available fake news datasets, Twitter and Weibo, for\nevaluations. Our model outperforms on Twitter by an average of 11% and keeps\ncompetitive performance on Weibo, within a 2.6% margin, while using 65% fewer\nparameters than the best comparable state-of-the-art baseline.\n","authors":["Mudit Dhawan","Shakshi Sharma","Aditya Kadam","Rajesh Sharma","Ponnurangam Kumaraguru"],"pdf_url":"https://arxiv.org/pdf/2202.12478v3.pdf","comment":"Accepted at SNAM 2024"},{"id":"http://arxiv.org/abs/2406.07874v1","updated":"2024-06-12T05:07:30Z","published":"2024-06-12T05:07:30Z","title":"Visual instrument co-design embracing the unique movement capabilities\n  of a dancer with physical disability","summary":"  This paper explores the design of an expressive visual instrument that\nembraces the unique movement style of a dancer living with physical disability.\nThrough a collaboration between the dancer and an interaction designer/visual\nartist, the creative qualities of wearable devices for motion tracking are\ninvestigated, with emphasis on integrating the dancer's specific movement\ncapabilities with their creative goals. The affordances of this technology for\nimagining new forms of creative expression play a critical role in the design\nprocess. These themes are drawn together through an experiential performance\nwhich augments an improvised dance with an ephemeral real-time visualisation of\nthe performer's movements. Through practice-based research, the design,\ndevelopment and presentation of this performance work is examined as a\n'testbed' for new ideas, allowing for the exploration of HCI concepts within a\ncreative context. This paper outlines the creative process behind the\ndevelopment of the work, the insights derived from the practice-based research\nenquiry, and the role of movement technology in encouraging new ways of moving\nthrough creative expression.\n","authors":["Sam Trolland","Melinda Smith","Alon Ilsar","Jon McCormack"],"pdf_url":"https://arxiv.org/pdf/2406.07874v1.pdf","comment":"Preprint of paper accepted at MOCO 24, The 9th International\n  Conference on Movement and Computing, Utrecht, Netherlands, May 30-June 02,\n  2024"},{"id":"http://arxiv.org/abs/2406.07871v1","updated":"2024-06-12T04:55:14Z","published":"2024-06-12T04:55:14Z","title":"Flexible Music-Conditioned Dance Generation with Style Description\n  Prompts","summary":"  Dance plays an important role as an artistic form and expression in human\nculture, yet the creation of dance remains a challenging task. Most dance\ngeneration methods primarily rely solely on music, seldom taking into\nconsideration intrinsic attributes such as music style or genre. In this work,\nwe introduce Flexible Dance Generation with Style Description Prompts (DGSDP),\na diffusion-based framework suitable for diversified tasks of dance generation\nby fully leveraging the semantics of music style. The core component of this\nframework is Music-Conditioned Style-Aware Diffusion (MCSAD), which comprises a\nTransformer-based network and a music Style Modulation module. The MCSAD seemly\nintegrates music conditions and style description prompts into the dance\ngeneration framework, ensuring that generated dances are consistent with the\nmusic content and style. To facilitate flexible dance generation and\naccommodate different tasks, a spatial-temporal masking strategy is effectively\napplied in the backward diffusion process. The proposed framework successfully\ngenerates realistic dance sequences that are accurately aligned with music for\na variety of tasks such as long-term generation, dance in-betweening, dance\ninpainting, and etc. We hope that this work has the potential to inspire dance\ngeneration and creation, with promising applications in entertainment, art, and\neducation.\n","authors":["Hongsong Wang","Yin Zhu","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2406.07871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07854v1","updated":"2024-06-12T04:06:56Z","published":"2024-06-12T04:06:56Z","title":"Zero-Shot Fake Video Detection by Audio-Visual Consistency","summary":"  Recent studies have advocated the detection of fake videos as a one-class\ndetection task, predicated on the hypothesis that the consistency between audio\nand visual modalities of genuine data is more significant than that of fake\ndata. This methodology, which solely relies on genuine audio-visual data while\nnegating the need for forged counterparts, is thus delineated as a `zero-shot'\ndetection paradigm. This paper introduces a novel zero-shot detection approach\nanchored in content consistency across audio and video. By employing\npre-trained ASR and VSR models, we recognize the audio and video content\nsequences, respectively. Then, the edit distance between the two sequences is\ncomputed to assess whether the claimed video is genuine. Experimental results\nindicate that, compared to two mainstream approaches based on semantic\nconsistency and temporal consistency, our approach achieves superior\ngeneralizability across various deepfake techniques and demonstrates strong\nrobustness against audio-visual perturbations. Finally, state-of-the-art\nperformance gains can be achieved by simply integrating the decision scores of\nthese three systems.\n","authors":["Xiaolou Li","Zehua Liu","Chen Chen","Lantian Li","Li Guo","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07854v1.pdf","comment":"to be published in INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.08564v1","updated":"2024-06-12T18:07:06Z","published":"2024-06-12T18:07:06Z","title":"A new approach for predicting the Quality of Experience in multimedia\n  services using machine learning","summary":"  In today's world, the Internet is recognized as one of the essentials of\nhuman life, playing a significant role in communications, business, and\nlifestyle. The quality of internet services can have widespread negative\nimpacts on individual and social levels. Consequently, Quality of Service (QoS)\nhas become a fundamental necessity for service providers in a competitive\nmarket aiming to offer superior services. The success and survival of these\nproviders depend on their ability to maintain high service quality and ensure\nsatisfaction.Alongside QoS, the concept of Quality of Experience (QoE) has\nemerged with the development of telephony networks. QoE focuses on the user's\nsatisfaction with the service, helping operators adjust their services to meet\nuser expectations. Recent research shows a trend towards utilizing machine\nlearning and deep learning techniques to predict QoE. Researchers aim to\ndevelop accurate models by leveraging large volumes of data from network and\nuser interactions, considering various real-world scenarios. Despite the\ncomplexity of network environments, this research provides a practical\nframework for improving and evaluating QoE. This study presents a comprehensive\nframework for evaluating QoE in multimedia services, adhering to the ITU-T\nP.1203 standard which includes automated data collection processes and uses\nmachine learning algorithms to predict user satisfaction based on key network\nparameters. By collecting over 20,000 data records from different network\nconditions and users, the Random Forest model achieved a prediction accuracy of\n95.8% for user satisfaction. This approach allows operators to dynamically\nallocate network resources in real-time, maintaining high levels of customer\nsatisfaction with minimal costs.\n","authors":["Parsa Hassani Shariat Panahi","Amir Hossein Jalilvand","Abolfazl Diyanat"],"pdf_url":"https://arxiv.org/pdf/2406.08564v1.pdf","comment":"11 pages, 5 figures"}]},"2024-06-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.09411v1","updated":"2024-06-13T17:59:52Z","published":"2024-06-13T17:59:52Z","title":"MuirBench: A Comprehensive Benchmark for Robust Multi-image\n  Understanding","summary":"  We introduce MuirBench, a comprehensive benchmark that focuses on robust\nmulti-image understanding capabilities of multimodal LLMs. MuirBench consists\nof 12 diverse multi-image tasks (e.g., scene understanding, ordering) that\ninvolve 10 categories of multi-image relations (e.g., multiview, temporal\nrelations). Comprising 11,264 images and 2,600 multiple-choice questions,\nMuirBench is created in a pairwise manner, where each standard instance is\npaired with an unanswerable variant that has minimal semantic differences, in\norder for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our\nresults reveal that even the best-performing models like GPT-4o and Gemini Pro\nfind it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.\nOpen-source multimodal LLMs trained on single images can hardly generalize to\nmulti-image questions, hovering below 33.3% in accuracy. These results\nhighlight the importance of MuirBench in encouraging the community to develop\nmultimodal LLMs that can look beyond a single image, suggesting potential\npathways for future improvements.\n","authors":["Fei Wang","Xingyu Fu","James Y. Huang","Zekun Li","Qin Liu","Xiaogeng Liu","Mingyu Derek Ma","Nan Xu","Wenxuan Zhou","Kai Zhang","Tianyi Lorena Yan","Wenjie Jacky Mo","Hsiang-Hui Liu","Pan Lu","Chunyuan Li","Chaowei Xiao","Kai-Wei Chang","Dan Roth","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09403v1","updated":"2024-06-13T17:59:31Z","published":"2024-06-13T17:59:31Z","title":"Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal\n  Language Models","summary":"  Humans draw to facilitate reasoning: we draw auxiliary lines when solving\ngeometry problems; we mark and circle when reasoning on maps; we use sketches\nto amplify our ideas and relieve our limited-capacity working memory. However,\nsuch actions are missing in current multimodal language models (LMs). Current\nchain-of-thought and tool-use paradigms only use text as intermediate reasoning\nsteps. In this work, we introduce Sketchpad, a framework that gives multimodal\nLMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts\nplanning and reasoning according to the visual artifacts it has drawn.\nDifferent from prior work, which uses text-to-image models to enable LMs to\ndraw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is\ncloser to human sketching and better facilitates reasoning. Sketchpad can also\nuse specialist vision models during the sketching process (e.g., draw bounding\nboxes with object detection models, draw masks with segmentation models), to\nfurther enhance visual perception and reasoning. We experiment with a wide\nrange of math tasks (including geometry, functions, graphs, and chess) and\ncomplex visual reasoning tasks. Sketchpad substantially improves performance on\nall tasks over strong base models with no sketching, yielding an average gain\nof 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a\nnew state of the art on all tasks, including V*Bench (80.3%), BLINK spatial\nreasoning (83.9%), and visual correspondence (80.8%). All codes and data are in\nhttps://visualsketchpad.github.io/.\n","authors":["Yushi Hu","Weijia Shi","Xingyu Fu","Dan Roth","Mari Ostendorf","Luke Zettlemoyer","Noah A Smith","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2406.09403v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2406.09393v1","updated":"2024-06-13T17:59:09Z","published":"2024-06-13T17:59:09Z","title":"Improving Autoregressive Training with Dynamic Oracles","summary":"  Many tasks within NLP can be framed as sequential decision problems, ranging\nfrom sequence tagging to text generation. However, for many tasks, the standard\ntraining methods, including maximum likelihood (teacher forcing) and scheduled\nsampling, suffer from exposure bias and a mismatch between metrics employed\nduring training and inference. DAgger provides a solution to mitigate these\nproblems, yet it requires a metric-specific dynamic oracle algorithm, which\ndoes not exist for many common metrics like span-based F1, ROUGE, and BLEU. In\nthis paper, we develop these novel dynamic oracles and show they maintain\nDAgger's no-regret guarantee for decomposable metrics like span-based F1. We\nevaluate the algorithm's performance on named entity recognition (NER), text\nsummarization, and machine translation (MT). While DAgger with dynamic oracle\nyields less favorable results in our MT experiments, it outperforms the\nbaseline techniques in NER and text summarization.\n","authors":["Jianing Yang","Harshine Visvanathan","Yilin Wang","Xinyi Hu","Matthew Gormley"],"pdf_url":"https://arxiv.org/pdf/2406.09393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00343v2","updated":"2024-06-13T17:53:45Z","published":"2024-06-01T07:36:59Z","title":"Beyond Metrics: Evaluating LLMs' Effectiveness in Culturally Nuanced,\n  Low-Resource Real-World Scenarios","summary":"  The deployment of Large Language Models (LLMs) in real-world applications\npresents both opportunities and challenges, particularly in multilingual and\ncode-mixed communication settings. This research evaluates the performance of\nseven leading LLMs in sentiment analysis on a dataset derived from multilingual\nand code-mixed WhatsApp chats, including Swahili, English and Sheng. Our\nevaluation includes both quantitative analysis using metrics like F1 score and\nqualitative assessment of LLMs' explanations for their predictions. We find\nthat, while Mistral-7b and Mixtral-8x7b achieved high F1 scores, they and other\nLLMs such as GPT-3.5-Turbo, Llama-2-70b, and Gemma-7b struggled with\nunderstanding linguistic and contextual nuances, as well as lack of\ntransparency in their decision-making process as observed from their\nexplanations. In contrast, GPT-4 and GPT-4-Turbo excelled in grasping diverse\nlinguistic inputs and managing various contextual information, demonstrating\nhigh consistency with human alignment and transparency in their decision-making\nprocess. The LLMs however, encountered difficulties in incorporating cultural\nnuance especially in non-English settings with GPT-4s doing so inconsistently.\nThe findings emphasize the necessity of continuous improvement of LLMs to\neffectively tackle the challenges of culturally nuanced, low-resource\nreal-world settings and the need for developing evaluation benchmarks for\ncapturing these issues.\n","authors":["Millicent Ochieng","Varun Gumma","Sunayana Sitaram","Jindong Wang","Vishrav Chaudhary","Keshet Ronen","Kalika Bali","Jacki O'Neill"],"pdf_url":"https://arxiv.org/pdf/2406.00343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12468v2","updated":"2024-06-13T17:32:00Z","published":"2024-05-21T03:04:14Z","title":"Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot\n  Dialogue State Tracking","summary":"  We demonstrate substantial performance gains in zero-shot dialogue state\ntracking (DST) by enhancing training data diversity through synthetic data\ngeneration. Existing DST datasets are severely limited in the number of\napplication domains and slot types they cover due to the high costs of data\ncollection, restricting their adaptability to new domains. This work addresses\nthis challenge with a novel, fully automatic data generation approach that\ncreates synthetic zero-shot DST datasets. Distinguished from previous methods,\nour approach can generate dialogues across a massive range of application\ndomains, complete with silver-standard dialogue state annotations and slot\ndescriptions. This technique is used to create the D0T dataset for training\nzero-shot DST models, encompassing an unprecedented 1,000+ domains. Experiments\non the MultiWOZ benchmark show that training models on diverse synthetic data\nimproves Joint Goal Accuracy by 6.7%, achieving results competitive with models\n13.5 times larger than ours.\n","authors":["James D. Finch","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2405.12468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09345v1","updated":"2024-06-13T17:28:13Z","published":"2024-06-13T17:28:13Z","title":"DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech\n  Units for Spoken Language Understanding","summary":"  The integration of pre-trained text-based large language models (LLM) with\nspeech input has enabled instruction-following capabilities for diverse speech\ntasks. This integration requires the use of a speech encoder, a speech adapter,\nand an LLM, trained on diverse tasks. We propose the use of discrete speech\nunits (DSU), rather than continuous-valued speech encoder outputs, that are\nconverted to the LLM token embedding space using the speech adapter. We\ngenerate DSU using a self-supervised speech encoder followed by k-means\nclustering. The proposed model shows robust performance on speech inputs from\nseen/unseen domains and instruction-following capability in spoken question\nanswering. We also explore various types of DSU extracted from different layers\nof the self-supervised speech encoder, as well as Mel frequency Cepstral\nCoefficients (MFCC). Our findings suggest that the ASR task and datasets are\nnot crucial in instruction-tuning for spoken question answering tasks.\n","authors":["Suwon Shon","Kwangyoun Kim","Yi-Te Hsu","Prashant Sridhar","Shinji Watanabe","Karen Livescu"],"pdf_url":"https://arxiv.org/pdf/2406.09345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10986v2","updated":"2024-06-13T17:24:50Z","published":"2024-02-16T05:05:12Z","title":"FinTral: A Family of GPT-4 Level Multimodal Financial Large Language\n  Models","summary":"  We introduce FinTral, a suite of state-of-the-art multimodal large language\nmodels (LLMs) built upon the Mistral-7b model and tailored for financial\nanalysis. FinTral integrates textual, numerical, tabular, and image data. We\nenhance FinTral with domain-specific pretraining, instruction fine-tuning, and\nRLAIF training by exploiting a large collection of textual and visual datasets\nwe curate for this work. We also introduce an extensive benchmark featuring\nnine tasks and 25 datasets for evaluation, including hallucinations in the\nfinancial domain. Our FinTral model trained with direct preference optimization\nemploying advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R,\ndemonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5\nin all tasks and surpasses GPT-4 in five out of nine tasks, marking a\nsignificant advancement in AI-driven financial technology. We also demonstrate\nthat FinTral has the potential to excel in real-time analysis and\ndecision-making in diverse financial contexts. The GitHub repository for\n\\textit{FinTral} is available at \\url{https://github.com/UBC-NLP/fintral}.\n","authors":["Gagan Bhatia","El Moatez Billah Nagoudi","Hasan Cavusoglu","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2402.10986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09334v1","updated":"2024-06-13T17:15:33Z","published":"2024-06-13T17:15:33Z","title":"ProxyLM: Predicting Language Model Performance on Multilingual Tasks via\n  Proxy Models","summary":"  Performance prediction is a method to estimate the performance of\nmultilingual language models (LMs), mitigating computational costs associated\nwith model capacity and data for fine-tuning. Our paper introduces ProxyLM, a\nscalable framework for predicting LM performance using proxy models in\nmultilingual tasks. These proxy models act as surrogates, approximating the\nperformance of fine-tuned LMs on specific downstream natural language\nprocessing (NLP) tasks. By leveraging proxy models, ProxyLM significantly\nreduces computational overhead on task evaluations, achieving up to a 37.08x\nspeedup compared to traditional methods, even with our smallest proxy models.\nAdditionally, our methodology showcases adaptability to previously unseen\nlanguages in pre-trained LMs, outperforming the state-of-the-art performance by\n1.89x as measured by root-mean-square-error (RMSE). This framework streamlines\nmodel selection, enabling efficient deployment and iterative LM enhancements\nwithout extensive computational resources.\n","authors":["David Anugraha","Genta Indra Winata","Chenyue Li","Patrick Amadeus Irawan","En-Shiun Annie Lee"],"pdf_url":"https://arxiv.org/pdf/2406.09334v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.09330v1","updated":"2024-06-13T17:08:58Z","published":"2024-06-13T17:08:58Z","title":"Learning from Natural Language Explanations for Generalizable Entity\n  Matching","summary":"  Entity matching is the task of linking records from different sources that\nrefer to the same real-world entity. Past work has primarily treated entity\nlinking as a standard supervised learning problem. However, supervised entity\nmatching models often do not generalize well to new data, and collecting\nexhaustive labeled training data is often cost prohibitive. Further, recent\nefforts have adopted LLMs for this task in few/zero-shot settings, exploiting\ntheir general knowledge. But LLMs are prohibitively expensive for performing\ninference at scale for real-world entity matching tasks.\n  As an efficient alternative, we re-cast entity matching as a conditional\ngeneration task as opposed to binary classification. This enables us to\n\"distill\" LLM reasoning into smaller entity matching models via natural\nlanguage explanations. This approach achieves strong performance, especially on\nout-of-domain generalization tests (10.85% F-1) where standalone generative\nmethods struggle. We perform ablations that highlight the importance of\nexplanations, both for performance and model robustness.\n","authors":["Somin Wadhwa","Adit Krishnan","Runhui Wang","Byron C. Wallace","Chris Kong"],"pdf_url":"https://arxiv.org/pdf/2406.09330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04539v2","updated":"2024-06-13T17:05:54Z","published":"2023-06-07T15:44:53Z","title":"Multimodal Learning Without Labeled Multimodal Data: Guarantees and\n  Applications","summary":"  In many machine learning systems that jointly learn from multiple modalities,\na core research question is to understand the nature of multimodal\ninteractions: how modalities combine to provide new task-relevant information\nthat was not present in either alone. We study this challenge of interaction\nquantification in a semi-supervised setting with only labeled unimodal data and\nnaturally co-occurring multimodal data (e.g., unlabeled images and captions,\nvideo and corresponding audio) but when labeling them is time-consuming. Using\na precise information-theoretic definition of interactions, our key\ncontribution is the derivation of lower and upper bounds to quantify the amount\nof multimodal interactions in this semi-supervised setting. We propose two\nlower bounds: one based on the shared information between modalities and the\nother based on disagreement between separately trained unimodal classifiers,\nand derive an upper bound through connections to approximate algorithms for\nmin-entropy couplings. We validate these estimated bounds and show how they\naccurately track true interactions. Finally, we show how these theoretical\nresults can be used to estimate multimodal model performance, guide data\ncollection, and select appropriate multimodal models for various tasks.\n","authors":["Paul Pu Liang","Chun Kai Ling","Yun Cheng","Alex Obolenskiy","Yudong Liu","Rohan Pandey","Alex Wilf","Louis-Philippe Morency","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2306.04539v2.pdf","comment":"ICLR 2024, Code available at: https://github.com/pliang279/PID"},{"id":"http://arxiv.org/abs/2406.09325v1","updated":"2024-06-13T17:02:32Z","published":"2024-06-13T17:02:32Z","title":"REVS: Unlearning Sensitive Information in Language Models via Rank\n  Editing in the Vocabulary Space","summary":"  Large language models (LLMs) risk inadvertently memorizing and divulging\nsensitive or personally identifiable information (PII) seen in training data,\ncausing privacy concerns. Current approaches to address this issue involve\ncostly dataset scrubbing, or model filtering through unlearning and model\nediting, which can be bypassed through extraction attacks. We propose REVS, a\nnovel model editing method for unlearning sensitive information from LLMs. REVS\nidentifies and modifies a small subset of neurons relevant for each piece of\nsensitive information. By projecting these neurons to the vocabulary space\n(unembedding), we pinpoint the components driving its generation. We then\ncompute a model edit based on the pseudo-inverse of the unembedding matrix, and\napply it to de-promote generation of the targeted sensitive data. To adequately\nevaluate our method on truly sensitive information, we curate two datasets: an\nemail dataset inherently memorized by GPT-J, and a synthetic social security\nnumber dataset that we tune the model to memorize. Compared to other\nstate-of-the-art model editing methods, REVS demonstrates superior performance\nin both eliminating sensitive information and robustness to extraction attacks,\nwhile retaining integrity of the underlying model. The code and a demo notebook\nare available at https://technion-cs-nlp.github.io/REVS.\n","authors":["Tomer Ashuach","Martin Tutek","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2406.09325v1.pdf","comment":"18 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.09324v1","updated":"2024-06-13T17:01:40Z","published":"2024-06-13T17:01:40Z","title":"Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs","summary":"  Although Large Language Models (LLMs) have demonstrated significant\ncapabilities in executing complex tasks in a zero-shot manner, they are\nsusceptible to jailbreak attacks and can be manipulated to produce harmful\noutputs. Recently, a growing body of research has categorized jailbreak attacks\ninto token-level and prompt-level attacks. However, previous work primarily\noverlooks the diverse key factors of jailbreak attacks, with most studies\nconcentrating on LLM vulnerabilities and lacking exploration of\ndefense-enhanced LLMs. To address these issues, we evaluate the impact of\nvarious attack settings on LLM performance and provide a baseline benchmark for\njailbreak attacks, encouraging the adoption of a standardized evaluation\nframework. Specifically, we evaluate the eight key factors of implementing\njailbreak attacks on LLMs from both target-level and attack-level perspectives.\nWe further conduct seven representative jailbreak attacks on six defense\nmethods across two widely used datasets, encompassing approximately 320\nexperiments with about 50,000 GPU hours on A800-80G. Our experimental results\nhighlight the need for standardized benchmarking to evaluate these attacks on\ndefense-enhanced LLMs. Our code is available at\nhttps://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking.\n","authors":["Zhao Xu","Fan Liu","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2406.09324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09321v1","updated":"2024-06-13T16:59:43Z","published":"2024-06-13T16:59:43Z","title":"JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts\n  Against Large Language Models","summary":"  Jailbreak attacks aim to induce Large Language Models (LLMs) to generate\nharmful responses for forbidden instructions, presenting severe misuse threats\nto LLMs. Up to now, research into jailbreak attacks and defenses is emerging,\nhowever, there is (surprisingly) no consensus on how to evaluate whether a\njailbreak attempt is successful. In other words, the methods to assess the\nharmfulness of an LLM's response are varied, such as manual annotation or\nprompting GPT-4 in specific ways. Each approach has its own set of strengths\nand weaknesses, impacting their alignment with human values, as well as the\ntime and financial cost. This diversity in evaluation presents challenges for\nresearchers in choosing suitable evaluation methods and conducting fair\ncomparisons across different jailbreak attacks and defenses. In this paper, we\nconduct a comprehensive analysis of jailbreak evaluation methodologies, drawing\nfrom nearly ninety jailbreak research released between May 2023 and April 2024.\nOur study introduces a systematic taxonomy of jailbreak evaluators, offering\nin-depth insights into their strengths and weaknesses, along with the current\nstatus of their adaptation. Moreover, to facilitate subsequent research, we\npropose JailbreakEval, a user-friendly toolkit focusing on the evaluation of\njailbreak attempts. It includes various well-known evaluators out-of-the-box,\nso that users can obtain evaluation results with only a single command.\nJailbreakEval also allows users to customize their own evaluation workflow in a\nunified framework with the ease of development and comparison. In summary, we\nregard JailbreakEval to be a catalyst that simplifies the evaluation process in\njailbreak research and fosters an inclusive standard for jailbreak evaluation\nwithin the community.\n","authors":["Delong Ran","Jinyuan Liu","Yichen Gong","Jingyi Zheng","Xinlei He","Tianshuo Cong","Anyu Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09321v1.pdf","comment":"Our code is available at https://github.com/ThuCCSLab/JailbreakEval"},{"id":"http://arxiv.org/abs/2406.09320v1","updated":"2024-06-13T16:58:02Z","published":"2024-06-13T16:58:02Z","title":"Khmer Semantic Search Engine: Digital Information Access and Document\n  Retrieval","summary":"  The search engine process is crucial for document content retrieval. For\nKhmer documents, a tool is needed to extract essential keywords. Despite the\ndaily generation of significant Khmer content, Cambodians struggle to find\nnecessary documents due to the lack of an effective semantic searching tool.\nEven Google does not deliver high accuracy for Khmer content. Semantic search\nengines improve search results by employing advanced algorithms to understand\nvarious content types. With the rise in Khmer digital content such as reports,\narticles, and social media feedback enhanced search capabilities are essential.\nThis research proposes the first Khmer Semantic Search Engine (KSE), designed\nto improve traditional Khmer search methods. Utilizing semantic matching\ntechniques and formally annotated semantic content, our tool extracts\nmeaningful keywords from user queries performs precise matching, and provides\nthe best matching offline documents and online URL documents. We propose two\nsemantic search frameworks based on keyword extraction and semantic search\nmatching. Additionally, we developed tools for data preparation, including\ndocument addition and manual keyword extraction. To evaluate performance, we\ncreated a ground truth dataset and discussed issues related to searching and\nsemantic search. Our findings show how understanding search term semantics can\nlead to more accurate results.\n","authors":["Nimol Thuon"],"pdf_url":"https://arxiv.org/pdf/2406.09320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09805v2","updated":"2024-06-13T16:54:51Z","published":"2024-05-16T04:25:53Z","title":"SecureLLM: Using Compositionality to Build Provably Secure Language\n  Models for Private, Sensitive, and Secret Data","summary":"  Traditional security mechanisms isolate resources from users who should not\naccess them. We reflect the compositional nature of such security mechanisms\nback into the structure of LLMs to build a provably secure LLM; that we term\nSecureLLM. Other approaches to LLM safety attempt to protect against bad actors\nor bad outcomes, but can only do so to an extent making them inappropriate for\nsensitive data. SecureLLM blends access security with fine-tuning methods. Each\ndata silo has associated with it a separate fine-tuning and a user has access\nonly to the collection of fine-tunings that they have permission for. The model\nmust then perform on compositional tasks at the intersection of those data\nsilos with the combination of those individual fine-tunings. While applicable\nto any task like document QA or making API calls, in this work we concern\nourselves with models that learn the layouts of new SQL databases to provide\nnatural-language-to-SQL translation capabilities. Existing fine-tuning\ncomposition methods fail in this challenging environment, as they are not\nwell-equipped for handling compositional tasks. Compositionality remains a\nchallenge for LLMs. We contribute both a difficult new compositional\nnatural-language-to-SQL translation task and a new perspective on LLM security\nthat allows models to be deployed to secure environments today.\n","authors":["Abdulrahman Alabdulkareem","Christian M Arnold","Yerim Lee","Pieter M Feenstra","Boris Katz","Andrei Barbu"],"pdf_url":"https://arxiv.org/pdf/2405.09805v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09308v1","updated":"2024-06-13T16:42:06Z","published":"2024-06-13T16:42:06Z","title":"Transformers meet Neural Algorithmic Reasoners","summary":"  Transformers have revolutionized machine learning with their simple yet\neffective architecture. Pre-training Transformers on massive text datasets from\nthe Internet has led to unmatched generalization for natural language\nunderstanding (NLU) tasks. However, such language models remain fragile when\ntasked with algorithmic forms of reasoning, where computations must be precise\nand robust. To address this limitation, we propose a novel approach that\ncombines the Transformer's language understanding with the robustness of graph\nneural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs\nproved effective as generic solvers for algorithmic tasks, when specified in\ngraph form. To make their embeddings accessible to a Transformer, we propose a\nhybrid architecture with a two-phase training procedure, allowing the tokens in\nthe language model to cross-attend to the node embeddings from the NAR. We\nevaluate our resulting TransNAR model on CLRS-Text, the text-based version of\nthe CLRS-30 benchmark, and demonstrate significant gains over Transformer-only\nmodels for algorithmic reasoning, both in and out of distribution.\n","authors":["Wilfried Bounsi","Borja Ibarz","Andrew Dudzik","Jessica B. Hamrick","Larisa Markeeva","Alex Vitvitskyi","Razvan Pascanu","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2406.09308v1.pdf","comment":"To appear at CVPR 2024 Multimodal Algorithmic Reasoning (MAR)\n  Workshop. 10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.09295v1","updated":"2024-06-13T16:30:14Z","published":"2024-06-13T16:30:14Z","title":"AlignMMBench: Evaluating Chinese Multimodal Alignment in Large\n  Vision-Language Models","summary":"  Evaluating the alignment capabilities of large Vision-Language Models (VLMs)\nis essential for determining their effectiveness as helpful assistants.\nHowever, existing benchmarks primarily focus on basic abilities using nonverbal\nmethods, such as yes-no and multiple-choice questions. In this paper, we\naddress this gap by introducing AlignMMBench, a comprehensive alignment\nbenchmark specifically designed for emerging Chinese VLMs. This benchmark is\nmeticulously curated from real-world scenarios and Chinese Internet sources,\nencompassing thirteen specific tasks across three categories, and includes both\nsingle-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite\nstrategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer\npairs. To facilitate the evaluation pipeline, we propose CritiqueVLM, a\nrule-calibrated evaluator that exceeds GPT-4's evaluation ability. Finally, we\nreport the performance of representative VLMs on AlignMMBench, offering\ninsights into the capabilities and limitations of different VLM architectures.\nAll evaluation codes and data are available on https://alignmmbench.github.io.\n","authors":["Yuhang Wu","Wenmeng Yu","Yean Cheng","Yan Wang","Xiaohan Zhang","Jiazheng Xu","Ming Ding","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2406.09295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15241v2","updated":"2024-06-13T16:28:47Z","published":"2024-01-26T23:17:31Z","title":"Unlearning Traces the Influential Training Data of Language Models","summary":"  Identifying the training datasets that influence a language model's outputs\nis essential for minimizing the generation of harmful content and enhancing its\nperformance. Ideally, we can measure the influence of each dataset by removing\nit from training; however, it is prohibitively expensive to retrain a model\nmultiple times. This paper presents UnTrac: unlearning traces the influence of\na training dataset on the model's performance. UnTrac is extremely simple; each\ntraining dataset is unlearned by gradient ascent, and we evaluate how much the\nmodel's predictions change after unlearning. Furthermore, we propose a more\nscalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the\nunlearned model on training datasets. UnTrac-Inv resembles UnTrac, while being\nefficient for massive training datasets. In the experiments, we examine if our\nmethods can assess the influence of pretraining datasets on generating toxic,\nbiased, and untruthful content. Our methods estimate their influence much more\naccurately than existing methods while requiring neither excessive memory space\nnor multiple checkpoints.\n","authors":["Masaru Isonuma","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2401.15241v2.pdf","comment":"14 pages, to appear in ACL2024 main conference (long paper)"},{"id":"http://arxiv.org/abs/2406.09290v1","updated":"2024-06-13T16:27:56Z","published":"2024-06-13T16:27:56Z","title":"Exploring Spoken Language Identification Strategies for Automatic\n  Transcription of Multilingual Broadcast and Institutional Speech","summary":"  This paper addresses spoken language identification (SLI) and speech\nrecognition of multilingual broadcast and institutional speech, real\napplication scenarios that have been rarely addressed in the SLI literature.\nObserving that in these domains language changes are mostly associated with\nspeaker changes, we propose a cascaded system consisting of speaker diarization\nand language identification and compare it with more traditional language\nidentification and language diarization systems. Results show that the proposed\nsystem often achieves lower language classification and language diarization\nerror rates (up to 10% relative language diarization error reduction and 60%\nrelative language confusion reduction) and leads to lower WERs on multilingual\ntest sets (more than 8% relative WER reduction), while at the same time does\nnot negatively affect speech recognition on monolingual audio (with an absolute\nWER increase between 0.1% and 0.7% w.r.t. monolingual ASR).\n","authors":["Martina Valente","Fabio Brugnara","Giovanni Morrone","Enrico Zovato","Leonardo Badino"],"pdf_url":"https://arxiv.org/pdf/2406.09290v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.09289v1","updated":"2024-06-13T16:26:47Z","published":"2024-06-13T16:26:47Z","title":"Understanding Jailbreak Success: A Study of Latent Space Dynamics in\n  Large Language Models","summary":"  Conversational Large Language Models are trained to refuse to answer harmful\nquestions. However, emergent jailbreaking techniques can still elicit unsafe\noutputs, presenting an ongoing challenge for model alignment. To better\nunderstand how different jailbreak types circumvent safeguards, this paper\nanalyses model activations on different jailbreak inputs. We find that it is\npossible to extract a jailbreak vector from a single class of jailbreaks that\nworks to mitigate jailbreak effectiveness from other classes. This may indicate\nthat different kinds of effective jailbreaks operate via similar internal\nmechanisms. We investigate a potential common mechanism of harmfulness feature\nsuppression, and provide evidence for its existence by looking at the\nharmfulness vector component. These findings offer actionable insights for\ndeveloping more robust jailbreak countermeasures and lay the groundwork for a\ndeeper, mechanistic understanding of jailbreak dynamics in language models.\n","authors":["Sarah Ball","Frauke Kreuter","Nina Rimsky"],"pdf_url":"https://arxiv.org/pdf/2406.09289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19154v2","updated":"2024-06-13T16:26:15Z","published":"2024-04-29T23:36:38Z","title":"RTF: Region-based Table Filling Method for Relational Triple Extraction","summary":"  Relational triple extraction is crucial work for the automatic construction\nof knowledge graphs. Existing methods only construct shallow representations\nfrom a token or token pair-level. However, previous works ignore local spatial\ndependencies of relational triples, resulting in a weakness of entity pair\nboundary detection. To tackle this problem, we propose a novel Region-based\nTable Filling method (RTF). We devise a novel region-based tagging scheme and\nbi-directional decoding strategy, which regard each relational triple as a\nregion on the relation-specific table, and identifies triples by determining\ntwo endpoints of each region. We also introduce convolution to construct\nregion-level table representations from a spatial perspective which makes\ntriples easier to be captured. In addition, we share partial tagging scores\namong different relations to improve learning efficiency of relation\nclassifier. Experimental results show that our method achieves state-of-the-art\nwith better generalization capability on three variants of two widely used\nbenchmark datasets.\n","authors":["Ning An","Lei Hei","Yong Jiang","Weiping Meng","Jingjing Hu","Boran Huang","Feiliang Ren"],"pdf_url":"https://arxiv.org/pdf/2404.19154v2.pdf","comment":"Rejected by EMNLP 2023"},{"id":"http://arxiv.org/abs/2406.09282v1","updated":"2024-06-13T16:22:37Z","published":"2024-06-13T16:22:37Z","title":"On the Effects of Heterogeneous Data Sources on Speech-to-Text\n  Foundation Models","summary":"  The Open Whisper-style Speech Model (OWSM) series was introduced to achieve\nfull transparency in building advanced speech-to-text (S2T) foundation models.\nTo this end, OWSM models are trained on 25 public speech datasets, which are\nheterogeneous in multiple ways. In this study, we advance the OWSM series by\nintroducing OWSM v3.2, which improves on prior models by investigating and\naddressing the impacts of this data heterogeneity. Our study begins with a\ndetailed analysis of each dataset, from which we derive two key strategies:\ndata filtering with proxy task to enhance data quality, and the incorporation\nof punctuation and true-casing using an open large language model (LLM). With\nall other configurations staying the same, OWSM v3.2 improves performance over\nthe OWSM v3.1 baseline while using 15% less training data.\n","authors":["Jinchuan Tian","Yifan Peng","William Chen","Kwanghee Choi","Karen Livescu","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2406.09282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09279v1","updated":"2024-06-13T16:17:21Z","published":"2024-06-13T16:17:21Z","title":"Unpacking DPO and PPO: Disentangling Best Practices for Learning from\n  Preference Feedback","summary":"  Learning from preference feedback has emerged as an essential step for\nimproving the generation quality and performance of modern language models\n(LMs). Despite its widespread use, the way preference-based learning is applied\nvaries wildly, with differing data, learning algorithms, and evaluations used,\nmaking disentangling the impact of each aspect difficult. In this work, we\nidentify four core aspects of preference-based learning: preference data,\nlearning algorithm, reward model, and policy training prompts, systematically\ninvestigate the impact of these components on downstream model performance, and\nsuggest a recipe for strong learning for preference feedback. Our findings\nindicate that all aspects are important for performance, with better preference\ndata leading to the largest improvements, followed by the choice of learning\nalgorithm, the use of improved reward models, and finally the use of additional\nunlabeled prompts for policy training. Notably, PPO outperforms DPO by up to\n2.5% in math and 1.2% in general domains. High-quality preference data leads to\nimprovements of up to 8% in instruction following and truthfulness. Despite\nsignificant gains of up to 5% in mathematical evaluation when scaling up reward\nmodels, we surprisingly observe marginal improvements in other categories.\n  We publicly release the code used for training\n(https://github.com/hamishivi/EasyLM) and evaluating\n(https://github.com/allenai/open-instruct) our models, along with the models\nand datasets themselves\n(https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618).\n","authors":["Hamish Ivison","Yizhong Wang","Jiacheng Liu","Zeqiu Wu","Valentina Pyatkin","Nathan Lambert","Noah A. Smith","Yejin Choi","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2406.09279v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.09277v1","updated":"2024-06-13T16:15:53Z","published":"2024-06-13T16:15:53Z","title":"End-to-end Streaming model for Low-Latency Speech Anonymization","summary":"  Speaker anonymization aims to conceal cues to speaker identity while\npreserving linguistic content. Current machine learning based approaches\nrequire substantial computational resources, hindering real-time streaming\napplications. To address these concerns, we propose a streaming model that\nachieves speaker anonymization with low latency. The system is trained in an\nend-to-end autoencoder fashion using a lightweight content encoder that\nextracts HuBERT-like information, a pretrained speaker encoder that extract\nspeaker identity, and a variance encoder that injects pitch and energy\ninformation. These three disentangled representations are fed to a decoder that\nresynthesizes the speech signal. We present evaluation results from two\nimplementations of our system, a full model that achieves a latency of 230ms,\nand a lite version (0.1x in size) that further reduces latency to 66ms while\nmaintaining state-of-the-art performance in naturalness, intelligibility, and\nprivacy preservation.\n","authors":["Waris Quamer","Ricardo Gutierrez-Osuna"],"pdf_url":"https://arxiv.org/pdf/2406.09277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09265v1","updated":"2024-06-13T16:04:11Z","published":"2024-06-13T16:04:11Z","title":"Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs","summary":"  Multilingual large language models (LLMs) have greatly increased the ceiling\nof performance on non-English tasks. However the mechanisms behind\nmultilingualism in these LLMs are poorly understood. Of particular interest is\nthe degree to which internal representations are shared between languages.\nRecent work on neuron analysis of LLMs has focused on the monolingual case, and\nthe limited work on the multilingual case has not considered the interaction\nbetween tasks and linguistic representations. In our work, we investigate how\nneuron activation is shared across languages by categorizing neurons into four\ndistinct groups according to their responses across different languages for a\nparticular input: all-shared, partial-shared, specific, and non-activated. This\ncategorization is combined with a study of neuron attribution, i.e. the\nimportance of a neuron w.r.t an output. Our analysis reveals the following\ninsights: (i) the linguistic sharing patterns are strongly affected by the type\nof task, but neuron behaviour changes across different inputs even for the same\ntask; (ii) all-shared neurons play a key role in generating correct responses;\n(iii) boosting multilingual alignment by increasing all-shared neurons can\nenhance accuracy on multilingual tasks. The code is available at\nhttps://github.com/weixuan-wang123/multilingual-neurons.\n","authors":["Weixuan Wang","Barry Haddow","Wei Peng","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2406.09265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09264v1","updated":"2024-06-13T16:03:25Z","published":"2024-06-13T16:03:25Z","title":"Towards Bidirectional Human-AI Alignment: A Systematic Review for\n  Clarifications, Framework, and Future Directions","summary":"  Recent advancements in general-purpose AI have highlighted the importance of\nguiding AI systems towards the intended goals, ethical principles, and values\nof individuals and groups, a concept broadly recognized as alignment. However,\nthe lack of clarified definitions and scopes of human-AI alignment poses a\nsignificant obstacle, hampering collaborative efforts across research domains\nto achieve this alignment. In particular, ML- and philosophy-oriented alignment\nresearch often views AI alignment as a static, unidirectional process (i.e.,\naiming to ensure that AI systems' objectives match humans) rather than an\nongoing, mutual alignment problem [429]. This perspective largely neglects the\nlong-term interaction and dynamic changes of alignment. To understand these\ngaps, we introduce a systematic review of over 400 papers published between\n2019 and January 2024, spanning multiple domains such as Human-Computer\nInteraction (HCI), Natural Language Processing (NLP), Machine Learning (ML),\nand others. We characterize, define and scope human-AI alignment. From this, we\npresent a conceptual framework of \"Bidirectional Human-AI Alignment\" to\norganize the literature from a human-centered perspective. This framework\nencompasses both 1) conventional studies of aligning AI to humans that ensures\nAI produces the intended outcomes determined by humans, and 2) a proposed\nconcept of aligning humans to AI, which aims to help individuals and society\nadjust to AI advancements both cognitively and behaviorally. Additionally, we\narticulate the key findings derived from literature analysis, including\ndiscussions about human values, interaction techniques, and evaluations. To\npave the way for future studies, we envision three key challenges for future\ndirections and propose examples of potential future solutions.\n","authors":["Hua Shen","Tiffany Knearem","Reshmi Ghosh","Kenan Alkiek","Kundan Krishna","Yachuan Liu","Ziqiao Ma","Savvas Petridis","Yi-Hao Peng","Li Qiwei","Sushrita Rakshit","Chenglei Si","Yutong Xie","Jeffrey P. Bigham","Frank Bentley","Joyce Chai","Zachary Lipton","Qiaozhu Mei","Rada Mihalcea","Michael Terry","Diyi Yang","Meredith Ringel Morris","Paul Resnick","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2406.09264v1.pdf","comment":"56 pages"},{"id":"http://arxiv.org/abs/2310.02409v2","updated":"2024-06-13T15:19:24Z","published":"2023-10-03T20:07:06Z","title":"Dodo: Dynamic Contextual Compression for Decoder-only LMs","summary":"  Transformer-based language models (LMs) are inefficient in long contexts. We\npropose Dodo, a solution for context compression. Instead of one vector per\ntoken in a standard transformer model, Dodo represents text with a dynamic\nnumber of hidden states at each layer, reducing the cost of self-attention to a\nfraction of typical time and space. Moreover, off-the-shelf models such as\nLLaMA can be adapted to Dodo by efficient parameter tuning methods such as\nLoRA. In use, Dodo can act as either an autoregressive LM or a context\ncompressor for downstream tasks. We demonstrate through experiments in language\nmodeling, question answering, and summarization that Dodo retains capabilities\nin these tasks, while drastically reducing the overhead during decoding. For\nexample, in the autoencoding task, Dodo shrinks context at a 20x compression\nratio with a BLEU score of 98% for reconstruction, achieving nearly lossless\nencoding.\n","authors":["Guanghui Qin","Corby Rosset","Ethan C. Chau","Nikhil Rao","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2310.02409v2.pdf","comment":"ACL 2024 camera-ready. 15 pages and 7 figures"},{"id":"http://arxiv.org/abs/2406.03339v2","updated":"2024-06-13T15:13:40Z","published":"2024-06-05T14:55:10Z","title":"The Challenges of Evaluating LLM Applications: An Analysis of Automated,\n  Human, and LLM-Based Approaches","summary":"  Chatbots have been an interesting application of natural language generation\nsince its inception. With novel transformer based Generative AI methods,\nbuilding chatbots have become trivial. Chatbots which are targeted at specific\ndomains for example medicine and psychology are implemented rapidly. This\nhowever, should not distract from the need to evaluate the chatbot responses.\nEspecially because the natural language generation community does not entirely\nagree upon how to effectively evaluate such applications. With this work we\ndiscuss the issue further with the increasingly popular LLM based evaluations\nand how they correlate with human evaluations. Additionally, we introduce a\ncomprehensive factored evaluation mechanism that can be utilized in conjunction\nwith both human and LLM-based evaluations. We present the results of an\nexperimental evaluation conducted using this scheme in one of our chatbot\nimplementations which consumed educational reports, and subsequently compare\nautomated, traditional human evaluation, factored human evaluation, and\nfactored LLM evaluation. Results show that factor based evaluation produces\nbetter insights on which aspects need to be improved in LLM applications and\nfurther strengthens the argument to use human evaluation in critical spaces\nwhere main functionality is not direct retrieval.\n","authors":["Bhashithe Abeysinghe","Ruhan Circi"],"pdf_url":"https://arxiv.org/pdf/2406.03339v2.pdf","comment":"Accepted in The First Workshop on Large Language Models for\n  Evaluation in Information Retrieval"},{"id":"http://arxiv.org/abs/2406.09206v1","updated":"2024-06-13T15:06:11Z","published":"2024-06-13T15:06:11Z","title":"Self-Training for Sample-Efficient Active Learning for Text\n  Classification with Pre-Trained Language Models","summary":"  Active learning is an iterative labeling process that is used to obtain a\nsmall labeled subset, despite the absence of labeled data, thereby enabling to\ntrain a model for supervised tasks such as text classification. While active\nlearning has made considerable progress in recent years due to improvements\nprovided by pre-trained language models, there is untapped potential in the\noften neglected unlabeled portion of the data, although it is available in\nconsiderably larger quantities than the usually small set of labeled data. Here\nwe investigate how self-training, a semi-supervised approach where a model is\nused to obtain pseudo-labels from the unlabeled data, can be used to improve\nthe efficiency of active learning for text classification. Starting with an\nextensive reproduction of four previous self-training approaches, some of which\nare evaluated for the first time in the context of active learning or natural\nlanguage processing, we devise HAST, a new and effective self-training\nstrategy, which is evaluated on four text classification benchmarks, on which\nit outperforms the reproduced self-training approaches and reaches\nclassification results comparable to previous experiments for three out of four\ndatasets, using only 25% of the data.\n","authors":["Christopher Schröder","Gerhard Heyer"],"pdf_url":"https://arxiv.org/pdf/2406.09206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12443v2","updated":"2024-06-13T15:04:18Z","published":"2023-09-21T19:36:22Z","title":"Active Learning for Multilingual Fingerspelling Corpora","summary":"  We apply active learning to help with data scarcity problems in sign\nlanguages. In particular, we perform a novel analysis of the effect of\npre-training. Since many sign languages are linguistic descendants of French\nsign language, they share hand configurations, which pre-training can hopefully\nexploit. We test this hypothesis on American, Chinese, German, and Irish\nfingerspelling corpora. We do observe a benefit from pre-training, but this may\nbe due to visual rather than linguistic similarities\n","authors":["Shuai Wang","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2309.12443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09205v1","updated":"2024-06-13T15:03:46Z","published":"2024-06-13T15:03:46Z","title":"ReadCtrl: Personalizing text generation with readability-controlled\n  instruction learning","summary":"  Content generation conditioning on users's readability is an important\napplication for personalization. In an era of large language models (LLMs),\nreadability-controlled text generation based on LLMs has become increasingly\nimportant. This paper introduces a novel methodology called\n\"Readability-Controlled Instruction Learning (ReadCtrl),\" which aims to\ninstruction-tune LLMs to tailor users' readability levels. Unlike the\ntraditional methods, which primarily focused on categorical readability\nadjustments typically classified as high, medium, and low or expert and\nlayperson levels with limited success, ReadCtrl introduces a dynamic framework\nthat enables LLMs to generate content at various (near continuous level)\ncomplexity levels, thereby enhancing their versatility across different\napplications. Our results show that the ReadCtrl-Mistral-7B models\nsignificantly outperformed strong baseline models such as GPT-4 and Claude-3,\nwith a win rate of 52.1%:35.7% against GPT-4 in human evaluations. Furthermore,\nRead-Ctrl has shown significant improvements in automatic evaluations, as\nevidenced by better readability metrics (e.g., FOG, FKGL) and generation\nquality metrics (e.g., BLEU, SARI, SummaC-Factuality, UniEval-Consistency and\nCoherence). These results underscore Read-Ctrl's effectiveness and tenacity in\nproducing high-quality, contextually appropriate outputs that closely align\nwith targeted readability levels, marking a significant advancement in\npersonalized content generation using LLMs.\n","authors":["Hieu Tran","Zonghai Yao","Lingxi Li","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2406.09205v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2311.16502v4","updated":"2024-06-13T15:02:39Z","published":"2023-11-27T17:33:21Z","title":"MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning\n  Benchmark for Expert AGI","summary":"  We introduce MMMU: a new benchmark designed to evaluate multimodal models on\nmassive multi-discipline tasks demanding college-level subject knowledge and\ndeliberate reasoning. MMMU includes 11.5K meticulously collected multimodal\nquestions from college exams, quizzes, and textbooks, covering six core\ndisciplines: Art & Design, Business, Science, Health & Medicine, Humanities &\nSocial Science, and Tech & Engineering. These questions span 30 subjects and\n183 subfields, comprising 30 highly heterogeneous image types, such as charts,\ndiagrams, maps, tables, music sheets, and chemical structures. Unlike existing\nbenchmarks, MMMU focuses on advanced perception and reasoning with\ndomain-specific knowledge, challenging models to perform tasks akin to those\nfaced by experts. The evaluation of 14 open-source LMMs as well as the\nproprietary GPT-4V(ision) and Gemini highlights the substantial challenges\nposed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve\naccuracies of 56% and 59% respectively, indicating significant room for\nimprovement. We believe MMMU will stimulate the community to build\nnext-generation multimodal foundation models towards expert artificial general\nintelligence.\n","authors":["Xiang Yue","Yuansheng Ni","Kai Zhang","Tianyu Zheng","Ruoqi Liu","Ge Zhang","Samuel Stevens","Dongfu Jiang","Weiming Ren","Yuxuan Sun","Cong Wei","Botao Yu","Ruibin Yuan","Renliang Sun","Ming Yin","Boyuan Zheng","Zhenzhu Yang","Yibo Liu","Wenhao Huang","Huan Sun","Yu Su","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2311.16502v4.pdf","comment":"CVPR 2024 Oral"},{"id":"http://arxiv.org/abs/2406.09202v1","updated":"2024-06-13T14:59:45Z","published":"2024-06-13T14:59:45Z","title":"Language Complexity and Speech Recognition Accuracy: Orthographic\n  Complexity Hurts, Phonological Complexity Doesn't","summary":"  We investigate what linguistic factors affect the performance of Automatic\nSpeech Recognition (ASR) models. We hypothesize that orthographic and\nphonological complexities both degrade accuracy. To examine this, we fine-tune\nthe multilingual self-supervised pretrained model Wav2Vec2-XLSR-53 on 25\nlanguages with 15 writing systems, and we compare their ASR accuracy, number of\ngraphemes, unigram grapheme entropy, logographicity (how much\nword/morpheme-level information is encoded in the writing system), and number\nof phonemes. The results demonstrate that orthographic complexities\nsignificantly correlate with low ASR accuracy, while phonological complexity\nshows no significant correlation.\n","authors":["Chihiro Taguchi","David Chiang"],"pdf_url":"https://arxiv.org/pdf/2406.09202v1.pdf","comment":"11 pages, 5 figures, 5 tables, submitted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.09200v1","updated":"2024-06-13T14:57:18Z","published":"2024-06-13T14:57:18Z","title":"Orthogonality and isotropy of speaker and phonetic information in\n  self-supervised speech representations","summary":"  Self-supervised speech representations can hugely benefit downstream speech\ntechnologies, yet the properties that make them useful are still poorly\nunderstood. Two candidate properties related to the geometry of the\nrepresentation space have been hypothesized to correlate well with downstream\ntasks: (1) the degree of orthogonality between the subspaces spanned by the\nspeaker centroids and phone centroids, and (2) the isotropy of the space, i.e.,\nthe degree to which all dimensions are effectively utilized. To study them, we\nintroduce a new measure, Cumulative Residual Variance (CRV), which can be used\nto assess both properties. Using linear classifiers for speaker and phone ID to\nprobe the representations of six different self-supervised models and two\nuntrained baselines, we ask whether either orthogonality or isotropy correlate\nwith linear probing accuracy. We find that both measures correlate with\nphonetic probing accuracy, though our results on isotropy are more nuanced.\n","authors":["Mukhtar Mohamed","Oli Danyi Liu","Hao Tang","Sharon Goldwater"],"pdf_url":"https://arxiv.org/pdf/2406.09200v1.pdf","comment":"Accepted to Interspeech"},{"id":"http://arxiv.org/abs/2406.09175v1","updated":"2024-06-13T14:37:04Z","published":"2024-06-13T14:37:04Z","title":"ReMI: A Dataset for Reasoning with Multiple Images","summary":"  With the continuous advancement of large language models (LLMs), it is\nessential to create new benchmarks to effectively evaluate their expanding\ncapabilities and identify areas for improvement. This work focuses on\nmulti-image reasoning, an emerging capability in state-of-the-art LLMs. We\nintroduce ReMI, a dataset designed to assess LLMs' ability to Reason with\nMultiple Images. This dataset encompasses a diverse range of tasks, spanning\nvarious reasoning domains such as math, physics, logic, code, table/chart\nunderstanding, and spatial and temporal reasoning. It also covers a broad\nspectrum of characteristics found in multi-image reasoning scenarios. We have\nbenchmarked several cutting-edge LLMs using ReMI and found a substantial gap\nbetween their performance and human-level proficiency. This highlights the\nchallenges in multi-image reasoning and the need for further research. Our\nanalysis also reveals the strengths and weaknesses of different models,\nshedding light on the types of reasoning that are currently attainable and\nareas where future models require improvement. To foster further research in\nthis area, we are releasing ReMI publicly:\nhttps://huggingface.co/datasets/mehrankazemi/ReMI.\n","authors":["Mehran Kazemi","Nishanth Dikkala","Ankit Anand","Petar Devic","Ishita Dasgupta","Fangyu Liu","Bahare Fatemi","Pranjal Awasthi","Dee Guo","Sreenivas Gollapudi","Ahmed Qureshi"],"pdf_url":"https://arxiv.org/pdf/2406.09175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09170v1","updated":"2024-06-13T14:31:19Z","published":"2024-06-13T14:31:19Z","title":"Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning","summary":"  Large language models (LLMs) have showcased remarkable reasoning\ncapabilities, yet they remain susceptible to errors, particularly in temporal\nreasoning tasks involving complex temporal logic. Existing research has\nexplored LLM performance on temporal reasoning using diverse datasets and\nbenchmarks. However, these studies often rely on real-world data that LLMs may\nhave encountered during pre-training or employ anonymization techniques that\ncan inadvertently introduce factual inconsistencies. In this work, we address\nthese limitations by introducing novel synthetic datasets specifically designed\nto assess LLM temporal reasoning abilities in various scenarios. The diversity\nof question types across these datasets enables systematic investigation into\nthe impact of the problem structure, size, question type, fact order, and other\nfactors on LLM performance. Our findings provide valuable insights into the\nstrengths and weaknesses of current LLMs in temporal reasoning tasks. To foster\nfurther research in this area, we are open-sourcing the datasets and evaluation\nframework used in our experiments: https://huggingface.co/datasets/baharef/ToT.\n","authors":["Bahare Fatemi","Mehran Kazemi","Anton Tsitsulin","Karishma Malkan","Jinyeong Yim","John Palowitch","Sungyong Seo","Jonathan Halcrow","Bryan Perozzi"],"pdf_url":"https://arxiv.org/pdf/2406.09170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09155v1","updated":"2024-06-13T14:18:13Z","published":"2024-06-13T14:18:13Z","title":"DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities,\nrevolutionizing the integration of AI in daily life applications. However, they\nare prone to hallucinations, generating claims that contradict established\nfacts, deviating from prompts, and producing inconsistent responses when the\nsame prompt is presented multiple times. Addressing these issues is challenging\ndue to the lack of comprehensive and easily assessable benchmark datasets. Most\nexisting datasets are small and rely on multiple-choice questions, which are\ninadequate for evaluating the generative prowess of LLMs. To measure\nhallucination in LLMs, this paper introduces a comprehensive benchmark dataset\ncomprising over 75,000 prompts across eight domains. These prompts are designed\nto elicit definitive, concise, and informative answers. The dataset is divided\ninto two segments: one publicly available for testing and assessing LLM\nperformance and a hidden segment for benchmarking various LLMs. In our\nexperiments, we tested six LLMs-GPT-3.5, LLama 2, LLama 3, Gemini, Mixtral, and\nZephyr-revealing that overall factual hallucination ranges from 59% to 82% on\nthe public dataset and 57% to 76% in the hidden benchmark. Prompt misalignment\nhallucination ranges from 6% to 95% in the public dataset and 17% to 94% in the\nhidden counterpart. Average consistency ranges from 21% to 61% and 22% to 63%,\nrespectively. Domain-wise analysis shows that LLM performance significantly\ndeteriorates when asked for specific numeric information while performing\nmoderately with person, location, and date queries. Our dataset demonstrates\nits efficacy and serves as a comprehensive benchmark for LLM performance\nevaluation. Our dataset and LLMs responses are available at\n\\href{https://github.com/ashikiut/DefAn}{https://github.com/ashikiut/DefAn}.\n","authors":["A B M Ashikur Rahman","Saeed Anwar","Muhammad Usman","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2406.09155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09154v1","updated":"2024-06-13T14:18:10Z","published":"2024-06-13T14:18:10Z","title":"Diffusion Gaussian Mixture Audio Denoise","summary":"  Recent diffusion models have achieved promising performances in\naudio-denoising tasks. The unique property of the reverse process could recover\nclean signals. However, the distribution of real-world noises does not comply\nwith a single Gaussian distribution and is even unknown. The sampling of\nGaussian noise conditions limits its application scenarios. To overcome these\nchallenges, we propose a DiffGMM model, a denoising model based on the\ndiffusion and Gaussian mixture models. We employ the reverse process to\nestimate parameters for the Gaussian mixture model. Given a noisy audio signal,\nwe first apply a 1D-U-Net to extract features and train linear layers to\nestimate parameters for the Gaussian mixture model, and we approximate the real\nnoise distributions. The noisy signal is continuously subtracted from the\nestimated noise to output clean audio signals. Extensive experimental results\ndemonstrate that the proposed DiffGMM model achieves state-of-the-art\nperformance.\n","authors":["Pu Wang","Junhui Li","Jialu Li","Liangdong Guo","Youshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.09154v1.pdf","comment":"INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.09153v1","updated":"2024-06-13T14:17:47Z","published":"2024-06-13T14:17:47Z","title":"LASER: Learning by Aligning Self-supervised Representations of Speech\n  for Improving Content-related Tasks","summary":"  Self-supervised learning (SSL)-based speech models are extensively used for\nfull-stack speech processing. However, it has been observed that improving\nSSL-based speech representations using unlabeled speech for content-related\ntasks is challenging and computationally expensive. Recent attempts have been\nmade to address this issue with cost-effective self-supervised fine-tuning\n(SSFT) approaches. Continuing in this direction, a cost-effective SSFT method\nnamed \"LASER: Learning by Aligning Self-supervised Representations\" is\npresented. LASER is based on the soft-DTW alignment loss with temporal\nregularisation term. Experiments are conducted with HuBERT and WavLM models and\nevaluated on the SUPERB benchmark for two content-related tasks: automatic\nspeech recognition (ASR) and phoneme recognition (PR). A relative improvement\nof 3.7% and 8.2% for HuBERT, and 4.1% and 11.7% for WavLM are observed, for the\nASR and PR tasks respectively, with only < 3 hours of fine-tuning on a single\nGPU.\n","authors":["Amit Meghanani","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2406.09153v1.pdf","comment":"Accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.09140v1","updated":"2024-06-13T14:08:56Z","published":"2024-06-13T14:08:56Z","title":"Investigating the translation capabilities of Large Language Models\n  trained on parallel data only","summary":"  In recent years, Large Language Models (LLMs) have demonstrated exceptional\nproficiency across a broad spectrum of Natural Language Processing (NLP) tasks,\nincluding Machine Translation. However, previous methods predominantly relied\non iterative processes such as instruction fine-tuning or continual\npre-training, leaving unexplored the challenges of training LLMs solely on\nparallel data. In this work, we introduce PLUME (Parallel Language Model), a\ncollection of three 2B LLMs featuring varying vocabulary sizes (32k, 128k, and\n256k) trained exclusively on Catalan-centric parallel examples. These models\nperform comparably to previous encoder-decoder architectures on 16 supervised\ntranslation directions and 56 zero-shot ones. Utilizing this set of models, we\nconduct a thorough investigation into the translation capabilities of LLMs,\nprobing their performance, the impact of the different elements of the prompt,\nand their cross-lingual representation space.\n","authors":["Javier García Gilabert","Carlos Escolano","Aleix Sant Savall","Francesca De Luca Fornaciari","Audrey Mash","Xixian Liao","Maite Melero"],"pdf_url":"https://arxiv.org/pdf/2406.09140v1.pdf","comment":"We release our code at: https://github.com/projecte-aina/Plume"},{"id":"http://arxiv.org/abs/2406.09138v1","updated":"2024-06-13T14:07:52Z","published":"2024-06-13T14:07:52Z","title":"Leveraging Explicit Reasoning for Inference Integration in\n  Commonsense-Augmented Dialogue Models","summary":"  Open-domain dialogue systems need to grasp social commonsense to understand\nand respond effectively to human users. Commonsense-augmented dialogue models\nhave been proposed that aim to infer commonsense knowledge from dialogue\ncontexts in order to improve response quality. However, existing approaches to\ncommonsense-augmented dialogue rely on implicit reasoning to integrate\ncommonsense inferences during response generation. In this study, we explore\nthe impact of explicit reasoning against implicit reasoning over commonsense\nfor dialogue response generation. Our findings demonstrate that separating\ncommonsense reasoning into explicit steps for generating, selecting, and\nintegrating commonsense into responses leads to better dialogue interactions,\nimproving naturalness, engagement, specificity, and overall quality. Subsequent\nanalyses of these findings unveil insights into the effectiveness of various\ntypes of commonsense in generating responses and the particular response traits\nenhanced through explicit reasoning for commonsense integration. Our work\nadvances research in open-domain dialogue by achieving a new state-of-the-art\nin commonsense-augmented response generation.\n","authors":["Sarah E. Finch","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2406.09138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09136v1","updated":"2024-06-13T14:07:02Z","published":"2024-06-13T14:07:02Z","title":"Chain of Preference Optimization: Improving Chain-of-Thought Reasoning\n  in LLMs","summary":"  The recent development of chain-of-thought (CoT) decoding has enabled large\nlanguage models (LLMs) to generate explicit logical reasoning paths for complex\nproblem-solving. However, research indicates that these paths are not always\ndeliberate and optimal. The tree-of-thought (ToT) method employs tree-searching\nto extensively explore the reasoning space and find better reasoning paths that\nCoT decoding might overlook. This deliberation, however, comes at the cost of\nsignificantly increased inference complexity. In this work, we demonstrate that\nfine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to\nachieve similar or better performance, thereby avoiding the substantial\ninference burden. This is achieved through Chain of Preference Optimization\n(CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths\nwith those of ToT using the inherent preference information in the tree-search\nprocess. Extensive experimental results show that CPO significantly improves\nLLM performance in solving a variety of complex problems, including question\nanswering, fact verification, and arithmetic reasoning, demonstrating its\neffectiveness. Our code is available at https://github.com/sail-sg/CPO.\n","authors":["Xuan Zhang","Chao Du","Tianyu Pang","Qian Liu","Wei Gao","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2406.09136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09133v1","updated":"2024-06-13T14:04:34Z","published":"2024-06-13T14:04:34Z","title":"RH-SQL: Refined Schema and Hardness Prompt for Text-to-SQL","summary":"  Text-to-SQL is a technology that converts natural language queries into the\nstructured query language SQL. A novel research approach that has recently\ngained attention focuses on methods based on the complexity of SQL queries,\nachieving notable performance improvements. However, existing methods entail\nsignificant storage and training costs, which hampers their practical\napplication. To address this issue, this paper introduces a method for\nText-to-SQL based on Refined Schema and Hardness Prompt. By filtering out\nlow-relevance schema information with a refined schema and identifying query\nhardness through a Language Model (LM) to form prompts, this method reduces\nstorage and training costs while maintaining performance. It's worth mentioning\nthat this method is applicable to any sequence-to-sequence (seq2seq) LM. Our\nexperiments on the Spider dataset, specifically with large-scale LMs, achieved\nan exceptional Execution accuracy (EX) of 82.6%, demonstrating the\neffectiveness and greater suitability of our method for real-world\napplications.\n","authors":["Jiawen Yi","Guo Chen","Zixiang Shen"],"pdf_url":"https://arxiv.org/pdf/2406.09133v1.pdf","comment":"4 pages, 2 figures, 2024 6th International Conference on Electronic\n  Engineering and Informatics (EEI 2024)"},{"id":"http://arxiv.org/abs/2406.09128v1","updated":"2024-06-13T14:01:08Z","published":"2024-06-13T14:01:08Z","title":"CoastTerm: a Corpus for Multidisciplinary Term Extraction in Coastal\n  Scientific Literature","summary":"  The growing impact of climate change on coastal areas, particularly active\nbut fragile regions, necessitates collaboration among diverse stakeholders and\ndisciplines to formulate effective environmental protection policies. We\nintroduce a novel specialized corpus comprising 2,491 sentences from 410\nscientific abstracts concerning coastal areas, for the Automatic Term\nExtraction (ATE) and Classification (ATC) tasks. Inspired by the ARDI\nframework, focused on the identification of Actors, Resources, Dynamics and\nInteractions, we automatically extract domain terms and their distinct roles in\nthe functioning of coastal systems by leveraging monolingual and multilingual\ntransformer models. The evaluation demonstrates consistent results, achieving\nan F1 score of approximately 80\\% for automated term extraction and F1 of 70\\%\nfor extracting terms and their labels. These findings are promising and signify\nan initial step towards the development of a specialized Knowledge Base\ndedicated to coastal areas.\n","authors":["Julien Delaunay","Hanh Thi Hong Tran","Carlos-Emiliano González-Gallardo","Georgeta Bordea","Mathilde Ducos","Nicolas Sidere","Antoine Doucet","Senja Pollak","Olivier De Viron"],"pdf_url":"https://arxiv.org/pdf/2406.09128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09861v3","updated":"2024-06-13T13:56:20Z","published":"2023-11-16T12:43:18Z","title":"ConceptPsy:A Benchmark Suite with Conceptual Comprehensiveness in\n  Psychology","summary":"  The critical field of psychology necessitates a comprehensive benchmark to\nenhance the evaluation and development of domain-specific Large Language Models\n(LLMs). Existing MMLU-type benchmarks, such as C-EVAL and CMMLU, include\npsychology-related subjects, but their limited number of questions and lack of\nsystematic concept sampling strategies mean they cannot cover the concepts\nrequired in psychology. Consequently, despite their broad subject coverage,\nthese benchmarks lack the necessary depth in the psychology domain, making them\ninadequate as psychology-specific evaluation suite. To address this issue, this\npaper presents ConceptPsy, designed to evaluate Chinese complex reasoning and\nknowledge abilities in psychology. ConceptPsy includes 12 core subjects and\n1383 manually collected concepts. Specifically, we prompt GPT-4 to generate\nquestions for each concept using carefully designed diverse prompts and hire\nprofessional psychologists to review these questions. To help to understand the\nfine-grained performances and enhance the weaknesses, we annotate each question\nwith a chapter label and provide chapter-wise accuracy. Based on ConceptPsy, we\nevaluate a broad range of LLMs. We observe that, although some LLMs achieve\nsimilar accuracies on overall performances, they exhibit significant\nperformance variations across different psychology concepts, even when they are\nmodels from the same series. We hope our work can facilitate the development of\nLLMs in the field of psychology.\n","authors":["Junlei Zhang","Hongliang He","Nirui Song","Zhanchao Zhou","Shuyuan He","Shuai Zhang","Huachuan Qiu","Anqi Li","Yong Dai","Lizhi Ma","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2311.09861v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2401.06112v2","updated":"2024-06-13T13:44:03Z","published":"2024-01-11T18:46:12Z","title":"Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed\n  Embeddings","summary":"  Word embedding is one of the most important components in natural language\nprocessing, but interpreting high-dimensional embeddings remains a challenging\nproblem. To address this problem, Independent Component Analysis (ICA) is\nidentified as an effective solution. ICA-transformed word embeddings reveal\ninterpretable semantic axes; however, the order of these axes are arbitrary. In\nthis study, we focus on this property and propose a novel method, Axis Tour,\nwhich optimizes the order of the axes. Inspired by Word Tour, a one-dimensional\nword embedding method, we aim to improve the clarity of the word embedding\nspace by maximizing the semantic continuity of the axes. Furthermore, we show\nthrough experiments on downstream tasks that Axis Tour yields better or\ncomparable low-dimensional embeddings compared to both PCA and ICA.\n","authors":["Hiroaki Yamagiwa","Yusuke Takase","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2401.06112v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09105v1","updated":"2024-06-13T13:31:49Z","published":"2024-06-13T13:31:49Z","title":"INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance\n  in Insurance","summary":"  Large Vision-Language Models (LVLMs) have demonstrated outstanding\nperformance in various general multimodal applications such as image\nrecognition and visual reasoning, and have also shown promising potential in\nspecialized domains. However, the application potential of LVLMs in the\ninsurance domain-characterized by rich application scenarios and abundant\nmultimodal data-has not been effectively explored. There is no systematic\nreview of multimodal tasks in the insurance domain, nor a benchmark\nspecifically designed to evaluate the capabilities of LVLMs in insurance. This\ngap hinders the development of LVLMs within the insurance domain. In this\npaper, we systematically review and distill multimodal tasks for four\nrepresentative types of insurance: auto insurance, property insurance, health\ninsurance, and agricultural insurance. We propose INS-MMBench, the first\ncomprehensive LVLMs benchmark tailored for the insurance domain. INS-MMBench\ncomprises a total of 2.2K thoroughly designed multiple-choice questions,\ncovering 12 meta-tasks and 22 fundamental tasks. Furthermore, we evaluate\nmultiple representative LVLMs, including closed-source models such as GPT-4o\nand open-source models like BLIP-2. This evaluation not only validates the\neffectiveness of our benchmark but also provides an in-depth performance\nanalysis of current LVLMs on various multimodal tasks in the insurance domain.\nWe hope that INS-MMBench will facilitate the further application of LVLMs in\nthe insurance domain and inspire interdisciplinary development. Our dataset and\nevaluation code are available at https://github.com/FDU-INS/INS-MMBench.\n","authors":["Chenwei Lin","Hanjia Lyu","Xian Xu","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2406.09105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09103v1","updated":"2024-06-13T13:31:04Z","published":"2024-06-13T13:31:04Z","title":"Chain-of-Though (CoT) prompting strategies for medical error detection\n  and correction","summary":"  This paper describes our submission to the MEDIQA-CORR 2024 shared task for\nautomatically detecting and correcting medical errors in clinical notes. We\nreport results for three methods of few-shot In-Context Learning (ICL)\naugmented with Chain-of-Thought (CoT) and reason prompts using a large language\nmodel (LLM). In the first method, we manually analyse a subset of train and\nvalidation dataset to infer three CoT prompts by examining error types in the\nclinical notes. In the second method, we utilise the training dataset to prompt\nthe LLM to deduce reasons about their correctness or incorrectness. The\nconstructed CoTs and reasons are then augmented with ICL examples to solve the\ntasks of error detection, span identification, and error correction. Finally,\nwe combine the two methods using a rule-based ensemble method. Across the three\nsub-tasks, our ensemble method achieves a ranking of 3rd for both sub-task 1\nand 2, while securing 7th place in sub-task 3 among all submissions.\n","authors":["Zhaolong Wu","Abul Hasan","Jinge Wu","Yunsoo Kim","Jason P. Y. Cheung","Teng Zhang","Honghan Wu"],"pdf_url":"https://arxiv.org/pdf/2406.09103v1.pdf","comment":"accepted as NAACL workshop"},{"id":"http://arxiv.org/abs/2402.00453v2","updated":"2024-06-13T13:28:37Z","published":"2024-02-01T09:43:30Z","title":"Instruction Makes a Difference","summary":"  We introduce Instruction Document Visual Question Answering (iDocVQA) dataset\nand Large Language Document (LLaDoc) model, for training Language-Vision (LV)\nmodels for document analysis and predictions on document images, respectively.\nUsually, deep neural networks for the DocVQA task are trained on datasets\nlacking instructions. We show that using instruction-following datasets\nimproves performance. We compare performance across document-related datasets\nusing the recent state-of-the-art (SotA) Large Language and Vision Assistant\n(LLaVA)1.5 as the base model. We also evaluate the performance of the derived\nmodels for object hallucination using the Polling-based Object Probing\nEvaluation (POPE) dataset. The results show that instruction-tuning performance\nranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over\nnon-instruction (traditional task) finetuning. Despite the gains, these still\nfall short of human performance (94.36%), implying there's much room for\nimprovement.\n","authors":["Tosin Adewumi","Nudrat Habib","Lama Alkhaled","Elisa Barney"],"pdf_url":"https://arxiv.org/pdf/2402.00453v2.pdf","comment":"Accepted at the 16th IAPR International Workshop On Document Analysis\n  Systems (DAS)"},{"id":"http://arxiv.org/abs/2406.09098v1","updated":"2024-06-13T13:27:52Z","published":"2024-06-13T13:27:52Z","title":"SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large\n  Language Models","summary":"  The burgeoning utilization of Large Language Models (LLMs) in scientific\nresearch necessitates advanced benchmarks capable of evaluating their\nunderstanding and application of scientific knowledge comprehensively. To\naddress this need, we introduce the SciKnowEval benchmark, a novel framework\nthat systematically evaluates LLMs across five progressive levels of scientific\nknowledge: studying extensively, inquiring earnestly, thinking profoundly,\ndiscerning clearly, and practicing assiduously. These levels aim to assess the\nbreadth and depth of scientific knowledge in LLMs, including knowledge\ncoverage, inquiry and exploration capabilities, reflection and reasoning\nabilities, ethic and safety considerations, as well as practice proficiency.\nSpecifically, we take biology and chemistry as the two instances of SciKnowEval\nand construct a dataset encompassing 50K multi-level scientific problems and\nsolutions. By leveraging this dataset, we benchmark 20 leading open-source and\nproprietary LLMs using zero-shot and few-shot prompting strategies. The results\nreveal that despite achieving state-of-the-art performance, the proprietary\nLLMs still have considerable room for improvement, particularly in addressing\nscientific computations and applications. We anticipate that SciKnowEval will\nestablish a comprehensive standard for benchmarking LLMs in science research\nand discovery, and promote the development of LLMs that integrate scientific\nknowledge with strong safety awareness. The dataset and code are publicly\navailable at https://github.com/hicai-zju/sciknoweval .\n","authors":["Kehua Feng","Keyan Ding","Weijie Wang","Xiang Zhuang","Zeyuan Wang","Ming Qin","Yu Zhao","Jianhua Yao","Qiang Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09098v1.pdf","comment":"48 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.00017v2","updated":"2024-06-13T13:26:56Z","published":"2024-05-23T01:16:45Z","title":"PTA: Enhancing Multimodal Sentiment Analysis through Pipelined\n  Prediction and Translation-based Alignment","summary":"  Multimodal aspect-based sentiment analysis (MABSA) aims to understand\nopinions in a granular manner, advancing human-computer interaction and other\nfields. Traditionally, MABSA methods use a joint prediction approach to\nidentify aspects and sentiments simultaneously. However, we argue that joint\nmodels are not always superior. Our analysis shows that joint models struggle\nto align relevant text tokens with image patches, leading to misalignment and\nineffective image utilization.\n  In contrast, a pipeline framework first identifies aspects through MATE\n(Multimodal Aspect Term Extraction) and then aligns these aspects with image\npatches for sentiment classification (MASC: Multimodal Aspect-Oriented\nSentiment Classification). This method is better suited for multimodal\nscenarios where effective image use is crucial. We present three key\nobservations: (a) MATE and MASC have different feature requirements, with MATE\nfocusing on token-level features and MASC on sequence-level features; (b) the\naspect identified by MATE is crucial for effective image utilization; and (c)\nimages play a trivial role in previous MABSA methods due to high noise.\n  Based on these observations, we propose a pipeline framework that first\npredicts the aspect and then uses translation-based alignment (TBA) to enhance\nmultimodal semantic consistency for better image utilization. Our method\nachieves state-of-the-art (SOTA) performance on widely used MABSA datasets\nTwitter-15 and Twitter-17. This demonstrates the effectiveness of the pipeline\napproach and its potential to provide valuable insights for future MABSA\nresearch.\n  For reproducibility, the code and checkpoint will be released.\n","authors":["Shezheng Song","Shasha Li","Shan Zhao","Chengyu Wang","Xiaopeng Li","Jie Yu","Qian Wan","Jun Ma","Tianwei Yan","Wentao Ma","Xiaoguang Mao"],"pdf_url":"https://arxiv.org/pdf/2406.00017v2.pdf","comment":"Code will be released upon publication"},{"id":"http://arxiv.org/abs/2406.09095v1","updated":"2024-06-13T13:25:50Z","published":"2024-06-13T13:25:50Z","title":"Modeling Comparative Logical Relation with Contrastive Learning for Text\n  Generation","summary":"  Data-to-Text Generation (D2T), a classic natural language generation problem,\naims at producing fluent descriptions for structured input data, such as a\ntable. Existing D2T works mainly focus on describing the superficial\nassociative relations among entities, while ignoring the deep comparative\nlogical relations, such as A is better than B in a certain aspect with a\ncorresponding opinion, which is quite common in our daily life. In this paper,\nwe introduce a new D2T task named comparative logical relation generation\n(CLRG). Additionally, we propose a Comparative Logic (CoLo) based text\ngeneration method, which generates texts following specific comparative logical\nrelations with contrastive learning. Specifically, we first construct various\npositive and negative samples by fine-grained perturbations in entities,\naspects and opinions. Then, we perform contrastive learning in the encoder\nlayer to have a better understanding of the comparative logical relations, and\nintegrate it in the decoder layer to guide the model to correctly generate the\nrelations. Noting the data scarcity problem, we construct a Chinese Comparative\nLogical Relation Dataset (CLRD), which is a high-quality human-annotated\ndataset and challenging for text generation with descriptions of multiple\nentities and annotations on their comparative logical relations. Extensive\nexperiments show that our method achieves impressive performance in both\nautomatic and human evaluations.\n","authors":["Yuhao Dan","Junfeng Tian","Jie Zhou","Ming Yan","Ji Zhang","Qin Chen","Liang He"],"pdf_url":"https://arxiv.org/pdf/2406.09095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11886v2","updated":"2024-06-13T13:18:43Z","published":"2024-03-18T15:39:14Z","title":"QueryAgent: A Reliable and Efficient Reasoning Framework with\n  Environmental Feedback-based Self-Correction","summary":"  Employing Large Language Models (LLMs) for semantic parsing has achieved\nremarkable success. However, we find existing methods fall short in terms of\nreliability and efficiency when hallucinations are encountered. In this paper,\nwe address these challenges with a framework called QueryAgent, which solves a\nquestion step-by-step and performs step-wise self-correction. We introduce an\nenvironmental feedback-based self-correction method called ERASER. Unlike\ntraditional approaches, ERASER leverages rich environmental feedback in the\nintermediate steps to perform selective and differentiated self-correction only\nwhen necessary. Experimental results demonstrate that QueryAgent notably\noutperforms all previous few-shot methods using only one example on GrailQA and\nGraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms\nof efficiency, including runtime, query overhead, and API invocation costs. By\nleveraging ERASER, we further improve another baseline (i.e., AgentBench) by\napproximately 10 points, revealing the strong transferability of our approach.\n","authors":["Xiang Huang","Sitao Cheng","Shanshan Huang","Jiayu Shen","Yong Xu","Chaoyun Zhang","Yuzhong Qu"],"pdf_url":"https://arxiv.org/pdf/2403.11886v2.pdf","comment":"Accepted by ACL 2024 main conference. 22 pages,7 figures, 13 tables"},{"id":"http://arxiv.org/abs/2402.17496v2","updated":"2024-06-13T13:09:48Z","published":"2024-02-27T13:22:47Z","title":"Emotional Voice Messages (EMOVOME) database: emotion recognition in\n  spontaneous voice messages","summary":"  Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing\n999 audio messages from real conversations on a messaging app from 100 Spanish\nspeakers, gender balanced. Voice messages were produced in-the-wild conditions\nbefore participants were recruited, avoiding any conscious bias due to\nlaboratory environment. Audios were labeled in valence and arousal dimensions\nby three non-experts and two experts, which were then combined to obtain a\nfinal label per dimension. The experts also provided an extra label\ncorresponding to seven emotion categories. To set a baseline for future\ninvestigations using EMOVOME, we implemented emotion recognition models using\nboth speech and audio transcriptions. For speech, we used the standard eGeMAPS\nfeature set and support vector machines, obtaining 49.27% and 44.71% unweighted\naccuracy for valence and arousal respectively. For text, we fine-tuned a\nmultilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for\nvalence and arousal respectively. This database will significantly contribute\nto research on emotion recognition in the wild, while also providing a unique\nnatural and freely accessible resource for Spanish.\n","authors":["Lucía Gómez Zaragozá","Rocío del Amor","Elena Parra Vargas","Valery Naranjo","Mariano Alcañiz Raya","Javier Marín-Morales"],"pdf_url":"https://arxiv.org/pdf/2402.17496v2.pdf","comment":"This paper has been superseded by arXiv:2403.02167 (merged from the\n  description of the EMOVOME database in arXiv:2402.17496v1 and the speech\n  emotion recognition models in arXiv:2403.02167v1)"},{"id":"http://arxiv.org/abs/2403.02167v2","updated":"2024-06-13T13:05:56Z","published":"2024-03-04T16:13:39Z","title":"EMOVOME Database: Advancing Emotion Recognition in Speech Beyond Staged\n  Scenarios","summary":"  Natural databases for Speech Emotion Recognition (SER) are scarce and often\nrely on staged scenarios, such as films or television shows, limiting their\napplication in real-world contexts. We developed and publicly released the\nEmotional Voice Messages (EMOVOME) database, including 999 voice messages from\nreal conversations of 100 Spanish speakers on a messaging app, labeled in\ncontinuous and discrete emotions by expert and non-expert annotators. We\nevaluated speaker-independent SER models using a standard set of acoustic\nfeatures and transformer-based models. We compared the results with reference\ndatabases including acted and elicited speech, and analyzed the influence of\nannotators and gender fairness. The pre-trained UniSpeech-SAT-Large model\nachieved the highest results, 61.64% and 55.57% Unweighted Accuracy (UA) for\n3-class valence and arousal prediction respectively on EMOVOME, a 10%\nimprovement over baseline models. For the emotion categories, 42.58% UA was\nobtained. EMOVOME performed lower than the acted RAVDESS database. The elicited\nIEMOCAP database also outperformed EMOVOME in predicting emotion categories,\nwhile similar results were obtained in valence and arousal. EMOVOME outcomes\nvaried with annotator labels, showing better results and fairness when\ncombining expert and non-expert annotations. This study highlights the gap\nbetween staged and real-life scenarios, supporting further advancements in\nrecognizing genuine emotions.\n","authors":["Lucía Gómez-Zaragozá","Rocío del Amor","María José Castro-Bleda","Valery Naranjo","Mariano Alcañiz Raya","Javier Marín-Morales"],"pdf_url":"https://arxiv.org/pdf/2403.02167v2.pdf","comment":"This article is a merged version of the description of the EMOVOME\n  database in arXiv:2402.17496v1 and the speech emotion recognition models in\n  arXiv:2403.02167v1. This work has been submitted to the IEEE for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible"},{"id":"http://arxiv.org/abs/2406.08316v2","updated":"2024-06-13T12:59:06Z","published":"2024-06-12T15:16:40Z","title":"Is Programming by Example solved by LLMs?","summary":"  Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n`solved' PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short.\n","authors":["Wen-Ding Li","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2406.08316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09076v1","updated":"2024-06-13T12:58:53Z","published":"2024-06-13T12:58:53Z","title":"3M: Multi-modal Multi-task Multi-teacher Learning for Game Event\n  Detection","summary":"  Esports has rapidly emerged as a global phenomenon with an ever-expanding\naudience via platforms, like YouTube. Due to the inherent complexity nature of\nthe game, it is challenging for newcomers to comprehend what the event entails.\nThe chaotic nature of online chat, the fast-paced speech of the game\ncommentator, and the game-specific user interface further compound the\ndifficulty for users in comprehending the gameplay. To overcome these\nchallenges, it is crucial to integrate the Multi-Modal (MM) information from\nthe platform and understand the event. The paper introduces a new MM\nmulti-teacher-based game event detection framework, with the ultimate goal of\nconstructing a comprehensive framework that enhances the comprehension of the\nongoing game situation. While conventional MM models typically prioritise\naligning MM data through concurrent training towards a unified objective, our\nframework leverages multiple teachers trained independently on different tasks\nto accomplish the Game Event Detection. The experiment clearly shows the\neffectiveness of the proposed MM multi-teacher framework.\n","authors":["Thye Shan Ng","Feiqi Cao","Soyeon Caren Han"],"pdf_url":"https://arxiv.org/pdf/2406.09076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09072v1","updated":"2024-06-13T12:56:21Z","published":"2024-06-13T12:56:21Z","title":"Living in the Moment: Can Large Language Models Grasp Co-Temporal\n  Reasoning?","summary":"  Temporal reasoning is fundamental for large language models (LLMs) to\ncomprehend the world. Current temporal reasoning datasets are limited to\nquestions about single or isolated events, falling short in mirroring the\nrealistic temporal characteristics involving concurrent nature and intricate\ntemporal interconnections. In this paper, we introduce CoTempQA, a\ncomprehensive co-temporal Question Answering (QA) benchmark containing four\nco-temporal scenarios (Equal, Overlap, During, Mix) with 4,748 samples for\nevaluating the co-temporal comprehension and reasoning abilities of LLMs. Our\nextensive experiments reveal a significant gap between the performance of\ncurrent LLMs and human-level reasoning on CoTempQA tasks. Even when enhanced\nwith Chain of Thought (CoT) methodologies, models consistently struggle with\nour task. In our preliminary exploration, we discovered that mathematical\nreasoning plays a significant role in handling co-temporal events and proposed\na strategy to boost LLMs' co-temporal reasoning from a mathematical\nperspective. We hope that our CoTempQA datasets will encourage further\nadvancements in improving the co-temporal reasoning capabilities of LLMs. Our\ncode is available at https://github.com/zhaochen0110/Cotempqa.\n","authors":["Zhaochen Su","Juntao Li","Jun Zhang","Tong Zhu","Xiaoye Qu","Pan Zhou","Yan Bowen","Yu Cheng","Min zhang"],"pdf_url":"https://arxiv.org/pdf/2406.09072v1.pdf","comment":"This paper has been accepted to the ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2406.09067v1","updated":"2024-06-13T12:54:20Z","published":"2024-06-13T12:54:20Z","title":"How structured are the representations in transformer-based vision\n  encoders? An analysis of multi-object representations in vision-language\n  models","summary":"  Forming and using symbol-like structured representations for reasoning has\nbeen considered essential for generalising over novel inputs. The primary tool\nthat allows generalisation outside training data distribution is the ability to\nabstract away irrelevant information into a compact form relevant to the task.\nAn extreme form of such abstract representations is symbols. Humans make use of\nsymbols to bind information while abstracting away irrelevant parts to utilise\nthe information consistently and meaningfully. This work estimates the state of\nsuch structured representations in vision encoders. Specifically, we evaluate\nimage encoders in large vision-language pre-trained models to address the\nquestion of which desirable properties their representations lack by applying\nthe criteria of symbolic structured reasoning described for LLMs to the image\nmodels. We test the representation space of image encoders like VIT, BLIP,\nCLIP, and FLAVA to characterise the distribution of the object representations\nin these models. In particular, we create decoding tasks using multi-object\nscenes from the COCO dataset, relating the token space to its input content for\nvarious objects in the scene. We use these tasks to characterise the network's\ntoken and layer-wise information modelling. Our analysis highlights that the\nCLS token, used for the downstream task, only focuses on a few objects\nnecessary for the trained downstream task. Still, other individual objects are\nwell-modelled separately by the tokens in the network originating from those\nobjects. We further observed a widespread distribution of scene information.\nThis demonstrates that information is far more entangled in tokens than optimal\nfor representing objects similar to symbols. Given these symbolic properties,\nwe show the network dynamics that cause failure modes of these models on basic\ndownstream tasks in a multi-object scene.\n","authors":["Tarun Khajuria","Braian Olmiro Dias","Jaan Aru"],"pdf_url":"https://arxiv.org/pdf/2406.09067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09056v1","updated":"2024-06-13T12:43:40Z","published":"2024-06-13T12:43:40Z","title":"CUDRT: Benchmarking the Detection of Human vs. Large Language Models\n  Generated Texts","summary":"  The proliferation of large language models (LLMs) has significantly enhanced\ntext generation capabilities across various industries. However, these models'\nability to generate human-like text poses substantial challenges in discerning\nbetween human and AI authorship. Despite the effectiveness of existing\nAI-generated text detectors, their development is hindered by the lack of\ncomprehensive, publicly available benchmarks. Current benchmarks are limited to\nspecific scenarios, such as question answering and text polishing, and\npredominantly focus on English texts, failing to capture the diverse\napplications and linguistic nuances of LLMs. To address these limitations, this\npaper constructs a comprehensive bilingual benchmark in both Chinese and\nEnglish to evaluate mainstream AI-generated text detectors. We categorize LLM\ntext generation into five distinct operations: Create, Update, Delete, Rewrite,\nand Translate (CUDRT), encompassing all current LLMs activities. We also\nestablish a robust benchmark evaluation framework to support scalable and\nreproducible experiments. For each CUDRT category, we have developed extensive\ndatasets to thoroughly assess detector performance. By employing the latest\nmainstream LLMs specific to each language, our datasets provide a thorough\nevaluation environment. Extensive experimental results offer critical insights\nfor optimizing AI-generated text detectors and suggest future research\ndirections to improve detection accuracy and generalizability across various\nscenarios.\n","authors":["Zhen Tao","Zhiyu Li","Dinghao Xi","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2406.09056v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2406.02795v2","updated":"2024-06-13T12:33:58Z","published":"2024-06-04T21:43:56Z","title":"ArguMentor: Augmenting User Experiences with Counter-Perspectives","summary":"  Opinion pieces (or op-eds) can provide valuable perspectives, but they often\nrepresent only one side of a story, which can make readers susceptible to\nconfirmation bias and echo chambers. Exposure to different perspectives can\nhelp readers overcome these obstacles and form more robust, nuanced views on\nimportant societal issues. We designed ArguMentor, a human-AI collaboration\nsystem that highlights claims in opinion pieces, identifies counter-arguments\nfor them using a LLM, and generates a context-based summary of based on current\nevents. It further enhances user understanding through additional features like\na Q&A bot (that answers user questions pertaining to the text), DebateMe (an\nagent that users can argue any side of the piece with) and highlighting (where\nusers can highlight a word or passage to get its definition or context). Our\nevaluation shows that participants can generate more arguments and\ncounter-arguments and have, on average, have more moderate views after engaging\nwith the system.\n","authors":["Priya Pitre","Kurt Luther"],"pdf_url":"https://arxiv.org/pdf/2406.02795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.13870v2","updated":"2024-06-13T12:31:31Z","published":"2021-12-27T19:17:07Z","title":"Can Social Ontological Knowledge Representations be Measured Using\n  Machine Learning?","summary":"  Personal Social Ontology (PSO), it is proposed, is how an individual\nperceives the ontological properties of terms. For example, an absolute\nfatalist would arguably use terms that remove any form of agency from a person.\nSuch fatalism has the impact of ontologically defining acts such as winning,\nvictory and success in a manner that is contrary to how a non-fatalist would\nontologically define them. While both the said fatalist and non-fatalist would\nagree on the dictionary definition of these terms, they would differ on\nspecifically how they can be brought about. This difference between the two\nindividuals can be induced from their usage of these terms, i.e., the\nco-occurrence of these terms with other terms. As such a quantification of this\nsuch co-occurrence offers an avenue to characterise the social ontological\nviews of the speaker. In this paper we ask, what specific term co-occurrence\nshould be measured in order to obtain a valid and reliable psychometric measure\nof a persons social ontology? We consider the social psychology and social\nneuroscience literature to arrive at a list of social concepts that can be\nconsidered principal features of personal social ontology, and then propose an\nNLP pipeline to capture the articulation of these terms in language.\n","authors":["Ahmed Izzidien"],"pdf_url":"https://arxiv.org/pdf/2112.13870v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2406.09044v1","updated":"2024-06-13T12:30:02Z","published":"2024-06-13T12:30:02Z","title":"MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM\n  Finetuning","summary":"  Efficient finetuning of large language models (LLMs) aims to adapt the LLMs\nwith reduced computation and memory cost. Previous LoRA-based approaches\ninitialize the low-rank matrices with gaussian distribution and zero values,\nwhile keeping the original weight matrices frozen. However, the trainable model\nparameters optimized in an unguided subspace might have interference with the\nwell-learned subspace of the pretrained weight matrix. In this paper, we\npropose MiLoRA, a simple yet effective LLM finetuning approach that only\nupdates the minor singular components of the weight matrix while keeping the\nprinciple singular components frozen. It is observed that the minor matrix\ncorresponds to the noisy or long-tail information, while the principle matrix\ncontains important knowledge. The MiLoRA initializes the low-rank matrices\nwithin a subspace that is orthogonal to the principle matrix, thus the\npretrained knowledge is expected to be well preserved. During finetuning,\nMiLoRA makes the most use of the less-optimized subspace for learning the\nfinetuning dataset. Extensive experiments on commonsense reasoning, math\nreasoning and instruction following benchmarks present the superior performance\nof our method.\n","authors":["Hanqing Wang","Zeguan Xiao","Yixia Li","Shuo Wang","Guanhua Chen","Yun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09043v1","updated":"2024-06-13T12:29:27Z","published":"2024-06-13T12:29:27Z","title":"Language Models are Crossword Solvers","summary":"  Crosswords are a form of word puzzle that require a solver to demonstrate a\nhigh degree of proficiency in natural language understanding, wordplay,\nreasoning, and world knowledge, along with adherence to character and length\nconstraints. In this paper we tackle the challenge of solving crosswords with\nLarge Language Models (LLMs). We demonstrate that the current generation of\nstate-of-the art (SoTA) language models show significant competence at\ndeciphering cryptic crossword clues, and outperform previously reported SoTA\nresults by a factor of 2-3 in relevant benchmarks. We also develop a search\nalgorithm that builds off this performance to tackle the problem of solving\nfull crossword grids with LLMs for the very first time, achieving an accuracy\nof 93\\% on New York Times crossword puzzles. Contrary to previous work in this\narea which concluded that LLMs lag human expert performance significantly, our\nresearch suggests this gap is a lot narrower.\n","authors":["Soumadeep Saha","Sutanoya Chakraborty","Saptarshi Saha","Utpal Garain"],"pdf_url":"https://arxiv.org/pdf/2406.09043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09041v1","updated":"2024-06-13T12:27:55Z","published":"2024-06-13T12:27:55Z","title":"ME-Switch: A Memory-Efficient Expert Switching Framework for Large\n  Language Models","summary":"  The typical process for developing LLMs involves pre-training a general\nfoundation model on massive data, followed by fine-tuning on task-specific data\nto create specialized experts. Serving these experts poses challenges, as\nloading all experts onto devices is impractical, and frequent switching between\nexperts in response to user requests incurs substantial I/O costs, increasing\nlatency and expenses. Previous approaches decompose expert weights into\npre-trained model weights and residual delta weights, then quantize the delta\nweights to reduce model size. However, these methods often lead to significant\nquantization errors at extremely low bitwidths and assume the appropriate model\nfor a user request is known in advance, which is not practical. To address\nthese issues, we introduce ME-Switch, a memory-efficient expert switching\nframework for LLM serving. ME-Switch uses mixed-precision quantization,\nselectively quantizing non-salient input channels of delta weights to extremely\nlow bits while keeping salient ones intact, significantly reducing storage\ndemands while maintaining performance. Additionally, we develop a routing\nmethod that efficiently directs user queries to the most suitable expert by\ntransforming the model selection problem into a domain classification problem.\nExtensive experiments show ME-Switch's promising memory efficiency and routing\nperformance. For example, when serving three models from the Mistral-7B family,\nME-Switch reduces model size by 1.74x while maintaining nearly lossless\nperformance on instruction, mathematical reasoning, and code generation tasks.\nFurthermore, ME-Switch can efficiently serve 16 models from the Mistral-7B\nfamily on a single NVIDIA A100 GPU.\n","authors":["Jing Liu","Ruihao Gong","Mingyang Zhang","Yefei He","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2406.09041v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2311.08894v3","updated":"2024-06-13T12:06:27Z","published":"2023-11-15T11:56:56Z","title":"Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing\n  Supervised Models with In-Context Learning","summary":"  Existing Knowledge Base Question Answering (KBQA) architectures are hungry\nfor annotated data, which make them costly and time-consuming to deploy. We\nintroduce the problem of few-shot transfer learning for KBQA, where the target\ndomain offers only a few labeled examples, but a large labeled training dataset\nis available in a source domain. We propose a novel KBQA architecture called\nFuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers,\nre-ranks using an LLM and uses this as input for LLM few-shot in-context\nlearning to generate logical forms. These are further refined using\nexecution-guided feedback. Experiments over multiple source-target KBQA pairs\nof varying complexity show that FuSIC-KBQA significantly outperforms\nadaptations of SoTA KBQA models for this setting. Additional experiments show\nthat FuSIC-KBQA also outperforms SoTA KBQA models in the in-domain setting when\ntraining data is limited.\n","authors":["Mayur Patidar","Riya Sawhney","Avinash Singh","Biswajit Chatterjee"," Mausam","Indrajit Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2311.08894v3.pdf","comment":"ACL-2024 camera-ready version"},{"id":"http://arxiv.org/abs/2311.03099v3","updated":"2024-06-13T11:56:04Z","published":"2023-11-06T13:43:07Z","title":"Language Models are Super Mario: Absorbing Abilities from Homologous\n  Models as a Free Lunch","summary":"  In this paper, we unveil that Language Models (LMs) can acquire new\ncapabilities by assimilating parameters from homologous models without\nretraining or GPUs. We first introduce DARE to set most delta parameters (i.e.,\nthe disparity between fine-tuned and pre-trained parameters) to zeros without\naffecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly\nDrops delta parameters with a ratio $p$ And REscales the remaining ones by $1 /\n(1 - p)$ to approximate the original embeddings. Then, we use DARE as a\nversatile plug-in to sparsify delta parameters of multiple SFT homologous\nmodels for mitigating parameter interference and merge them into a single model\nby parameter fusing. We experiment with encoder- and decoder-based LMs, showing\nthat: (1) SFT delta parameter value ranges are typically small (within 0.002)\nwith extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of\nthem; (2) DARE can merge multiple task-specific LMs into one LM with diverse\ncapabilities. Notably, this phenomenon is more pronounced in large-scale LMs,\nwhere the merged LM reveals the potential to surpass the performance of any\nsource LM, providing a new discovery. We also utilize DARE to create a merged\nLM that ranks first among models with 7 billion parameters on the Open LLM\nLeaderboard.\n","authors":["Le Yu","Bowen Yu","Haiyang Yu","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2311.03099v3.pdf","comment":"Accepted at ICML 2024"},{"id":"http://arxiv.org/abs/2406.09012v1","updated":"2024-06-13T11:33:30Z","published":"2024-06-13T11:33:30Z","title":"Bayesian Statistical Modeling with Predictors from LLMs","summary":"  State of the art large language models (LLMs) have shown impressive\nperformance on a variety of benchmark tasks and are increasingly used as\ncomponents in larger applications, where LLM-based predictions serve as proxies\nfor human judgements or decision. This raises questions about the\nhuman-likeness of LLM-derived information, alignment with human intuition, and\nwhether LLMs could possibly be considered (parts of) explanatory models of\n(aspects of) human cognition or language use. To shed more light on these\nissues, we here investigate the human-likeness of LLMs' predictions for\nmultiple-choice decision tasks from the perspective of Bayesian statistical\nmodeling. Using human data from a forced-choice experiment on pragmatic\nlanguage use, we find that LLMs do not capture the variance in the human data\nat the item-level. We suggest different ways of deriving full distributional\npredictions from LLMs for aggregate, condition-level data, and find that some,\nbut not all ways of obtaining condition-level predictions yield adequate fits\nto human data. These results suggests that assessment of LLM performance\ndepends strongly on seemingly subtle choices in methodology, and that LLMs are\nat best predictors of human behavior at the aggregate, condition-level, for\nwhich they are, however, not designed to, or usually used to, make predictions\nin the first place.\n","authors":["Michael Franke","Polina Tsvilodub","Fausto Carcassi"],"pdf_url":"https://arxiv.org/pdf/2406.09012v1.pdf","comment":"20 pages, 10 figures, parallel submission to a journal"},{"id":"http://arxiv.org/abs/2406.09008v1","updated":"2024-06-13T11:19:50Z","published":"2024-06-13T11:19:50Z","title":"LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large\n  Language Models","summary":"  Topic modeling has been a widely used tool for unsupervised text analysis.\nHowever, comprehensive evaluations of a topic model remain challenging.\nExisting evaluation methods are either less comparable across different models\n(e.g., perplexity) or focus on only one specific aspect of a model (e.g., topic\nquality or document representation quality) at a time, which is insufficient to\nreflect the overall model performance. In this paper, we propose WALM (Words\nAgreement with Language Model), a new evaluation method for topic modeling that\ncomprehensively considers the semantic quality of document representations and\ntopics in a joint manner, leveraging the power of large language models (LLMs).\nWith extensive experiments involving different types of topic models, WALM is\nshown to align with human judgment and can serve as a complementary evaluation\nmethod to the existing ones, bringing a new perspective to topic modeling. Our\nsoftware package will be available at\nhttps://github.com/Xiaohao-Yang/Topic_Model_Evaluation, which can be integrated\nwith many widely used topic models.\n","authors":["Xiaohao Yang","He Zhao","Dinh Phung","Wray Buntine","Lan Du"],"pdf_url":"https://arxiv.org/pdf/2406.09008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07350v2","updated":"2024-06-13T10:47:48Z","published":"2024-03-12T06:16:33Z","title":"VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark","summary":"  Recently, knowledge editing on large language models (LLMs) has received\nconsiderable attention. Compared to this, editing Large Vision-Language Models\n(LVLMs) faces extra challenges from diverse data modalities and complicated\nmodel components, and data for LVLMs editing are limited. The existing LVLM\nediting benchmark, which comprises three metrics (Reliability, Locality, and\nGenerality), falls short in the quality of synthesized evaluation images and\ncannot assess whether models apply edited knowledge in relevant content.\nTherefore, we employ more reliable data collection methods to construct a new\nLarge $\\textbf{V}$ision-$\\textbf{L}$anguage Model $\\textbf{K}$nowledge\n$\\textbf{E}$diting $\\textbf{B}$enchmark, $\\textbf{VLKEB}$, and extend the\nPortability metric for more comprehensive evaluation. Leveraging a multi-modal\nknowledge graph, our image data are bound with knowledge entities. This can be\nfurther used to extract entity-related knowledge, which constitutes the base of\nediting data. We conduct experiments of different editing methods on five\nLVLMs, and thoroughly analyze how do they impact the models. The results reveal\nstrengths and deficiencies of these methods and hopefully provide insights for\nfuture research. The codes and dataset are available at:\n$\\href{https://github.com/VLKEB/VLKEB}{\\text{https://github.com/VLKEB/VLKEB}}$.\n","authors":["Han Huang","Haitian Zhong","Tao Yu","Qiang Liu","Shu Wu","Liang Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2403.07350v2.pdf","comment":"9+11 pages (main+appendix), 7 figures, 13 tables.\n  $\\href{https://github.com/VLKEB/VLKEB}{\\text{get code and data}}$"},{"id":"http://arxiv.org/abs/2406.06579v2","updated":"2024-06-13T10:29:45Z","published":"2024-06-04T13:52:54Z","title":"From Redundancy to Relevance: Enhancing Explainability in Multimodal\n  Large Language Models","summary":"  Recently, multimodal large language models have exploded with an endless\nvariety, most of the popular Large Vision Language Models (LVLMs) depend on\nsequential visual representation, where images are converted into hundreds or\nthousands of tokens before being input into the Large Language Model (LLM)\nalong with language prompts. The black-box design hinders the interpretability\nof visual-language models, especially regarding more complex reasoning tasks.\nTo explore the interaction process between image and text in complex reasoning\ntasks, we introduce the information flow method to visualize the interaction\nmechanism. By analyzing the dynamic flow of the information flow, we find that\nthe information flow appears to converge in the shallow layer. Further\ninvestigation revealed a redundancy of the image token in the shallow layer.\nConsequently, a truncation strategy was introduced to aggregate image tokens\nwithin these shallow layers. This approach has been validated through\nexperiments across multiple models, yielding consistent improvements.\n","authors":["Xiaofeng Zhang","Chen Shen","Xiaosong Yuan","Shaotian Yan","Liang Xie","Wenxiao Wang","Chaochen Gu","Hao Tang","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2406.06579v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17682v2","updated":"2024-06-13T10:21:03Z","published":"2024-02-27T16:56:30Z","title":"NextLevelBERT: Masked Language Modeling with Higher-Level\n  Representations for Long Documents","summary":"  While (large) language models have significantly improved over the last\nyears, they still struggle to sensibly process long sequences found, e.g., in\nbooks, due to the quadratic scaling of the underlying attention mechanism. To\naddress this, we propose NextLevelBERT, a Masked Language Model operating not\non tokens, but on higher-level semantic representations in the form of text\nembeddings. We pretrain NextLevelBERT to predict the vector representation of\nentire masked text chunks and evaluate the effectiveness of the resulting\ndocument vectors on three types of tasks: 1) Semantic Textual Similarity via\nzero-shot document embeddings, 2) Long document classification, 3)\nMultiple-choice question answering. We find that next-level Masked Language\nModeling is an effective technique to tackle long-document use cases and can\noutperfor much larger embedding models as long as the required level of detail\nof semantic information is not too fine. Our models and code are publicly\navailable online.\n","authors":["Tamara Czinczoll","Christoph Hönes","Maximilian Schall","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2402.17682v2.pdf","comment":"accepted at ACL 2024; camera-ready version; 9 pages"},{"id":"http://arxiv.org/abs/2406.08979v1","updated":"2024-06-13T10:18:36Z","published":"2024-06-13T10:18:36Z","title":"Multi-Agent Software Development through Cross-Team Collaboration","summary":"  The latest breakthroughs in Large Language Models (LLMs), eg., ChatDev, have\ncatalyzed profound transformations, particularly through multi-agent\ncollaboration for software development. LLM agents can collaborate in teams\nlike humans, and follow the waterfall model to sequentially work on\nrequirements analysis, development, review, testing, and other phases to\nperform autonomous software generation. However, for an agent team, each phase\nin a single development process yields only one possible outcome. This results\nin the completion of only one development chain, thereby losing the opportunity\nto explore multiple potential decision paths within the solution space.\nConsequently, this may lead to obtaining suboptimal results. To address this\nchallenge, we introduce Cross-Team Collaboration (CTC), a scalable multi-team\nframework that enables orchestrated teams to jointly propose various decisions\nand communicate with their insights in a cross-team collaboration environment\nfor superior content generation. Experimental results in software development\nreveal a notable increase in quality compared to state-of-the-art baselines,\nunderscoring the efficacy of our framework. The significant improvements in\nstory generation demonstrate the promising generalization ability of our\nframework across various domains. We anticipate that our work will guide LLM\nagents towards a cross-team paradigm and contribute to their significant growth\nin but not limited to software development. The code and data will be available\nat https://github.com/OpenBMB/ChatDev.\n","authors":["Zhuoyun Du","Chen Qian","Wei Liu","Zihao Xie","Yifei Wang","Yufan Dang","Weize Chen","Cheng Yang"],"pdf_url":"https://arxiv.org/pdf/2406.08979v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2406.08407v2","updated":"2024-06-13T09:37:50Z","published":"2024-06-12T16:54:54Z","title":"MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation\n  in Videos","summary":"  Multimodal Language Language Models (MLLMs) demonstrate the emerging\nabilities of \"world models\" -- interpreting and reasoning about complex\nreal-world dynamics. To assess these abilities, we posit videos are the ideal\nmedium, as they encapsulate rich representations of real-world dynamics and\ncausalities. To this end, we introduce MMWorld, a new benchmark for\nmulti-discipline, multi-faceted multimodal video understanding. MMWorld\ndistinguishes itself from previous video understanding benchmarks with two\nunique advantages: (1) multi-discipline, covering various disciplines that\noften require domain expertise for comprehensive understanding; (2)\nmulti-faceted reasoning, including explanation, counterfactual thinking, future\nprediction, etc. MMWorld consists of a human-annotated dataset to evaluate\nMLLMs with questions about the whole videos and a synthetic dataset to analyze\nMLLMs within a single modality of perception. Together, MMWorld encompasses\n1,910 videos across seven broad disciplines and 69 subdisciplines, complete\nwith 6,627 question-answer pairs and associated captions. The evaluation\nincludes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld\n(e.g., GPT-4V performs the best with only 52.3\\% accuracy), showing large room\nfor improvement. Further ablation studies reveal other interesting findings\nsuch as models' different skill sets from humans. We hope MMWorld can serve as\nan essential step towards world model evaluation in videos.\n","authors":["Xuehai He","Weixi Feng","Kaizhi Zheng","Yujie Lu","Wanrong Zhu","Jiachen Li","Yue Fan","Jianfeng Wang","Linjie Li","Zhengyuan Yang","Kevin Lin","William Yang Wang","Lijuan Wang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08940v1","updated":"2024-06-13T09:10:16Z","published":"2024-06-13T09:10:16Z","title":"Word Order in English-Japanese Simultaneous Interpretation: Analyses and\n  Evaluation using Chunk-wise Monotonic Translation","summary":"  This paper analyzes the features of monotonic translations, which follow the\nword order of the source language, in simultaneous interpreting (SI). The word\norder differences are one of the biggest challenges in SI, especially for\nlanguage pairs with significant structural differences like English and\nJapanese. We analyzed the characteristics of monotonic translations using the\nNAIST English-to-Japanese Chunk-wise Monotonic Translation Evaluation Dataset\nand found some grammatical structures that make monotonic translation difficult\nin English-Japanese SI. We further investigated the features of monotonic\ntranslations through evaluating the output from the existing speech translation\n(ST) and simultaneous speech translation (simulST) models on NAIST\nEnglish-to-Japanese Chunk-wise Monotonic Translation Evaluation Dataset as well\nas on existing test sets. The results suggest that the existing SI-based test\nset underestimates the model performance. We also found that the\nmonotonic-translation-based dataset would better evaluate simulST models, while\nusing an offline-based test set for evaluating simulST models underestimates\nthe model performance.\n","authors":["Kosuke Doi","Yuka Ko","Mana Makinae","Katsuhito Sudoh","Satoshi Nakamura"],"pdf_url":"https://arxiv.org/pdf/2406.08940v1.pdf","comment":"Accepted to IWSLT2024"},{"id":"http://arxiv.org/abs/2406.08931v1","updated":"2024-06-13T09:00:14Z","published":"2024-06-13T09:00:14Z","title":"Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging\n  Co-Attention Cues in Multitask Learning","summary":"  Advent of modern deep learning techniques has given rise to advancements in\nthe field of Speech Emotion Recognition (SER). However, most systems prevalent\nin the field fail to generalize to speakers not seen during training. This\nstudy focuses on handling challenges of multilingual SER, specifically on\nunseen speakers. We introduce CAMuLeNet, a novel architecture leveraging\nco-attention based fusion and multitask learning to address this problem.\nAdditionally, we benchmark pretrained encoders of Whisper, HuBERT, Wav2Vec2.0,\nand WavLM using 10-fold leave-speaker-out cross-validation on five existing\nmultilingual benchmark datasets: IEMOCAP, RAVDESS, CREMA-D, EmoDB and CaFE and,\nrelease a novel dataset for SER on the Hindi language (BhavVani). CAMuLeNet\nshows an average improvement of approximately 8% over all benchmarks on unseen\nspeakers determined by our cross-validation strategy.\n","authors":["Arnav Goel","Medha Hira","Anubha Gupta"],"pdf_url":"https://arxiv.org/pdf/2406.08931v1.pdf","comment":"5 pages, Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.06852v2","updated":"2024-06-13T08:52:44Z","published":"2024-06-10T23:54:21Z","title":"A Survey of Backdoor Attacks and Defenses on Large Language Models:\n  Implications for Security Measures","summary":"  The large language models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LMMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and attacks without fine-tuning.\nBased on insights from a substantial review, we also discuss crucial issues for\nfuture research on backdoor attacks, such as further exploring attack\nalgorithms that do not require fine-tuning, or developing more covert attack\nalgorithms.\n","authors":["Shuai Zhao","Meihuizi Jia","Zhongliang Guo","Leilei Gan","Jie Fu","Yichao Feng","Fengjun Pan","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2406.06852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01924v2","updated":"2024-06-13T08:42:05Z","published":"2024-03-04T10:41:52Z","title":"To Generate or to Retrieve? On the Effectiveness of Artificial Contexts\n  for Medical Open-Domain Question Answering","summary":"  Medical open-domain question answering demands substantial access to\nspecialized knowledge. Recent efforts have sought to decouple knowledge from\nmodel parameters, counteracting architectural scaling and allowing for training\non common low-resource hardware. The retrieve-then-read paradigm has become\nubiquitous, with model predictions grounded on relevant knowledge pieces from\nexternal repositories such as PubMed, textbooks, and UMLS. An alternative path,\nstill under-explored but made possible by the advent of domain-specific large\nlanguage models, entails constructing artificial contexts through prompting. As\na result, \"to generate or to retrieve\" is the modern equivalent of Hamlet's\ndilemma. This paper presents MedGENIE, the first generate-then-read framework\nfor multiple-choice question answering in medicine. We conduct extensive\nexperiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical\nperspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new\nstate-of-the-art in the open-book setting of each testbed, allowing a\nsmall-scale reader to outcompete zero-shot closed-book 175B baselines while\nusing up to 706$\\times$ fewer parameters. Our findings reveal that generated\npassages are more effective than retrieved ones in attaining higher accuracy.\n","authors":["Giacomo Frisoni","Alessio Cocchieri","Alex Presepi","Gianluca Moro","Zaiqiao Meng"],"pdf_url":"https://arxiv.org/pdf/2403.01924v2.pdf","comment":"ACL 2024 (camera-ready paper)"},{"id":"http://arxiv.org/abs/2403.00835v4","updated":"2024-06-13T08:41:28Z","published":"2024-02-28T20:17:04Z","title":"CLLMs: Consistency Large Language Models","summary":"  Parallel decoding methods such as Jacobi decoding show promise for more\nefficient LLM inference as it breaks the sequential nature of the LLM decoding\nprocess and transforms it into parallelizable computation. However, in\npractice, it achieves little speedup compared to traditional autoregressive\n(AR) decoding, primarily because Jacobi decoding seldom accurately predicts\nmore than one token in a single fixed-point iteration step. To address this, we\ndevelop a new approach aimed at realizing fast convergence from any state to\nthe fixed point on a Jacobi trajectory. This is accomplished by refining the\ntarget LLM to consistently predict the fixed point given any state as input.\nExtensive experiments demonstrate the effectiveness of our method, showing\n2.4$\\times$ to 3.4$\\times$ improvements in generation speed while preserving\ngeneration quality across both domain-specific and open-domain benchmarks.\n","authors":["Siqi Kou","Lanxiang Hu","Zhezhi He","Zhijie Deng","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.00835v4.pdf","comment":"In the proceedings of the 41st International Conference on Machine\n  Learning (ICML) 2024"},{"id":"http://arxiv.org/abs/2406.08922v1","updated":"2024-06-13T08:37:01Z","published":"2024-06-13T08:37:01Z","title":"Navigating the Shadows: Unveiling Effective Disturbances for Modern AI\n  Content Detectors","summary":"  With the launch of ChatGPT, large language models (LLMs) have attracted\nglobal attention. In the realm of article writing, LLMs have witnessed\nextensive utilization, giving rise to concerns related to intellectual property\nprotection, personal privacy, and academic integrity. In response, AI-text\ndetection has emerged to distinguish between human and machine-generated\ncontent. However, recent research indicates that these detection systems often\nlack robustness and struggle to effectively differentiate perturbed texts.\nCurrently, there is a lack of systematic evaluations regarding detection\nperformance in real-world applications, and a comprehensive examination of\nperturbation techniques and detector robustness is also absent. To bridge this\ngap, our work simulates real-world scenarios in both informal and professional\nwriting, exploring the out-of-the-box performance of current detectors.\nAdditionally, we have constructed 12 black-box text perturbation methods to\nassess the robustness of current detection models across various perturbation\ngranularities. Furthermore, through adversarial learning experiments, we\ninvestigate the impact of perturbation data augmentation on the robustness of\nAI-text detectors. We have released our code and data at\nhttps://github.com/zhouying20/ai-text-detector-evaluation.\n","authors":["Ying Zhou","Ben He","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2406.08922v1.pdf","comment":"Accepted by ACL 2024, Main Conference"},{"id":"http://arxiv.org/abs/2406.02856v3","updated":"2024-06-13T08:35:47Z","published":"2024-06-05T02:12:06Z","title":"Xmodel-LM Technical Report","summary":"  We introduce Xmodel-LM, a compact and efficient 1.1B language model\npre-trained on around 2 trillion tokens. Trained on our self-built dataset\n(Xdata), which balances Chinese and English corpora based on downstream task\noptimization, Xmodel-LM exhibits remarkable performance despite its smaller\nsize. It notably surpasses existing open-source language models of similar\nscale. Our model checkpoints and code are publicly accessible on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM.\n","authors":["Yichuan Wang","Yang Liu","Yu Yan","Qun Wang","Shulei Wu","Xucheng Huang","Ling Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.02856v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08911v1","updated":"2024-06-13T08:16:52Z","published":"2024-06-13T08:16:52Z","title":"An Initial Investigation of Language Adaptation for TTS Systems under\n  Low-resource Scenarios","summary":"  Self-supervised learning (SSL) representations from massively multilingual\nmodels offer a promising solution for low-resource language speech tasks.\nDespite advancements, language adaptation in TTS systems remains an open\nproblem. This paper explores the language adaptation capability of ZMM-TTS, a\nrecent SSL-based multilingual TTS system proposed in our previous work. We\nconducted experiments on 12 languages using limited data with various\nfine-tuning configurations. We demonstrate that the similarity in phonetics\nbetween the pre-training and target languages, as well as the language\ncategory, affects the target language's adaptation performance. Additionally,\nwe find that the fine-tuning dataset size and number of speakers influence\nadaptability. Surprisingly, we also observed that using paired data for\nfine-tuning is not always optimal compared to audio-only data. Beyond speech\nintelligibility, our analysis covers speaker similarity, language\nidentification, and predicted MOS.\n","authors":["Cheng Gong","Erica Cooper","Xin Wang","Chunyu Qiang","Mengzhe Geng","Dan Wells","Longbiao Wang","Jianwu Dang","Marc Tessier","Aidan Pine","Korin Richmond","Junichi Yamagishi"],"pdf_url":"https://arxiv.org/pdf/2406.08911v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2402.17019v2","updated":"2024-06-13T08:10:39Z","published":"2024-02-26T20:56:06Z","title":"Leveraging Large Language Models for Learning Complex Legal Concepts\n  through Storytelling","summary":"  Making legal knowledge accessible to non-experts is crucial for enhancing\ngeneral legal literacy and encouraging civic participation in democracy.\nHowever, legal documents are often challenging to understand for people without\nlegal backgrounds. In this paper, we present a novel application of large\nlanguage models (LLMs) in legal education to help non-experts learn intricate\nlegal concepts through storytelling, an effective pedagogical tool in conveying\ncomplex and abstract concepts. We also introduce a new dataset LegalStories,\nwhich consists of 294 complex legal doctrines, each accompanied by a story and\na set of multiple-choice questions generated by LLMs. To construct the dataset,\nwe experiment with various LLMs to generate legal stories explaining these\nconcepts. Furthermore, we use an expert-in-the-loop approach to iteratively\ndesign multiple-choice questions. Then, we evaluate the effectiveness of\nstorytelling with LLMs through randomized controlled trials (RCTs) with legal\nnovices on 10 samples from the dataset. We find that LLM-generated stories\nenhance comprehension of legal concepts and interest in law among non-native\nspeakers compared to only definitions. Moreover, stories consistently help\nparticipants relate legal concepts to their lives. Finally, we find that\nlearning with stories shows a higher retention rate for non-native speakers in\nthe follow-up assessment. Our work has strong implications for using LLMs in\npromoting teaching and learning in the legal field and beyond.\n","authors":["Hang Jiang","Xiajie Zhang","Robert Mahari","Daniel Kessler","Eric Ma","Tal August","Irene Li","Alex 'Sandy' Pentland","Yoon Kim","Jad Kabbara","Deb Roy"],"pdf_url":"https://arxiv.org/pdf/2402.17019v2.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.08903v1","updated":"2024-06-13T07:57:27Z","published":"2024-06-13T07:57:27Z","title":"Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for\n  Large Language Models","summary":"  Fine-tuning is a crucial process for adapting large language models (LLMs) to\ndiverse applications. In certain scenarios, such as multi-tenant serving,\ndeploying multiple LLMs becomes necessary to meet complex demands. Recent\nstudies suggest decomposing a fine-tuned LLM into a base model and\ncorresponding delta weights, which are then compressed using low-rank or\nlow-bit approaches to reduce costs. In this work, we observe that existing\nlow-rank and low-bit compression methods can significantly harm the model\nperformance for task-specific fine-tuned LLMs (e.g., WizardMath for math\nproblems). Motivated by the long-tail distribution of singular values in the\ndelta weights, we propose a delta quantization approach using mixed-precision.\nThis method employs higher-bit representation for singular vectors\ncorresponding to larger singular values. We evaluate our approach on various\nfine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.\nExperimental results demonstrate that our approach performs comparably to full\nfine-tuned LLMs, surpassing both low-rank and low-bit baselines by a\nconsiderable margin. Additionally, we show that our method is compatible with\nvarious backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its\ngeneralizability.\n","authors":["Bowen Ping","Shuo Wang","Hanqing Wang","Xu Han","Yuzhuang Xu","Yukun Yan","Yun Chen","Baobao Chang","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2406.08903v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2406.01638v2","updated":"2024-06-13T07:53:12Z","published":"2024-06-03T00:27:29Z","title":"TimeCMA: Towards LLM-Empowered Time Series Forecasting via\n  Cross-Modality Alignment","summary":"  The widespread adoption of scalable mobile sensing has led to large amounts\nof time series data for real-world applications. A fundamental application is\nmultivariate time series forecasting (MTSF), which aims to predict future time\nseries values based on historical observations. Existing MTSF methods suffer\nfrom limited parameterization and small-scale training data. Recently, Large\nlanguage models (LLMs) have been introduced in time series, which achieve\npromising forecasting performance but incur heavy computational costs. To solve\nthese challenges, we propose TimeCMA, an LLM-empowered framework for time\nseries forecasting with cross-modality alignment. We design a dual-modality\nencoding module with two branches, where the time series encoding branch\nextracts relatively low-quality yet pure embeddings of time series through an\ninverted Transformer. In addition, the LLM-empowered encoding branch wraps the\nsame time series as prompts to obtain high-quality yet entangled prompt\nembeddings via a Pre-trained LLM. Then, we design a cross-modality alignment\nmodule to retrieve high-quality and pure time series embeddings from the prompt\nembeddings. Moreover, we develop a time series forecasting module to decode the\naligned embeddings while capturing dependencies among multiple variables for\nforecasting. Notably, we tailor the prompt to encode sufficient temporal\ninformation into a last token and design the last token embedding storage to\nreduce computational costs. Extensive experiments on real data offer insight\ninto the accuracy and efficiency of the proposed framework.\n","authors":["Chenxi Liu","Qianxiong Xu","Hao Miao","Sun Yang","Lingzheng Zhang","Cheng Long","Ziyue Li","Rui Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.01638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08881v1","updated":"2024-06-13T07:35:37Z","published":"2024-06-13T07:35:37Z","title":"No perspective, no perception!! Perspective-aware Healthcare Answer\n  Summarization","summary":"  Healthcare Community Question Answering (CQA) forums offer an accessible\nplatform for individuals seeking information on various healthcare-related\ntopics. People find such platforms suitable for self-disclosure, seeking\nmedical opinions, finding simplified explanations for their medical conditions,\nand answering others' questions. However, answers on these forums are typically\ndiverse and prone to off-topic discussions. It can be challenging for readers\nto sift through numerous answers and extract meaningful insights, making answer\nsummarization a crucial task for CQA forums. While several efforts have been\nmade to summarize the community answers, most of them are limited to the open\ndomain and overlook the different perspectives offered by these answers. To\naddress this problem, this paper proposes a novel task of perspective-specific\nanswer summarization. We identify various perspectives, within\nhealthcare-related responses and frame a perspective-driven abstractive summary\ncovering all responses. To achieve this, we annotate 3167 CQA threads with 6193\nperspective-aware summaries in our PUMA dataset. Further, we propose PLASMA, a\nprompt-driven controllable summarization model. To encapsulate the\nperspective-specific conditions, we design an energy-controlled loss function\nfor the optimization. We also leverage the prefix tuner to learn the\nintricacies of the health-care perspective summarization. Our evaluation\nagainst five baselines suggests the superior performance of PLASMA by a margin\nof 1.5-21% improvement. We supplement our experiments with ablation and\nqualitative analysis.\n","authors":["Gauri Naik","Sharad Chandakacherla","Shweta Yadav","Md. Shad Akhtar"],"pdf_url":"https://arxiv.org/pdf/2406.08881v1.pdf","comment":"ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2306.01931v3","updated":"2024-06-13T07:06:52Z","published":"2023-06-02T22:12:05Z","title":"Simple Data Augmentation Techniques for Chinese Disease Normalization","summary":"  Disease name normalization is an important task in the medical domain. It\nclassifies disease names written in various formats into standardized names,\nserving as a fundamental component in smart healthcare systems for various\ndisease-related functions. Nevertheless, the most significant obstacle to\nexisting disease name normalization systems is the severe shortage of training\ndata. Consequently, we present a novel data augmentation approach that includes\na series of data augmentation techniques and some supporting modules to help\nmitigate the problem. Our proposed methods rely on the Structural Invariance\nproperty of disease names and the Hierarchy property of the disease\nclassification system. The goal is to equip the models with extensive\nunderstanding of the disease names and the hierarchical structure of the\ndisease name classification system. Through extensive experimentation, we\nillustrate that our proposed approach exhibits significant performance\nimprovements across various baseline models and training objectives,\nparticularly in scenarios with limited training data.\n","authors":["Wenqian Cui","Xiangling Fu","Shaohui Liu","Mingjun Gu","Xien Liu","Ji Wu","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2306.01931v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08860v1","updated":"2024-06-13T06:49:03Z","published":"2024-06-13T06:49:03Z","title":"Plan, Generate and Complicate: Improving Low-resource Dialogue State\n  Tracking via Easy-to-Difficult Zero-shot Data Augmentation","summary":"  Data augmentation methods have been a promising direction to improve the\nperformance of small models for low-resource dialogue state tracking. However,\ntraditional methods rely on pre-defined user goals and neglect the importance\nof data complexity in this task. In this paper, we propose EDZ-DA, an\nEasy-to-Difficult Zero-shot Data Augmentation framework for low-resource\ndialogue state tracking that utilizes large language models to automatically\ncatch the relationships of different domains and then generate the dialogue\ndata. We also complicate the dialogues based on the domain relation to enhance\nthe model's capability for co-reference slot tracking. Furthermore, we permute\nslot values to mitigate the influence of output orders and the problem of\nincomplete value generation. Experimental results illustrate the superiority of\nour proposed method compared to previous strong data augmentation baselines on\nMultiWOZ.\n","authors":["Ming Gu","Yan Yang"],"pdf_url":"https://arxiv.org/pdf/2406.08860v1.pdf","comment":"Accepted by ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.06144v2","updated":"2024-06-13T06:46:14Z","published":"2024-06-10T10:03:16Z","title":"Language Models Resist Alignment","summary":"  Large language models (LLMs) may exhibit undesirable behaviors. Recent\nefforts have focused on aligning these models to prevent harmful generation.\nDespite these efforts, studies have shown that even a well-conducted alignment\nprocess can be easily circumvented, whether intentionally or accidentally. Do\nalignment fine-tuning have robust effects on models, or are merely superficial?\nIn this work, we answer this question through both theoretical and empirical\nmeans. Empirically, we demonstrate the elasticity of post-alignment models,\ni.e., the tendency to revert to the behavior distribution formed during the\npre-training phase upon further fine-tuning. Using compression theory, we\nformally derive that such fine-tuning process disproportionately undermines\nalignment compared to pre-training, potentially by orders of magnitude. We\nconduct experimental validations to confirm the presence of elasticity across\nmodels of varying types and sizes. Specifically, we find that model performance\ndeclines rapidly before reverting to the pre-training distribution, after which\nthe rate of decline drops significantly. We further reveal that elasticity\npositively correlates with increased model size and the expansion of\npre-training data. Our discovery signifies the importance of taming the\ninherent elasticity of LLMs, thereby overcoming the resistance of LLMs to\nalignment finetuning.\n","authors":["Jiaming Ji","Kaile Wang","Tianyi Qiu","Boyuan Chen","Jiayi Zhou","Changye Li","Hantao Lou","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.06144v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2310.03309v4","updated":"2024-06-13T06:26:46Z","published":"2023-10-05T04:47:49Z","title":"Concise and Organized Perception Facilitates Reasoning in Large Language\n  Models","summary":"  Exploiting large language models (LLMs) to tackle reasoning has garnered\ngrowing attention. It still remains highly challenging to achieve satisfactory\nresults in complex logical problems, characterized by plenty of premises within\nthe prompt and requiring multi-hop reasoning. In particular, the reasoning\ncapabilities of LLMs are brittle to disorder and distractibility. In this work,\nwe first examine the mechanism from the perspective of information flow and\nreveal that LLMs exhibit failure patterns akin to human-like cognitive biases\nwhen dealing with disordered and irrelevant content in reasoning tasks.\nHowever, in contrast to LLMs, disordered and irrelevant content does not\nsignificantly decrease human performance, as humans have a propensity to\ndistill the most relevant information and systematically organize their\nthoughts, aiding them in responding to questions. Stem from that, we further\npropose a novel reasoning approach named Concise and Organized Perception\n(COP). COP carefully analyzes the given statements to identify the most\npertinent information while eliminating redundancy efficiently. It then prompts\nthe LLMs in a more organized form that adapts to the model's inference process.\nBy perceiving concise and organized context, the reasoning abilities of LLMs\ncan be better elicited. Extensive experimental results on several popular\nlogical benchmarks (ProofWriter, PrOntoQA, PrOntoQA-OOD, and FOLIO) and math\nbenchmark (DI-GSM) show that COP significantly outperforms previous\nstate-of-the-art methods.\n","authors":["Junjie Liu","Shaotian Yan","Chen Shen","Liang Xie","Wenxiao Wang","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2310.03309v4.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2406.08848v1","updated":"2024-06-13T06:24:52Z","published":"2024-06-13T06:24:52Z","title":"An Approach to Build Zero-Shot Slot-Filling System for Industry-Grade\n  Conversational Assistants","summary":"  We present an approach to build Large Language Model (LLM) based slot-filling\nsystem to perform Dialogue State Tracking in conversational assistants serving\nacross a wide variety of industry-grade applications. Key requirements of this\nsystem include: 1) usage of smaller-sized models to meet low latency\nrequirements and to enable convenient and cost-effective cloud and customer\npremise deployments, and 2) zero-shot capabilities to serve across a wide\nvariety of domains, slot types and conversational scenarios. We adopt a\nfine-tuning approach where a pre-trained LLM is fine-tuned into a slot-filling\nmodel using task specific data. The fine-tuning data is prepared carefully to\ncover a wide variety of slot-filling task scenarios that the model is expected\nto face across various domains. We give details of the data preparation and\nmodel building process. We also give a detailed analysis of the results of our\nexperimental evaluations. Results show that our prescribed approach for\nslot-filling model building has resulted in 6.9% relative improvement of F1\nmetric over the best baseline on a realistic benchmark, while at the same time\nreducing the latency by 57%. More over, the data we prepared has helped improve\nF1 on an average by 4.2% relative across various slot-types.\n","authors":["G P Shrivatsa Bhargav","Sumit Neelam","Udit Sharma","Shajith Ikbal","Dheeraj Sreedhar","Hima Karanam","Sachindra Joshi","Pankaj Dhoolia","Dinesh Garg","Kyle Croutwater","Haode Qi","Eric Wayne","J William Murdock"],"pdf_url":"https://arxiv.org/pdf/2406.08848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08842v1","updated":"2024-06-13T06:08:04Z","published":"2024-06-13T06:08:04Z","title":"ContraSolver: Self-Alignment of Language Models by Resolving Internal\n  Preference Contradictions","summary":"  While substantial advancements have been made in developing large language\nmodels (LLMs), achieving control over their behavior can be difficult. Direct\npreference optimization (DPO) assumes the existence of a latent reward function\nto evaluate the responses of LLMs. This assumption indicates a strict\npreference ordering of different responses to the same input. However, there\nalways exist contradictions of preference in LLMs according to our experimental\nobservations. In this paper, we construct a graph structure of the preference\nrelationship among different responses with self-annotation to find\ncontradictions in the preference order. We propose ContraSolver, an algorithm\nthat traverses all edges on the preference graph to identify those that might\ncause contradictions. ContraSolver initializes the graph with a maximum\nspanning tree and identifies contradictory edges, prioritizing the resolution\nof low-confidence preferences while preserving high-confidence ones.\nExperimental results on four different generation tasks show that the\nperformance of different LLMs can be largely improved through our completely\nunsupervised self-alignment. Furthermore, by analyzing the preference graphs of\nLLMs with and without self-alignment by ContraSolver, we quantify the reduction\nin contradictions, suggesting that resolving preference contradictions is\ncrucial for achieving better alignment performance.\n","authors":["Xu Zhang","Xunjian Yin","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2406.08842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08838v1","updated":"2024-06-13T06:03:59Z","published":"2024-06-13T06:03:59Z","title":"Research on Optimization of Natural Language Processing Model Based on\n  Multimodal Deep Learning","summary":"  This project intends to study the image representation based on attention\nmechanism and multimodal data. By adding multiple pattern layers to the\nattribute model, the semantic and hidden layers of image content are\nintegrated. The word vector is quantified by the Word2Vec method and then\nevaluated by a word embedding convolutional neural network. The published\nexperimental results of the two groups were tested. The experimental results\nshow that this method can convert discrete features into continuous characters,\nthus reducing the complexity of feature preprocessing. Word2Vec and natural\nlanguage processing technology are integrated to achieve the goal of direct\nevaluation of missing image features. The robustness of the image feature\nevaluation model is improved by using the excellent feature analysis\ncharacteristics of a convolutional neural network. This project intends to\nimprove the existing image feature identification methods and eliminate the\nsubjective influence in the evaluation process. The findings from the\nsimulation indicate that the novel approach has developed is viable,\neffectively augmenting the features within the produced representations.\n","authors":["Dan Sun","Yaxin Liang","Yining Yang","Yuhan Ma","Qishi Zhan","Erdi Gao"],"pdf_url":"https://arxiv.org/pdf/2406.08838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05644v2","updated":"2024-06-13T05:39:31Z","published":"2024-06-09T05:04:37Z","title":"How Alignment and Jailbreak Work: Explain LLM Safety through\n  Intermediate Hidden States","summary":"  Large language models (LLMs) rely on safety alignment to avoid responding to\nmalicious user inputs. Unfortunately, jailbreak can circumvent safety\nguardrails, resulting in LLMs generating harmful content and raising concerns\nabout LLM safety. Due to language models with intensive parameters often\nregarded as black boxes, the mechanisms of alignment and jailbreak are\nchallenging to elucidate. In this paper, we employ weak classifiers to explain\nLLM safety through the intermediate hidden states. We first confirm that LLMs\nlearn ethical concepts during pre-training rather than alignment and can\nidentify malicious and normal inputs in the early layers. Alignment actually\nassociates the early concepts with emotion guesses in the middle layers and\nthen refines them to the specific reject tokens for safe generations. Jailbreak\ndisturbs the transformation of early unethical classification into negative\nemotions. We conduct experiments on models from 7B to 70B across various model\nfamilies to prove our conclusion. Overall, our paper indicates the intrinsical\nmechanism of LLM safety and how jailbreaks circumvent safety guardrails,\noffering a new perspective on LLM safety and reducing concerns. Our code is\navailable at https://github.com/ydyjya/LLM-IHS-Explanation.\n","authors":["Zhenhong Zhou","Haiyang Yu","Xinghua Zhang","Rongwu Xu","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2406.05644v2.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2406.08824v1","updated":"2024-06-13T05:31:49Z","published":"2024-06-13T05:31:49Z","title":"LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful\n  Actions","summary":"  Members of the Human-Robot Interaction (HRI) and Artificial Intelligence (AI)\ncommunities have proposed Large Language Models (LLMs) as a promising resource\nfor robotics tasks such as natural language interactions, doing household and\nworkplace tasks, approximating `common sense reasoning', and modeling humans.\nHowever, recent research has raised concerns about the potential for LLMs to\nproduce discriminatory outcomes and unsafe behaviors in real-world robot\nexperiments and applications. To address these concerns, we conduct an\nHRI-based evaluation of discrimination and safety criteria on several\nhighly-rated LLMs. Our evaluation reveals that LLMs currently lack robustness\nwhen encountering people across a diverse range of protected identity\ncharacteristics (e.g., race, gender, disability status, nationality, religion,\nand their intersections), producing biased outputs consistent with directly\ndiscriminatory outcomes -- e.g. `gypsy' and `mute' people are labeled\nuntrustworthy, but not `european' or `able-bodied' people. Furthermore, we test\nmodels in settings with unconstrained natural language (open vocabulary)\ninputs, and find they fail to act safely, generating responses that accept\ndangerous, violent, or unlawful instructions -- such as incident-causing\nmisstatements, taking people's mobility aids, and sexual predation. Our results\nunderscore the urgent need for systematic, routine, and comprehensive risk\nassessments and assurances to improve outcomes and ensure LLMs only operate on\nrobots when it is safe, effective, and just to do so. Data and code will be\nmade available.\n","authors":["Rumaisa Azeem","Andrew Hundt","Masoumeh Mansouri","Martim Brandão"],"pdf_url":"https://arxiv.org/pdf/2406.08824v1.pdf","comment":"40 pages (52 with references), 21 Figures, 6 Tables"},{"id":"http://arxiv.org/abs/2406.08820v1","updated":"2024-06-13T05:23:22Z","published":"2024-06-13T05:23:22Z","title":"DisfluencySpeech -- Single-Speaker Conversational Speech Dataset with\n  Paralanguage","summary":"  Laughing, sighing, stuttering, and other forms of paralanguage do not\ncontribute any direct lexical meaning to speech, but they provide crucial\npropositional context that aids semantic and pragmatic processes such as irony.\nIt is thus important for artificial social agents to both understand and be\nable to generate speech with semantically-important paralanguage. Most speech\ndatasets do not include transcribed non-lexical speech sounds and disfluencies,\nwhile those that do are typically multi-speaker datasets where each speaker\nprovides relatively little audio. This makes it challenging to train\nconversational Text-to-Speech (TTS) synthesis models that include such\nparalinguistic components.\n  We thus present DisfluencySpeech, a studio-quality labeled English speech\ndataset with paralanguage. A single speaker recreates nearly 10 hours of\nexpressive utterances from the Switchboard-1 Telephone Speech Corpus\n(Switchboard), simulating realistic informal conversations. To aid the\ndevelopment of a TTS model that is able to predictively synthesise paralanguage\nfrom text without such components, we provide three different transcripts at\ndifferent levels of information removal (removal of non-speech events, removal\nof non-sentence elements, and removal of false starts), as well as benchmark\nTTS models trained on each of these levels.\n","authors":["Kyra Wang","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2406.08820v1.pdf","comment":"4 pages, 1 figure, submitted to IEEE TENCON 2024"},{"id":"http://arxiv.org/abs/2406.08818v1","updated":"2024-06-13T05:20:42Z","published":"2024-06-13T05:20:42Z","title":"Linguistic Bias in ChatGPT: Language Models Reinforce Dialect\n  Discrimination","summary":"  We present a large-scale study of linguistic bias exhibited by ChatGPT\ncovering ten dialects of English (Standard American English, Standard British\nEnglish, and eight widely spoken non-\"standard\" varieties from around the\nworld). We prompted GPT-3.5 Turbo and GPT-4 with text by native speakers of\neach variety and analyzed the responses via detailed linguistic feature\nannotation and native speaker evaluation. We find that the models default to\n\"standard\" varieties of English; based on evaluation by native speakers, we\nalso find that model responses to non-\"standard\" varieties consistently exhibit\na range of issues: lack of comprehension (10% worse compared to \"standard\"\nvarieties), stereotyping (16% worse), demeaning content (22% worse), and\ncondescending responses (12% worse). We also find that if these models are\nasked to imitate the writing style of prompts in non-\"standard\" varieties, they\nproduce text that exhibits lower comprehension of the input and is especially\nprone to stereotyping. GPT-4 improves on GPT-3.5 in terms of comprehension,\nwarmth, and friendliness, but it also results in a marked increase in\nstereotyping (+17%). The results suggest that GPT-3.5 Turbo and GPT-4 exhibit\nlinguistic discrimination in ways that can exacerbate harms for speakers of\nnon-\"standard\" varieties.\n","authors":["Eve Fleisig","Genevieve Smith","Madeline Bossi","Ishita Rustagi","Xavier Yin","Dan Klein"],"pdf_url":"https://arxiv.org/pdf/2406.08818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08817v1","updated":"2024-06-13T05:19:51Z","published":"2024-06-13T05:19:51Z","title":"Automated Essay Scoring Using Grammatical Variety and Errors with\n  Multi-Task Learning and Item Response Theory","summary":"  This study examines the effect of grammatical features in automatic essay\nscoring (AES). We use two kinds of grammatical features as input to an AES\nmodel: (1) grammatical items that writers used correctly in essays, and (2) the\nnumber of grammatical errors. Experimental results show that grammatical\nfeatures improve the performance of AES models that predict the holistic scores\nof essays. Multi-task learning with the holistic and grammar scores, alongside\nusing grammatical features, resulted in a larger improvement in model\nperformance. We also show that a model using grammar abilities estimated using\nItem Response Theory (IRT) as the labels for the auxiliary task achieved\ncomparable performance to when we used grammar scores assigned by human raters.\nIn addition, we weight the grammatical features using IRT to consider the\ndifficulty of grammatical items and writers' grammar abilities. We found that\nweighting grammatical features with the difficulty led to further improvement\nin performance.\n","authors":["Kosuke Doi","Katsuhito Sudoh","Satoshi Nakamura"],"pdf_url":"https://arxiv.org/pdf/2406.08817v1.pdf","comment":"Accepted to BEA2024"},{"id":"http://arxiv.org/abs/2402.16040v3","updated":"2024-06-13T05:15:33Z","published":"2024-02-25T09:41:50Z","title":"EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using\n  Discharge Summaries","summary":"  Discharge summaries in Electronic Health Records (EHRs) are crucial for\nclinical decision-making, but their length and complexity make information\nextraction challenging, especially when dealing with accumulated summaries\nacross multiple patient admissions. Large Language Models (LLMs) show promise\nin addressing this challenge by efficiently analyzing vast and complex data.\nExisting benchmarks, however, fall short in properly evaluating LLMs'\ncapabilities in this context, as they typically focus on single-note\ninformation or limited topics, failing to reflect the real-world inquiries\nrequired by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel\nbenchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each\nlinked to distinct patients' discharge summaries. Every QA pair is initially\ngenerated using GPT-4 and then manually reviewed and refined by three\nclinicians to ensure clinical relevance. EHRNoteQA includes questions that\nrequire information across multiple discharge summaries and covers eight\ndiverse topics, mirroring the complexity and diversity of real clinical\ninquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice\nquestion answering, and propose a reliable evaluation method for each. We\nevaluate 27 LLMs using EHRNoteQA and examine various factors affecting the\nmodel performance (e.g., the length and number of discharge summaries).\nFurthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations\nin clinical practice, we measure the correlation between the LLM performance on\nEHRNoteQA, and the LLM performance manually evaluated by clinicians. Results\nshow that LLM performance on EHRNoteQA have higher correlation with\nclinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to\nother benchmarks, demonstrating its practical relevance in evaluating LLMs in\nclinical settings.\n","authors":["Sunjun Kweon","Jiyoun Kim","Heeyoung Kwak","Dongchul Cha","Hangyul Yoon","Kwanghyun Kim","Jeewon Yang","Seunghyun Won","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2402.16040v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.07971v2","updated":"2024-06-13T05:13:50Z","published":"2024-06-12T07:52:17Z","title":"It Takes Two: On the Seamlessness between Reward and Policy Model in\n  RLHF","summary":"  Reinforcement Learning from Human Feedback (RLHF) involves training policy\nmodels (PMs) and reward models (RMs) to align language models with human\npreferences. Instead of focusing solely on PMs and RMs independently, we\npropose to examine their interactions during fine-tuning, introducing the\nconcept of seamlessness. Our study starts with observing the saturation\nphenomenon, where continual improvements in RM and PM do not translate into\nRLHF progress. Our analysis shows that RMs fail to assign proper scores to PM\nresponses, resulting in a 35% mismatch rate with human preferences,\nhighlighting a significant discrepancy between PM and RM. To measure\nseamlessness between PM and RM without human effort, we propose an automatic\nmetric, SEAM. SEAM quantifies the discrepancies between PM and RM judgments\ninduced by data samples. We validate the effectiveness of SEAM in data\nselection and model augmentation. Our experiments demonstrate that (1) using\nSEAM-filtered data for RL training improves RLHF performance by 4.5%, and (2)\nSEAM-guided model augmentation results in a 4% performance improvement over\nstandard augmentation methods.\n","authors":["Taiming Lu","Lingfeng Shen","Xinyu Yang","Weiting Tan","Beidi Chen","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2406.07971v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.00237v3","updated":"2024-06-13T05:04:33Z","published":"2023-09-01T04:01:20Z","title":"Publicly Shareable Clinical Large Language Model Built on Synthetic\n  Clinical Notes","summary":"  The development of large language models tailored for handling patients'\nclinical notes is often hindered by the limited accessibility and usability of\nthese notes due to strict privacy regulations. To address these challenges, we\nfirst create synthetic large-scale clinical notes using publicly available case\nreports extracted from biomedical literature. We then use these synthetic notes\nto train our specialized clinical large language model, Asclepius. While\nAsclepius is trained on synthetic data, we assess its potential performance in\nreal-world applications by evaluating it using real clinical notes. We\nbenchmark Asclepius against several other large language models, including\nGPT-3.5-turbo and other open-source alternatives. To further validate our\napproach using synthetic notes, we also compare Asclepius with its variants\ntrained on real clinical notes. Our findings convincingly demonstrate that\nsynthetic clinical notes can serve as viable substitutes for real ones when\nconstructing high-performing clinical language models. This conclusion is\nsupported by detailed evaluations conducted by both GPT-4 and medical\nprofessionals. All resources including weights, codes, and data used in the\ndevelopment of Asclepius are made publicly accessible for future research.\n(https://github.com/starmpcc/Asclepius)\n","authors":["Sunjun Kweon","Junu Kim","Jiyoun Kim","Sujeong Im","Eunbyeol Cho","Seongsu Bae","Jungwoo Oh","Gyubok Lee","Jong Hak Moon","Seng Chan You","Seungjin Baek","Chang Hoon Han","Yoon Bin Jung","Yohan Jo","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2309.00237v3.pdf","comment":"ACL 2024 (Findings)"},{"id":"http://arxiv.org/abs/2406.08811v1","updated":"2024-06-13T05:01:28Z","published":"2024-06-13T05:01:28Z","title":"Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large\n  Language Models","summary":"  Large language models (LLMs) are typically fine-tuned on diverse and\nextensive datasets sourced from various origins to develop a comprehensive\nrange of skills, such as writing, reasoning, chatting, coding, and more. Each\nskill has unique characteristics, and these datasets are often heterogeneous\nand imbalanced, making the fine-tuning process highly challenging. Balancing\nthe development of each skill while ensuring the model maintains its overall\nperformance requires sophisticated techniques and careful dataset curation. In\nthis work, we propose a general, model-agnostic, reinforcement learning\nframework, Mixture-of-Skills (MoS), that learns to optimize data usage\nautomatically during the fine-tuning process. This framework ensures the\noptimal comprehensive skill development of LLMs by dynamically adjusting the\nfocus on different datasets based on their current learning state. To validate\nthe effectiveness of MoS, we conduct extensive experiments using three diverse\nLLM backbones on two widely used benchmarks and demonstrate that MoS\nsubstantially enhances model performance. Building on the success of MoS, we\npropose MoSpec, an adaptation for task-specific fine-tuning, which harnesses\nthe utilities of various datasets for a specific purpose. Our work underlines\nthe significance of dataset rebalancing and present MoS as a powerful, general\nsolution for optimizing data usage in the fine-tuning of LLMs for various\npurposes.\n","authors":["Minghao Wu","Thuy-Trang Vu","Lizhen Qu","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.08811v1.pdf","comment":"Work in progress; 15 pages, 7 tables, 4 figures"},{"id":"http://arxiv.org/abs/2402.19167v2","updated":"2024-06-13T04:58:21Z","published":"2024-02-29T13:50:47Z","title":"Teaching Large Language Models an Unseen Language on the Fly","summary":"  Existing large language models struggle to support numerous low-resource\nlanguages, particularly the extremely low-resource ones, for which there is\nminimal training data available for effective parameter updating. We thus\ninvestigate whether LLMs can learn a new language on the fly solely through\nprompting. To study this question, we collect a research suite for Zhuang, a\nlanguage supported by no LLMs currently. We introduce DiPMT++, a framework for\nadapting LLMs to unseen languages by in-context learning. Using a dictionary\nand 5K parallel sentences only, DiPMT++ significantly enhances the performance\nof GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32\nBLEU for Zhuang-to-Chinese translation. We also validate the effectiveness of\nour framework on Kalamang, another unseen language. Furthermore, we demonstrate\nthe practical utility of DiPMT++ in aiding humans in translating completely\nunseen languages, which could contribute to the preservation of linguistic\ndiversity.\n","authors":["Chen Zhang","Xiao Liu","Jiuheng Lin","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2402.19167v2.pdf","comment":"ACL 2024 https://github.com/luciusssss/ZhuangBench"},{"id":"http://arxiv.org/abs/2311.08348v2","updated":"2024-06-13T04:36:11Z","published":"2023-11-14T17:45:50Z","title":"MC$^2$: Towards Transparent and Culturally-Aware NLP for Minority\n  Languages in China","summary":"  Current large language models demonstrate deficiencies in understanding\nlow-resource languages, particularly the minority languages in China. This\nlimitation stems from the scarcity of available pre-training data. To address\nthis accessibility challenge, we present MC$^2$, a Multilingual Corpus of\nMinority Languages in China, which is the largest open-source corpus of its\nkind so far. MC$^2$ includes four underrepresented languages: Tibetan, Uyghur,\nKazakh, and Mongolian. Notably, we focus on the less common writing systems of\nKazakh and Mongolian, i.e., Kazakh Arabic script and traditional Mongolian\nscript, respectively, which have been long neglected in previous corpus\nconstruction efforts. Recognizing the prevalence of language contamination\nwithin existing corpora, we adopt a quality-centric solution for collecting\nMC$^2$, prioritizing accuracy while enhancing diversity. Furthermore, we\nunderscore the importance of attending to the multiplicity of writing systems,\nwhich is closely related to the cultural awareness of the resulting models. The\nMC$^2$ corpus and related models are made public to the community.\n","authors":["Chen Zhang","Mingxu Tao","Quzhe Huang","Jiuheng Lin","Zhibin Chen","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2311.08348v2.pdf","comment":"ACL 2024 https://github.com/luciusssss/mc2_corpus"},{"id":"http://arxiv.org/abs/2309.09838v3","updated":"2024-06-13T04:32:08Z","published":"2023-09-18T14:55:21Z","title":"HypR: A comprehensive study for ASR hypothesis revising with a reference\n  corpus","summary":"  With the development of deep learning, automatic speech recognition (ASR) has\nmade significant progress. To further enhance the performance of ASR, revising\nrecognition results is one of the lightweight but efficient manners. Various\nmethods can be roughly classified into N-best reranking modeling and error\ncorrection modeling. The former aims to select the hypothesis with the lowest\nerror rate from a set of candidates generated by ASR for a given input speech.\nThe latter focuses on detecting recognition errors in a given hypothesis and\ncorrecting these errors to obtain an enhanced result. However, we observe that\nthese studies are hardly comparable to each other, as they are usually\nevaluated on different corpora, paired with different ASR models, and even use\ndifferent datasets to train the models. Accordingly, we first concentrate on\nproviding an ASR hypothesis revising (HypR) dataset in this study. HypR\ncontains several commonly used corpora (AISHELL-1, TED-LIUM 2, and LibriSpeech)\nand provides 50 recognition hypotheses for each speech utterance. The\ncheckpoint models of ASR are also published. In addition, we implement and\ncompare several classic and representative methods, showing the recent research\nprogress in revising speech recognition results. We hope that the publicly\navailable HypR dataset can become a reference benchmark for subsequent research\nand promote this field of research to an advanced level.\n","authors":["Yi-Wei Wang","Ke-Han Lu","Kuan-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2309.09838v3.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.08796v1","updated":"2024-06-13T04:10:17Z","published":"2024-06-13T04:10:17Z","title":"Deep Exploration of Cross-Lingual Zero-Shot Generalization in\n  Instruction Tuning","summary":"  Instruction tuning has emerged as a powerful technique, significantly\nboosting zero-shot performance on unseen tasks. While recent work has explored\ncross-lingual generalization by applying instruction tuning to multilingual\nmodels, previous studies have primarily focused on English, with a limited\nexploration of non-English tasks. For an in-depth exploration of cross-lingual\ngeneralization in instruction tuning, we perform instruction tuning\nindividually for two distinct language meta-datasets. Subsequently, we assess\nthe performance on unseen tasks in a language different from the one used for\ntraining. To facilitate this investigation, we introduce a novel non-English\nmeta-dataset named \"KORANI\" (Korean Natural Instruction), comprising 51 Korean\nbenchmarks. Moreover, we design cross-lingual templates to mitigate\ndiscrepancies in language and instruction-format of the template between\ntraining and inference within the cross-lingual setting. Our experiments reveal\nconsistent improvements through cross-lingual generalization in both English\nand Korean, outperforming baseline by average scores of 20.7\\% and 13.6\\%,\nrespectively. Remarkably, these enhancements are comparable to those achieved\nby monolingual instruction tuning and even surpass them in some tasks. The\nresult underscores the significance of relevant data acquisition across\nlanguages over linguistic congruence with unseen tasks during instruction\ntuning.\n","authors":["Janghoon Han","Changho Lee","Joongbo Shin","Stanley Jungkyu Choi","Honglak Lee","Kynghoon Bae"],"pdf_url":"https://arxiv.org/pdf/2406.08796v1.pdf","comment":"Findings of ACL 2024 (Camera-ready), by Janghoon Han and Changho Lee,\n  with equal contribution"},{"id":"http://arxiv.org/abs/2305.13669v3","updated":"2024-06-13T03:44:03Z","published":"2023-05-23T04:22:50Z","title":"The Knowledge Alignment Problem: Bridging Human and External Knowledge\n  for Large Language Models","summary":"  Large language models often necessitate grounding on external knowledge to\ngenerate faithful and reliable answers. Yet even with the correct groundings in\nthe reference, they can ignore them and rely on wrong groundings or their\ninherent biases to hallucinate when users, being largely unaware of the\nspecifics of the stored information, pose questions that might not directly\ncorrelate with the retrieved groundings. In this work, we formulate this\nknowledge alignment problem and introduce MixAlign, a framework that interacts\nwith both the human user and the knowledge base to obtain and integrate\nclarifications on how the user question relates to the stored information.\nMixAlign employs a language model to achieve automatic knowledge alignment and,\nif necessary, further enhances this alignment through human user\nclarifications. Experimental results highlight the crucial role of knowledge\nalignment in boosting model performance and mitigating hallucination, with\nimprovements noted up to 22.2% and 27.1% respectively. We also demonstrate the\neffectiveness of MixAlign in improving knowledge alignment by producing\nhigh-quality, user-centered clarifications.\n","authors":["Shuo Zhang","Liangming Pan","Junzhou Zhao","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2305.13669v3.pdf","comment":"ACL 2024, Findings"},{"id":"http://arxiv.org/abs/2402.04333v3","updated":"2024-06-13T03:42:02Z","published":"2024-02-06T19:18:04Z","title":"LESS: Selecting Influential Data for Targeted Instruction Tuning","summary":"  Instruction tuning has unlocked powerful capabilities in large language\nmodels (LLMs), effectively using combined datasets to develop generalpurpose\nchatbots. However, real-world applications often require a specialized suite of\nskills (e.g., reasoning). The challenge lies in identifying the most relevant\ndata from these extensive datasets to effectively develop specific\ncapabilities, a setting we frame as targeted instruction tuning. We propose\nLESS, an optimizer-aware and practically efficient algorithm to effectively\nestimate data influences and perform Low-rank gradiEnt Similarity Search for\ninstruction data selection. Crucially, LESS adapts existing influence\nformulations to work with the Adam optimizer and variable-length instruction\ndata. LESS first constructs a highly reusable and transferable gradient\ndatastore with low-dimensional gradient features and then selects examples\nbased on their similarity to few-shot examples embodying a specific capability.\nExperiments show that training on a LESS-selected 5% of the data can often\noutperform training on the full dataset across diverse downstream tasks.\nFurthermore, the selected data is highly transferable: smaller models can be\nleveraged to select useful data for larger models and models from different\nfamilies. Our qualitative analysis shows that our method goes beyond surface\nform cues to identify data that exemplifies the necessary reasoning skills for\nthe intended downstream application.\n","authors":["Mengzhou Xia","Sadhika Malladi","Suchin Gururangan","Sanjeev Arora","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2402.04333v3.pdf","comment":"ICML 2024; Code and data are available at\n  https://github.com/princeton-nlp/LESS"},{"id":"http://arxiv.org/abs/2405.20215v2","updated":"2024-06-13T03:35:22Z","published":"2024-05-30T16:17:40Z","title":"TS-Align: A Teacher-Student Collaborative Framework for Scalable\n  Iterative Finetuning of Large Language Models","summary":"  Mainstream approaches to aligning large language models (LLMs) heavily rely\non human preference data, particularly when models require periodic updates.\nThe standard process for iterative alignment of LLMs involves collecting new\nhuman feedback for each update. However, the data collection process is costly\nand challenging to scale. To address this issue, we introduce the \"TS-Align\"\nframework, which fine-tunes a policy model using pairwise feedback data\nautomatically mined from its outputs. This automatic mining process is\nefficiently accomplished through the collaboration between a large-scale\nteacher model and a small-scale student model. The policy fine-tuning process\ncan be iteratively repeated using on-policy generations within our proposed\nteacher-student collaborative framework. Through extensive experiments, we\ndemonstrate that our final aligned policy outperforms the base policy model\nwith an average win rate of 69.7% across seven conversational or\ninstruction-following datasets. Furthermore, we show that the ranking\ncapability of the teacher is effectively distilled into the student through our\npipeline, resulting in a small-scale yet effective reward model for policy\nmodel alignment.\n","authors":["Chen Zhang","Chengguang Tang","Dading Chong","Ke Shi","Guohua Tang","Feng Jiang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2405.20215v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08101v2","updated":"2024-06-13T03:16:47Z","published":"2024-06-12T11:27:10Z","title":"CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI\n  Systems","summary":"  Conversational explainable artificial intelligence (ConvXAI) systems based on\nlarge language models (LLMs) have garnered significant interest from the\nresearch community in natural language processing (NLP) and human-computer\ninteraction (HCI). Such systems can provide answers to user questions about\nexplanations in dialogues, have the potential to enhance users' comprehension\nand offer more information about the decision-making and generation processes\nof LLMs. Currently available ConvXAI systems are based on intent recognition\nrather than free chat, as this has been found to be more precise and reliable\nin identifying users' intentions. However, the recognition of intents still\npresents a challenge in the case of ConvXAI, since little training data exist\nand the domain is highly specific, as there is a broad range of XAI methods to\nmap requests onto. In order to bridge this gap, we present CoXQL, the first\ndataset for user intent recognition in ConvXAI, covering 31 intents, seven of\nwhich require filling multiple slots. Subsequently, we enhance an existing\nparsing approach by incorporating template validations, and conduct an\nevaluation of several LLMs on CoXQL using different parsing strategies. We\nconclude that the improved parsing approach (MP+) surpasses the performance of\nprevious approaches. We also discover that intents with multiple slots remain\nhighly challenging for LLMs.\n","authors":["Qianli Wang","Tatiana Anikina","Nils Feldhus","Simon Ostermann","Sebastian Möller"],"pdf_url":"https://arxiv.org/pdf/2406.08101v2.pdf","comment":"4 pages, short paper"},{"id":"http://arxiv.org/abs/2406.08772v1","updated":"2024-06-13T03:04:28Z","published":"2024-06-13T03:04:28Z","title":"MMFakeBench: A Mixed-Source Multimodal Misinformation Detection\n  Benchmark for LVLMs","summary":"  Current multimodal misinformation detection (MMD) methods often assume a\nsingle source and type of forgery for each sample, which is insufficient for\nreal-world scenarios where multiple forgery sources coexist. The lack of a\nbenchmark for mixed-source misinformation has hindered progress in this field.\nTo address this, we introduce MMFakeBench, the first comprehensive benchmark\nfor mixed-source MMD. MMFakeBench includes 3 critical sources: textual veracity\ndistortion, visual veracity distortion, and cross-modal consistency distortion,\nalong with 12 sub-categories of misinformation forgery types. We further\nconduct an extensive evaluation of 6 prevalent detection methods and 15 large\nvision-language models (LVLMs) on MMFakeBench under a zero-shot setting. The\nresults indicate that current methods struggle under this challenging and\nrealistic mixed-source MMD setting. Additionally, we propose an innovative\nunified framework, which integrates rationales, actions, and tool-use\ncapabilities of LVLM agents, significantly enhancing accuracy and\ngeneralization. We believe this study will catalyze future research into more\nrealistic mixed-source multimodal misinformation and provide a fair evaluation\nof misinformation detection methods.\n","authors":["Xuannan Liu","Zekun Li","Peipei Li","Shuhan Xia","Xing Cui","Linzhi Huang","Huaibo Huang","Weihong Deng","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2406.08772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00480v2","updated":"2024-06-13T02:38:07Z","published":"2023-12-01T10:23:15Z","title":"Japanese Tort-case Dataset for Rationale-supported Legal Judgment\n  Prediction","summary":"  This paper presents the first dataset for Japanese Legal Judgment Prediction\n(LJP), the Japanese Tort-case Dataset (JTD), which features two tasks: tort\nprediction and its rationale extraction. The rationale extraction task\nidentifies the court's accepting arguments from alleged arguments by plaintiffs\nand defendants, which is a novel task in the field. JTD is constructed based on\nannotated 3,477 Japanese Civil Code judgments by 41 legal experts, resulting in\n7,978 instances with 59,697 of their alleged arguments from the involved\nparties. Our baseline experiments show the feasibility of the proposed two\ntasks, and our error analysis by legal experts identifies sources of errors and\nsuggests future directions of the LJP research.\n","authors":["Hiroaki Yamada","Takenobu Tokunaga","Ryutaro Ohara","Akira Tokutsu","Keisuke Takeshita","Mihoko Sumida"],"pdf_url":"https://arxiv.org/pdf/2312.00480v2.pdf","comment":"14 pages, 5 figures. This is the final preprint version. Accepted and\n  published at Artificial Intelligence and Law (2024)"},{"id":"http://arxiv.org/abs/2406.08757v1","updated":"2024-06-13T02:35:55Z","published":"2024-06-13T02:35:55Z","title":"SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction\n  Benchmark in Form Understanding","summary":"  Accurately identifying and organizing textual content is crucial for the\nautomation of document processing in the field of form understanding. Existing\ndatasets, such as FUNSD and XFUND, support entity classification and\nrelationship prediction tasks but are typically limited to local and\nentity-level annotations. This limitation overlooks the hierarchically\nstructured representation of documents, constraining comprehensive\nunderstanding of complex forms. To address this issue, we present the SRFUND, a\nhierarchically structured multi-task form understanding benchmark. SRFUND\nprovides refined annotations on top of the original FUNSD and XFUND datasets,\nencompassing five tasks: (1) word to text-line merging, (2) text-line to entity\nmerging, (3) entity category classification, (4) item table localization, and\n(5) entity-based full-document hierarchical structure recovery. We meticulously\nsupplemented the original dataset with missing annotations at various levels of\ngranularity and added detailed annotations for multi-item table regions within\nthe forms. Additionally, we introduce global hierarchical structure\ndependencies for entity relation prediction tasks, surpassing traditional local\nkey-value associations. The SRFUND dataset includes eight languages including\nEnglish, Chinese, Japanese, German, French, Spanish, Italian, and Portuguese,\nmaking it a powerful tool for cross-lingual form understanding. Extensive\nexperimental results demonstrate that the SRFUND dataset presents new\nchallenges and significant opportunities in handling diverse layouts and global\nhierarchical structures of forms, thus providing deep insights into the field\nof form understanding. The original dataset and implementations of baseline\nmethods are available at https://sprateam-ustc.github.io/SRFUND\n","authors":["Jiefeng Ma","Yan Wang","Chenyu Liu","Jun Du","Yu Hu","Zhenrong Zhang","Pengfei Hu","Qing Wang","Jianshu Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08757v1.pdf","comment":"NeurIPS 2024 Track on Datasets and Benchmarks under review"},{"id":"http://arxiv.org/abs/2310.16776v5","updated":"2024-06-13T02:31:28Z","published":"2023-10-25T17:06:42Z","title":"DEFT: Data Efficient Fine-Tuning for Pre-Trained Language Models via\n  Unsupervised Core-Set Selection","summary":"  Recent advances have led to the availability of many pre-trained language\nmodels (PLMs); however, a question that remains is how much data is truly\nneeded to fine-tune PLMs for downstream tasks? In this work, we introduce\nDEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised\ncore-set selection to identify a smaller, representative dataset that reduces\nthe amount of data needed to fine-tune PLMs for downstream tasks. We examine\nthe efficacy of DEFT-UCS in the context of text-editing LMs, and compare to the\nstate-of-the art text-editing model, CoEDIT. Our results demonstrate that\nDEFT-UCS models are just as accurate as CoEDIT, across eight different datasets\nconsisting of six different editing tasks, while finetuned on 70% less data.\n","authors":["Devleena Das","Vivek Khetan"],"pdf_url":"https://arxiv.org/pdf/2310.16776v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08754v1","updated":"2024-06-13T02:24:08Z","published":"2024-06-13T02:24:08Z","title":"StructuralSleight: Automated Jailbreak Attacks on Large Language Models\n  Utilizing Uncommon Text-Encoded Structure","summary":"  Large Language Models (LLMs) are widely used in natural language processing\nbut face the risk of jailbreak attacks that maliciously induce them to generate\nharmful content. Existing jailbreak attacks, including character-level and\ncontext-level attacks, mainly focus on the prompt of the plain text without\nspecifically exploring the significant influence of its structure. In this\npaper, we focus on studying how prompt structure contributes to the jailbreak\nattack. We introduce a novel structure-level attack method based on tail\nstructures that are rarely used during LLM training, which we refer to as\nUncommon Text-Encoded Structure (UTES). We extensively study 12 UTESs templates\nand 6 obfuscation methods to build an effective automated jailbreak tool named\nStructuralSleight that contains three escalating attack strategies: Structural\nAttack, Structural and Character/Context Obfuscation Attack, and Fully\nObfuscated Structural Attack. Extensive experiments on existing LLMs show that\nStructuralSleight significantly outperforms baseline methods. In particular,\nthe attack success rate reaches 94.62\\% on GPT-4o, which has not been addressed\nby state-of-the-art techniques.\n","authors":["Bangxin Li","Hengrui Xing","Chao Huang","Jin Qian","Huangqing Xiao","Linfeng Feng","Cong Tian"],"pdf_url":"https://arxiv.org/pdf/2406.08754v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2307.12564v4","updated":"2024-06-13T02:14:30Z","published":"2023-07-24T07:17:33Z","title":"Towards Generalising Neural Topical Representations","summary":"  Topic models have evolved from conventional Bayesian probabilistic models to\nrecent Neural Topic Models (NTMs). Although NTMs have shown promising\nperformance when trained and tested on a specific corpus, their generalisation\nability across corpora has yet to be studied. In practice, we often expect that\nan NTM trained on a source corpus can still produce quality topical\nrepresentation (i.e., latent distribution over topics) for the document from\ndifferent target corpora to a certain degree. In this work, we aim to improve\nNTMs further so that their representation power for documents generalises\nreliably across corpora and tasks. To do so, we propose to enhance NTMs by\nnarrowing the semantic distance between similar documents, with the underlying\nassumption that documents from different corpora may share similar semantics.\nSpecifically, we obtain a similar document for each training document by text\ndata augmentation. Then, we optimise NTMs further by minimising the semantic\ndistance between each pair, measured by the Topical Optimal Transport\n(TopicalOT) distance, which computes the optimal transport distance between\ntheir topical representations. Our framework can be readily applied to most\nNTMs as a plug-and-play module. Extensive experiments show that our framework\nsignificantly improves the generalisation ability regarding neural topical\nrepresentation across corpora. Our code and datasets are available at:\nhttps://github.com/Xiaohao-Yang/Topic_Model_Generalisation.\n","authors":["Xiaohao Yang","He Zhao","Dinh Phung","Lan Du"],"pdf_url":"https://arxiv.org/pdf/2307.12564v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08747v1","updated":"2024-06-13T02:08:28Z","published":"2024-06-13T02:08:28Z","title":"StreamBench: Towards Benchmarking Continuous Improvement of Language\n  Agents","summary":"  Recent works have shown that large language model (LLM) agents are able to\nimprove themselves from experience, which is an important ability for\ncontinuous enhancement post-deployment. However, existing benchmarks primarily\nevaluate their innate capabilities and do not assess their ability to improve\nover time. To address this gap, we introduce StreamBench, a pioneering\nbenchmark designed to evaluate the continuous improvement of LLM agents over an\ninput-feedback sequence. StreamBench simulates an online learning environment\nwhere LLMs receive a continuous flow of feedback stream and iteratively enhance\ntheir performance. In addition, we propose several simple yet effective\nbaselines for improving LLMs on StreamBench, and provide a comprehensive\nanalysis to identify critical components that contribute to successful\nstreaming strategies. Our work serves as a stepping stone towards developing\neffective online learning strategies for LLMs, paving the way for more adaptive\nAI systems in streaming scenarios.\n","authors":["Cheng-Kuang Wu","Zhi Rui Tam","Chieh-Yen Lin","Yun-Nung Chen","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2406.08747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03589v2","updated":"2024-06-13T01:12:56Z","published":"2024-06-05T19:14:21Z","title":"Ranking Manipulation for Conversational Search Engines","summary":"  Major search engine providers are rapidly incorporating Large Language Model\n(LLM)-generated content in response to user queries. These conversational\nsearch engines operate by loading retrieved website text into the LLM context\nfor summarization and interpretation. Recent research demonstrates that LLMs\nare highly vulnerable to jailbreaking and prompt injection attacks, which\ndisrupt the safety and quality goals of LLMs using adversarial strings. This\nwork investigates the impact of prompt injections on the ranking order of\nsources referenced by conversational search engines. To this end, we introduce\na focused dataset of real-world consumer product websites and formalize\nconversational search ranking as an adversarial problem. Experimentally, we\nanalyze conversational search rankings in the absence of adversarial injections\nand show that different LLMs vary significantly in prioritizing product name,\ndocument content, and context position. We then present a tree-of-attacks-based\njailbreaking technique which reliably promotes low-ranked products.\nImportantly, these attacks transfer effectively to state-of-the-art\nconversational search engines such as perplexity.ai. Given the strong financial\nincentive for website owners to boost their search ranking, we argue that our\nproblem formulation is of critical importance for future robustness work.\n","authors":["Samuel Pfrommer","Yatong Bai","Tanmay Gautam","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2406.03589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08726v1","updated":"2024-06-13T01:08:40Z","published":"2024-06-13T01:08:40Z","title":"Standard Language Ideology in AI-Generated Language","summary":"  In this position paper, we explore standard language ideology in language\ngenerated by large language models (LLMs). First, we outline how standard\nlanguage ideology is reflected and reinforced in LLMs. We then present a\ntaxonomy of open problems regarding standard language ideology in AI-generated\nlanguage with implications for minoritized language communities. We introduce\nthe concept of standard AI-generated language ideology, the process by which\nAI-generated language regards Standard American English (SAE) as a linguistic\ndefault and reinforces a linguistic bias that SAE is the most \"appropriate\"\nlanguage. Finally, we discuss tensions that remain, including reflecting on\nwhat desirable system behavior looks like, as well as advantages and drawbacks\nof generative AI tools imitating--or often not--different English language\nvarieties. Throughout, we discuss standard language ideology as a manifestation\nof existing global power structures in and through AI-generated language before\nending with questions to move towards alternative, more emancipatory digital\nfutures.\n","authors":["Genevieve Smith","Eve Fleisig","Madeline Bossi","Ishita Rustagi","Xavier Yin"],"pdf_url":"https://arxiv.org/pdf/2406.08726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08723v1","updated":"2024-06-13T00:59:55Z","published":"2024-06-13T00:59:55Z","title":"ECBD: Evidence-Centered Benchmark Design for NLP","summary":"  Benchmarking is seen as critical to assessing progress in NLP. However,\ncreating a benchmark involves many design decisions (e.g., which datasets to\ninclude, which metrics to use) that often rely on tacit, untested assumptions\nabout what the benchmark is intended to measure or is actually measuring. There\nis currently no principled way of analyzing these decisions and how they impact\nthe validity of the benchmark's measurements. To address this gap, we draw on\nevidence-centered design in educational assessments and propose\nEvidence-Centered Benchmark Design (ECBD), a framework which formalizes the\nbenchmark design process into five modules. ECBD specifies the role each module\nplays in helping practitioners collect evidence about capabilities of interest.\nSpecifically, each module requires benchmark designers to describe, justify,\nand support benchmark design choices -- e.g., clearly specifying the\ncapabilities the benchmark aims to measure or how evidence about those\ncapabilities is collected from model responses. To demonstrate the use of ECBD,\nwe conduct case studies with three benchmarks: BoolQ, SuperGLUE, and HELM. Our\nanalysis reveals common trends in benchmark design and documentation that could\nthreaten the validity of benchmarks' measurements.\n","authors":["Yu Lu Liu","Su Lin Blodgett","Jackie Chi Kit Cheung","Q. Vera Liao","Alexandra Olteanu","Ziang Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.08723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08718v1","updated":"2024-06-13T00:48:44Z","published":"2024-06-13T00:48:44Z","title":"Enhancing Psychotherapy Counseling: A Data Augmentation Pipeline\n  Leveraging Large Language Models for Counseling Conversations","summary":"  We introduce a pipeline that leverages Large Language Models (LLMs) to\ntransform single-turn psychotherapy counseling sessions into multi-turn\ninteractions. While AI-supported online counseling services for individuals\nwith mental disorders exist, they are often constrained by the limited\navailability of multi-turn training datasets and frequently fail to fully\nutilize therapists' expertise. Our proposed pipeline effectively addresses\nthese limitations. The pipeline comprises two main steps: 1) Information\nExtraction and 2) Multi-turn Counseling Generation. Each step is meticulously\ndesigned to extract and generate comprehensive multi-turn counseling\nconversations from the available datasets. Experimental results from both\nzero-shot and few-shot generation scenarios demonstrate that our approach\nsignificantly enhances the ability of LLMs to produce higher quality multi-turn\ndialogues in the context of mental health counseling. Our pipeline and dataset\nare publicly available\nhttps://github.com/jwkim-chat/A-Data-Augmentation-Pipeline-Leveraging-Large-Language-Models-for-Counseling-Conversations.\n","authors":["Jun-Woo Kim","Ji-Eun Han","Jun-Seok Koh","Hyeon-Tae Seo","Du-Seong Chang"],"pdf_url":"https://arxiv.org/pdf/2406.08718v1.pdf","comment":"IJCAI 2024 AI4Research workshop"},{"id":"http://arxiv.org/abs/2406.08707v1","updated":"2024-06-13T00:13:32Z","published":"2024-06-13T00:13:32Z","title":"mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus","summary":"  Multimodal Large Language Models (mLLMs) are trained on a large amount of\ntext-image data. While most mLLMs are trained on caption-like data only,\nAlayrac et al. [2022] showed that additionally training them on interleaved\nsequences of text and images can lead to the emergence of in-context learning\ncapabilities. However, the dataset they used, M3W, is not public and is only in\nEnglish. There have been attempts to reproduce their results but the released\ndatasets are English-only. In contrast, current multilingual and multimodal\ndatasets are either composed of caption-like only or medium-scale or fully\nprivate data. This limits mLLM research for the 7,000 other languages spoken in\nthe world. We therefore introduce mOSCAR, to the best of our knowledge the\nfirst large-scale multilingual and multimodal document corpus crawled from the\nweb. It covers 163 languages, 315M documents, 214B tokens and 1.2B images. We\ncarefully conduct a set of filtering and evaluation steps to make sure mOSCAR\nis sufficiently safe, diverse and of good quality. We additionally train two\ntypes of multilingual model to prove the benefits of mOSCAR: (1) a model\ntrained on a subset of mOSCAR and captioning data and (2) a model train on\ncaptioning data only. The model additionally trained on mOSCAR shows a strong\nboost in few-shot learning performance across various multilingual image-text\ntasks and benchmarks, confirming previous findings for English-only mLLMs.\n","authors":["Matthieu Futeral","Armel Zebaze","Pedro Ortiz Suarez","Julien Abadji","Rémi Lacroix","Cordelia Schmid","Rachel Bawden","Benoît Sagot"],"pdf_url":"https://arxiv.org/pdf/2406.08707v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2406.08702v1","updated":"2024-06-13T00:00:20Z","published":"2024-06-13T00:00:20Z","title":"VLind-Bench: Measuring Language Priors in Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have demonstrated outstanding\nperformance across various multimodal tasks. However, they suffer from a\nproblem known as language prior, where responses are generated based solely on\ntextual patterns while disregarding image information. Addressing the issue of\nlanguage prior is crucial, as it can lead to undesirable biases or\nhallucinations when dealing with images that are out of training distribution.\nDespite its importance, current methods for accurately measuring language\npriors in LVLMs are poorly studied. Although existing benchmarks based on\ncounterfactual or out-of-distribution images can partially be used to measure\nlanguage priors, they fail to disentangle language priors from other\nconfounding factors. To this end, we propose a new benchmark called\nVLind-Bench, which is the first benchmark specifically designed to measure the\nlanguage priors, or blindness, of LVLMs. It not only includes tests on\ncounterfactual images to assess language priors but also involves a series of\ntests to evaluate more basic capabilities such as commonsense knowledge, visual\nperception, and commonsense biases. For each instance in our benchmark, we\nensure that all these basic tests are passed before evaluating the language\npriors, thereby minimizing the influence of other factors on the assessment.\nThe evaluation and analysis of recent LVLMs in our benchmark reveal that almost\nall models exhibit a significant reliance on language priors, presenting a\nstrong challenge in the field.\n","authors":["Kang-il Lee","Minbeom Kim","Seunghyun Yoon","Minsung Kim","Dongryeol Lee","Hyukhun Koh","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2406.08702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05777v2","updated":"2024-06-13T22:56:47Z","published":"2024-01-11T09:27:50Z","title":"How Proficient Are Large Language Models in Formal Languages? An\n  In-Depth Insight for Knowledge Base Question Answering","summary":"  Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions based on facts in knowledge bases. A typical approach to KBQA is\nsemantic parsing, which translates a question into an executable logical form\nin a formal language. Recent works leverage the capabilities of large language\nmodels (LLMs) for logical form generation to improve performance. However,\nalthough it is validated that LLMs are capable of solving some KBQA problems,\nthere has been little discussion on the differences in LLMs' proficiency in\nformal languages used in semantic parsing. In this work, we propose to evaluate\nthe understanding and generation ability of LLMs to deal with differently\nstructured logical forms by examining the inter-conversion of natural and\nformal language through in-context learning of LLMs. Extensive experiments with\nmodels of different sizes show that state-of-the-art LLMs can understand formal\nlanguages as well as humans, but generating correct logical forms given a few\nexamples remains a challenge. Most importantly, our results also indicate that\nLLMs exhibit considerable sensitivity. In general, the formal language with a\nlower formalization level, i.e., the more similar it is to natural language, is\nmore friendly to LLMs.\n","authors":["Jinxin Liu","Shulin Cao","Jiaxin Shi","Tingjian Zhang","Lunyiu Nie","Linmei Hu","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2401.05777v2.pdf","comment":"ACL Findings 2024"},{"id":"http://arxiv.org/abs/2406.09618v1","updated":"2024-06-13T22:55:22Z","published":"2024-06-13T22:55:22Z","title":"Multi-Modal Retrieval For Large Language Model Based Speech Recognition","summary":"  Retrieval is a widely adopted approach for improving language models\nleveraging external information. As the field moves towards multi-modal large\nlanguage models, it is important to extend the pure text based methods to\nincorporate other modalities in retrieval as well for applications across the\nwide spectrum of machine learning tasks and data types. In this work, we\npropose multi-modal retrieval with two approaches: kNN-LM and cross-attention\ntechniques. We demonstrate the effectiveness of our retrieval approaches\nempirically by applying them to automatic speech recognition tasks with access\nto external information. Under this setting, we show that speech-based\nmulti-modal retrieval outperforms text based retrieval, and yields up to 50 %\nimprovement in word error rate over the multi-modal language model baseline.\nFurthermore, we achieve state-of-the-art recognition results on the\nSpoken-Squad question answering dataset.\n","authors":["Jari Kolehmainen","Aditya Gourav","Prashanth Gurunath Shivakumar","Yile Gu","Ankur Gandhe","Ariya Rastrow","Grant Strimel","Ivan Bulyko"],"pdf_url":"https://arxiv.org/pdf/2406.09618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09617v1","updated":"2024-06-13T22:52:07Z","published":"2024-06-13T22:52:07Z","title":"Multimodal Large Language Models with Fusion Low Rank Adaptation for\n  Device Directed Speech Detection","summary":"  Although Large Language Models (LLMs) have shown promise for human-like\nconversations, they are primarily pre-trained on text data. Incorporating audio\nor video improves performance, but collecting large-scale multimodal data and\npre-training multimodal LLMs is challenging. To this end, we propose a Fusion\nLow Rank Adaptation (FLoRA) technique that efficiently adapts a pre-trained\nunimodal LLM to consume new, previously unseen modalities via low rank\nadaptation. For device-directed speech detection, using FLoRA, the multimodal\nLLM achieves 22% relative reduction in equal error rate (EER) over the\ntext-only approach and attains performance parity with its full fine-tuning\n(FFT) counterpart while needing to tune only a fraction of its parameters.\nFurthermore, with the newly introduced adapter dropout, FLoRA is robust to\nmissing data, improving over FFT by 20% lower EER and 56% lower false accept\nrate. The proposed approach scales well for model sizes from 16M to 3B\nparameters.\n","authors":["Shruti Palaskar","Oggi Rudovic","Sameer Dharur","Florian Pesce","Gautam Krishna","Aswin Sivaraman","Jack Berkowitz","Ahmed Hussen Abdelaziz","Saurabh Adya","Ahmed Tewfik"],"pdf_url":"https://arxiv.org/pdf/2406.09617v1.pdf","comment":"Accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2312.03633v2","updated":"2024-06-13T22:32:58Z","published":"2023-12-06T17:29:45Z","title":"Exploring the Reversal Curse and Other Deductive Logical Reasoning in\n  BERT and GPT-Based Large Language Models","summary":"  The term \"Reversal Curse\" refers to the scenario where auto-regressive\ndecoder large language models (LLMs), such as ChatGPT, trained on \"A is B\" fail\nto learn \"B is A,\" assuming that B and A are distinct and can be uniquely\nidentified from each other, demonstrating a basic failure of logical deduction.\nThis raises a red flag in the use of GPT models for certain general tasks such\nas constructing knowledge graphs, considering their adherence to this symmetric\nprinciple. In our study, we examined a bidirectional LLM, BERT, and found that\nit is immune to the reversal curse. Driven by ongoing efforts to construct\nbiomedical knowledge graphs with LLMs, we also embarked on evaluating more\ncomplex but essential deductive reasoning capabilities. This process included\nfirst training encoder and decoder language models to master the intersection\nand union operations on two sets and then moving on to assess their capability\nto infer different combinations of union and intersection operations on three\nnewly created sets. The findings showed that while both encoder and decoder\nlanguage models, trained for tasks involving two sets (union/intersection),\nwere proficient in such scenarios, they encountered difficulties when dealing\nwith operations that included three sets (various combinations of union and\nintersection). Our research highlights the distinct characteristics of encoder\nand decoder models in simple and complex logical reasoning. In practice, the\nchoice between BERT and GPT should be guided by the specific requirements and\nnature of the task at hand, leveraging their respective strengths in\nbidirectional context comprehension and sequence prediction.\n","authors":["Da Wu","Jingye Yang","Kai Wang"],"pdf_url":"https://arxiv.org/pdf/2312.03633v2.pdf","comment":"Major revisions"},{"id":"http://arxiv.org/abs/2310.14211v2","updated":"2024-06-13T21:40:02Z","published":"2023-10-22T07:26:21Z","title":"LUNA: A Model-Based Universal Analysis Framework for Large Language\n  Models","summary":"  Over the past decade, Artificial Intelligence (AI) has had great success\nrecently and is being used in a wide range of academic and industrial fields.\nMore recently, LLMs have made rapid advancements that have propelled AI to a\nnew level, enabling even more diverse applications and industrial domains with\nintelligence, particularly in areas like software engineering and natural\nlanguage processing. Nevertheless, a number of emerging trustworthiness\nconcerns and issues exhibited in LLMs have already recently received much\nattention, without properly solving which the widespread adoption of LLMs could\nbe greatly hindered in practice. The distinctive characteristics of LLMs, such\nas the self-attention mechanism, extremely large model scale, and\nautoregressive generation schema, differ from classic AI software based on CNNs\nand RNNs and present new challenges for quality analysis. Up to the present, it\nstill lacks universal and systematic analysis techniques for LLMs despite the\nurgent industrial demand. Towards bridging this gap, we initiate an early\nexploratory study and propose a universal analysis framework for LLMs, LUNA,\ndesigned to be general and extensible, to enable versatile analysis of LLMs\nfrom multiple quality perspectives in a human-interpretable manner. In\nparticular, we first leverage the data from desired trustworthiness\nperspectives to construct an abstract model as an auxiliary analysis asset,\nwhich is empowered by various abstract model construction methods. To assess\nthe quality of the abstract model, we collect and define a number of evaluation\nmetrics, aiming at both abstract model level and the semantics level. Then, the\nsemantics, which is the degree of satisfaction of the LLM w.r.t. the\ntrustworthiness perspective, is bound to and enriches the abstract model with\nsemantics, which enables more detailed analysis applications for diverse\npurposes.\n","authors":["Da Song","Xuan Xie","Jiayang Song","Derui Zhu","Yuheng Huang","Felix Juefei-Xu","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2310.14211v2.pdf","comment":"34 pages, 13 figures, To appear in Transactions on Software\n  Engineering (Journal First)"},{"id":"http://arxiv.org/abs/2402.09573v2","updated":"2024-06-13T21:22:02Z","published":"2024-02-14T20:48:58Z","title":"Changes by Butterflies: Farsighted Forecasting with Group Reservoir\n  Transformer","summary":"  In Chaos, a minor divergence between two initial conditions exhibits\nexponential amplification over time, leading to far-away outcomes, known as the\nbutterfly effect. Thus, the distant future is full of uncertainty and hard to\nforecast. We introduce Group Reservoir Transformer to predict long-term events\nmore accurately and robustly by overcoming two challenges in Chaos: (1) the\nextensive historical sequences and (2) the sensitivity to initial conditions. A\nreservoir is attached to a Transformer to efficiently handle arbitrarily long\nhistorical lengths, with an extension of a group of reservoirs to reduce the\nsensitivity to the initialization variations. Our architecture consistently\noutperforms state-of-the-art models in multivariate time series, including\nTimeLLM, GPT2TS, PatchTST, DLinear, TimeNet, and the baseline Transformer, with\nan error reduction of up to -59\\% in various fields such as ETTh, ETTm, and air\nquality, demonstrating that an ensemble of butterfly learning can improve the\nadequacy and certainty of event prediction, despite of the traveling time to\nthe unknown future.\n","authors":["Md Kowsher","Abdul Rafae Khan","Jia Xu"],"pdf_url":"https://arxiv.org/pdf/2402.09573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09066v3","updated":"2024-06-13T21:15:26Z","published":"2023-11-15T16:05:55Z","title":"Identifying Self-Disclosures of Use, Misuse and Addiction in\n  Community-based Social Media Posts","summary":"  In the last decade, the United States has lost more than 500,000 people from\nan overdose involving prescription and illicit opioids making it a national\npublic health emergency (USDHHS, 2017). Medical practitioners require robust\nand timely tools that can effectively identify at-risk patients.\nCommunity-based social media platforms such as Reddit allow self-disclosure for\nusers to discuss otherwise sensitive drug-related behaviors. We present a\nmoderate size corpus of 2500 opioid-related posts from various subreddits\nlabeled with six different phases of opioid use: Medical Use, Misuse,\nAddiction, Recovery, Relapse, Not Using. For every post, we annotate span-level\nextractive explanations and crucially study their role both in annotation\nquality and model development. We evaluate several state-of-the-art models in a\nsupervised, few-shot, or zero-shot setting. Experimental results and error\nanalysis show that identifying the phases of opioid use disorder is highly\ncontextual and challenging. However, we find that using explanations during\nmodeling leads to a significant boost in classification accuracy demonstrating\ntheir beneficial role in a high-stakes domain such as studying the opioid use\ndisorder continuum.\n","authors":["Chenghao Yang","Tuhin Chakrabarty","Karli R Hochstatter","Melissa N Slavin","Nabila El-Bassel","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2311.09066v3.pdf","comment":"NAACL 2024 Findings (Camera-Ready Version). Codes and Data are\n  available at https://github.com/yangalan123/OpioidID"},{"id":"http://arxiv.org/abs/2204.13805v2","updated":"2024-06-13T21:04:26Z","published":"2022-04-28T22:33:36Z","title":"Investigating writing style as a contributor to gender gaps in science\n  and technology","summary":"  A growing stream of research finds that scientific contributions are\nevaluated differently depending on the gender of the author. In this article,\nwe consider whether gender differences in writing styles - how men and women\ncommunicate their work - may contribute to these observed gender gaps. We\nground our investigation in a framework for characterizing the linguistic style\nof written text, with two sets of features - informational (i.e., features that\nemphasize facts) and involved (i.e., features that emphasize relationships).\nUsing a large sample of academic papers and patents, we find significant\ndifferences in writing style by gender, with women using more involved features\nin their writing. Papers and patents with more involved features also tend to\nbe cited more by women. Our findings suggest that scientific text is not devoid\nof personal character, which could contribute to bias in evaluation, thereby\ncompromising the norm of universalism as a foundational principle of science.\n","authors":["Kara Kedrick","Ekaterina Levitskaya","Russell J. Funk"],"pdf_url":"https://arxiv.org/pdf/2204.13805v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03823v4","updated":"2024-06-13T20:58:03Z","published":"2024-03-06T16:10:01Z","title":"A Modular Approach for Multimodal Summarization of TV Shows","summary":"  In this paper we address the task of summarizing television shows, which\ntouches key areas in AI research: complex reasoning, multiple modalities, and\nlong narratives. We present a modular approach where separate components\nperform specialized sub-tasks which we argue affords greater flexibility\ncompared to end-to-end methods. Our modules involve detecting scene boundaries,\nreordering scenes so as to minimize the number of cuts between different\nevents, converting visual information to text, summarizing the dialogue in each\nscene, and fusing the scene summaries into a final summary for the entire\nepisode. We also present a new metric, PREFS (Precision and Recall Evaluation\nof Summary FactS), to measure both precision and recall of generated summaries,\nwhich we decompose into atomic facts. Tested on the recently released\nSummScreen3D dataset Papalampidi and Lapata (2023), our method produces higher\nquality summaries than comparison models, as measured with ROUGE and our new\nfact-based metric.\n","authors":["Louis Mahon","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2403.03823v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09573v1","updated":"2024-06-13T20:23:59Z","published":"2024-06-13T20:23:59Z","title":"Analyzing Gender Polarity in Short Social Media Texts with BERT: The\n  Role of Emojis and Emoticons","summary":"  In this effort we fine tuned different models based on BERT to detect the\ngender polarity of twitter accounts. We specially focused on analyzing the\neffect of using emojis and emoticons in performance of our model in classifying\ntask. We were able to demonstrate that the use of these none word inputs\nalongside the mention of other accounts in a short text format like tweet has\nan impact in detecting the account holder's gender.\n","authors":["Saba Yousefian Jazi","Amir Mirzaeinia","Sina Yousefian Jazi"],"pdf_url":"https://arxiv.org/pdf/2406.09573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09569v1","updated":"2024-06-13T20:20:29Z","published":"2024-06-13T20:20:29Z","title":"Speech ReaLLM -- Real-time Streaming Speech Recognition with Multimodal\n  LLMs by Teaching the Flow of Time","summary":"  We introduce Speech ReaLLM, a new ASR architecture that marries\n\"decoder-only\" ASR with the RNN-T to make multimodal LLM architectures capable\nof real-time streaming. This is the first \"decoder-only\" ASR architecture\ndesigned to handle continuous audio without explicit end-pointing. Speech\nReaLLM is a special case of the more general ReaLLM (\"real-time LLM\") approach,\nalso introduced here for the first time. The idea is inspired by RNN-T: Instead\nof generating a response only at the end of a user prompt, generate after every\ninput token received in real time (it is often empty). On Librispeech \"test\",\nan 80M Speech ReaLLM achieves WERs of 3.0% and 7.4% in real time (without an\nexternal LM or auxiliary loss). This is only slightly above a 3x larger\nAttention-Encoder-Decoder baseline. We also show that this way, an LLM\narchitecture can learn to represent and reproduce the flow of time; and that a\npre-trained 7B LLM can be fine-tuned to do reasonably well on this task.\n","authors":["Frank Seide","Morrie Doulaty","Yangyang Shi","Yashesh Gaur","Junteng Jia","Chunyang Wu"],"pdf_url":"https://arxiv.org/pdf/2406.09569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09559v1","updated":"2024-06-13T19:55:20Z","published":"2024-06-13T19:55:20Z","title":"Decoding the Diversity: A Review of the Indic AI Research Landscape","summary":"  This review paper provides a comprehensive overview of large language model\n(LLM) research directions within Indic languages. Indic languages are those\nspoken in the Indian subcontinent, including India, Pakistan, Bangladesh, Sri\nLanka, Nepal, and Bhutan, among others. These languages have a rich cultural\nand linguistic heritage and are spoken by over 1.5 billion people worldwide.\nWith the tremendous market potential and growing demand for natural language\nprocessing (NLP) based applications in diverse languages, generative\napplications for Indic languages pose unique challenges and opportunities for\nresearch. Our paper deep dives into the recent advancements in Indic generative\nmodeling, contributing with a taxonomy of research directions, tabulating 84\nrecent publications. Research directions surveyed in this paper include LLM\ndevelopment, fine-tuning existing LLMs, development of corpora, benchmarking\nand evaluation, as well as publications around specific techniques, tools, and\napplications. We found that researchers across the publications emphasize the\nchallenges associated with limited data availability, lack of standardization,\nand the peculiar linguistic complexities of Indic languages. This work aims to\nserve as a valuable resource for researchers and practitioners working in the\nfield of NLP, particularly those focused on Indic languages, and contributes to\nthe development of more accurate and efficient LLM applications for these\nlanguages.\n","authors":["Sankalp KJ","Vinija Jain","Sreyoshi Bhaduri","Tamoghna Roy","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2406.09559v1.pdf","comment":"27 pages, 1 figure"},{"id":"http://arxiv.org/abs/2311.07564v3","updated":"2024-06-13T19:54:53Z","published":"2023-11-13T18:54:17Z","title":"Can Authorship Attribution Models Distinguish Speakers in Speech\n  Transcripts?","summary":"  Authorship verification is the task of determining if two distinct writing\nsamples share the same author and is typically concerned with the attribution\nof written text. In this paper, we explore the attribution of transcribed\nspeech, which poses novel challenges. The main challenge is that many stylistic\nfeatures, such as punctuation and capitalization, are not informative in this\nsetting. On the other hand, transcribed speech exhibits other patterns, such as\nfiller words and backchannels (e.g., 'um', 'uh-huh'), which may be\ncharacteristic of different speakers. We propose a new benchmark for speaker\nattribution focused on human-transcribed conversational speech transcripts. To\nlimit spurious associations of speakers with topic, we employ both conversation\nprompts and speakers participating in the same conversation to construct\nverification trials of varying difficulties. We establish the state of the art\non this new benchmark by comparing a suite of neural and non-neural baselines,\nfinding that although written text attribution models achieve surprisingly good\nperformance in certain settings, they perform markedly worse as conversational\ntopic is increasingly controlled. We present analyses of the impact of\ntranscription style on performance as well as the ability of fine-tuning on\nspeech transcripts to improve performance.\n","authors":["Cristina Aggazzotti","Nicholas Andrews","Elizabeth Allyn Smith"],"pdf_url":"https://arxiv.org/pdf/2311.07564v3.pdf","comment":"To be published in Transactions of the Association for Computational\n  Linguistics (pre-MIT Press publication version); revision includes additional\n  experiments and evaluations"},{"id":"http://arxiv.org/abs/2406.09549v1","updated":"2024-06-13T19:30:32Z","published":"2024-06-13T19:30:32Z","title":"Exploring Syntactic Patterns in Urdu: A Deep Dive into Dependency\n  Analysis","summary":"  Parsing is the process of breaking a sentence into its grammatical components\nand identifying the syntactic structure of the sentence. The syntactically\ncorrect sentence structure is achieved by assigning grammatical labels to its\nconstituents using lexicon and syntactic rules. In linguistics, parser is\nextremely useful due to the number of different applications like name entity\nrecognition, QA systems and information extraction, etc. The two most common\ntechniques used for parsing are phrase structure and dependency Structure.\nBecause Urdu is a low-resource language, there has been little progress in\nbuilding an Urdu parser. A comparison of several parsers revealed that the\ndependency parsing approach is better suited for order-free languages such as\nUrdu. We have made significant progress in parsing Urdu, a South Asian language\nwith a complex morphology. For Urdu dependency parsing, a basic feature model\nconsisting of word location, word head, and dependency relation is employed as\na starting point, followed by more complex feature models. The dependency\ntagset is designed after careful consideration of the complex morphological\nstructure of the Urdu language, word order variation, and lexical ambiguity and\nit contains 22 tags. Our dataset comprises of sentences from news articles, and\nwe tried to include sentences of different complexity (which is quite\nchallenging), to get reliable results. All experiments are performed using\nMaltParser, exploring all 9 algorithms and classifiers. We have achieved a 70\npercent overall best-labeled accuracy (LA), as well as an 84 percent overall\nbest-unlabeled attachment score (UAS) using the Nivreeager algorithm. The\ncomparison of output data with treebank test data that has been manually parsed\nis then used to carry out error assessment and to identify the errors produced\nby the parser.\n","authors":["Nudrat Habib"],"pdf_url":"https://arxiv.org/pdf/2406.09549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17455v4","updated":"2024-06-13T19:15:53Z","published":"2023-05-27T12:07:21Z","title":"CrossGET: Cross-Guided Ensemble of Tokens for Accelerating\n  Vision-Language Transformers","summary":"  Recent vision-language models have achieved tremendous advances. However,\ntheir computational costs are also escalating dramatically, making model\nacceleration exceedingly critical. To pursue more efficient vision-language\nTransformers, this paper introduces Cross-Guided Ensemble of Tokens (CrossGET),\na general acceleration framework for vision-language Transformers. This\nframework adaptively combines tokens in real-time during inference,\nsignificantly reducing computational costs while maintaining high performance.\nCrossGET features two primary innovations: 1) Cross-Guided Matching and\nEnsemble. CrossGET leverages cross-modal guided token matching and ensemble to\neffectively utilize cross-modal information, achieving wider applicability\nacross both modality-independent models, e.g., CLIP, and modality-dependent\nones, e.g., BLIP2. 2) Complete-Graph Soft Matching. CrossGET introduces an\nalgorithm for the token-matching mechanism, ensuring reliable matching results\nwhile facilitating parallelizability and high efficiency. Extensive experiments\nhave been conducted on various vision-language tasks, such as image-text\nretrieval, visual reasoning, image captioning, and visual question answering.\nThe performance on both classic multimodal architectures and emerging\nmultimodal LLMs demonstrates the framework's effectiveness and versatility. The\ncode is available at https://github.com/sdc17/CrossGET.\n","authors":["Dachuan Shi","Chaofan Tao","Anyi Rao","Zhendong Yang","Chun Yuan","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2305.17455v4.pdf","comment":"ICML 2024. Code: https://github.com/sdc17/CrossGET"},{"id":"http://arxiv.org/abs/2402.09739v2","updated":"2024-06-13T18:55:23Z","published":"2024-02-15T06:36:07Z","title":"QuRating: Selecting High-Quality Data for Training Language Models","summary":"  Selecting high-quality pre-training data is important for creating capable\nlanguage models, but existing methods rely on simple heuristics. We introduce\nQuRating, a method for selecting pre-training data that can capture human\nintuitions about data quality. In this paper, we investigate four qualities -\nwriting style, required expertise, facts & trivia, and educational value - and\nfind that LLMs are able to discern these qualities, especially when making\npairwise judgments of texts. We train a QuRater model to learn scalar ratings\nfrom pairwise judgments, and use it to annotate a 260B training corpus with\nquality ratings for each of the four criteria. In our experiments, we select\n30B tokens according to the different quality ratings and train 1.3B-parameter\nlanguage models on the selected data. We find that it is important to balance\nquality and diversity. When we sample using quality ratings as logits over\ndocuments, our models obtain lower perplexity and stronger in-context learning\nperformance than baselines. Our best model is based on educational value and\nperforms similarly to a model trained with uniform sampling for 50% more steps.\nBeyond data selection, we use the quality ratings to construct a training\ncurriculum which improves performance without changing the training dataset. We\nextensively analyze the quality ratings and discuss their characteristics,\nbiases, and wider implications.\n","authors":["Alexander Wettig","Aatmik Gupta","Saumya Malik","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2402.09739v2.pdf","comment":"Accepted at ICML 2024. The results for top-k selection have been\n  corrected. The code, models and data are available at\n  https://github.com/princeton-nlp/QuRating. arXiv admin note: text overlap\n  with arXiv:2002.04059, arXiv:hep-th/9607006, arXiv:2107.06981,\n  arXiv:2008.09340 by other authors"},{"id":"http://arxiv.org/abs/2406.06455v2","updated":"2024-06-13T18:48:17Z","published":"2024-06-10T16:44:48Z","title":"A Large Language Model Pipeline for Breast Cancer Oncology","summary":"  Large language models (LLMs) have demonstrated potential in the innovation of\nmany disciplines. However, how they can best be developed for oncology remains\nunderdeveloped. State-of-the-art OpenAI models were fine-tuned on a clinical\ndataset and clinical guidelines text corpus for two important cancer treatment\nfactors, adjuvant radiation therapy and chemotherapy, using a novel Langchain\nprompt engineering pipeline. A high accuracy (0.85+) was achieved in the\nclassification of adjuvant radiation therapy and chemotherapy for breast cancer\npatients. Furthermore, a confidence interval was formed from observational data\non the quality of treatment from human oncologists to estimate the proportion\nof scenarios in which the model must outperform the original oncologist in its\ntreatment prediction to be a better solution overall as 8.2% to 13.3%. Due to\nindeterminacy in the outcomes of cancer treatment decisions, future\ninvestigation, potentially a clinical trial, would be required to determine if\nthis threshold was met by the models. Nevertheless, with 85% of U.S. cancer\npatients receiving treatment at local community facilities, these kinds of\nmodels could play an important part in expanding access to quality care with\noutcomes that lie, at minimum, close to a human oncologist.\n","authors":["Tristen Pool","Dennis Trujillo"],"pdf_url":"https://arxiv.org/pdf/2406.06455v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09520v1","updated":"2024-06-13T18:16:27Z","published":"2024-06-13T18:16:27Z","title":"A Systematic Review of Generative AI for Teaching and Learning Practice","summary":"  The use of generative artificial intelligence (GenAI) in academia is a\nsubjective and hotly debated topic. Currently, there are no agreed guidelines\ntowards the usage of GenAI systems in higher education (HE) and, thus, it is\nstill unclear how to make effective use of the technology for teaching and\nlearning practice. This paper provides an overview of the current state of\nresearch on GenAI for teaching and learning in HE. To this end, this study\nconducted a systematic review of relevant studies indexed by Scopus, using the\npreferred reporting items for systematic reviews and meta-analyses (PRISMA)\nguidelines. The search criteria revealed a total of 625 research papers, of\nwhich 355 met the final inclusion criteria. The findings from the review showed\nthe current state and the future trends in documents, citations, document\nsources/authors, keywords, and co-authorship. The research gaps identified\nsuggest that while some authors have looked at understanding the detection of\nAI-generated text, it may be beneficial to understand how GenAI can be\nincorporated into supporting the educational curriculum for assessments,\nteaching, and learning delivery. Furthermore, there is a need for additional\ninterdisciplinary, multidimensional studies in HE through collaboration. This\nwill strengthen the awareness and understanding of students, tutors, and other\nstakeholders, which will be instrumental in formulating guidelines, frameworks,\nand policies for GenAI usage.\n","authors":["Bayode Ogunleye","Kudirat Ibilola Zakariyyah","Oluwaseun Ajao","Olakunle Olayinka","Hemlata Sharma"],"pdf_url":"https://arxiv.org/pdf/2406.09520v1.pdf","comment":"20 pages, 10 figures, article published in Education Sciences"},{"id":"http://arxiv.org/abs/2406.09519v1","updated":"2024-06-13T18:12:01Z","published":"2024-06-13T18:12:01Z","title":"Talking Heads: Understanding Inter-layer Communication in Transformer\n  Language Models","summary":"  Although it is known that transformer language models (LMs) pass features\nfrom early layers to later layers, it is not well understood how this\ninformation is represented and routed by the model. By analyzing particular\nmechanism LMs use to accomplish this, we find that it is also used to recall\nitems from a list, and show that this mechanism can explain an otherwise\narbitrary-seeming sensitivity of the model to the order of items in the prompt.\nSpecifically, we find that models write into low-rank subspaces of the residual\nstream to represent features which are then read out by specific later layers,\nforming low-rank communication channels between layers. By decomposing\nattention head weight matrices with the Singular Value Decomposition (SVD), we\nfind that previously described interactions between heads separated by one or\nmore layers can be predicted via analysis of their weight matrices. We show\nthat it is possible to manipulate the internal model representations as well as\nedit model weights based on the mechanism we discover in order to significantly\nimprove performance on our synthetic Laundry List task, which requires recall\nfrom a list, often improving task accuracy by over 20%. Our analysis reveals a\nsurprisingly intricate interpretable structure learned from language model\npretraining, and helps us understand why sophisticated LMs sometimes fail in\nsimple domains, facilitating future analysis of more complex behaviors.\n","authors":["Jack Merullo","Carsten Eickhoff","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2406.09519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09490v1","updated":"2024-06-13T16:20:05Z","published":"2024-06-13T16:20:05Z","title":"Newswire: A Large-Scale Structured Database of a Century of Historical\n  News","summary":"  In the U.S. historically, local newspapers drew their content largely from\nnewswires like the Associated Press. Historians argue that newswires played a\npivotal role in creating a national identity and shared understanding of the\nworld, but there is no comprehensive archive of the content sent over\nnewswires. We reconstruct such an archive by applying a customized deep\nlearning pipeline to hundreds of terabytes of raw image scans from thousands of\nlocal newspapers. The resulting dataset contains 2.7 million unique public\ndomain U.S. newswire articles, written between 1878 and 1977. Locations in\nthese articles are georeferenced, topics are tagged using customized neural\ntopic classification, named entities are recognized, and individuals are\ndisambiguated to Wikipedia using a novel entity disambiguation model. To\nconstruct the Newswire dataset, we first recognize newspaper layouts and\ntranscribe around 138 millions structured article texts from raw image scans.\nWe then use a customized neural bi-encoder model to de-duplicate reproduced\narticles, in the presence of considerable abridgement and noise, quantifying\nhow widely each article was reproduced. A text classifier is used to ensure\nthat we only include newswire articles, which historically are in the public\ndomain. The structured data that accompany the texts provide rich information\nabout the who (disambiguated individuals), what (topics), and where\n(georeferencing) of the news that millions of Americans read over the course of\na century. We also include Library of Congress metadata information about the\nnewspapers that ran the articles on their front pages. The Newswire dataset is\nuseful both for large language modeling - expanding training data beyond what\nis available from modern web texts - and for studying a diversity of questions\nin computational linguistics, social science, and the digital humanities.\n","authors":["Emily Silcock","Abhishek Arora","Luca D'Amico-Wong","Melissa Dell"],"pdf_url":"https://arxiv.org/pdf/2406.09490v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2306.17810,\n  arXiv:2308.12477"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.09418v1","updated":"2024-06-13T17:59:59Z","published":"2024-06-13T17:59:59Z","title":"VideoGPT+: Integrating Image and Video Encoders for Enhanced Video\n  Understanding","summary":"  Building on the advances of language models, Large Multimodal Models (LMMs)\nhave contributed significant improvements in video understanding. While the\ncurrent video LMMs utilize advanced Large Language Models (LLMs), they rely on\neither image or video encoders to process visual inputs, each of which has its\nown limitations. Image encoders excel at capturing rich spatial details from\nframe sequences but lack explicit temporal context, which can be important in\nvideos with intricate action sequences. On the other hand, video encoders\nprovide temporal context but are often limited by computational constraints\nthat lead to processing only sparse frames at lower resolutions, resulting in\nreduced contextual and spatial understanding. To this end, we introduce\nVideoGPT+, which combines the complementary benefits of the image encoder (for\ndetailed spatial understanding) and the video encoder (for global temporal\ncontext modeling). The model processes videos by dividing them into smaller\nsegments and applies an adaptive pooling strategy on features extracted by both\nimage and video encoders. Our architecture showcases improved performance\nacross multiple video benchmarks, including VCGBench, MVBench and Zero-shot\nquestion-answering. Further, we develop 112K video-instruction set using a\nnovel semi-automatic annotation pipeline which further improves the model\nperformance. Additionally, to comprehensively evaluate video LMMs, we present\nVCGBench-Diverse, covering 18 broad video categories such as lifestyle, sports,\nscience, gaming, and surveillance videos. This benchmark with 4,354\nquestion-answer pairs evaluates the generalization of existing LMMs on dense\nvideo captioning, spatial and temporal understanding, and complex reasoning,\nensuring comprehensive assessment across diverse video types and dynamics.\nCode: https://github.com/mbzuai-oryx/VideoGPT-plus.\n","authors":["Muhammad Maaz","Hanoona Rasheed","Salman Khan","Fahad Khan"],"pdf_url":"https://arxiv.org/pdf/2406.09418v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2406.09415v1","updated":"2024-06-13T17:59:58Z","published":"2024-06-13T17:59:58Z","title":"An Image is Worth More Than 16x16 Patches: Exploring Transformers on\n  Individual Pixels","summary":"  This work does not introduce a new method. Instead, we present an interesting\nfinding that questions the necessity of the inductive bias -- locality in\nmodern computer vision architectures. Concretely, we find that vanilla\nTransformers can operate by directly treating each individual pixel as a token\nand achieve highly performant results. This is substantially different from the\npopular design in Vision Transformer, which maintains the inductive bias from\nConvNets towards local neighborhoods (e.g. by treating each 16x16 patch as a\ntoken). We mainly showcase the effectiveness of pixels-as-tokens across three\nwell-studied tasks in computer vision: supervised learning for object\nclassification, self-supervised learning via masked autoencoding, and image\ngeneration with diffusion models. Although directly operating on individual\npixels is less computationally practical, we believe the community must be\naware of this surprising piece of knowledge when devising the next generation\nof neural architectures for computer vision.\n","authors":["Duy-Kien Nguyen","Mahmoud Assran","Unnat Jain","Martin R. Oswald","Cees G. M. Snoek","Xinlei Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09415v1.pdf","comment":"Technical report, 23 pages"},{"id":"http://arxiv.org/abs/2406.09416v1","updated":"2024-06-13T17:59:58Z","published":"2024-06-13T17:59:58Z","title":"Alleviating Distortion in Image Generation via Multi-Resolution\n  Diffusion Models","summary":"  This paper presents innovative enhancements to diffusion models by\nintegrating a novel multi-resolution network and time-dependent layer\nnormalization. Diffusion models have gained prominence for their effectiveness\nin high-fidelity image generation. While conventional approaches rely on\nconvolutional U-Net architectures, recent Transformer-based designs have\ndemonstrated superior performance and scalability. However, Transformer\narchitectures, which tokenize input data (via \"patchification\"), face a\ntrade-off between visual fidelity and computational complexity due to the\nquadratic nature of self-attention operations concerning token length. While\nlarger patch sizes enable attention computation efficiency, they struggle to\ncapture fine-grained visual details, leading to image distortions. To address\nthis challenge, we propose augmenting the Diffusion model with the\nMulti-Resolution network (DiMR), a framework that refines features across\nmultiple resolutions, progressively enhancing detail from low to high\nresolution. Additionally, we introduce Time-Dependent Layer Normalization\n(TD-LN), a parameter-efficient approach that incorporates time-dependent\nparameters into layer normalization to inject time information and achieve\nsuperior performance. Our method's efficacy is demonstrated on the\nclass-conditional ImageNet generation benchmark, where DiMR-XL variants\noutperform prior diffusion models, setting new state-of-the-art FID scores of\n1.70 on ImageNet 256 x 256 and 2.89 on ImageNet 512 x 512. Project page:\nhttps://qihao067.github.io/projects/DiMR\n","authors":["Qihao Liu","Zhanpeng Zeng","Ju He","Qihang Yu","Xiaohui Shen","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09416v1.pdf","comment":"Introducing DiMR, a new diffusion backbone that surpasses all\n  existing image generation models of various sizes on ImageNet 256 with only\n  505M parameters. Project page: https://qihao067.github.io/projects/DiMR"},{"id":"http://arxiv.org/abs/2406.09417v1","updated":"2024-06-13T17:59:58Z","published":"2024-06-13T17:59:58Z","title":"Rethinking Score Distillation as a Bridge Between Image Distributions","summary":"  Score distillation sampling (SDS) has proven to be an important tool,\nenabling the use of large-scale diffusion priors for tasks operating in\ndata-poor domains. Unfortunately, SDS has a number of characteristic artifacts\nthat limit its usefulness in general-purpose applications. In this paper, we\nmake progress toward understanding the behavior of SDS and its variants by\nviewing them as solving an optimal-cost transport path from a source\ndistribution to a target distribution. Under this new interpretation, these\nmethods seek to transport corrupted images (source) to the natural image\ndistribution (target). We argue that current methods' characteristic artifacts\nare caused by (1) linear approximation of the optimal path and (2) poor\nestimates of the source distribution. We show that calibrating the text\nconditioning of the source distribution can produce high-quality generation and\ntranslation results with little extra overhead. Our method can be easily\napplied across many domains, matching or beating the performance of specialized\nmethods. We demonstrate its utility in text-to-2D, text-based NeRF\noptimization, translating paintings to real images, optical illusion\ngeneration, and 3D sketch-to-real. We compare our method to existing approaches\nfor score distillation sampling and show that it can produce high-frequency\ndetails with realistic colors.\n","authors":["David McAllister","Songwei Ge","Jia-Bin Huang","David W. Jacobs","Alexei A. Efros","Aleksander Holynski","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2406.09417v1.pdf","comment":"Project webpage: https://sds-bridge.github.io/"},{"id":"http://arxiv.org/abs/2406.09413v1","updated":"2024-06-13T17:59:56Z","published":"2024-06-13T17:59:56Z","title":"Interpreting the Weight Space of Customized Diffusion Models","summary":"  We investigate the space of weights spanned by a large collection of\ncustomized diffusion models. We populate this space by creating a dataset of\nover 60,000 models, each of which is a base model fine-tuned to insert a\ndifferent person's visual identity. We model the underlying manifold of these\nweights as a subspace, which we term weights2weights. We demonstrate three\nimmediate applications of this space -- sampling, editing, and inversion.\nFirst, as each point in the space corresponds to an identity, sampling a set of\nweights from it results in a model encoding a novel identity. Next, we find\nlinear directions in this space corresponding to semantic edits of the identity\n(e.g., adding a beard). These edits persist in appearance across generated\nsamples. Finally, we show that inverting a single image into this space\nreconstructs a realistic identity, even if the input image is out of\ndistribution (e.g., a painting). Our results indicate that the weight space of\nfine-tuned diffusion models behaves as an interpretable latent space of\nidentities.\n","authors":["Amil Dravid","Yossi Gandelsman","Kuan-Chieh Wang","Rameen Abdal","Gordon Wetzstein","Alexei A. Efros","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2406.09413v1.pdf","comment":"Project Page: https://snap-research.github.io/weights2weights"},{"id":"http://arxiv.org/abs/2406.09414v1","updated":"2024-06-13T17:59:56Z","published":"2024-06-13T17:59:56Z","title":"Depth Anything V2","summary":"  This work presents Depth Anything V2. Without pursuing fancy techniques, we\naim to reveal crucial findings to pave the way towards building a powerful\nmonocular depth estimation model. Notably, compared with V1, this version\nproduces much finer and more robust depth predictions through three key\npractices: 1) replacing all labeled real images with synthetic images, 2)\nscaling up the capacity of our teacher model, and 3) teaching student models\nvia the bridge of large-scale pseudo-labeled real images. Compared with the\nlatest models built on Stable Diffusion, our models are significantly more\nefficient (more than 10x faster) and more accurate. We offer models of\ndifferent scales (ranging from 25M to 1.3B params) to support extensive\nscenarios. Benefiting from their strong generalization capability, we fine-tune\nthem with metric depth labels to obtain our metric depth models. In addition to\nour models, considering the limited diversity and frequent noise in current\ntest sets, we construct a versatile evaluation benchmark with precise\nannotations and diverse scenes to facilitate future research.\n","authors":["Lihe Yang","Bingyi Kang","Zilong Huang","Zhen Zhao","Xiaogang Xu","Jiashi Feng","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.09414v1.pdf","comment":"Project page: https://depth-anything-v2.github.io"},{"id":"http://arxiv.org/abs/2406.09412v1","updated":"2024-06-13T17:59:53Z","published":"2024-06-13T17:59:53Z","title":"Explore the Limits of Omni-modal Pretraining at Scale","summary":"  We propose to build omni-modal intelligence, which is capable of\nunderstanding any modality and learning universal representations. In specific,\nwe propose a scalable pretraining paradigm, named Multimodal Context (MiCo),\nwhich can scale up the numbers of modalities and amount of data, together with\nthe model parameters, in the pretraining process. With MiCo, the pretrained\nmodels show significant emergent abilities in multimodal learning, which are\nevaluated on the following tasks: i) single-modality perception benchmarks of\n10 different modalities, ii) 25 cross-modality understanding tasks of\nretrieval, question-answering, captioning, and iii) 18 multimodal large\nlanguage model benchmarks. Our models establish 37 new records for\nstate-of-the-art performance. We hope that our research could contribute to the\ndevelopment of omni-modal intelligence. Code and Models are at\nhttps://github.com/invictus717/MiCo\n","authors":["Yiyuan Zhang","Handong Li","Jing Liu","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2406.09412v1.pdf","comment":"Project Website: https://invictus717.github.io/MiCo/"},{"id":"http://arxiv.org/abs/2406.09411v1","updated":"2024-06-13T17:59:52Z","published":"2024-06-13T17:59:52Z","title":"MuirBench: A Comprehensive Benchmark for Robust Multi-image\n  Understanding","summary":"  We introduce MuirBench, a comprehensive benchmark that focuses on robust\nmulti-image understanding capabilities of multimodal LLMs. MuirBench consists\nof 12 diverse multi-image tasks (e.g., scene understanding, ordering) that\ninvolve 10 categories of multi-image relations (e.g., multiview, temporal\nrelations). Comprising 11,264 images and 2,600 multiple-choice questions,\nMuirBench is created in a pairwise manner, where each standard instance is\npaired with an unanswerable variant that has minimal semantic differences, in\norder for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our\nresults reveal that even the best-performing models like GPT-4o and Gemini Pro\nfind it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.\nOpen-source multimodal LLMs trained on single images can hardly generalize to\nmulti-image questions, hovering below 33.3% in accuracy. These results\nhighlight the importance of MuirBench in encouraging the community to develop\nmultimodal LLMs that can look beyond a single image, suggesting potential\npathways for future improvements.\n","authors":["Fei Wang","Xingyu Fu","James Y. Huang","Zekun Li","Qin Liu","Xiaogeng Liu","Mingyu Derek Ma","Nan Xu","Wenxuan Zhou","Kai Zhang","Tianyi Lorena Yan","Wenjie Jacky Mo","Hsiang-Hui Liu","Pan Lu","Chunyuan Li","Chaowei Xiao","Kai-Wei Chang","Dan Roth","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09410v1","updated":"2024-06-13T17:59:51Z","published":"2024-06-13T17:59:51Z","title":"Scene Graph Generation in Large-Size VHR Satellite Imagery: A\n  Large-Scale Dataset and A Context-Aware Approach","summary":"  Scene graph generation (SGG) in satellite imagery (SAI) benefits promoting\nintelligent understanding of geospatial scenarios from perception to cognition.\nIn SAI, objects exhibit great variations in scales and aspect ratios, and there\nexist rich relationships between objects (even between spatially disjoint\nobjects), which makes it necessary to holistically conduct SGG in large-size\nvery-high-resolution (VHR) SAI. However, the lack of SGG datasets with\nlarge-size VHR SAI has constrained the advancement of SGG in SAI. Due to the\ncomplexity of large-size VHR SAI, mining triplets <subject, relationship,\nobject> in large-size VHR SAI heavily relies on long-range contextual\nreasoning. Consequently, SGG models designed for small-size natural imagery are\nnot directly applicable to large-size VHR SAI. To address the scarcity of\ndatasets, this paper constructs a large-scale dataset for SGG in large-size VHR\nSAI with image sizes ranging from 512 x 768 to 27,860 x 31,096 pixels, named\nRSG, encompassing over 210,000 objects and more than 400,000 triplets. To\nrealize SGG in large-size VHR SAI, we propose a context-aware cascade cognition\n(CAC) framework to understand SAI at three levels: object detection (OBD), pair\npruning and relationship prediction. As a fundamental prerequisite for SGG in\nlarge-size SAI, a holistic multi-class object detection network (HOD-Net) that\ncan flexibly integrate multi-scale contexts is proposed. With the consideration\nthat there exist a huge amount of object pairs in large-size SAI but only a\nminority of object pairs contain meaningful relationships, we design a pair\nproposal generation (PPG) network via adversarial reconstruction to select\nhigh-value pairs. Furthermore, a relationship prediction network with\ncontext-aware messaging (RPCM) is proposed to predict the relationship types of\nthese pairs.\n","authors":["Yansheng Li","Linlin Wang","Tingzhu Wang","Xue Yang","Junwei Luo","Qi Wang","Youming Deng","Wenbin Wang","Xian Sun","Haifeng Li","Bo Dang","Yongjun Zhang","Yi Yu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2406.09410v1.pdf","comment":"This paper releases a SAI-oriented SGG toolkit with about 30 OBD\n  methods and 10 SGG methods, and develops a benchmark based on RSG where our\n  HOD-Net and RPCM significantly outperform the state-of-the-art methods in\n  both OBD and SGG tasks. The RSG dataset and SAI-oriented toolkit will be made\n  publicly available at https://linlin-dev.github.io/project/RSG"},{"id":"http://arxiv.org/abs/2406.09409v1","updated":"2024-06-13T17:59:46Z","published":"2024-06-13T17:59:46Z","title":"CodedEvents: Optimal Point-Spread-Function Engineering for 3D-Tracking\n  with Event Cameras","summary":"  Point-spread-function (PSF) engineering is a well-established computational\nimaging technique that uses phase masks and other optical elements to embed\nextra information (e.g., depth) into the images captured by conventional CMOS\nimage sensors. To date, however, PSF-engineering has not been applied to\nneuromorphic event cameras; a powerful new image sensing technology that\nresponds to changes in the log-intensity of light.\n  This paper establishes theoretical limits (Cram\\'er Rao bounds) on 3D point\nlocalization and tracking with PSF-engineered event cameras. Using these\nbounds, we first demonstrate that existing Fisher phase masks are already\nnear-optimal for localizing static flashing point sources (e.g., blinking\nfluorescent molecules). We then demonstrate that existing designs are\nsub-optimal for tracking moving point sources and proceed to use our theory to\ndesign optimal phase masks and binary amplitude masks for this task. To\novercome the non-convexity of the design problem, we leverage novel implicit\nneural representation based parameterizations of the phase and amplitude masks.\nWe demonstrate the efficacy of our designs through extensive simulations. We\nalso validate our method with a simple prototype.\n","authors":["Sachin Shah","Matthew Albert Chan","Haoming Cai","Jingxi Chen","Sakshum Kulshrestha","Chahat Deep Singh","Yiannis Aloimonos","Christopher Metzler"],"pdf_url":"https://arxiv.org/pdf/2406.09409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09407v1","updated":"2024-06-13T17:59:44Z","published":"2024-06-13T17:59:44Z","title":"Towards Evaluating the Robustness of Visual State Space Models","summary":"  Vision State Space Models (VSSMs), a novel architecture that combines the\nstrengths of recurrent neural networks and latent variable models, have\ndemonstrated remarkable performance in visual perception tasks by efficiently\ncapturing long-range dependencies and modeling complex visual dynamics.\nHowever, their robustness under natural and adversarial perturbations remains a\ncritical concern. In this work, we present a comprehensive evaluation of VSSMs'\nrobustness under various perturbation scenarios, including occlusions, image\nstructure, common corruptions, and adversarial attacks, and compare their\nperformance to well-established architectures such as transformers and\nConvolutional Neural Networks. Furthermore, we investigate the resilience of\nVSSMs to object-background compositional changes on sophisticated benchmarks\ndesigned to test model performance in complex visual scenes. We also assess\ntheir robustness on object detection and segmentation tasks using corrupted\ndatasets that mimic real-world scenarios. To gain a deeper understanding of\nVSSMs' adversarial robustness, we conduct a frequency analysis of adversarial\nattacks, evaluating their performance against low-frequency and high-frequency\nperturbations. Our findings highlight the strengths and limitations of VSSMs in\nhandling complex visual corruptions, offering valuable insights for future\nresearch and improvements in this promising field. Our code and models will be\navailable at https://github.com/HashmatShadab/MambaRobustness.\n","authors":["Hashmat Shadab Malik","Fahad Shamshad","Muzammal Naseer","Karthik Nandakumar","Fahad Shahbaz Khan","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2406.09407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09408v1","updated":"2024-06-13T17:59:44Z","published":"2024-06-13T17:59:44Z","title":"Data Attribution for Text-to-Image Models by Unlearning Synthesized\n  Images","summary":"  The goal of data attribution for text-to-image models is to identify the\ntraining images that most influence the generation of a new image. We can\ndefine \"influence\" by saying that, for a given output, if a model is retrained\nfrom scratch without that output's most influential images, the model should\nthen fail to generate that output image. Unfortunately, directly searching for\nthese influential images is computationally infeasible, since it would require\nrepeatedly retraining from scratch. We propose a new approach that efficiently\nidentifies highly-influential images. Specifically, we simulate unlearning the\nsynthesized image, proposing a method to increase the training loss on the\noutput image, without catastrophic forgetting of other, unrelated concepts.\nThen, we find training images that are forgotten by proxy, identifying ones\nwith significant loss deviations after the unlearning process, and label these\nas influential. We evaluate our method with a computationally intensive but\n\"gold-standard\" retraining from scratch and demonstrate our method's advantages\nover previous methods.\n","authors":["Sheng-Yu Wang","Aaron Hertzmann","Alexei A. Efros","Jun-Yan Zhu","Richard Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.09408v1.pdf","comment":"Project page: https://peterwang512.github.io/AttributeByUnlearning\n  Code: https://github.com/PeterWang512/AttributeByUnlearning"},{"id":"http://arxiv.org/abs/2406.09406v1","updated":"2024-06-13T17:59:42Z","published":"2024-06-13T17:59:42Z","title":"4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities","summary":"  Current multimodal and multitask foundation models like 4M or UnifiedIO show\npromising results, but in practice their out-of-the-box abilities to accept\ndiverse inputs and perform diverse tasks are limited by the (usually rather\nsmall) number of modalities and tasks they are trained on. In this paper, we\nexpand upon the capabilities of them by training a single model on tens of\nhighly diverse modalities and by performing co-training on large-scale\nmultimodal datasets and text corpora. This includes training on several\nsemantic and geometric modalities, feature maps from recent state of the art\nmodels like DINOv2 and ImageBind, pseudo labels of specialist models like SAM\nand 4DHumans, and a range of new modalities that allow for novel ways to\ninteract with the model and steer the generation, for example image metadata or\ncolor palettes. A crucial step in this process is performing discrete\ntokenization on various modalities, whether they are image-like, neural network\nfeature maps, vectors, structured data like instance segmentation or human\nposes, or data that can be represented as text. Through this, we expand on the\nout-of-the-box capabilities of multimodal models and specifically show the\npossibility of training one model to solve at least 3x more tasks/modalities\nthan existing ones and doing so without a loss in performance. This enables\nmore fine-grained and controllable multimodal generation capabilities and\nallows us to study the distillation of models trained on diverse data and\nobjectives into a unified model. We successfully scale the training to a three\nbillion parameter model using tens of modalities and different datasets. The\nresulting models and training code are open sourced at 4m.epfl.ch.\n","authors":["Roman Bachmann","Oğuzhan Fatih Kar","David Mizrahi","Ali Garjani","Mingfei Gao","David Griffiths","Jiaming Hu","Afshin Dehghan","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2406.09406v1.pdf","comment":"Project page at 4m.epfl.ch"},{"id":"http://arxiv.org/abs/2406.09404v1","updated":"2024-06-13T17:59:32Z","published":"2024-06-13T17:59:32Z","title":"ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene\n  Editing","summary":"  This paper proposes ConsistDreamer - a novel framework that lifts 2D\ndiffusion models with 3D awareness and 3D consistency, thus enabling\nhigh-fidelity instruction-guided scene editing. To overcome the fundamental\nlimitation of missing 3D consistency in 2D diffusion models, our key insight is\nto introduce three synergetic strategies that augment the input of the 2D\ndiffusion model to become 3D-aware and to explicitly enforce 3D consistency\nduring the training process. Specifically, we design surrounding views as\ncontext-rich input for the 2D diffusion model, and generate 3D-consistent,\nstructured noise instead of image-independent noise. Moreover, we introduce\nself-supervised consistency-enforcing training within the per-scene editing\nprocedure. Extensive evaluation shows that our ConsistDreamer achieves\nstate-of-the-art performance for instruction-guided scene editing across\nvarious scenes and editing instructions, particularly in complicated\nlarge-scale indoor scenes from ScanNet++, with significantly improved sharpness\nand fine-grained textures. Notably, ConsistDreamer stands as the first work\ncapable of successfully editing complex (e.g., plaid/checkered) patterns. Our\nproject page is at immortalco.github.io/ConsistDreamer.\n","authors":["Jun-Kun Chen","Samuel Rota Bulò","Norman Müller","Lorenzo Porzi","Peter Kontschieder","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09404v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.09403v1","updated":"2024-06-13T17:59:31Z","published":"2024-06-13T17:59:31Z","title":"Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal\n  Language Models","summary":"  Humans draw to facilitate reasoning: we draw auxiliary lines when solving\ngeometry problems; we mark and circle when reasoning on maps; we use sketches\nto amplify our ideas and relieve our limited-capacity working memory. However,\nsuch actions are missing in current multimodal language models (LMs). Current\nchain-of-thought and tool-use paradigms only use text as intermediate reasoning\nsteps. In this work, we introduce Sketchpad, a framework that gives multimodal\nLMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts\nplanning and reasoning according to the visual artifacts it has drawn.\nDifferent from prior work, which uses text-to-image models to enable LMs to\ndraw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is\ncloser to human sketching and better facilitates reasoning. Sketchpad can also\nuse specialist vision models during the sketching process (e.g., draw bounding\nboxes with object detection models, draw masks with segmentation models), to\nfurther enhance visual perception and reasoning. We experiment with a wide\nrange of math tasks (including geometry, functions, graphs, and chess) and\ncomplex visual reasoning tasks. Sketchpad substantially improves performance on\nall tasks over strong base models with no sketching, yielding an average gain\nof 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a\nnew state of the art on all tasks, including V*Bench (80.3%), BLINK spatial\nreasoning (83.9%), and visual correspondence (80.8%). All codes and data are in\nhttps://visualsketchpad.github.io/.\n","authors":["Yushi Hu","Weijia Shi","Xingyu Fu","Dan Roth","Mari Ostendorf","Luke Zettlemoyer","Noah A Smith","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2406.09403v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2406.09401v1","updated":"2024-06-13T17:59:30Z","published":"2024-06-13T17:59:30Z","title":"MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded\n  Language Annotations","summary":"  With the emergence of LLMs and their integration with other data modalities,\nmulti-modal 3D perception attracts more attention due to its connectivity to\nthe physical world and makes rapid progress. However, limited by existing\ndatasets, previous works mainly focus on understanding object properties or\ninter-object spatial relationships in a 3D scene. To tackle this problem, this\npaper builds the first largest ever multi-modal 3D scene dataset and benchmark\nwith hierarchical grounded language annotations, MMScan. It is constructed\nbased on a top-down logic, from region to object level, from a single target to\ninter-target relationships, covering holistic aspects of spatial and attribute\nunderstanding. The overall pipeline incorporates powerful VLMs via carefully\ndesigned prompts to initialize the annotations efficiently and further involve\nhumans' correction in the loop to ensure the annotations are natural, correct,\nand comprehensive. Built upon existing 3D scanning data, the resulting\nmulti-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects\nand 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding\nand question-answering benchmarks. We evaluate representative baselines on our\nbenchmarks, analyze their capabilities in different aspects, and showcase the\nkey problems to be addressed in the future. Furthermore, we use this\nhigh-quality dataset to train state-of-the-art 3D visual grounding and LLMs and\nobtain remarkable performance improvement both on existing benchmarks and\nin-the-wild evaluation. Codes, datasets, and benchmarks will be available at\nhttps://github.com/OpenRobotLab/EmbodiedScan.\n","authors":["Ruiyuan Lyu","Tai Wang","Jingli Lin","Shuai Yang","Xiaohan Mao","Yilun Chen","Runsen Xu","Haifeng Huang","Chenming Zhu","Dahua Lin","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2406.09401v1.pdf","comment":"Follow-up of EmbodiedScan. A multi-modal 3D dataset with the\n  most-ever comprehensive language annotations for 3D-LLMs. Project page:\n  https://tai-wang.github.io/mmscan/"},{"id":"http://arxiv.org/abs/2406.09402v1","updated":"2024-06-13T17:59:30Z","published":"2024-06-13T17:59:30Z","title":"Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D\n  Diffusion","summary":"  This paper proposes Instruct 4D-to-4D that achieves 4D awareness and\nspatial-temporal consistency for 2D diffusion models to generate high-quality\ninstruction-guided dynamic scene editing results. Traditional applications of\n2D diffusion models in dynamic scene editing often result in inconsistency,\nprimarily due to their inherent frame-by-frame editing methodology. Addressing\nthe complexities of extending instruction-guided editing to 4D, our key insight\nis to treat a 4D scene as a pseudo-3D scene, decoupled into two sub-problems:\nachieving temporal consistency in video editing and applying these edits to the\npseudo-3D scene. Following this, we first enhance the Instruct-Pix2Pix (IP2P)\nmodel with an anchor-aware attention module for batch processing and consistent\nediting. Additionally, we integrate optical flow-guided appearance propagation\nin a sliding window fashion for more precise frame-to-frame editing and\nincorporate depth-based projection to manage the extensive data of pseudo-3D\nscenes, followed by iterative editing to achieve convergence. We extensively\nevaluate our approach in various scenes and editing instructions, and\ndemonstrate that it achieves spatially and temporally consistent editing\nresults, with significantly enhanced detail and sharpness over the prior art.\nNotably, Instruct 4D-to-4D is general and applicable to both monocular and\nchallenging multi-camera scenes. Code and more results are available at\nimmortalco.github.io/Instruct-4D-to-4D.\n","authors":["Linzhan Mou","Jun-Kun Chen","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09402v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.09400v1","updated":"2024-06-13T17:59:29Z","published":"2024-06-13T17:59:29Z","title":"Yo'LLaVA: Your Personalized Language and Vision Assistant","summary":"  Large Multimodal Models (LMMs) have shown remarkable capabilities across a\nvariety of tasks (e.g., image captioning, visual question answering). While\nbroad, their knowledge remains generic (e.g., recognizing a dog), and they are\nunable to handle personalized subjects (e.g., recognizing a user's pet dog).\nHuman reasoning, in contrast, typically operates within the context of specific\nsubjects in our surroundings. For example, one might ask, \"What should I buy\nfor my dog's birthday?\"; as opposed to a generic inquiry about \"What should I\nbuy for a dog's birthday?\". Similarly, when looking at a friend's image, the\ninterest lies in seeing their activities (e.g., \"my friend is holding a cat\"),\nrather than merely observing generic human actions (e.g., \"a man is holding a\ncat\"). In this paper, we introduce the novel task of personalizing LMMs, so\nthat they can have conversations about a specific subject. We propose Yo'LLaVA,\nwhich learns to embed a personalized subject into a set of latent tokens given\na handful of example images of the subject. Our qualitative and quantitative\nanalyses reveal that Yo'LLaVA can learn the concept more efficiently using\nfewer tokens and more effectively encode the visual attributes compared to\nstrong prompting baselines (e.g., LLaVA).\n","authors":["Thao Nguyen","Haotian Liu","Yuheng Li","Mu Cai","Utkarsh Ojha","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2406.09400v1.pdf","comment":"Project page: https://thaoshibe.github.io/YoLLaVA"},{"id":"http://arxiv.org/abs/2406.09399v1","updated":"2024-06-13T17:59:26Z","published":"2024-06-13T17:59:26Z","title":"OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation","summary":"  Tokenizer, serving as a translator to map the intricate visual data into a\ncompact latent space, lies at the core of visual generative models. Based on\nthe finding that existing tokenizers are tailored to image or video inputs,\nthis paper presents OmniTokenizer, a transformer-based tokenizer for joint\nimage and video tokenization. OmniTokenizer is designed with a spatial-temporal\ndecoupled architecture, which integrates window and causal attention for\nspatial and temporal modeling. To exploit the complementary nature of image and\nvideo data, we further propose a progressive training strategy, where\nOmniTokenizer is first trained on image data on a fixed resolution to develop\nthe spatial encoding capacity and then jointly trained on image and video data\non multiple resolutions to learn the temporal dynamics. OmniTokenizer, for the\nfirst time, handles both image and video inputs within a unified framework and\nproves the possibility of realizing their synergy. Extensive experiments\ndemonstrate that OmniTokenizer achieves state-of-the-art (SOTA) reconstruction\nperformance on various image and video datasets, e.g., 1.11 reconstruction FID\non ImageNet and 42 reconstruction FVD on UCF-101, beating the previous SOTA\nmethods by 13% and 26%, respectively. Additionally, we also show that when\nintegrated with OmniTokenizer, both language model-based approaches and\ndiffusion models can realize advanced visual synthesis performance,\nunderscoring the superiority and versatility of our method. Code is available\nat https://github.com/FoundationVision/OmniTokenizer.\n","authors":["Junke Wang","Yi Jiang","Zehuan Yuan","Binyue Peng","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.09399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09398v1","updated":"2024-06-13T17:59:23Z","published":"2024-06-13T17:59:23Z","title":"Real-Time Deepfake Detection in the Real-World","summary":"  Recent improvements in generative AI made synthesizing fake images easy; as\nthey can be used to cause harm, it is crucial to develop accurate techniques to\nidentify them. This paper introduces \"Locally Aware Deepfake Detection\nAlgorithm\" (LaDeDa), that accepts a single 9x9 image patch and outputs its\ndeepfake score. The image deepfake score is the pooled score of its patches.\nWith merely patch-level information, LaDeDa significantly improves over the\nstate-of-the-art, achieving around 99% mAP on current benchmarks. Owing to the\npatch-level structure of LaDeDa, we hypothesize that the generation artifacts\ncan be detected by a simple model. We therefore distill LaDeDa into\nTiny-LaDeDa, a highly efficient model consisting of only 4 convolutional\nlayers. Remarkably, Tiny-LaDeDa has 375x fewer FLOPs and is 10,000x more\nparameter-efficient than LaDeDa, allowing it to run efficiently on edge devices\nwith a minor decrease in accuracy. These almost-perfect scores raise the\nquestion: is the task of deepfake detection close to being solved? Perhaps\nsurprisingly, our investigation reveals that current training protocols prevent\nmethods from generalizing to real-world deepfakes extracted from social media.\nTo address this issue, we introduce WildRF, a new deepfake detection dataset\ncurated from several popular social networks. Our method achieves the top\nperformance of 93.7% mAP on WildRF, however the large gap from perfect accuracy\nshows that reliable real-world deepfake detection is still unsolved.\n","authors":["Bar Cavia","Eliahu Horwitz","Tal Reiss","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2406.09398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09397v1","updated":"2024-06-13T17:59:20Z","published":"2024-06-13T17:59:20Z","title":"Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks\n  and Algorithms","summary":"  Modern vision models are trained on very large noisy datasets. While these\nmodels acquire strong capabilities, they may not follow the user's intent to\noutput the desired results in certain aspects, e.g., visual aesthetic,\npreferred style, and responsibility. In this paper, we target the realm of\nvisual aesthetics and aim to align vision models with human aesthetic standards\nin a retrieval system. Advanced retrieval systems usually adopt a cascade of\naesthetic models as re-rankers or filters, which are limited to low-level\nfeatures like saturation and perform poorly when stylistic, cultural or\nknowledge contexts are involved. We find that utilizing the reasoning ability\nof large language models (LLMs) to rephrase the search query and extend the\naesthetic expectations can make up for this shortcoming. Based on the above\nfindings, we propose a preference-based reinforcement learning method that\nfine-tunes the vision models to distill the knowledge from both LLMs reasoning\nand the aesthetic models to better align the vision models with human\naesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval\nsystems, we leverage large multi-modality model (LMM) to evaluate the aesthetic\nperformance with their strong abilities. As aesthetic assessment is one of the\nmost subjective tasks, to validate the robustness of LMM, we further propose a\nnovel dataset named HPIR to benchmark the alignment with human aesthetics.\nExperiments demonstrate that our method significantly enhances the aesthetic\nbehaviors of the vision models, under several metrics. We believe the proposed\nalgorithm can be a general practice for aligning vision models with human\nvalues.\n","authors":["Miaosen Zhang","Yixuan Wei","Zhen Xing","Yifei Ma","Zuxuan Wu","Ji Li","Zheng Zhang","Qi Dai","Chong Luo","Xin Geng","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2406.09397v1.pdf","comment":"28 pages, 26 figures, under review"},{"id":"http://arxiv.org/abs/2406.09396v1","updated":"2024-06-13T17:59:16Z","published":"2024-06-13T17:59:16Z","title":"Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video\n  QA","summary":"  Long-form videos that span across wide temporal intervals are highly\ninformation redundant and contain multiple distinct events or entities that are\noften loosely-related. Therefore, when performing long-form video question\nanswering (LVQA),all information necessary to generate a correct response can\noften be contained within a small subset of frames. Recent literature explore\nthe use of large language models (LLMs) in LVQA benchmarks, achieving\nexceptional performance, while relying on vision language models (VLMs) to\nconvert all visual content within videos into natural language. Such VLMs often\nindependently caption a large number of frames uniformly sampled from long\nvideos, which is not efficient and can mostly be redundant. Questioning these\ndecision choices, we explore optimal strategies for key-frame selection and\nsequence-aware captioning, that can significantly reduce these redundancies. We\npropose two novel approaches that improve each of aspects, namely Hierarchical\nKeyframe Selector and Sequential Visual LLM. Our resulting framework termed\nLVNet achieves state-of-the-art performance across three benchmark LVQA\ndatasets. Our code will be released publicly.\n","authors":["Jongwoo Park","Kanchana Ranasinghe","Kumara Kahatapitiya","Wonjeong Ryoo","Donghyun Kim","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2406.09396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17075v3","updated":"2024-06-13T17:59:14Z","published":"2023-10-26T00:36:03Z","title":"HyperFields: Towards Zero-Shot Generation of NeRFs from Text","summary":"  We introduce HyperFields, a method for generating text-conditioned Neural\nRadiance Fields (NeRFs) with a single forward pass and (optionally) some\nfine-tuning. Key to our approach are: (i) a dynamic hypernetwork, which learns\na smooth mapping from text token embeddings to the space of NeRFs; (ii) NeRF\ndistillation training, which distills scenes encoded in individual NeRFs into\none dynamic hypernetwork. These techniques enable a single network to fit over\na hundred unique scenes. We further demonstrate that HyperFields learns a more\ngeneral map between text and NeRFs, and consequently is capable of predicting\nnovel in-distribution and out-of-distribution scenes -- either zero-shot or\nwith a few finetuning steps. Finetuning HyperFields benefits from accelerated\nconvergence thanks to the learned general map, and is capable of synthesizing\nnovel scenes 5 to 10 times faster than existing neural optimization-based\nmethods. Our ablation experiments show that both the dynamic architecture and\nNeRF distillation are critical to the expressivity of HyperFields.\n","authors":["Sudarshan Babu","Richard Liu","Avery Zhou","Michael Maire","Greg Shakhnarovich","Rana Hanocka"],"pdf_url":"https://arxiv.org/pdf/2310.17075v3.pdf","comment":"Accepted to ICML 2024, Project page:\n  https://threedle.github.io/hyperfields/"},{"id":"http://arxiv.org/abs/2406.09395v1","updated":"2024-06-13T17:59:11Z","published":"2024-06-13T17:59:11Z","title":"Modeling Ambient Scene Dynamics for Free-view Synthesis","summary":"  We introduce a novel method for dynamic free-view synthesis of an ambient\nscenes from a monocular capture bringing a immersive quality to the viewing\nexperience. Our method builds upon the recent advancements in 3D Gaussian\nSplatting (3DGS) that can faithfully reconstruct complex static scenes.\nPrevious attempts to extend 3DGS to represent dynamics have been confined to\nbounded scenes or require multi-camera captures, and often fail to generalize\nto unseen motions, limiting their practical application. Our approach overcomes\nthese constraints by leveraging the periodicity of ambient motions to learn the\nmotion trajectory model, coupled with careful regularization. We also propose\nimportant practical strategies to improve the visual quality of the baseline\n3DGS static reconstructions and to improve memory efficiency critical for\nGPU-memory intensive learning. We demonstrate high-quality photorealistic novel\nview synthesis of several ambient natural scenes with intricate textures and\nfine structural elements.\n","authors":["Meng-Li Shih","Jia-Bin Huang","Changil Kim","Rajvi Shah","Johannes Kopf","Chen Gao"],"pdf_url":"https://arxiv.org/pdf/2406.09395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09394v1","updated":"2024-06-13T17:59:10Z","published":"2024-06-13T17:59:10Z","title":"WonderWorld: Interactive 3D Scene Generation from a Single Image","summary":"  We present WonderWorld, a novel framework for \\emph{interactive} 3D scene\nextrapolation that enables users to explore and shape virtual environments\nbased on a single input image and user-specified text. While significant\nimprovements have been made to the visual quality of scene generation, existing\nmethods are run offline, taking tens of minutes to hours to generate a scene.\nBy leveraging Fast Gaussian Surfels and a guided diffusion-based depth\nestimation method, WonderWorld generates geometrically consistent extrapolation\nwhile significantly reducing computational time. Our framework generates\nconnected and diverse 3D scenes in less than 10 seconds on a single A6000 GPU,\nenabling real-time user interaction and exploration. We demonstrate the\npotential of WonderWorld for applications in virtual reality, gaming, and\ncreative design, where users can quickly generate and navigate immersive,\npotentially infinite virtual worlds from a single image. Our approach\nrepresents a significant advancement in interactive 3D scene generation,\nopening up new possibilities for user-driven content creation and exploration\nin virtual environments. We will release full code and software for\nreproducibility. Project website: https://WonderWorld-2024.github.io/\n","authors":["Hong-Xing Yu","Haoyi Duan","Charles Herrmann","William T. Freeman","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2406.09394v1.pdf","comment":"Project website: https://WonderWorld-2024.github.io/"},{"id":"http://arxiv.org/abs/2406.09390v1","updated":"2024-06-13T17:59:05Z","published":"2024-06-13T17:59:05Z","title":"LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities\n  of Living","summary":"  Large Language Vision Models (LLVMs) have demonstrated effectiveness in\nprocessing internet videos, yet they struggle with the visually perplexing\ndynamics present in Activities of Daily Living (ADL) due to limited pertinent\ndatasets and models tailored to relevant cues. To this end, we propose a\nframework for curating ADL multiview datasets to fine-tune LLVMs, resulting in\nthe creation of ADL-X, comprising 100K RGB video-instruction pairs, language\ndescriptions, 3D skeletons, and action-conditioned object trajectories. We\nintroduce LLAVIDAL, an LLVM capable of incorporating 3D poses and relevant\nobject trajectories to understand the intricate spatiotemporal relationships\nwithin ADLs. Furthermore, we present a novel benchmark, ADLMCQ, for quantifying\nLLVM effectiveness in ADL scenarios. When trained on ADL-X, LLAVIDAL\nconsistently achieves state-of-the-art performance across all ADL evaluation\nmetrics. Qualitative analysis reveals LLAVIDAL's temporal reasoning\ncapabilities in understanding ADL. The link to the dataset is provided at:\nhttps://adl-x.github.io/\n","authors":["Rajatsubhra Chakraborty","Arkaprava Sinha","Dominick Reilly","Manish Kumar Govind","Pu Wang","Francois Bremond","Srijan Das"],"pdf_url":"https://arxiv.org/pdf/2406.09390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09389v1","updated":"2024-06-13T17:58:40Z","published":"2024-06-13T17:58:40Z","title":"Sagiri: Low Dynamic Range Image Enhancement with Generative Diffusion\n  Prior","summary":"  Capturing High Dynamic Range (HDR) scenery using 8-bit cameras often suffers\nfrom over-/underexposure, loss of fine details due to low bit-depth\ncompression, skewed color distributions, and strong noise in dark areas.\nTraditional LDR image enhancement methods primarily focus on color mapping,\nwhich enhances the visual representation by expanding the image's color range\nand adjusting the brightness. However, these approaches fail to effectively\nrestore content in dynamic range extremes, which are regions with pixel values\nclose to 0 or 255. To address the full scope of challenges in HDR imaging and\nsurpass the limitations of current models, we propose a novel two-stage\napproach. The first stage maps the color and brightness to an appropriate range\nwhile keeping the existing details, and the second stage utilizes a diffusion\nprior to generate content in dynamic range extremes lost during capture. This\ngenerative refinement module can also be used as a plug-and-play module to\nenhance and complement existing LDR enhancement models. The proposed method\nmarkedly improves the quality and details of LDR images, demonstrating superior\nperformance through rigorous experimental validation. The project page is at\nhttps://sagiri0208.github.io\n","authors":["Baiang Li","Sizhuo Ma","Yanhong Zeng","Xiaogang Xu","Youqing Fang","Zhao Zhang","Jian Wang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09389v1.pdf","comment":"https://sagiri0208.github.io"},{"id":"http://arxiv.org/abs/2406.09388v1","updated":"2024-06-13T17:58:39Z","published":"2024-06-13T17:58:39Z","title":"Exploring the Spectrum of Visio-Linguistic Compositionality and\n  Recognition","summary":"  Vision and language models (VLMs) such as CLIP have showcased remarkable\nzero-shot recognition abilities yet face challenges in visio-linguistic\ncompositionality, particularly in linguistic comprehension and fine-grained\nimage-text alignment. This paper explores the intricate relationship between\ncompositionality and recognition -- two pivotal aspects of VLM capability. We\nconduct a comprehensive evaluation of existing VLMs, covering both pre-training\napproaches aimed at recognition and the fine-tuning methods designed to improve\ncompositionality. Our evaluation employs 12 benchmarks for compositionality,\nalong with 21 zero-shot classification and two retrieval benchmarks for\nrecognition. In our analysis from 274 CLIP model checkpoints, we reveal\npatterns and trade-offs that emerge between compositional understanding and\nrecognition accuracy. Ultimately, this necessitates strategic efforts towards\ndeveloping models that improve both capabilities, as well as the meticulous\nformulation of benchmarks for compositionality. We open our evaluation\nframework at https://github.com/ytaek-oh/vl_compo.\n","authors":["Youngtaek Oh","Pyunghwan Ahn","Jinhyung Kim","Gwangmo Song","Soonyoung Lee","In So Kweon","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2406.09388v1.pdf","comment":"Accepted to CVPRW 2024 on 'What is Next in Multimodal Foundation\n  Models?'. Code: https://github.com/ytaek-oh/vl_compo"},{"id":"http://arxiv.org/abs/2406.09386v1","updated":"2024-06-13T17:58:32Z","published":"2024-06-13T17:58:32Z","title":"SimGen: Simulator-conditioned Driving Scene Generation","summary":"  Controllable synthetic data generation can substantially lower the annotation\ncost of training data in autonomous driving research and development. Prior\nworks use diffusion models to generate driving images conditioned on the 3D\nobject layout. However, those models are trained on small-scale datasets like\nnuScenes, which lack appearance and layout diversity. Moreover, the trained\nmodels can only generate images based on the real-world layout data from the\nvalidation set of the same dataset, where overfitting might happen. In this\nwork, we introduce a simulator-conditioned scene generation framework called\nSimGen that can learn to generate diverse driving scenes by mixing data from\nthe simulator and the real world. It uses a novel cascade diffusion pipeline to\naddress challenging sim-to-real gaps and multi-condition conflicts. A driving\nvideo dataset DIVA is collected to enhance the generative diversity of SimGen,\nwhich contains over 147.5 hours of real-world driving videos from 73 locations\nworldwide and simulated driving data from the MetaDrive simulator. SimGen\nachieves superior generation quality and diversity while preserving\ncontrollability based on the text prompt and the layout pulled from a\nsimulator. We further demonstrate the improvements brought by SimGen for\nsynthetic data augmentation on the BEV detection and segmentation task and\nshowcase its capability in safety-critical data generation. Code, data, and\nmodels will be made available.\n","authors":["Yunsong Zhou","Michael Simon","Zhenghao Peng","Sicheng Mo","Hongzi Zhu","Minyi Guo","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.09386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09385v1","updated":"2024-06-13T17:57:30Z","published":"2024-06-13T17:57:30Z","title":"Towards Vision-Language Geo-Foundation Model: A Survey","summary":"  Vision-Language Foundation Models (VLFMs) have made remarkable progress on\nvarious multimodal tasks, such as image captioning, image-text retrieval,\nvisual question answering, and visual grounding. However, most methods rely on\ntraining with general image datasets, and the lack of geospatial data leads to\npoor performance on earth observation. Numerous geospatial image-text pair\ndatasets and VLFMs fine-tuned on them have been proposed recently. These new\napproaches aim to leverage large-scale, multimodal geospatial data to build\nversatile intelligent models with diverse geo-perceptive capabilities, which we\nrefer to as Vision-Language Geo-Foundation Models (VLGFMs). This paper\nthoroughly reviews VLGFMs, summarizing and analyzing recent developments in the\nfield. In particular, we introduce the background and motivation behind the\nrise of VLGFMs, highlighting their unique research significance. Then, we\nsystematically summarize the core technologies employed in VLGFMs, including\ndata construction, model architectures, and applications of various multimodal\ngeospatial tasks. Finally, we conclude with insights, issues, and discussions\nregarding future research directions. To the best of our knowledge, this is the\nfirst comprehensive literature review of VLGFMs. We keep tracing related works\nat https://github.com/zytx121/Awesome-VLGFM.\n","authors":["Yue Zhou","Litong Feng","Yiping Ke","Xue Jiang","Junchi Yan","Xue Yang","Wayne Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.09385v1.pdf","comment":"18 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.09384v1","updated":"2024-06-13T17:57:10Z","published":"2024-06-13T17:57:10Z","title":"Reflecting on the State of Rehearsal-free Continual Learning with\n  Pretrained Models","summary":"  With the advent and recent ubiquity of foundation models, continual learning\n(CL) has recently shifted from continual training from scratch to the continual\nadaptation of pretrained models, seeing particular success on rehearsal-free CL\nbenchmarks (RFCL). To achieve this, most proposed methods adapt and restructure\nparameter-efficient finetuning techniques (PEFT) to suit the continual nature\nof the problem. Based most often on input-conditional query-mechanisms or\nregularizations on top of prompt- or adapter-based PEFT, these PEFT-style RFCL\n(P-RFCL) approaches report peak performances; often convincingly outperforming\nexisting CL techniques. However, on the other end, critical studies have\nrecently highlighted competitive results by training on just the first task or\nvia simple non-parametric baselines. Consequently, questions arise about the\nrelationship between methodological choices in P-RFCL and their reported high\nbenchmark scores. In this work, we tackle these questions to better understand\nthe true drivers behind strong P-RFCL performances, their placement w.r.t.\nrecent first-task adaptation studies, and their relation to preceding CL\nstandards such as EWC or SI. In particular, we show: (1) P-RFCL techniques\nrelying on input-conditional query mechanisms work not because, but rather\ndespite them by collapsing towards standard PEFT shortcut solutions. (2)\nIndeed, we show how most often, P-RFCL techniques can be matched by a simple\nand lightweight PEFT baseline. (3) Using this baseline, we identify the\nimplicit bound on tunable parameters when deriving RFCL approaches from PEFT\nmethods as a potential denominator behind P-RFCL efficacy. Finally, we (4)\nbetter disentangle continual versus first-task adaptation, and (5) motivate\nstandard RFCL techniques s.a. EWC or SI in light of recent P-RFCL methods.\n","authors":["Lukas Thede","Karsten Roth","Olivier J. Hénaff","Matthias Bethge","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2406.09384v1.pdf","comment":"3rd Conference on Lifelong Learning Agents (CoLLAs) 2024"},{"id":"http://arxiv.org/abs/2406.09383v1","updated":"2024-06-13T17:56:56Z","published":"2024-06-13T17:56:56Z","title":"Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset","summary":"  Large-scale datasets have fueled recent advancements in AI-based autonomous\nvehicle research. However, these datasets are usually collected from a single\nvehicle's one-time pass of a certain location, lacking multiagent interactions\nor repeated traversals of the same place. Such information could lead to\ntransformative enhancements in autonomous vehicles' perception, prediction, and\nplanning capabilities. To bridge this gap, in collaboration with the\nself-driving company May Mobility, we present the MARS dataset which unifies\nscenarios that enable MultiAgent, multitraveRSal, and multimodal autonomous\nvehicle research. More specifically, MARS is collected with a fleet of\nautonomous vehicles driving within a certain geographical area. Each vehicle\nhas its own route and different vehicles may appear at nearby locations. Each\nvehicle is equipped with a LiDAR and surround-view RGB cameras. We curate two\nsubsets in MARS: one facilitates collaborative driving with multiple vehicles\nsimultaneously present at the same location, and the other enables memory\nretrospection through asynchronous traversals of the same location by multiple\nvehicles. We conduct experiments in place recognition and neural\nreconstruction. More importantly, MARS introduces new research opportunities\nand challenges such as multitraversal 3D reconstruction, multiagent perception,\nand unsupervised object discovery. Our data and codes can be found at\nhttps://ai4ce.github.io/MARS/.\n","authors":["Yiming Li","Zhiheng Li","Nuo Chen","Moonjun Gong","Zonglin Lyu","Zehong Wang","Peili Jiang","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2406.09383v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2405.15891v2","updated":"2024-06-13T17:56:53Z","published":"2024-05-24T19:22:09Z","title":"Score Distillation via Reparametrized DDIM","summary":"  While 2D diffusion models generate realistic, high-detail images, 3D shape\ngeneration methods like Score Distillation Sampling (SDS) built on these 2D\ndiffusion models produce cartoon-like, over-smoothed shapes. To help explain\nthis discrepancy, we show that the image guidance used in Score Distillation\ncan be understood as the velocity field of a 2D denoising generative process,\nup to the choice of a noise term. In particular, after a change of variables,\nSDS resembles a high-variance version of Denoising Diffusion Implicit Models\n(DDIM) with a differently-sampled noise term: SDS introduces noise i.i.d.\nrandomly at each step, while DDIM infers it from the previous noise\npredictions. This excessive variance can lead to over-smoothing and unrealistic\noutputs. We show that a better noise approximation can be recovered by\ninverting DDIM in each SDS update step. This modification makes SDS's\ngenerative process for 2D images almost identical to DDIM. In 3D, it removes\nover-smoothing, preserves higher-frequency detail, and brings the generation\nquality closer to that of 2D samplers. Experimentally, our method achieves\nbetter or similar 3D generation quality compared to other state-of-the-art\nScore Distillation methods, all without training additional neural networks or\nmulti-view supervision, and providing useful insights into relationship between\n2D and 3D asset generation with diffusion models.\n","authors":["Artem Lukoianov","Haitz Sáez de Ocáriz Borde","Kristjan Greenewald","Vitor Campagnolo Guizilini","Timur Bagautdinov","Vincent Sitzmann","Justin Solomon"],"pdf_url":"https://arxiv.org/pdf/2405.15891v2.pdf","comment":"Preprint. 25 pages, 26 figures. Revision : added missed comparisons,\n  fixed typos, fixed PDF compatibility issues"},{"id":"http://arxiv.org/abs/2406.09377v1","updated":"2024-06-13T17:54:38Z","published":"2024-06-13T17:54:38Z","title":"GGHead: Fast and Generalizable 3D Gaussian Heads","summary":"  Learning 3D head priors from large 2D image collections is an important step\ntowards high-quality 3D-aware human modeling. A core requirement is an\nefficient architecture that scales well to large-scale datasets and large image\nresolutions. Unfortunately, existing 3D GANs struggle to scale to generate\nsamples at high resolutions due to their relatively slow train and render\nspeeds, and typically have to rely on 2D superresolution networks at the\nexpense of global 3D consistency. To address these challenges, we propose\nGenerative Gaussian Heads (GGHead), which adopts the recent 3D Gaussian\nSplatting representation within a 3D GAN framework. To generate a 3D\nrepresentation, we employ a powerful 2D CNN generator to predict Gaussian\nattributes in the UV space of a template head mesh. This way, GGHead exploits\nthe regularity of the template's UV layout, substantially facilitating the\nchallenging task of predicting an unstructured set of 3D Gaussians. We further\nimprove the geometric fidelity of the generated 3D representations with a novel\ntotal variation loss on rendered UV coordinates. Intuitively, this\nregularization encourages that neighboring rendered pixels should stem from\nneighboring Gaussians in the template's UV space. Taken together, our pipeline\ncan efficiently generate 3D heads trained only from single-view 2D image\nobservations. Our proposed framework matches the quality of existing 3D head\nGANs on FFHQ while being both substantially faster and fully 3D consistent. As\na result, we demonstrate real-time generation and rendering of high-quality\n3D-consistent heads at $1024^2$ resolution for the first time.\n","authors":["Tobias Kirschstein","Simon Giebenhain","Jiapeng Tang","Markos Georgopoulos","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2406.09377v1.pdf","comment":"Project Page: https://tobias-kirschstein.github.io/gghead/ ; YouTube\n  Video: https://www.youtube.com/watch?v=1iyC74neQXc"},{"id":"http://arxiv.org/abs/2406.09374v1","updated":"2024-06-13T17:52:47Z","published":"2024-06-13T17:52:47Z","title":"Scale-Invariant Monocular Depth Estimation via SSI Depth","summary":"  Existing methods for scale-invariant monocular depth estimation (SI MDE)\noften struggle due to the complexity of the task, and limited and non-diverse\ndatasets, hindering generalizability in real-world scenarios. This is while\nshift-and-scale-invariant (SSI) depth estimation, simplifying the task and\nenabling training with abundant stereo datasets achieves high performance. We\npresent a novel approach that leverages SSI inputs to enhance SI depth\nestimation, streamlining the network's role and facilitating in-the-wild\ngeneralization for SI depth estimation while only using a synthetic dataset for\ntraining. Emphasizing the generation of high-resolution details, we introduce a\nnovel sparse ordinal loss that substantially improves detail generation in SSI\nMDE, addressing critical limitations in existing approaches. Through\nin-the-wild qualitative examples and zero-shot evaluation we substantiate the\npractical utility of our approach in computational photography applications,\nshowcasing its ability to generate highly detailed SI depth maps and achieve\ngeneralization in diverse scenarios.\n","authors":["S. Mahdi H. Miangoleh","Mahesh Reddy","Yağız Aksoy"],"pdf_url":"https://arxiv.org/pdf/2406.09374v1.pdf","comment":"To appear in Proc. SIGGRAPH, 2024. Project webpage:\n  https://yaksoy.github.io/sidepth/"},{"id":"http://arxiv.org/abs/2406.09371v1","updated":"2024-06-13T17:51:00Z","published":"2024-06-13T17:51:00Z","title":"LRM-Zero: Training Large Reconstruction Models with Synthesized Data","summary":"  We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on\nsynthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The\ncore of LRM-Zero is our procedural 3D dataset, Zeroverse, which is\nautomatically synthesized from simple primitive shapes with random texturing\nand augmentations (e.g., height fields, boolean differences, and wireframes).\nUnlike previous 3D datasets (e.g., Objaverse) which are often captured or\ncrafted by humans to approximate real 3D data, Zeroverse completely ignores\nrealistic global semantics but is rich in complex geometric and texture details\nthat are locally similar to or even more intricate than real objects. We\ndemonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse,\ncan achieve high visual quality in the reconstruction of real-world objects,\ncompetitive with models trained on Objaverse. We also analyze several critical\ndesign choices of Zeroverse that contribute to LRM-Zero's capability and\ntraining stability. Our work demonstrates that 3D reconstruction, one of the\ncore tasks in 3D vision, can potentially be addressed without the semantics of\nreal-world objects. The Zeroverse's procedural synthesis code and interactive\nvisualization are available at: https://desaixie.github.io/lrm-zero/.\n","authors":["Desai Xie","Sai Bi","Zhixin Shu","Kai Zhang","Zexiang Xu","Yi Zhou","Sören Pirk","Arie Kaufman","Xin Sun","Hao Tan"],"pdf_url":"https://arxiv.org/pdf/2406.09371v1.pdf","comment":"23 pages, 8 figures. Our code and interactive visualization are\n  available at: https://desaixie.github.io/lrm-zero/"},{"id":"http://arxiv.org/abs/2406.09368v1","updated":"2024-06-13T17:50:28Z","published":"2024-06-13T17:50:28Z","title":"CLIPAway: Harmonizing Focused Embeddings for Removing Objects via\n  Diffusion Models","summary":"  Advanced image editing techniques, particularly inpainting, are essential for\nseamlessly removing unwanted elements while preserving visual integrity.\nTraditional GAN-based methods have achieved notable success, but recent\nadvancements in diffusion models have produced superior results due to their\ntraining on large-scale datasets, enabling the generation of remarkably\nrealistic inpainted images. Despite their strengths, diffusion models often\nstruggle with object removal tasks without explicit guidance, leading to\nunintended hallucinations of the removed object. To address this issue, we\nintroduce CLIPAway, a novel approach leveraging CLIP embeddings to focus on\nbackground regions while excluding foreground elements. CLIPAway enhances\ninpainting accuracy and quality by identifying embeddings that prioritize the\nbackground, thus achieving seamless object removal. Unlike other methods that\nrely on specialized training datasets or costly manual annotations, CLIPAway\nprovides a flexible, plug-and-play solution compatible with various\ndiffusion-based inpainting techniques.\n","authors":["Yigit Ekin","Ahmet Burak Yildirim","Erdem Eren Caglar","Aykut Erdem","Erkut Erdem","Aysegul Dundar"],"pdf_url":"https://arxiv.org/pdf/2406.09368v1.pdf","comment":"Project page: https://yigitekin.github.io/CLIPAway/"},{"id":"http://arxiv.org/abs/2406.09367v1","updated":"2024-06-13T17:50:05Z","published":"2024-06-13T17:50:05Z","title":"Needle In A Video Haystack: A Scalable Synthetic Framework for\n  Benchmarking Video MLLMs","summary":"  Video understanding is a crucial next step for multimodal large language\nmodels (MLLMs). To probe specific aspects of video understanding ability,\nexisting video benchmarks typically require careful video selection based on\nthe target capability, along with laborious annotation of query-response pairs\nto match the specific video content. This process is both challenging and\nresource-intensive. In this paper, we propose VideoNIAH (Video Needle In A\nHaystack), a benchmark construction framework through synthetic video\ngeneration. VideoNIAH decouples test video content from their query-responses\nby inserting unrelated image/text 'needles' into original videos. It generates\nannotations solely from these needles, ensuring diversity in video sources and\na variety of query-responses. Additionally, by inserting multiple needles,\nVideoNIAH rigorously evaluates the temporal understanding capabilities of\nmodels. We utilized VideoNIAH to compile a video benchmark VNBench, including\ntasks such as retrieval, ordering, and counting. VNBench can efficiently\nevaluate the fine-grained understanding ability and spatio-temporal modeling\nability of a video model, while also supporting the long-context evaluation.\nAdditionally, we evaluated recent video-centric multimodal large language\nmodels (MLLMs), both open-source and proprietary, providing a comprehensive\nanalysis. We found that although proprietary models have significant advantages\nover open-source models, all existing video models still perform poorly on\nlong-distance dependency tasks. VideoNIAH is a simple yet highly scalable\nbenchmark construction framework, and we believe it will inspire future video\nbenchmark works. The code and data are available at\nhttps://github.com/joez17/VideoNIAH.\n","authors":["Zijia Zhao","Haoyu Lu","Yuqi Huo","Yifan Du","Tongtian Yue","Longteng Guo","Bingning Wang","Weipeng Chen","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2406.09367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09366v1","updated":"2024-06-13T17:49:56Z","published":"2024-06-13T17:49:56Z","title":"Towards an Improved Understanding and Utilization of Maximum Manifold\n  Capacity Representations","summary":"  Maximum Manifold Capacity Representations (MMCR) is a recent multi-view\nself-supervised learning (MVSSL) method that matches or surpasses other leading\nMVSSL methods. MMCR is intriguing because it does not fit neatly into any of\nthe commonplace MVSSL lineages, instead originating from a statistical\nmechanical perspective on the linear separability of data manifolds. In this\npaper, we seek to improve our understanding and our utilization of MMCR. To\nbetter understand MMCR, we leverage tools from high dimensional probability to\ndemonstrate that MMCR incentivizes alignment and uniformity of learned\nembeddings. We then leverage tools from information theory to show that such\nembeddings maximize a well-known lower bound on mutual information between\nviews, thereby connecting the geometric perspective of MMCR to the\ninformation-theoretic perspective commonly discussed in MVSSL. To better\nutilize MMCR, we mathematically predict and experimentally confirm\nnon-monotonic changes in the pretraining loss akin to double descent but with\nrespect to atypical hyperparameters. We also discover compute scaling laws that\nenable predicting the pretraining loss as a function of gradients steps, batch\nsize, embedding dimension and number of views. We then show that MMCR,\noriginally applied to image data, is performant on multimodal image-text data.\nBy more deeply understanding the theoretical and empirical behavior of MMCR,\nour work reveals insights on improving MVSSL methods.\n","authors":["Rylan Schaeffer","Victor Lecomte","Dhruv Bhandarkar Pai","Andres Carranza","Berivan Isik","Alyssa Unell","Mikail Khona","Thomas Yerxa","Yann LeCun","SueYeon Chung","Andrey Gromov","Ravid Shwartz-Ziv","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2406.09366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09356v1","updated":"2024-06-13T17:41:37Z","published":"2024-06-13T17:41:37Z","title":"CMC-Bench: Towards a New Paradigm of Visual Signal Compression","summary":"  Ultra-low bitrate image compression is a challenging and demanding topic.\nWith the development of Large Multimodal Models (LMMs), a Cross Modality\nCompression (CMC) paradigm of Image-Text-Image has emerged. Compared with\ntraditional codecs, this semantic-level compression can reduce image data size\nto 0.1\\% or even lower, which has strong potential applications. However, CMC\nhas certain defects in consistency with the original image and perceptual\nquality. To address this problem, we introduce CMC-Bench, a benchmark of the\ncooperative performance of Image-to-Text (I2T) and Text-to-Image (T2I) models\nfor image compression. This benchmark covers 18,000 and 40,000 images\nrespectively to verify 6 mainstream I2T and 12 T2I models, including 160,000\nsubjective preference scores annotated by human experts. At ultra-low bitrates,\nthis paper proves that the combination of some I2T and T2I models has surpassed\nthe most advanced visual signal codecs; meanwhile, it highlights where LMMs can\nbe further optimized toward the compression task. We encourage LMM developers\nto participate in this test to promote the evolution of visual signal codec\nprotocols.\n","authors":["Chunyi Li","Xiele Wu","Haoning Wu","Donghui Feng","Zicheng Zhang","Guo Lu","Xiongkuo Min","Xiaohong Liu","Guangtao Zhai","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2406.09356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09353v1","updated":"2024-06-13T17:40:15Z","published":"2024-06-13T17:40:15Z","title":"Enhancing Domain Adaptation through Prompt Gradient Alignment","summary":"  Prior Unsupervised Domain Adaptation (UDA) methods often aim to train a\ndomain-invariant feature extractor, which may hinder the model from learning\nsufficiently discriminative features. To tackle this, a line of works based on\nprompt learning leverages the power of large-scale pre-trained vision-language\nmodels to learn both domain-invariant and specific features through a set of\ndomain-agnostic and domain-specific learnable prompts. Those studies typically\nenforce invariant constraints on representation, output, or prompt space to\nlearn such prompts. Differently, we cast UDA as a multiple-objective\noptimization problem in which each objective is represented by a domain loss.\nUnder this new framework, we propose aligning per-objective gradients to foster\nconsensus between them. Additionally, to prevent potential overfitting when\nfine-tuning this deep learning architecture, we penalize the norm of these\ngradients. To achieve these goals, we devise a practical gradient update\nprocedure that can work under both single-source and multi-source UDA.\nEmpirically, our method consistently surpasses other prompt-based baselines by\na large margin on different UDA benchmarks\n","authors":["Hoang Phan","Lam Tran","Quyen Tran","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2406.09353v1.pdf","comment":"26 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.08474v2","updated":"2024-06-13T17:38:12Z","published":"2024-06-12T17:57:06Z","title":"Real2Code: Reconstruct Articulated Objects via Code Generation","summary":"  We present Real2Code, a novel approach to reconstructing articulated objects\nvia code generation. Given visual observations of an object, we first\nreconstruct its part geometry using an image segmentation model and a shape\ncompletion model. We then represent the object parts with oriented bounding\nboxes, which are input to a fine-tuned large language model (LLM) to predict\njoint articulation as code. By leveraging pre-trained vision and language\nmodels, our approach scales elegantly with the number of articulated parts, and\ngeneralizes from synthetic training data to real world objects in unstructured\nenvironments. Experimental results demonstrate that Real2Code significantly\noutperforms previous state-of-the-art in reconstruction accuracy, and is the\nfirst approach to extrapolate beyond objects' structural complexity in the\ntraining set, and reconstructs objects with up to 10 articulated parts. When\nincorporated with a stereo reconstruction model, Real2Code also generalizes to\nreal world objects from a handful of multi-view RGB images, without the need\nfor depth or camera information.\n","authors":["Zhao Mandi","Yijia Weng","Dominik Bauer","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2406.08474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09335v1","updated":"2024-06-13T17:17:31Z","published":"2024-06-13T17:17:31Z","title":"Instance-level quantitative saliency in multiple sclerosis lesion\n  segmentation","summary":"  In recent years, explainable methods for artificial intelligence (XAI) have\ntried to reveal and describe models' decision mechanisms in the case of\nclassification tasks. However, XAI for semantic segmentation and in particular\nfor single instances has been little studied to date. Understanding the process\nunderlying automatic segmentation of single instances is crucial to reveal what\ninformation was used to detect and segment a given object of interest. In this\nstudy, we proposed two instance-level explanation maps for semantic\nsegmentation based on SmoothGrad and Grad-CAM++ methods. Then, we investigated\ntheir relevance for the detection and segmentation of white matter lesions\n(WML), a magnetic resonance imaging (MRI) biomarker in multiple sclerosis (MS).\n687 patients diagnosed with MS for a total of 4043 FLAIR and MPRAGE MRI scans\nwere collected at the University Hospital of Basel, Switzerland. Data were\nrandomly split into training, validation and test sets to train a 3D U-Net for\nMS lesion segmentation. We observed 3050 true positive (TP), 1818 false\npositive (FP), and 789 false negative (FN) cases. We generated instance-level\nexplanation maps for semantic segmentation, by developing two XAI methods based\non SmoothGrad and Grad-CAM++. We investigated: 1) the distribution of gradients\nin saliency maps with respect to both input MRI sequences; 2) the model's\nresponse in the case of synthetic lesions; 3) the amount of perilesional tissue\nneeded by the model to segment a lesion. Saliency maps (based on SmoothGrad) in\nFLAIR showed positive values inside a lesion and negative in its neighborhood.\nPeak values of saliency maps generated for these four groups of volumes\npresented distributions that differ significantly from one another, suggesting\na quantitative nature of the proposed saliency. Contextual information of 7mm\naround the lesion border was required for their segmentation.\n","authors":["Federico Spagnolo","Nataliia Molchanova","Roger Schaer","Meritxell Bach Cuadra","Mario Ocampo Pineda","Lester Melie-Garcia","Cristina Granziera","Vincent Andrearczyk","Adrien Depeursinge"],"pdf_url":"https://arxiv.org/pdf/2406.09335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09333v1","updated":"2024-06-13T17:14:30Z","published":"2024-06-13T17:14:30Z","title":"Memory-Efficient Sparse Pyramid Attention Networks for Whole Slide Image\n  Analysis","summary":"  Whole Slide Images (WSIs) are crucial for modern pathological diagnosis, yet\ntheir gigapixel-scale resolutions and sparse informative regions pose\nsignificant computational challenges. Traditional dense attention mechanisms,\nwidely used in computer vision and natural language processing, are impractical\nfor WSI analysis due to the substantial data scale and the redundant processing\nof uninformative areas. To address these challenges, we propose\nMemory-Efficient Sparse Pyramid Attention Networks with Shifted Windows (SPAN),\ndrawing inspiration from state-of-the-art sparse attention techniques in other\ndomains. SPAN introduces a sparse pyramid attention architecture that\nhierarchically focuses on informative regions within the WSI, aiming to reduce\nmemory overhead while preserving critical features. Additionally, the\nincorporation of shifted windows enables the model to capture long-range\ncontextual dependencies essential for accurate classification. We evaluated\nSPAN on multiple public WSI datasets, observing its competitive performance.\nUnlike existing methods that often struggle to model spatial and contextual\ninformation due to memory constraints, our approach enables the accurate\nmodeling of these crucial features. Our study also highlights the importance of\nkey design elements in attention mechanisms, such as the shifted-window scheme\nand the hierarchical structure, which contribute substantially to the\neffectiveness of SPAN in WSI analysis. The potential of SPAN for\nmemory-efficient and effective analysis of WSI data is thus demonstrated, and\nthe code will be made publicly available following the publication of this\nwork.\n","authors":["Weiyi Wu","Chongyang Gao","Xinwen Xu","Siting Li","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2406.09333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09327v1","updated":"2024-06-13T17:06:15Z","published":"2024-06-13T17:06:15Z","title":"Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN\n  Pipeline applied on PSMA PET/CT Scans","summary":"  Assessing tumor response to systemic therapies is one of the main\napplications of PET/CT. Routinely, only a small subset of index lesions out of\nmultiple lesions is analyzed. However, this operator dependent selection may\nbias the results due to possible significant inter-metastatic heterogeneity of\nresponse to therapy. Automated, AI based approaches for lesion tracking hold\npromise in enabling the analysis of many more lesions and thus providing a\nbetter assessment of tumor response. This work introduces a Siamese CNN\napproach for lesion tracking between PET/CT scans. Our approach is applied on\nthe laborious task of tracking a high number of bone lesions in full-body\nbaseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cycles\nof [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancer\npatients. Data preparation includes lesion segmentation and affine\nregistration. Our algorithm extracts suitable lesion patches and forwards them\ninto a Siamese CNN trained to classify the lesion patch pairs as corresponding\nor non-corresponding lesions. Experiments have been performed with different\ninput patch types and a Siamese network in 2D and 3D. The CNN model\nsuccessfully learned to classify lesion assignments, reaching a lesion tracking\naccuracy of 83 % in its best configuration with an AUC = 0.91. For remaining\nlesions the pipeline accomplished a re-identification rate of 89 %. We proved\nthat a CNN may facilitate the tracking of multiple lesions in PSMA PET/CT\nscans. Future clinical studies are necessary if this improves the prediction of\nthe outcome of therapies.\n","authors":["Stefan P. Hein","Manuel Schultheiss","Andrei Gafita","Raphael Zaum","Farid Yagubbayli","Isabel Rauscher","Matthias Eiber","Franz Pfeiffer","Wolfgang A. Weber"],"pdf_url":"https://arxiv.org/pdf/2406.09327v1.pdf","comment":"25 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2306.04539v2","updated":"2024-06-13T17:05:54Z","published":"2023-06-07T15:44:53Z","title":"Multimodal Learning Without Labeled Multimodal Data: Guarantees and\n  Applications","summary":"  In many machine learning systems that jointly learn from multiple modalities,\na core research question is to understand the nature of multimodal\ninteractions: how modalities combine to provide new task-relevant information\nthat was not present in either alone. We study this challenge of interaction\nquantification in a semi-supervised setting with only labeled unimodal data and\nnaturally co-occurring multimodal data (e.g., unlabeled images and captions,\nvideo and corresponding audio) but when labeling them is time-consuming. Using\na precise information-theoretic definition of interactions, our key\ncontribution is the derivation of lower and upper bounds to quantify the amount\nof multimodal interactions in this semi-supervised setting. We propose two\nlower bounds: one based on the shared information between modalities and the\nother based on disagreement between separately trained unimodal classifiers,\nand derive an upper bound through connections to approximate algorithms for\nmin-entropy couplings. We validate these estimated bounds and show how they\naccurately track true interactions. Finally, we show how these theoretical\nresults can be used to estimate multimodal model performance, guide data\ncollection, and select appropriate multimodal models for various tasks.\n","authors":["Paul Pu Liang","Chun Kai Ling","Yun Cheng","Alex Obolenskiy","Yudong Liu","Rohan Pandey","Alex Wilf","Louis-Philippe Morency","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2306.04539v2.pdf","comment":"ICLR 2024, Code available at: https://github.com/pliang279/PID"},{"id":"http://arxiv.org/abs/2406.09326v1","updated":"2024-06-13T17:05:23Z","published":"2024-06-13T17:05:23Z","title":"PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in\n  Piano Performance","summary":"  Recently, artificial intelligence techniques for education have been received\nincreasing attentions, while it still remains an open problem to design the\neffective music instrument instructing systems. Although key presses can be\ndirectly derived from sheet music, the transitional movements among key presses\nrequire more extensive guidance in piano performance. In this work, we\nconstruct a piano-hand motion generation benchmark to guide hand movements and\nfingerings for piano playing. To this end, we collect an annotated dataset,\nPianoMotion10M, consisting of 116 hours of piano playing videos from a\nbird's-eye view with 10 million annotated hand poses. We also introduce a\npowerful baseline model that generates hand motions from piano audios through a\nposition predictor and a position-guided gesture generator. Furthermore, a\nseries of evaluation metrics are designed to assess the performance of the\nbaseline model, including motion similarity, smoothness, positional accuracy of\nleft and right hands, and overall fidelity of movement distribution. Despite\nthat piano key presses with respect to music scores or audios are already\naccessible, PianoMotion10M aims to provide guidance on piano fingering for\ninstruction purposes. The dataset and source code can be accessed at\nhttps://agnjason.github.io/PianoMotion-page.\n","authors":["Qijun Gan","Song Wang","Shengtao Wu","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.09326v1.pdf","comment":"Codes and Dataset: https://agnjason.github.io/PianoMotion-page"},{"id":"http://arxiv.org/abs/2406.09317v1","updated":"2024-06-13T16:53:57Z","published":"2024-06-13T16:53:57Z","title":"Common and Rare Fundus Diseases Identification Using Vision-Language\n  Foundation Model with Knowledge of Over 400 Diseases","summary":"  The current retinal artificial intelligence models were trained using data\nwith a limited category of diseases and limited knowledge. In this paper, we\npresent a retinal vision-language foundation model (RetiZero) with knowledge of\nover 400 fundus diseases. Specifically, we collected 341,896 fundus images\npaired with text descriptions from 29 publicly available datasets, 180\nophthalmic books, and online resources, encompassing over 400 fundus diseases\nacross multiple countries and ethnicities. RetiZero achieved outstanding\nperformance across various downstream tasks, including zero-shot retinal\ndisease recognition, image-to-image retrieval, internal domain and cross-domain\nretinal disease classification, and few-shot fine-tuning. Specially, in the\nzero-shot scenario, RetiZero achieved a Top5 score of 0.8430 and 0.7561 on 15\nand 52 fundus diseases respectively. In the image-retrieval task, RetiZero\nachieved a Top5 score of 0.9500 and 0.8860 on 15 and 52 retinal diseases\nrespectively. Furthermore, clinical evaluations by ophthalmology experts from\ndifferent countries demonstrate that RetiZero can achieve performance\ncomparable to experienced ophthalmologists using zero-shot and image retrieval\nmethods without requiring model retraining. These capabilities of retinal\ndisease identification strengthen our RetiZero foundation model in clinical\nimplementation.\n","authors":["Meng Wang","Tian Lin","Kai Yu","Aidi Lin","Yuanyuan Peng","Lianyu Wang","Cheng Chen","Ke Zou","Huiyu Liang","Man Chen","Xue Yao","Meiqin Zhang","Binwei Huang","Chaoxin Zheng","Wei Chen","Yilong Luo","Yifan Chen","Jingcheng Wang","Yih Chung Tham","Dianbo Liu","Wendy Wong","Sahil Thakur","Beau Fenner","Yanda Meng","Yukun Zhou","Zehua Jiang","Minghui Qiu","Changqing Zhang","Xinjian Chen","Sophia Y. Wang","Cecilia S. Lee","Lucia Sobrin","Pearse A. Keane","Ching-Yu Cheng","Haoyu Chen","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2406.09317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14014v3","updated":"2024-06-13T16:51:50Z","published":"2024-05-22T21:48:17Z","title":"RadarOcc: Robust 3D Occupancy Prediction with 4D Imaging Radar","summary":"  3D occupancy-based perception pipeline has significantly advanced autonomous\ndriving by capturing detailed scene descriptions and demonstrating strong\ngeneralizability across various object categories and shapes. Current methods\npredominantly rely on LiDAR or camera inputs for 3D occupancy prediction. These\nmethods are susceptible to adverse weather conditions, limiting the all-weather\ndeployment of self-driving cars. To improve perception robustness, we leverage\nthe recent advances in automotive radars and introduce a novel approach that\nutilizes 4D imaging radar sensors for 3D occupancy prediction. Our method,\nRadarOcc, circumvents the limitations of sparse radar point clouds by directly\nprocessing the 4D radar tensor, thus preserving essential scene details.\nRadarOcc innovatively addresses the challenges associated with the voluminous\nand noisy 4D radar data by employing Doppler bins descriptors, sidelobe-aware\nspatial sparsification, and range-wise self-attention mechanisms. To minimize\nthe interpolation errors associated with direct coordinate transformations, we\nalso devise a spherical-based feature encoding followed by\nspherical-to-Cartesian feature aggregation. We benchmark various baseline\nmethods based on distinct modalities on the public K-Radar dataset. The results\ndemonstrate RadarOcc's state-of-the-art performance in radar-based 3D occupancy\nprediction and promising results even when compared with LiDAR- or camera-based\nmethods. Additionally, we present qualitative evidence of the superior\nperformance of 4D radar in adverse weather conditions and explore the impact of\nkey pipeline components through ablation studies.\n","authors":["Fangqiang Ding","Xiangyu Wen","Lawrence Zhu","Yiming Li","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2405.14014v3.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.09315v1","updated":"2024-06-13T16:51:33Z","published":"2024-06-13T16:51:33Z","title":"Vertical LoRA: Dense Expectation-Maximization Interpretation of\n  Transformers","summary":"  In this paper, we show how Transformers can be interpreted as dense\nExpectation-Maximization algorithms performed on Bayesian Nets. Based on the\nabove interpretation, we propose a new model design paradigm, namely Vertical\nLoRA (VLoRA), which reduces the parameter count dramatically while preserving\nperformance. In VLoRA, a model consists of layers, each of which recursively\nlearns an increment based on the previous layer. We then apply LoRA\ndecomposition to the increments. VLoRA works on the base model, which is\northogonal to LoRA, meaning they can be used together. We do experiments on\nvarious tasks and models. The results show that 1) with VLoRA, the Transformer\nmodel parameter count can be reduced dramatically and 2) the performance of the\noriginal model is preserved. The source code is available at\n\\url{https://github.com/neverUseThisName/vlora}\n","authors":["Zhuolin Fu"],"pdf_url":"https://arxiv.org/pdf/2406.09315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09871v3","updated":"2024-06-13T16:51:26Z","published":"2024-03-14T21:01:06Z","title":"ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric\n  Thermal Images","summary":"  In this work, we present ThermoHands, a new benchmark for thermal image-based\negocentric 3D hand pose estimation, aimed at overcoming challenges like varying\nlighting conditions and obstructions (e.g., handwear). The benchmark includes a\nmulti-view and multi-spectral dataset collected from 28 subjects performing\nhand-object and hand-virtual interactions under diverse scenarios, accurately\nannotated with 3D hand poses through an automated process. We introduce a new\nbaseline method, TherFormer, utilizing dual transformer modules for effective\negocentric 3D hand pose estimation in thermal imagery. Our experimental results\nhighlight TherFormer's leading performance and affirm thermal imaging's\neffectiveness in enabling robust 3D hand pose estimation in adverse conditions.\n","authors":["Fangqiang Ding","Lawrence Zhu","Xiangyu Wen","Gaowen Liu","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2403.09871v3.pdf","comment":"15 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.11530v2","updated":"2024-06-13T16:49:57Z","published":"2024-02-18T10:09:10Z","title":"Efficient Multimodal Learning from Data-centric Perspective","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated notable\ncapabilities in general visual understanding and reasoning tasks. However,\ntheir deployment is hindered by substantial computational costs in both\ntraining and inference, limiting accessibility to the broader research and user\ncommunities. A straightforward solution is to leverage smaller pre-trained\nvision and language models, which inevitably cause significant performance\ndrops. In this paper, we demonstrate the possibility of training a smaller but\nbetter MLLM with high-quality training data. Specifically, we introduce Bunny,\na family of lightweight MLLMs with flexible vision and language backbones for\nefficient multimodal learning from selected training data. Experiments show\nthat our Bunny-4B/8B outperforms the state-of-the-art large MLLMs on multiple\nbenchmarks. We expect that this work can provide the community with a clean and\nflexible open-source tool for further research and development. The code,\nmodels, and data can be found in https://github.com/BAAI-DCAI/Bunny.\n","authors":["Muyang He","Yexin Liu","Boya Wu","Jianhao Yuan","Yueze Wang","Tiejun Huang","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.11530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04470v2","updated":"2024-06-13T16:46:22Z","published":"2024-06-06T19:50:33Z","title":"DiffuSyn Bench: Evaluating Vision-Language Models on Real-World\n  Complexities with Diffusion-Generated Synthetic Benchmarks","summary":"  This study assesses the ability of Large Vision-Language Models (LVLMs) to\ndifferentiate between AI-generated and human-generated images. It introduces a\nnew automated benchmark construction method for this evaluation. The experiment\ncompared common LVLMs with human participants using a mixed dataset of AI and\nhuman-created images. Results showed that LVLMs could distinguish between the\nimage types to some extent but exhibited a rightward bias, and perform\nsignificantly worse compared to humans. To build on these findings, we\ndeveloped an automated benchmark construction process using AI. This process\ninvolved topic retrieval, narrative script generation, error embedding, and\nimage generation, creating a diverse set of text-image pairs with intentional\nerrors. We validated our method through constructing two caparable benchmarks.\nThis study highlights the strengths and weaknesses of LVLMs in real-world\nunderstanding and advances benchmark construction techniques, providing a\nscalable and automatic approach for AI model evaluation.\n","authors":["Haokun Zhou","Yipeng Hong"],"pdf_url":"https://arxiv.org/pdf/2406.04470v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09305v1","updated":"2024-06-13T16:40:39Z","published":"2024-06-13T16:40:39Z","title":"Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven\n  Text-to-Image Generation","summary":"  In subject-driven text-to-image generation, recent works have achieved\nsuperior performance by training the model on synthetic datasets containing\nnumerous image pairs. Trained on these datasets, generative models can produce\ntext-aligned images for specific subject from arbitrary testing image in a\nzero-shot manner. They even outperform methods which require additional\nfine-tuning on testing images. However, the cost of creating such datasets is\nprohibitive for most researchers. To generate a single training pair, current\nmethods fine-tune a pre-trained text-to-image model on the subject image to\ncapture fine-grained details, then use the fine-tuned model to create images\nfor the same subject based on creative text prompts. Consequently, constructing\na large-scale dataset with millions of subjects can require hundreds of\nthousands of GPU hours. To tackle this problem, we propose Toffee, an efficient\nmethod to construct datasets for subject-driven editing and generation.\nSpecifically, our dataset construction does not need any subject-level\nfine-tuning. After pre-training two generative models, we are able to generate\ninfinite number of high-quality samples. We construct the first large-scale\ndataset for subject-driven image editing and generation, which contains 5\nmillion image pairs, text prompts, and masks. Our dataset is 5 times the size\nof previous largest dataset, yet our cost is tens of thousands of GPU hours\nlower. To test the proposed dataset, we also propose a model which is capable\nof both subject-driven image editing and generation. By simply training the\nmodel on our proposed dataset, it obtains competitive results, illustrating the\neffectiveness of the proposed dataset construction framework.\n","authors":["Yufan Zhou","Ruiyi Zhang","Kaizhi Zheng","Nanxuan Zhao","Jiuxiang Gu","Zichao Wang","Xin Eric Wang","Tong Sun"],"pdf_url":"https://arxiv.org/pdf/2406.09305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09296v1","updated":"2024-06-13T16:30:32Z","published":"2024-06-13T16:30:32Z","title":"Parameter-Efficient Active Learning for Foundational models","summary":"  Foundational vision transformer models have shown impressive few shot\nperformance on many vision tasks. This research presents a novel investigation\ninto the application of parameter efficient fine-tuning methods within an\nactive learning (AL) framework, to advance the sampling selection process in\nextremely budget constrained classification tasks. The focus on image datasets,\nknown for their out-of-distribution characteristics, adds a layer of complexity\nand relevance to our study. Through a detailed evaluation, we illustrate the\nimproved AL performance on these challenging datasets, highlighting the\nstrategic advantage of merging parameter efficient fine tuning methods with\nfoundation models. This contributes to the broader discourse on optimizing AL\nstrategies, presenting a promising avenue for future exploration in leveraging\nfoundation models for efficient and effective data annotation in specialized\ndomains.\n","authors":["Athmanarayanan Lakshmi Narayanan","Ranganath Krishnan","Amrutha Machireddy","Mahesh Subedar"],"pdf_url":"https://arxiv.org/pdf/2406.09296v1.pdf","comment":"Accepted for CVPR2024 Transformers for Vision Workshop"},{"id":"http://arxiv.org/abs/2406.09295v1","updated":"2024-06-13T16:30:14Z","published":"2024-06-13T16:30:14Z","title":"AlignMMBench: Evaluating Chinese Multimodal Alignment in Large\n  Vision-Language Models","summary":"  Evaluating the alignment capabilities of large Vision-Language Models (VLMs)\nis essential for determining their effectiveness as helpful assistants.\nHowever, existing benchmarks primarily focus on basic abilities using nonverbal\nmethods, such as yes-no and multiple-choice questions. In this paper, we\naddress this gap by introducing AlignMMBench, a comprehensive alignment\nbenchmark specifically designed for emerging Chinese VLMs. This benchmark is\nmeticulously curated from real-world scenarios and Chinese Internet sources,\nencompassing thirteen specific tasks across three categories, and includes both\nsingle-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite\nstrategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer\npairs. To facilitate the evaluation pipeline, we propose CritiqueVLM, a\nrule-calibrated evaluator that exceeds GPT-4's evaluation ability. Finally, we\nreport the performance of representative VLMs on AlignMMBench, offering\ninsights into the capabilities and limitations of different VLM architectures.\nAll evaluation codes and data are available on https://alignmmbench.github.io.\n","authors":["Yuhang Wu","Wenmeng Yu","Yean Cheng","Yan Wang","Xiaohan Zhang","Jiazheng Xu","Ming Ding","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2406.09295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09294v1","updated":"2024-06-13T16:30:03Z","published":"2024-06-13T16:30:03Z","title":"You Don't Need Data-Augmentation in Self-Supervised Learning","summary":"  Self-Supervised learning (SSL) with Joint-Embedding Architectures (JEA) has\nled to outstanding performances. All instantiations of this paradigm were\ntrained using strong and well-established hand-crafted data augmentations,\nleading to the general belief that they are required for the proper training\nand performance of such models. On the other hand, generative\nreconstruction-based models such as BEIT and MAE or Joint-Embedding Predictive\nArchitectures such as I-JEPA have shown strong performance without using data\naugmentations except masking. In this work, we challenge the importance of\ninvariance and data-augmentation in JEAs at scale. By running a case-study on a\nrecent SSL foundation model - DINOv2 - we show that strong image\nrepresentations can be obtained with JEAs and only cropping without resizing\nprovided the training data is large enough, reaching state-of-the-art results\nand using the least amount of augmentation in the literature. Through this\nstudy, we also discuss the impact of compute constraints on the outcomes of\nexperimental deep learning research, showing that they can lead to very\ndifferent conclusions.\n","authors":["Théo Moutakanni","Maxime Oquab","Marc Szafraniec","Maria Vakalopoulou","Piotr Bojanowski"],"pdf_url":"https://arxiv.org/pdf/2406.09294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09293v1","updated":"2024-06-13T16:29:46Z","published":"2024-06-13T16:29:46Z","title":"StableMaterials: Enhancing Diversity in Material Generation via\n  Semi-Supervised Learning","summary":"  We introduce StableMaterials, a novel approach for generating photorealistic\nphysical-based rendering (PBR) materials that integrate semi-supervised\nlearning with Latent Diffusion Models (LDMs). Our method employs adversarial\ntraining to distill knowledge from existing large-scale image generation\nmodels, minimizing the reliance on annotated data and enhancing the diversity\nin generation. This distillation approach aligns the distribution of the\ngenerated materials with that of image textures from an SDXL model, enabling\nthe generation of novel materials that are not present in the initial training\ndataset. Furthermore, we employ a diffusion-based refiner model to improve the\nvisual quality of the samples and achieve high-resolution generation. Finally,\nwe distill a latent consistency model for fast generation in just four steps\nand propose a new tileability technique that removes visual artifacts typically\nassociated with fewer diffusion steps. We detail the architecture and training\nprocess of StableMaterials, the integration of semi-supervised training within\nexisting LDM frameworks and show the advantages of our approach. Comparative\nevaluations with state-of-the-art methods show the effectiveness of\nStableMaterials, highlighting its potential applications in computer graphics\nand beyond. StableMaterials is publicly available at\nhttps://gvecchio.com/stablematerials.\n","authors":["Giuseppe Vecchio"],"pdf_url":"https://arxiv.org/pdf/2406.09293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09292v1","updated":"2024-06-13T16:29:18Z","published":"2024-06-13T16:29:18Z","title":"Neural Assets: 3D-Aware Multi-Object Scene Synthesis with Image\n  Diffusion Models","summary":"  We address the problem of multi-object 3D pose control in image diffusion\nmodels. Instead of conditioning on a sequence of text tokens, we propose to use\na set of per-object representations, Neural Assets, to control the 3D pose of\nindividual objects in a scene. Neural Assets are obtained by pooling visual\nrepresentations of objects from a reference image, such as a frame in a video,\nand are trained to reconstruct the respective objects in a different image,\ne.g., a later frame in the video. Importantly, we encode object visuals from\nthe reference image while conditioning on object poses from the target frame.\nThis enables learning disentangled appearance and pose features. Combining\nvisual and 3D pose representations in a sequence-of-tokens format allows us to\nkeep the text-to-image architecture of existing models, with Neural Assets in\nplace of text tokens. By fine-tuning a pre-trained text-to-image diffusion\nmodel with this information, our approach enables fine-grained 3D pose and\nplacement control of individual objects in a scene. We further demonstrate that\nNeural Assets can be transferred and recomposed across different scenes. Our\nmodel achieves state-of-the-art multi-object editing results on both synthetic\n3D scene datasets, as well as two real-world video datasets (Objectron, Waymo\nOpen).\n","authors":["Ziyi Wu","Yulia Rubanova","Rishabh Kabra","Drew A. Hudson","Igor Gilitschenski","Yusuf Aytar","Sjoerd van Steenkiste","Kelsey R. Allen","Thomas Kipf"],"pdf_url":"https://arxiv.org/pdf/2406.09292v1.pdf","comment":"Additional details and video results are available at\n  https://neural-assets-paper.github.io/"},{"id":"http://arxiv.org/abs/2406.04251v2","updated":"2024-06-13T16:28:31Z","published":"2024-06-06T16:55:07Z","title":"Gaussian Splatting with Localized Points Management","summary":"  Point management is a critical component in optimizing 3D Gaussian Splatting\n(3DGS) models, as the point initiation (e.g., via structure from motion) is\ndistributionally inappropriate. Typically, the Adaptive Density Control (ADC)\nalgorithm is applied, leveraging view-averaged gradient magnitude thresholding\nfor point densification, opacity thresholding for pruning, and regular\nall-points opacity reset. However, we reveal that this strategy is limited in\ntackling intricate/special image regions (e.g., transparent) as it is unable to\nidentify all the 3D zones that require point densification, and lacking an\nappropriate mechanism to handle the ill-conditioned points with negative\nimpacts (occlusion due to false high opacity). To address these limitations, we\npropose a Localized Point Management (LPM) strategy, capable of identifying\nthose error-contributing zones in the highest demand for both point addition\nand geometry calibration. Zone identification is achieved by leveraging the\nunderlying multiview geometry constraints, with the guidance of image rendering\nerrors. We apply point densification in the identified zone, whilst resetting\nthe opacity of those points residing in front of these regions so that a new\nopportunity is created to correct ill-conditioned points. Serving as a\nversatile plugin, LPM can be seamlessly integrated into existing 3D Gaussian\nSplatting models. Experimental evaluation across both static 3D and dynamic 4D\nscenes validate the efficacy of our LPM strategy in boosting a variety of\nexisting 3DGS models both quantitatively and qualitatively. Notably, LPM\nimproves both vanilla 3DGS and SpaceTimeGS to achieve state-of-the-art\nrendering quality while retaining real-time speeds, outperforming on\nchallenging datasets such as Tanks & Temples and the Neural 3D Video Dataset.\n","authors":["Haosen Yang","Chenhao Zhang","Wenqing Wang","Marco Volino","Adrian Hilton","Li Zhang","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.04251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01373v2","updated":"2024-06-13T16:28:14Z","published":"2023-11-02T16:31:49Z","title":"Optimization Efficient Open-World Visual Region Recognition","summary":"  Understanding the semantics of individual regions or patches of unconstrained\nimages, such as open-world object detection, remains a critical yet challenging\ntask in computer vision. Building on the success of powerful image-level\nvision-language (ViL) foundation models like CLIP, recent efforts have sought\nto harness their capabilities by either training a contrastive model from\nscratch with an extensive collection of region-label pairs or aligning the\noutputs of a detection model with image-level representations of region\nproposals. Despite notable progress, these approaches are plagued by\ncomputationally intensive training requirements, susceptibility to data noise,\nand deficiency in contextual information. To address these limitations, we\nexplore the synergistic potential of off-the-shelf foundation models,\nleveraging their respective strengths in localization and semantics. We\nintroduce a novel, generic, and efficient architecture, named RegionSpot,\ndesigned to integrate position-aware localization knowledge from a localization\nfoundation model (e.g., SAM) with semantic information from a ViL model (e.g.,\nCLIP). To fully exploit pretrained knowledge while minimizing training\noverhead, we keep both foundation models frozen, focusing optimization efforts\nsolely on a lightweight attention-based knowledge integration module. Extensive\nexperiments in open-world object recognition show that our RegionSpot achieves\nsignificant performance gain over prior alternatives, along with substantial\ncomputational savings (e.g., training our model with 3 million data in a single\nday using 8 V100 GPUs). RegionSpot outperforms GLIP-L by 2.9 in mAP on LVIS val\nset, with an even larger margin of 13.1 AP for more challenging and rare\ncategories, and a 2.5 AP increase on ODinW. Furthermore, it exceeds\nGroundingDINO-L by 11.0 AP for rare categories on the LVIS minival set.\n","authors":["Haosen Yang","Chuofan Ma","Bin Wen","Yi Jiang","Zehuan Yuan","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.01373v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09272v1","updated":"2024-06-13T16:10:19Z","published":"2024-06-13T16:10:19Z","title":"Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric\n  Videos","summary":"  Generating realistic audio for human interactions is important for many\napplications, such as creating sound effects for films or virtual reality\ngames. Existing approaches implicitly assume total correspondence between the\nvideo and audio during training, yet many sounds happen off-screen and have\nweak to no correspondence with the visuals -- resulting in uncontrolled ambient\nsounds or hallucinations at test time. We propose a novel ambient-aware audio\ngeneration model, AV-LDM. We devise a novel audio-conditioning mechanism to\nlearn to disentangle foreground action sounds from the ambient background\nsounds in in-the-wild training videos. Given a novel silent video, our model\nuses retrieval-augmented generation to create audio that matches the visual\ncontent both semantically and temporally. We train and evaluate our model on\ntwo in-the-wild egocentric video datasets Ego4D and EPIC-KITCHENS. Our model\noutperforms an array of existing methods, allows controllable generation of the\nambient sound, and even shows promise for generalizing to computer graphics\ngame clips. Overall, our work is the first to focus video-to-audio generation\nfaithfully on the observed visual content despite training from uncurated clips\nwith natural background sounds.\n","authors":["Changan Chen","Puyuan Peng","Ami Baid","Zihui Xue","Wei-Ning Hsu","David Harwarth","Kristen Grauman"],"pdf_url":"https://arxiv.org/pdf/2406.09272v1.pdf","comment":"Project page: https://vision.cs.utexas.edu/projects/action2sound"},{"id":"http://arxiv.org/abs/2406.09260v1","updated":"2024-06-13T16:01:22Z","published":"2024-06-13T16:01:22Z","title":"Deep Transformer Network for Monocular Pose Estimation of Ship-Based UAV","summary":"  This paper introduces a deep transformer network for estimating the relative\n6D pose of a Unmanned Aerial Vehicle (UAV) with respect to a ship using\nmonocular images. A synthetic dataset of ship images is created and annotated\nwith 2D keypoints of multiple ship parts. A Transformer Neural Network model is\ntrained to detect these keypoints and estimate the 6D pose of each part. The\nestimates are integrated using Bayesian fusion. The model is tested on\nsynthetic data and in-situ flight experiments, demonstrating robustness and\naccuracy in various lighting conditions. The position estimation error is\napproximately 0.8\\% and 1.0\\% of the distance to the ship for the synthetic\ndata and the flight experiments, respectively. The method has potential\napplications for ship-based autonomous UAV landing and navigation.\n","authors":["Maneesha Wickramasuriya","Taeyoung Lee","Murray Snyder"],"pdf_url":"https://arxiv.org/pdf/2406.09260v1.pdf","comment":"23 pages, 25 figures, 3 tables"},{"id":"http://arxiv.org/abs/2406.09257v1","updated":"2024-06-13T15:58:37Z","published":"2024-06-13T15:58:37Z","title":"Assessing Model Generalization in Vicinity","summary":"  This paper evaluates the generalization ability of classification models on\nout-of-distribution test sets without depending on ground truth labels. Common\napproaches often calculate an unsupervised metric related to a specific model\nproperty, like confidence or invariance, which correlates with\nout-of-distribution accuracy. However, these metrics are typically computed for\neach test sample individually, leading to potential issues caused by spurious\nmodel responses, such as overly high or low confidence. To tackle this\nchallenge, we propose incorporating responses from neighboring test samples\ninto the correctness assessment of each individual sample. In essence, if a\nmodel consistently demonstrates high correctness scores for nearby samples, it\nincreases the likelihood of correctly predicting the target sample, and vice\nversa. The resulting scores are then averaged across all test samples to\nprovide a holistic indication of model accuracy. Developed under the vicinal\nrisk formulation, this approach, named vicinal risk proxy (VRP), computes\naccuracy without relying on labels. We show that applying the VRP method to\nexisting generalization indicators, such as average confidence and effective\ninvariance, consistently improves over these baselines both methodologically\nand experimentally. This yields a stronger correlation with model accuracy,\nespecially on challenging out-of-distribution test sets.\n","authors":["Yuchi Liu","Yifan Sun","Jingdong Wang","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.09257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09250v1","updated":"2024-06-13T15:55:04Z","published":"2024-06-13T15:55:04Z","title":"MirrorCheck: Efficient Adversarial Defense for Vision-Language Models","summary":"  Vision-Language Models (VLMs) are becoming increasingly vulnerable to\nadversarial attacks as various novel attack strategies are being proposed\nagainst these models. While existing defenses excel in unimodal contexts, they\ncurrently fall short in safeguarding VLMs against adversarial threats. To\nmitigate this vulnerability, we propose a novel, yet elegantly simple approach\nfor detecting adversarial samples in VLMs. Our method leverages Text-to-Image\n(T2I) models to generate images based on captions produced by target VLMs.\nSubsequently, we calculate the similarities of the embeddings of both input and\ngenerated images in the feature space to identify adversarial samples.\nEmpirical evaluations conducted on different datasets validate the efficacy of\nour approach, outperforming baseline methods adapted from image classification\ndomains. Furthermore, we extend our methodology to classification tasks,\nshowcasing its adaptability and model-agnostic nature. Theoretical analyses and\nempirical findings also show the resilience of our approach against adaptive\nattacks, positioning it as an excellent defense mechanism for real-world\ndeployment against adversarial threats.\n","authors":["Samar Fares","Klea Ziu","Toluwani Aremu","Nikita Durasov","Martin Takáč","Pascal Fua","Karthik Nandakumar","Ivan Laptev"],"pdf_url":"https://arxiv.org/pdf/2406.09250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09240v1","updated":"2024-06-13T15:43:59Z","published":"2024-06-13T15:43:59Z","title":"Comparison Visual Instruction Tuning","summary":"  Comparing two images in terms of Commonalities and Differences (CaD) is a\nfundamental human capability that forms the basis of advanced visual reasoning\nand interpretation. It is essential for the generation of detailed and\ncontextually relevant descriptions, performing comparative analysis, novelty\ndetection, and making informed decisions based on visual data. However,\nsurprisingly, little attention has been given to these fundamental concepts in\nthe best current mimic of human visual intelligence - Large Multimodal Models\n(LMMs). We develop and contribute a new two-phase approach CaD-VI for\ncollecting synthetic visual instructions, together with an\ninstruction-following dataset CaD-Inst containing 349K image pairs with CaD\ninstructions collected using CaD-VI. Our approach significantly improves the\nCaD spotting capabilities in LMMs, advancing the SOTA on a diverse set of\nrelated tasks by up to 17.5%. It is also complementary to existing\ndifference-only instruction datasets, allowing automatic targeted refinement of\nthose resources increasing their effectiveness for CaD tuning by up to 10%.\nAdditionally, we propose an evaluation benchmark with 7.5K open-ended QAs to\nassess the CaD understanding abilities of LMMs.\n","authors":["Wei Lin","Muhammad Jehanzeb Mirza","Sivan Doveh","Rogerio Feris","Raja Giryes","Sepp Hochreiter","Leonid Karlinsky"],"pdf_url":"https://arxiv.org/pdf/2406.09240v1.pdf","comment":"Project page: https://wlin-at.github.io/cad_vi ; Huggingface dataset\n  repo: https://huggingface.co/datasets/wlin21at/CaD-Inst"},{"id":"http://arxiv.org/abs/2403.15943v2","updated":"2024-06-13T15:30:02Z","published":"2024-03-23T22:07:32Z","title":"Advanced Feature Manipulation for Enhanced Change Detection Leveraging\n  Natural Language Models","summary":"  Change detection is a fundamental task in computer vision that processes a\nbi-temporal image pair to differentiate between semantically altered and\nunaltered regions. Large language models (LLMs) have been utilized in various\ndomains for their exceptional feature extraction capabilities and have shown\npromise in numerous downstream applications. In this study, we harness the\npower of a pre-trained LLM, extracting feature maps from extensive datasets,\nand employ an auxiliary network to detect changes. Unlike existing LLM-based\nchange detection methods that solely focus on deriving high-quality feature\nmaps, our approach emphasizes the manipulation of these feature maps to enhance\nsemantic relevance.\n","authors":["Zhenglin Li","Yangchen Huang","Mengran Zhu","Jingyu Zhang","JingHao Chang","Houze Liu"],"pdf_url":"https://arxiv.org/pdf/2403.15943v2.pdf","comment":"This version is not our full version based on our new progress,\n  related data, and methodology we are dealing with, and based on the rules and\n  the laws, we are adjusting our current version"},{"id":"http://arxiv.org/abs/2406.09229v1","updated":"2024-06-13T15:29:37Z","published":"2024-06-13T15:29:37Z","title":"MGRQ: Post-Training Quantization For Vision Transformer With Mixed\n  Granularity Reconstruction","summary":"  Post-training quantization (PTQ) efficiently compresses vision models, but\nunfortunately, it accompanies a certain degree of accuracy degradation.\nReconstruction methods aim to enhance model performance by narrowing the gap\nbetween the quantized model and the full-precision model, often yielding\npromising results. However, efforts to significantly improve the performance of\nPTQ through reconstruction in the Vision Transformer (ViT) have shown limited\nefficacy. In this paper, we conduct a thorough analysis of the reasons for this\nlimited effectiveness and propose MGRQ (Mixed Granularity Reconstruction\nQuantization) as a solution to address this issue. Unlike previous\nreconstruction schemes, MGRQ introduces a mixed granularity reconstruction\napproach. Specifically, MGRQ enhances the performance of PTQ by introducing\nExtra-Block Global Supervision and Intra-Block Local Supervision, building upon\nOptimized Block-wise Reconstruction. Extra-Block Global Supervision considers\nthe relationship between block outputs and the model's output, aiding\nblock-wise reconstruction through global supervision. Meanwhile, Intra-Block\nLocal Supervision reduces generalization errors by aligning the distribution of\noutputs at each layer within a block. Subsequently, MGRQ is further optimized\nfor reconstruction through Mixed Granularity Loss Fusion. Extensive experiments\nconducted on various ViT models illustrate the effectiveness of MGRQ. Notably,\nMGRQ demonstrates robust performance in low-bit quantization, thereby enhancing\nthe practicality of the quantized model.\n","authors":["Lianwei Yang","Zhikai Li","Junrui Xiao","Haisong Gong","Qingyi Gu"],"pdf_url":"https://arxiv.org/pdf/2406.09229v1.pdf","comment":"Accepted by 2024 IEEE International Conference on Image Processing"},{"id":"http://arxiv.org/abs/2406.09211v1","updated":"2024-06-13T15:15:07Z","published":"2024-06-13T15:15:07Z","title":"WildlifeReID-10k: Wildlife re-identification dataset with 10k individual\n  animals","summary":"  We introduce a new wildlife re-identification dataset WildlifeReID-10k with\nmore than 214k images of 10k individual animals. It is a collection of 30\nexisting wildlife re-identification datasets with additional processing steps.\nWildlifeReID-10k contains animals as diverse as marine turtles, primates,\nbirds, African herbivores, marine mammals and domestic animals. Due to the\nubiquity of similar images in datasets, we argue that the standard (random)\nsplits into training and testing sets are inadequate for wildlife\nre-identification and propose a new similarity-aware split based on the\nsimilarity of extracted features. To promote fair method comparison, we include\nsimilarity-aware splits both for closed-set and open-set settings, use\nMegaDescriptor - a foundational model for wildlife re-identification - for\nbaseline performance and host a leaderboard with the best results. We publicly\npublish the dataset and the codes used to create it in the wildlife-datasets\nlibrary, making WildlifeReID-10k both highly curated and easy to use.\n","authors":["Lukáš Adam","Vojtěch Čermák","Kostas Papafitsoros","Lukas Picek"],"pdf_url":"https://arxiv.org/pdf/2406.09211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12443v2","updated":"2024-06-13T15:04:18Z","published":"2023-09-21T19:36:22Z","title":"Active Learning for Multilingual Fingerspelling Corpora","summary":"  We apply active learning to help with data scarcity problems in sign\nlanguages. In particular, we perform a novel analysis of the effect of\npre-training. Since many sign languages are linguistic descendants of French\nsign language, they share hand configurations, which pre-training can hopefully\nexploit. We test this hypothesis on American, Chinese, German, and Irish\nfingerspelling corpora. We do observe a benefit from pre-training, but this may\nbe due to visual rather than linguistic similarities\n","authors":["Shuai Wang","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2309.12443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16502v4","updated":"2024-06-13T15:02:39Z","published":"2023-11-27T17:33:21Z","title":"MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning\n  Benchmark for Expert AGI","summary":"  We introduce MMMU: a new benchmark designed to evaluate multimodal models on\nmassive multi-discipline tasks demanding college-level subject knowledge and\ndeliberate reasoning. MMMU includes 11.5K meticulously collected multimodal\nquestions from college exams, quizzes, and textbooks, covering six core\ndisciplines: Art & Design, Business, Science, Health & Medicine, Humanities &\nSocial Science, and Tech & Engineering. These questions span 30 subjects and\n183 subfields, comprising 30 highly heterogeneous image types, such as charts,\ndiagrams, maps, tables, music sheets, and chemical structures. Unlike existing\nbenchmarks, MMMU focuses on advanced perception and reasoning with\ndomain-specific knowledge, challenging models to perform tasks akin to those\nfaced by experts. The evaluation of 14 open-source LMMs as well as the\nproprietary GPT-4V(ision) and Gemini highlights the substantial challenges\nposed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve\naccuracies of 56% and 59% respectively, indicating significant room for\nimprovement. We believe MMMU will stimulate the community to build\nnext-generation multimodal foundation models towards expert artificial general\nintelligence.\n","authors":["Xiang Yue","Yuansheng Ni","Kai Zhang","Tianyu Zheng","Ruoqi Liu","Ge Zhang","Samuel Stevens","Dongfu Jiang","Weiming Ren","Yuxuan Sun","Cong Wei","Botao Yu","Ruibin Yuan","Renliang Sun","Ming Yin","Boyuan Zheng","Zhenzhu Yang","Yibo Liu","Wenhao Huang","Huan Sun","Yu Su","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2311.16502v4.pdf","comment":"CVPR 2024 Oral"},{"id":"http://arxiv.org/abs/2406.09203v1","updated":"2024-06-13T15:00:17Z","published":"2024-06-13T15:00:17Z","title":"Optimizing Visual Question Answering Models for Driving: Bridging the\n  Gap Between Human and Machine Attention Patterns","summary":"  Visual Question Answering (VQA) models play a critical role in enhancing the\nperception capabilities of autonomous driving systems by allowing vehicles to\nanalyze visual inputs alongside textual queries, fostering natural interaction\nand trust between the vehicle and its occupants or other road users. This study\ninvestigates the attention patterns of humans compared to a VQA model when\nanswering driving-related questions, revealing disparities in the objects\nobserved. We propose an approach integrating filters to optimize the model's\nattention mechanisms, prioritizing relevant objects and improving accuracy.\nUtilizing the LXMERT model for a case study, we compare attention patterns of\nthe pre-trained and Filter Integrated models, alongside human answers using\nimages from the NuImages dataset, gaining insights into feature prioritization.\nWe evaluated the models using a Subjective scoring framework which shows that\nthe integration of the feature encoder filter has enhanced the performance of\nthe VQA model by refining its attention mechanisms.\n","authors":["Kaavya Rekanar","Martin Hayes","Ganesh Sistu","Ciaran Eising"],"pdf_url":"https://arxiv.org/pdf/2406.09203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09201v1","updated":"2024-06-13T14:59:45Z","published":"2024-06-13T14:59:45Z","title":"Enhanced Object Detection: A Study on Vast Vocabulary Object Detection\n  Track for V3Det Challenge 2024","summary":"  In this technical report, we present our findings from the research conducted\non the Vast Vocabulary Visual Detection (V3Det) dataset for Supervised Vast\nVocabulary Visual Detection task. How to deal with complex categories and\ndetection boxes has become a difficulty in this track. The original supervised\ndetector is not suitable for this task. We have designed a series of\nimprovements, including adjustments to the network structure, changes to the\nloss function, and design of training strategies. Our model has shown\nimprovement over the baseline and achieved excellent rankings on the\nLeaderboard for both the Vast Vocabulary Object Detection (Supervised) track\nand the Open Vocabulary Object Detection (OVD) track of the V3Det Challenge\n2024.\n","authors":["Peixi Wu","Bosong Chai","Xuan Nie","Longquan Yan","Zeyu Wang","Qifan Zhou","Boning Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17648v4","updated":"2024-06-13T14:58:23Z","published":"2023-05-28T06:44:33Z","title":"Z-GMOT: Zero-shot Generic Multiple Object Tracking","summary":"  Despite recent significant progress, Multi-Object Tracking (MOT) faces\nlimitations such as reliance on prior knowledge and predefined categories and\nstruggles with unseen objects. To address these issues, Generic Multiple Object\nTracking (GMOT) has emerged as an alternative approach, requiring less prior\ninformation. However, current GMOT methods often rely on initial bounding boxes\nand struggle to handle variations in factors such as viewpoint, lighting,\nocclusion, and scale, among others. Our contributions commence with the\nintroduction of the \\textit{Referring GMOT dataset} a collection of videos,\neach accompanied by detailed textual descriptions of their attributes.\nSubsequently, we propose $\\mathtt{Z-GMOT}$, a cutting-edge tracking solution\ncapable of tracking objects from \\textit{never-seen categories} without the\nneed of initial bounding boxes or predefined categories. Within our\n$\\mathtt{Z-GMOT}$ framework, we introduce two novel components: (i)\n$\\mathtt{iGLIP}$, an improved Grounded language-image pretraining, for\naccurately detecting unseen objects with specific characteristics. (ii)\n$\\mathtt{MA-SORT}$, a novel object association approach that adeptly integrates\nmotion and appearance-based matching strategies to tackle the complex task of\ntracking objects with high similarity. Our contributions are benchmarked\nthrough extensive experiments conducted on the Referring GMOT dataset for GMOT\ntask. Additionally, to assess the generalizability of the proposed\n$\\mathtt{Z-GMOT}$, we conduct ablation studies on the DanceTrack and MOT20\ndatasets for the MOT task. Our dataset, code, and models are released at:\nhttps://fsoft-aic.github.io/Z-GMOT.\n","authors":["Kim Hoang Tran","Anh Duy Le Dinh","Tien Phat Nguyen","Thinh Phan","Pha Nguyen","Khoa Luu","Donald Adjeroh","Gianfranco Doretto","Ngan Hoang Le"],"pdf_url":"https://arxiv.org/pdf/2305.17648v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09198v1","updated":"2024-06-13T14:56:07Z","published":"2024-06-13T14:56:07Z","title":"CLIP-Driven Cloth-Agnostic Feature Learning for Cloth-Changing Person\n  Re-Identification","summary":"  Contrastive Language-Image Pre-Training (CLIP) has shown impressive\nperformance in short-term Person Re-Identification (ReID) due to its ability to\nextract high-level semantic features of pedestrians, yet its direct application\nto Cloth-Changing Person Re-Identification (CC-ReID) faces challenges due to\nCLIP's image encoder overly focusing on clothes clues. To address this, we\npropose a novel framework called CLIP-Driven Cloth-Agnostic Feature Learning\n(CCAF) for CC-ReID. Accordingly, two modules were custom-designed: the\nInvariant Feature Prompting (IFP) and the Clothes Feature Minimization (CFM).\nThese modules guide the model to extract cloth-agnostic features positively and\nattenuate clothes-related features negatively. Specifically, IFP is designed to\nextract fine-grained semantic features unrelated to clothes from the raw image,\nguided by the cloth-agnostic text prompts. This module first covers the clothes\nin the raw image at the pixel level to obtain the shielding image and then\nutilizes CLIP's knowledge to generate cloth-agnostic text prompts.\nSubsequently, it aligns the raw image-text and the raw image-shielding image in\nthe feature space, emphasizing discriminative clues related to identity but\nunrelated to clothes. Furthermore, CFM is designed to examine and weaken the\nimage encoder's ability to extract clothes features. It first generates text\nprompts corresponding to clothes pixels. Then, guided by these clothes text\nprompts, it iteratively examines and disentangles clothes features from\npedestrian features, ultimately retaining inherent discriminative features.\nExtensive experiments have demonstrated the effectiveness of the proposed CCAF,\nachieving new state-of-the-art performance on several popular CC-ReID\nbenchmarks without any additional inference time.\n","authors":["Shuang Li","Jiaxu Leng","Guozhang Li","Ji Gan","Haosheng chen","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2406.09198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09196v1","updated":"2024-06-13T14:55:11Z","published":"2024-06-13T14:55:11Z","title":"Adaptive Slot Attention: Object Discovery with Dynamic Slot Number","summary":"  Object-centric learning (OCL) extracts the representation of objects with\nslots, offering an exceptional blend of flexibility and interpretability for\nabstracting low-level perceptual features. A widely adopted method within OCL\nis slot attention, which utilizes attention mechanisms to iteratively refine\nslot representations. However, a major drawback of most object-centric models,\nincluding slot attention, is their reliance on predefining the number of slots.\nThis not only necessitates prior knowledge of the dataset but also overlooks\nthe inherent variability in the number of objects present in each instance. To\novercome this fundamental limitation, we present a novel complexity-aware\nobject auto-encoder framework. Within this framework, we introduce an adaptive\nslot attention (AdaSlot) mechanism that dynamically determines the optimal\nnumber of slots based on the content of the data. This is achieved by proposing\na discrete slot sampling module that is responsible for selecting an\nappropriate number of slots from a candidate list. Furthermore, we introduce a\nmasked slot decoder that suppresses unselected slots during the decoding\nprocess. Our framework, tested extensively on object discovery tasks with\nvarious datasets, shows performance matching or exceeding top fixed-slot\nmodels. Moreover, our analysis substantiates that our method exhibits the\ncapability to dynamically adapt the slot number according to each instance's\ncomplexity, offering the potential for further exploration in slot attention\nresearch. Project will be available at https://kfan21.github.io/AdaSlot/\n","authors":["Ke Fan","Zechen Bai","Tianjun Xiao","Tong He","Max Horn","Yanwei Fu","Francesco Locatello","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.09196v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.09188v1","updated":"2024-06-13T14:49:28Z","published":"2024-06-13T14:49:28Z","title":"Reducing Task Discrepancy of Text Encoders for Zero-Shot Composed Image\n  Retrieval","summary":"  Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nreference image and conditioning text, enabling controllable searches. Due to\nthe expensive dataset construction cost for CIR triplets, a zero-shot (ZS) CIR\nsetting has been actively studied to eliminate the need for human-collected\ntriplet datasets. The mainstream of ZS-CIR employs an efficient projection\nmodule that projects a CLIP image embedding to the CLIP text token embedding\nspace, while fixing the CLIP encoders. Using the projected image embedding,\nthese methods generate image-text composed features by using the pre-trained\ntext encoder. However, their CLIP image and text encoders suffer from the task\ndiscrepancy between the pre-training task (text $\\leftrightarrow$ image) and\nthe target CIR task (image + text $\\leftrightarrow$ image). Conceptually, we\nneed expensive triplet samples to reduce the discrepancy, but we use cheap text\ntriplets instead and update the text encoder. To that end, we introduce the\nReducing Task Discrepancy of text encoders for Composed Image Retrieval (RTD),\na plug-and-play training scheme for the text encoder that enhances its\ncapability using a novel target-anchored text contrastive learning. We also\npropose two additional techniques to improve the proposed learning scheme: a\nhard negatives-based refined batch sampling strategy and a sophisticated\nconcatenation scheme. Integrating RTD into the state-of-the-art\nprojection-based ZS-CIR methods significantly improves performance across\nvarious datasets and backbones, demonstrating its efficiency and\ngeneralizability.\n","authors":["Jaeseok Byun","Seokhyeon Jeong","Wonjae Kim","Sanghyuk Chun","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2406.09188v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2406.09185v1","updated":"2024-06-13T14:47:57Z","published":"2024-06-13T14:47:57Z","title":"Thoracic Surgery Video Analysis for Surgical Phase Recognition","summary":"  This paper presents an approach for surgical phase recognition using video\ndata, aiming to provide a comprehensive understanding of surgical procedures\nfor automated workflow analysis. The advent of robotic surgery, digitized\noperating rooms, and the generation of vast amounts of data have opened doors\nfor the application of machine learning and computer vision in the analysis of\nsurgical videos. Among these advancements, Surgical Phase Recognition(SPR)\nstands out as an emerging technology that has the potential to recognize and\nassess the ongoing surgical scenario, summarize the surgery, evaluate surgical\nskills, offer surgical decision support, and facilitate medical training. In\nthis paper, we analyse and evaluate both frame-based and video clipping-based\nphase recognition on thoracic surgery dataset consisting of 11 classes of\nphases. Specifically, we utilize ImageNet ViT for image-based classification\nand VideoMAE as the baseline model for video-based classification. We show that\nMasked Video Distillation(MVD) exhibits superior performance, achieving a top-1\naccuracy of 72.9%, compared to 52.31% achieved by ImageNet ViT. These findings\nunderscore the efficacy of video-based classifiers over their image-based\ncounterparts in surgical phase recognition tasks.\n","authors":["Syed Abdul Mateen","Niharika Malvia","Syed Abdul Khader","Danny Wang","Deepti Srinivasan","Chi-Fu Jeffrey Yang","Lana Schumacher","Sandeep Manjanna"],"pdf_url":"https://arxiv.org/pdf/2406.09185v1.pdf","comment":"2 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.09181v1","updated":"2024-06-13T14:42:59Z","published":"2024-06-13T14:42:59Z","title":"A Large-scale Universal Evaluation Benchmark For Face Forgery Detection","summary":"  With the rapid development of AI-generated content (AIGC) technology, the\nproduction of realistic fake facial images and videos that deceive human visual\nperception has become possible. Consequently, various face forgery detection\ntechniques have been proposed to identify such fake facial content. However,\nevaluating the effectiveness and generalizability of these detection techniques\nremains a significant challenge. To address this, we have constructed a\nlarge-scale evaluation benchmark called DeepFaceGen, aimed at quantitatively\nassessing the effectiveness of face forgery detection and facilitating the\niterative development of forgery detection technology. DeepFaceGen consists of\n776,990 real face image/video samples and 773,812 face forgery image/video\nsamples, generated using 34 mainstream face generation techniques. During the\nconstruction process, we carefully consider important factors such as content\ndiversity, fairness across ethnicities, and availability of comprehensive\nlabels, in order to ensure the versatility and convenience of DeepFaceGen.\nSubsequently, DeepFaceGen is employed in this study to evaluate and analyze the\nperformance of 13 mainstream face forgery detection techniques from various\nperspectives. Through extensive experimental analysis, we derive significant\nfindings and propose potential directions for future research. The code and\ndataset for DeepFaceGen are available at\nhttps://anonymous.4open.science/r/DeepFaceGen-47D1.\n","authors":["Yijun Bei","Hengrui Lou","Jinsong Geng","Erteng Liu","Lechao Cheng","Jie Song","Mingli Song","Zunlei Feng"],"pdf_url":"https://arxiv.org/pdf/2406.09181v1.pdf","comment":"This is a paper about constructing a large-scale universal evaluation\n  benchmark for face forgery detection.The main text is 9 pages and the full\n  text is 30 pages"},{"id":"http://arxiv.org/abs/2406.09175v1","updated":"2024-06-13T14:37:04Z","published":"2024-06-13T14:37:04Z","title":"ReMI: A Dataset for Reasoning with Multiple Images","summary":"  With the continuous advancement of large language models (LLMs), it is\nessential to create new benchmarks to effectively evaluate their expanding\ncapabilities and identify areas for improvement. This work focuses on\nmulti-image reasoning, an emerging capability in state-of-the-art LLMs. We\nintroduce ReMI, a dataset designed to assess LLMs' ability to Reason with\nMultiple Images. This dataset encompasses a diverse range of tasks, spanning\nvarious reasoning domains such as math, physics, logic, code, table/chart\nunderstanding, and spatial and temporal reasoning. It also covers a broad\nspectrum of characteristics found in multi-image reasoning scenarios. We have\nbenchmarked several cutting-edge LLMs using ReMI and found a substantial gap\nbetween their performance and human-level proficiency. This highlights the\nchallenges in multi-image reasoning and the need for further research. Our\nanalysis also reveals the strengths and weaknesses of different models,\nshedding light on the types of reasoning that are currently attainable and\nareas where future models require improvement. To foster further research in\nthis area, we are releasing ReMI publicly:\nhttps://huggingface.co/datasets/mehrankazemi/ReMI.\n","authors":["Mehran Kazemi","Nishanth Dikkala","Ankit Anand","Petar Devic","Ishita Dasgupta","Fangyu Liu","Bahare Fatemi","Pranjal Awasthi","Dee Guo","Sreenivas Gollapudi","Ahmed Qureshi"],"pdf_url":"https://arxiv.org/pdf/2406.09175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09168v1","updated":"2024-06-13T14:30:35Z","published":"2024-06-13T14:30:35Z","title":"SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image\n  Super-Resolution","summary":"  Confocal fluorescence microscopy is one of the most accessible and widely\nused imaging techniques for the study of biological processes. Scanning\nconfocal microscopy allows the capture of high-quality images from 3D samples,\nyet suffers from well-known limitations such as photobleaching and\nphototoxicity of specimens caused by intense light exposure, which limits its\nuse in some applications, especially for living cells. Cellular damage can be\nalleviated by changing imaging parameters to reduce light exposure, often at\nthe expense of image quality. Machine/deep learning methods for single-image\nsuper-resolution (SISR) can be applied to restore image quality by upscaling\nlower-resolution (LR) images to produce high-resolution images (HR). These SISR\nmethods have been successfully applied to photo-realistic images due partly to\nthe abundance of publicly available data. In contrast, the lack of publicly\navailable data partly limits their application and success in scanning confocal\nmicroscopy. In this paper, we introduce a large scanning confocal microscopy\ndataset named SR-CACO-2 that is comprised of low- and high-resolution image\npairs marked for three different fluorescent markers. It allows the evaluation\nof performance of SISR methods on three different upscaling levels (X2, X4,\nX8). SR-CACO-2 contains the human epithelial cell line Caco-2 (ATCC HTB-37),\nand it is composed of 22 tiles that have been translated in the form of 9,937\nimage patches for experiments with SISR methods. Given the new SR-CACO-2\ndataset, we also provide benchmarking results for 15 state-of-the-art methods\nthat are representative of the main SISR families. Results show that these\nmethods have limited success in producing high-resolution textures, indicating\nthat SR-CACO-2 represents a challenging problem. Our dataset, code and\npretrained weights are available: https://github.com/sbelharbi/sr-caco-2.\n","authors":["Soufiane Belharbi","Mara KM Whitford","Phuong Hoang","Shakeeb Murtaza","Luke McCaffrey","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2406.09168v1.pdf","comment":"23 pages, 13 figures"},{"id":"http://arxiv.org/abs/2406.09166v1","updated":"2024-06-13T14:27:53Z","published":"2024-06-13T14:27:53Z","title":"Fine-Grained Domain Generalization with Feature Structuralization","summary":"  Fine-grained domain generalization (FGDG) is a more challenging task due to\nits small inter-class variations and relatively large intra-class disparities.\nWhen domain distribution changes, the fragility of subtle features leads to a\npronounced deterioration in model performance.Nevertheless, humans inherently\ndemonstrate the capacity for generalizing to out-of-distribution data,\nleveraging structured multi-granularity knowledge that emerges from discerning\nboth the commonality and specificity within categories.Likewise, we propose a\nFeature Structuralized Domain Generalization (FSDG) model, wherein features\nexperience structuralization into common, specific, and confounding segments,\nharmoniously aligned with their relevant semantic concepts, to elevate\nperformance in FGDG. Specifically, feature structuralization (FS) is achieved\nthrough a decorrelation function on disentangled segments, constraints on\ncommon feature consistency, specific feature distinctiveness, and a prediction\ncalibration operation across granularities. By imposing these stipulations,\nFSDG is prompted to disentangle and align features based on multi-granularity\nknowledge, facilitating robust subtle distinctions among categories. Extensive\nexperimentation on three benchmarks consistently validates the superiority of\nFSDG over state-of-the-art counterparts, with an average improvement of 6.1% in\nterms of FGDG performance. Beyond that, the explainability analysis and\nexperiments on various mainstream model architectures confirm the validity of\nFS.\n","authors":["Wenlong Yu","Dongyue Chen","Qilong Wang","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2406.09166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09162v1","updated":"2024-06-13T14:26:43Z","published":"2024-06-13T14:26:43Z","title":"EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal\n  Prompts","summary":"  Recent advancements in image generation have enabled the creation of\nhigh-quality images from text conditions. However, when facing multi-modal\nconditions, such as text combined with reference appearances, existing methods\nstruggle to balance multiple conditions effectively, typically showing a\npreference for one modality over others. To address this challenge, we\nintroduce EMMA, a novel image generation model accepting multi-modal prompts\nbuilt upon the state-of-the-art text-to-image (T2I) diffusion model, ELLA. EMMA\nseamlessly incorporates additional modalities alongside text to guide image\ngeneration through an innovative Multi-modal Feature Connector design, which\neffectively integrates textual and supplementary modal information using a\nspecial attention mechanism. By freezing all parameters in the original T2I\ndiffusion model and only adjusting some additional layers, we reveal an\ninteresting finding that the pre-trained T2I diffusion model can secretly\naccept multi-modal prompts. This interesting property facilitates easy\nadaptation to different existing frameworks, making EMMA a flexible and\neffective tool for producing personalized and context-aware images and even\nvideos. Additionally, we introduce a strategy to assemble learned EMMA modules\nto produce images conditioned on multiple modalities simultaneously,\neliminating the need for additional training with mixed multi-modal prompts.\nExtensive experiments demonstrate the effectiveness of EMMA in maintaining high\nfidelity and detail in generated images, showcasing its potential as a robust\nsolution for advanced multi-modal conditional image generation tasks.\n","authors":["Yucheng Han","Rui Wang","Chi Zhang","Juntao Hu","Pei Cheng","Bin Fu","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.09162v1.pdf","comment":"https://tencentqqgylab.github.io/EMMA"},{"id":"http://arxiv.org/abs/2406.09160v1","updated":"2024-06-13T14:22:59Z","published":"2024-06-13T14:22:59Z","title":"Beyond the Frontier: Predicting Unseen Walls from Occupancy Grids by\n  Learning from Floor Plans","summary":"  In this paper, we tackle the challenge of predicting the unseen walls of a\npartially observed environment as a set of 2D line segments, conditioned on\noccupancy grids integrated along the trajectory of a 360{\\deg} LIDAR sensor. A\ndataset of such occupancy grids and their corresponding target wall segments is\ncollected by navigating a virtual robot between a set of randomly sampled\nwaypoints in a collection of office-scale floor plans from a university campus.\nThe line segment prediction task is formulated as an autoregressive sequence\nprediction task, and an attention-based deep network is trained on the dataset.\nThe sequence-based autoregressive formulation is evaluated through predicted\ninformation gain, as in frontier-based autonomous exploration, demonstrating\nsignificant improvements over both non-predictive estimation and\nconvolution-based image prediction found in the literature. Ablations on key\ncomponents are evaluated, as well as sensor range and the occupancy grid's\nmetric area. Finally, model generality is validated by predicting walls in a\nnovel floor plan reconstructed on-the-fly in a real-world office environment.\n","authors":["Ludvig Ericson","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2406.09160v1.pdf","comment":"RA-L, 8 pages"},{"id":"http://arxiv.org/abs/2406.09156v1","updated":"2024-06-13T14:18:56Z","published":"2024-06-13T14:18:56Z","title":"Towards Multilingual Audio-Visual Question Answering","summary":"  In this paper, we work towards extending Audio-Visual Question Answering\n(AVQA) to multilingual settings. Existing AVQA research has predominantly\nrevolved around English and replicating it for addressing AVQA in other\nlanguages requires a substantial allocation of resources. As a scalable\nsolution, we leverage machine translation and present two multilingual AVQA\ndatasets for eight languages created from existing benchmark AVQA datasets.\nThis prevents extra human annotation efforts of collecting questions and\nanswers manually. To this end, we propose, MERA framework, by leveraging\nstate-of-the-art (SOTA) video, audio, and textual foundation models for AVQA in\nmultiple languages. We introduce a suite of models namely MERA-L, MERA-C,\nMERA-T with varied model architectures to benchmark the proposed datasets. We\nbelieve our work will open new research directions and act as a reference\nbenchmark for future works in multilingual AVQA.\n","authors":["Orchid Chetia Phukan","Priyabrata Mallick","Swarup Ranjan Behera","Aalekhya Satya Narayani","Arun Balaji Buduru","Rajesh Sharma"],"pdf_url":"https://arxiv.org/pdf/2406.09156v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.09155v1","updated":"2024-06-13T14:18:13Z","published":"2024-06-13T14:18:13Z","title":"DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities,\nrevolutionizing the integration of AI in daily life applications. However, they\nare prone to hallucinations, generating claims that contradict established\nfacts, deviating from prompts, and producing inconsistent responses when the\nsame prompt is presented multiple times. Addressing these issues is challenging\ndue to the lack of comprehensive and easily assessable benchmark datasets. Most\nexisting datasets are small and rely on multiple-choice questions, which are\ninadequate for evaluating the generative prowess of LLMs. To measure\nhallucination in LLMs, this paper introduces a comprehensive benchmark dataset\ncomprising over 75,000 prompts across eight domains. These prompts are designed\nto elicit definitive, concise, and informative answers. The dataset is divided\ninto two segments: one publicly available for testing and assessing LLM\nperformance and a hidden segment for benchmarking various LLMs. In our\nexperiments, we tested six LLMs-GPT-3.5, LLama 2, LLama 3, Gemini, Mixtral, and\nZephyr-revealing that overall factual hallucination ranges from 59% to 82% on\nthe public dataset and 57% to 76% in the hidden benchmark. Prompt misalignment\nhallucination ranges from 6% to 95% in the public dataset and 17% to 94% in the\nhidden counterpart. Average consistency ranges from 21% to 61% and 22% to 63%,\nrespectively. Domain-wise analysis shows that LLM performance significantly\ndeteriorates when asked for specific numeric information while performing\nmoderately with person, location, and date queries. Our dataset demonstrates\nits efficacy and serves as a comprehensive benchmark for LLM performance\nevaluation. Our dataset and LLMs responses are available at\n\\href{https://github.com/ashikiut/DefAn}{https://github.com/ashikiut/DefAn}.\n","authors":["A B M Ashikur Rahman","Saeed Anwar","Muhammad Usman","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2406.09155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06361v2","updated":"2024-06-13T14:17:04Z","published":"2024-03-11T01:18:49Z","title":"See Through Their Minds: Learning Transferable Neural Representation\n  from Cross-Subject fMRI","summary":"  Deciphering visual content from functional Magnetic Resonance Imaging (fMRI)\nhelps illuminate the human vision system. However, the scarcity of fMRI data\nand noise hamper brain decoding model performance. Previous approaches\nprimarily employ subject-specific models, sensitive to training sample size. In\nthis paper, we explore a straightforward but overlooked solution to address\ndata scarcity. We propose shallow subject-specific adapters to map\ncross-subject fMRI data into unified representations. Subsequently, a shared\ndeeper decoding model decodes cross-subject features into the target feature\nspace. During training, we leverage both visual and textual supervision for\nmulti-modal brain decoding. Our model integrates a high-level perception\ndecoding pipeline and a pixel-wise reconstruction pipeline guided by high-level\nperceptions, simulating bottom-up and top-down processes in neuroscience.\nEmpirical experiments demonstrate robust neural representation learning across\nsubjects for both pipelines. Moreover, merging high-level and low-level\ninformation improves both low-level and high-level reconstruction metrics.\nAdditionally, we successfully transfer learned general knowledge to new\nsubjects by training new adapters with limited training data. Compared to\nprevious state-of-the-art methods, notably pre-training-based methods (Mind-Vis\nand fMRI-PTE), our approach achieves comparable or superior results across\ndiverse tasks, showing promise as an alternative method for cross-subject fMRI\ndata pre-training. Our code and pre-trained weights will be publicly released\nat https://github.com/YulongBonjour/See_Through_Their_Minds.\n","authors":["Yulong Liu","Yongqiang Ma","Guibo Zhu","Haodong Jing","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.06361v2.pdf","comment":"A versatile brain decoding method learning from cross-subject fMRI\n  data"},{"id":"http://arxiv.org/abs/2306.12153v4","updated":"2024-06-13T14:16:10Z","published":"2023-06-21T10:03:56Z","title":"DIAS: A Dataset and Benchmark for Intracranial Artery Segmentation in\n  DSA sequences","summary":"  The automated segmentation of Intracranial Arteries (IA) in Digital\nSubtraction Angiography (DSA) plays a crucial role in the quantification of\nvascular morphology, significantly contributing to computer-assisted stroke\nresearch and clinical practice. Current research primarily focuses on the\nsegmentation of single-frame DSA using proprietary datasets. However, these\nmethods face challenges due to the inherent limitation of single-frame DSA,\nwhich only partially displays vascular contrast, thereby hindering accurate\nvascular structure representation. In this work, we introduce DIAS, a dataset\nspecifically developed for IA segmentation in DSA sequences. We establish a\ncomprehensive benchmark for evaluating DIAS, covering full, weak, and\nsemi-supervised segmentation methods. Specifically, we propose the vessel\nsequence segmentation network, in which the sequence feature extraction module\neffectively captures spatiotemporal representations of intravascular contrast,\nachieving intracranial artery segmentation in 2D+Time DSA sequences. For\nweakly-supervised IA segmentation, we propose a novel scribble learning-based\nimage segmentation framework, which, under the guidance of scribble labels,\nemploys cross pseudo-supervision and consistency regularization to improve the\nperformance of the segmentation network. Furthermore, we introduce the random\npatch-based self-training framework, aimed at alleviating the performance\nconstraints encountered in IA segmentation due to the limited availability of\nannotated DSA data. Our extensive experiments on the DIAS dataset demonstrate\nthe effectiveness of these methods as potential baselines for future research\nand clinical applications. The dataset and code are publicly available at\nhttps://doi.org/10.5281/zenodo.11396520 and https://github.com/lseventeen/DIAS.\n","authors":["Wentao Liu","Tong Tian","Lemeng Wang","Weijin Xu","Lei Li","Haoyuan Li","Wenyi Zhao","Siyu Tian","Xipeng Pan","Huihua Yang","Feng Gao","Yiming Deng","Xin Yang","Ruisheng Su"],"pdf_url":"https://arxiv.org/pdf/2306.12153v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09143v1","updated":"2024-06-13T14:11:19Z","published":"2024-06-13T14:11:19Z","title":"Generative AI-based Prompt Evolution Engineering Design Optimization\n  With Vision-Language Model","summary":"  Engineering design optimization requires an efficient combination of a 3D\nshape representation, an optimization algorithm, and a design performance\nevaluation method, which is often computationally expensive. We present a\nprompt evolution design optimization (PEDO) framework contextualized in a\nvehicle design scenario that leverages a vision-language model for penalizing\nimpractical car designs synthesized by a generative model. The backbone of our\nframework is an evolutionary strategy coupled with an optimization objective\nfunction that comprises a physics-based solver and a vision-language model for\npractical or functional guidance in the generated car designs. In the prompt\nevolutionary search, the optimizer iteratively generates a population of text\nprompts, which embed user specifications on the aerodynamic performance and\nvisual preferences of the 3D car designs. Then, in addition to the\ncomputational fluid dynamics simulations, the pre-trained vision-language model\nis used to penalize impractical designs and, thus, foster the evolutionary\nalgorithm to seek more viable designs. Our investigations on a car design\noptimization problem show a wide spread of potential car designs generated at\nthe early phase of the search, which indicates a good diversity of designs in\nthe initial populations, and an increase of over 20\\% in the probability of\ngenerating practical designs compared to a baseline framework without using a\nvision-language model. Visual inspection of the designs against the performance\nresults demonstrates prompt evolution as a very promising paradigm for finding\nnovel designs with good optimization performance while providing ease of use in\nspecifying design specifications and preferences via a natural language\ninterface.\n","authors":["Melvin Wong","Thiago Rios","Stefan Menzel","Yew Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2406.09143v1.pdf","comment":"Accepted and to be published in IEEE Congress on Evolutionary\n  Computation 2024"},{"id":"http://arxiv.org/abs/2406.09135v1","updated":"2024-06-13T14:06:12Z","published":"2024-06-13T14:06:12Z","title":"AdaRevD: Adaptive Patch Exiting Reversible Decoder Pushes the Limit of\n  Image Deblurring","summary":"  Despite the recent progress in enhancing the efficacy of image deblurring,\nthe limited decoding capability constrains the upper limit of State-Of-The-Art\n(SOTA) methods. This paper proposes a pioneering work, Adaptive Patch Exiting\nReversible Decoder (AdaRevD), to explore their insufficient decoding\ncapability. By inheriting the weights of the well-trained encoder, we refactor\na reversible decoder which scales up the single-decoder training to\nmulti-decoder training while remaining GPU memory-friendly. Meanwhile, we show\nthat our reversible structure gradually disentangles high-level degradation\ndegree and low-level blur pattern (residual of the blur image and its sharp\ncounterpart) from compact degradation representation. Besides, due to the\nspatially-variant motion blur kernels, different blur patches have various\ndeblurring difficulties. We further introduce a classifier to learn the\ndegradation degree of image patches, enabling them to exit at different\nsub-decoders for speedup. Experiments show that our AdaRevD pushes the limit of\nimage deblurring, e.g., achieving 34.60 dB in PSNR on GoPro dataset.\n","authors":["Xintian Mao","Qingli Li","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09768v2","updated":"2024-06-13T14:00:24Z","published":"2024-04-15T13:13:56Z","title":"Contrastive Pretraining for Visual Concept Explanations of Socioeconomic\n  Outcomes","summary":"  Predicting socioeconomic indicators from satellite imagery with deep learning\nhas become an increasingly popular research direction. Post-hoc concept-based\nexplanations can be an important step towards broader adoption of these models\nin policy-making as they enable the interpretation of socioeconomic outcomes\nbased on visual concepts that are intuitive to humans. In this paper, we study\nthe interplay between representation learning using an additional task-specific\ncontrastive loss and post-hoc concept explainability for socioeconomic studies.\nOur results on two different geographical locations and tasks indicate that the\ntask-specific pretraining imposes a continuous ordering of the latent space\nembeddings according to the socioeconomic outcomes. This improves the model's\ninterpretability as it enables the latent space of the model to associate\nconcepts encoding typical urban and natural area patterns with continuous\nintervals of socioeconomic outcomes. Further, we illustrate how analyzing the\nmodel's conceptual sensitivity for the intervals of socioeconomic outcomes can\nshed light on new insights for urban studies.\n","authors":["Ivica Obadic","Alex Levering","Lars Pennig","Dario Oliveira","Diego Marcos","Xiaoxiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.09768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09126v1","updated":"2024-06-13T13:59:47Z","published":"2024-06-13T13:59:47Z","title":"Auto-Vocabulary Segmentation for LiDAR Points","summary":"  Existing perception methods for autonomous driving fall short of recognizing\nunknown entities not covered in the training data. Open-vocabulary methods\noffer promising capabilities in detecting any object but are limited by\nuser-specified queries representing target classes. We propose AutoVoc3D, a\nframework for automatic object class recognition and open-ended segmentation.\nEvaluation on nuScenes showcases AutoVoc3D's ability to generate precise\nsemantic classes and accurate point-wise segmentation. Moreover, we introduce\nText-Point Semantic Similarity, a new metric to assess the semantic similarity\nbetween text and point cloud without eliminating novel classes.\n","authors":["Weijie Wei","Osman Ülger","Fatemeh Karimi Najadasl","Theo Gevers","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2406.09126v1.pdf","comment":"Accepted by CVPR 2024 OpenSun3D Workshop"},{"id":"http://arxiv.org/abs/2406.09121v1","updated":"2024-06-13T13:51:59Z","published":"2024-06-13T13:51:59Z","title":"MMRel: A Relation Understanding Dataset and Benchmark in the MLLM Era","summary":"  Despite the recent advancements in Multi-modal Large Language Models (MLLMs),\nunderstanding inter-object relations, i.e., interactions or associations\nbetween distinct objects, remains a major challenge for such models. This issue\nsignificantly hinders their advanced reasoning capabilities and is primarily\ndue to the lack of large-scale, high-quality, and diverse multi-modal data\nessential for training and evaluating MLLMs. In this paper, we provide a\ntaxonomy of inter-object relations and introduce Multi-Modal Relation\nUnderstanding (MMRel), a comprehensive dataset designed to bridge this gap by\nproviding large-scale, high-quality and diverse data for studying inter-object\nrelations with MLLMs. MMRel features three distinctive attributes: (i) It\nincludes over 15K question-answer pairs, which are sourced from three distinct\ndomains, ensuring large scale and high diversity; (ii) It contains a subset\nfeaturing highly unusual relations, on which MLLMs often fail due to\nhallucinations, thus are very challenging; (iii) It provides manually verified\nhigh-quality labels for inter-object relations. Thanks to these features, MMRel\nis ideal for evaluating MLLMs on relation understanding, as well as being used\nto fine-tune MLLMs to enhance relation understanding and even benefit overall\nperformance in various vision-language tasks. Extensive experiments on various\npopular MLLMs validate the effectiveness of MMRel. Both MMRel dataset and the\ncomplete labeling scripts have been made publicly available.\n","authors":["Jiahao Nie","Gongjie Zhang","Wenbin An","Yap-Peng Tan","Alex C. Kot","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2406.09121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09117v1","updated":"2024-06-13T13:44:31Z","published":"2024-06-13T13:44:31Z","title":"PC-LoRA: Low-Rank Adaptation for Progressive Model Compression with\n  Knowledge Distillation","summary":"  Low-rank adaption (LoRA) is a prominent method that adds a small number of\nlearnable parameters to the frozen pre-trained weights for parameter-efficient\nfine-tuning. Prompted by the question, ``Can we make its representation enough\nwith LoRA weights solely at the final phase of finetuning without the\npre-trained weights?'' In this work, we introduce Progressive Compression\nLoRA~(PC-LoRA), which utilizes low-rank adaptation (LoRA) to simultaneously\nperform model compression and fine-tuning. The PC-LoRA method gradually removes\nthe pre-trained weights during the training process, eventually leaving only\nthe low-rank adapters in the end. Thus, these low-rank adapters replace the\nwhole pre-trained weights, achieving the goals of compression and fine-tuning\nat the same time. Empirical analysis across various models demonstrates that\nPC-LoRA achieves parameter and FLOPs compression rates of 94.36%/89.1% for\nvision models, e.g., ViT-B, and 93.42%/84.2% parameters and FLOPs compressions\nfor language models, e.g., BERT.\n","authors":["Injoon Hwang","Haewon Park","Youngwan Lee","Jooyoung Yang","SunJae Maeng"],"pdf_url":"https://arxiv.org/pdf/2406.09117v1.pdf","comment":"Accepted at T4V@CVPR"},{"id":"http://arxiv.org/abs/2406.09112v1","updated":"2024-06-13T13:43:01Z","published":"2024-06-13T13:43:01Z","title":"Large-Scale Evaluation of Open-Set Image Classification Techniques","summary":"  The goal for classification is to correctly assign labels to unseen samples.\nHowever, most methods misclassify samples with unseen labels and assign them to\none of the known classes. Open-Set Classification (OSC) algorithms aim to\nmaximize both closed and open-set recognition capabilities. Recent studies\nshowed the utility of such algorithms on small-scale data sets, but limited\nexperimentation makes it difficult to assess their performances in real-world\nproblems. Here, we provide a comprehensive comparison of various OSC\nalgorithms, including training-based (SoftMax, Garbage, EOS) and\npost-processing methods (Maximum SoftMax Scores, Maximum Logit Scores, OpenMax,\nEVM, PROSER), the latter are applied on features from the former. We perform\nour evaluation on three large-scale protocols that mimic real-world challenges,\nwhere we train on known and negative open-set samples, and test on known and\nunknown instances. Our results show that EOS helps to improve performance of\nalmost all post-processing algorithms. Particularly, OpenMax and PROSER are\nable to exploit better-trained networks, demonstrating the utility of hybrid\nmodels. However, while most algorithms work well on negative test samples --\nsamples of open-set classes seen during training -- they tend to perform poorly\nwhen tested on samples of previously unseen unknown classes, especially in\nchallenging conditions.\n","authors":["Halil Bisgin","Andres Palechor","Mike Suter","Manuel Günther"],"pdf_url":"https://arxiv.org/pdf/2406.09112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09105v1","updated":"2024-06-13T13:31:49Z","published":"2024-06-13T13:31:49Z","title":"INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance\n  in Insurance","summary":"  Large Vision-Language Models (LVLMs) have demonstrated outstanding\nperformance in various general multimodal applications such as image\nrecognition and visual reasoning, and have also shown promising potential in\nspecialized domains. However, the application potential of LVLMs in the\ninsurance domain-characterized by rich application scenarios and abundant\nmultimodal data-has not been effectively explored. There is no systematic\nreview of multimodal tasks in the insurance domain, nor a benchmark\nspecifically designed to evaluate the capabilities of LVLMs in insurance. This\ngap hinders the development of LVLMs within the insurance domain. In this\npaper, we systematically review and distill multimodal tasks for four\nrepresentative types of insurance: auto insurance, property insurance, health\ninsurance, and agricultural insurance. We propose INS-MMBench, the first\ncomprehensive LVLMs benchmark tailored for the insurance domain. INS-MMBench\ncomprises a total of 2.2K thoroughly designed multiple-choice questions,\ncovering 12 meta-tasks and 22 fundamental tasks. Furthermore, we evaluate\nmultiple representative LVLMs, including closed-source models such as GPT-4o\nand open-source models like BLIP-2. This evaluation not only validates the\neffectiveness of our benchmark but also provides an in-depth performance\nanalysis of current LVLMs on various multimodal tasks in the insurance domain.\nWe hope that INS-MMBench will facilitate the further application of LVLMs in\nthe insurance domain and inspire interdisciplinary development. Our dataset and\nevaluation code are available at https://github.com/FDU-INS/INS-MMBench.\n","authors":["Chenwei Lin","Hanjia Lyu","Xian Xu","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2406.09105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00453v2","updated":"2024-06-13T13:28:37Z","published":"2024-02-01T09:43:30Z","title":"Instruction Makes a Difference","summary":"  We introduce Instruction Document Visual Question Answering (iDocVQA) dataset\nand Large Language Document (LLaDoc) model, for training Language-Vision (LV)\nmodels for document analysis and predictions on document images, respectively.\nUsually, deep neural networks for the DocVQA task are trained on datasets\nlacking instructions. We show that using instruction-following datasets\nimproves performance. We compare performance across document-related datasets\nusing the recent state-of-the-art (SotA) Large Language and Vision Assistant\n(LLaVA)1.5 as the base model. We also evaluate the performance of the derived\nmodels for object hallucination using the Polling-based Object Probing\nEvaluation (POPE) dataset. The results show that instruction-tuning performance\nranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over\nnon-instruction (traditional task) finetuning. Despite the gains, these still\nfall short of human performance (94.36%), implying there's much room for\nimprovement.\n","authors":["Tosin Adewumi","Nudrat Habib","Lama Alkhaled","Elisa Barney"],"pdf_url":"https://arxiv.org/pdf/2402.00453v2.pdf","comment":"Accepted at the 16th IAPR International Workshop On Document Analysis\n  Systems (DAS)"},{"id":"http://arxiv.org/abs/2406.09087v1","updated":"2024-06-13T13:13:17Z","published":"2024-06-13T13:13:17Z","title":"Suitability of KANs for Computer Vision: A preliminary investigation","summary":"  Kolmogorov-Arnold Networks (KANs) introduce a paradigm of neural modeling\nthat implements learnable functions on the edges of the networks, diverging\nfrom the traditional node-centric activations in neural networks. This work\nassesses the applicability and efficacy of KANs in visual modeling, focusing on\nthe image recognition task. We mainly analyze the performance and efficiency of\ndifferent network architectures built using KAN concepts along with\nconventional building blocks of convolutional and linear layers, enabling a\ncomparative analysis with the conventional models. Our findings are aimed at\ncontributing to understanding the potential of KANs in computer vision,\nhighlighting both their strengths and areas for further research. Our\nevaluation shows that whereas KAN-based architectures perform in-line with the\noriginal claims of KAN paper for performance and model-complexity in the case\nof simpler vision datasets like MNIST, the advantages seem to diminish even for\nslightly more complex datasets like CIFAR-10.\n","authors":["Basim Azam","Naveed Akhtar"],"pdf_url":"https://arxiv.org/pdf/2406.09087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06953v2","updated":"2024-06-13T13:11:29Z","published":"2024-06-11T05:25:25Z","title":"Stepwise Regression and Pre-trained Edge for Robust Stereo Matching","summary":"  Due to the difficulty in obtaining real samples and ground truth, the\ngeneralization performance and the fine-tuned performance are critical for the\nfeasibility of stereo matching methods in real-world applications. However, the\npresence of substantial disparity distributions and density variations across\ndifferent datasets presents significant challenges for the generalization and\nfine-tuning of the model. In this paper, we propose a novel stereo matching\nmethod, called SR-Stereo, which mitigates the distributional differences across\ndifferent datasets by predicting the disparity clips and uses a loss weight\nrelated to the regression target scale to improve the accuracy of the disparity\nclips. Moreover, this stepwise regression architecture can be easily extended\nto existing iteration-based methods to improve the performance without changing\nthe structure. In addition, to mitigate the edge blurring of the fine-tuned\nmodel on sparse ground truth, we propose Domain Adaptation Based on Pre-trained\nEdges (DAPE). Specifically, we use the predicted disparity and RGB image to\nestimate the edge map of the target domain image. The edge map is filtered to\ngenerate edge map background pseudo-labels, which together with the sparse\nground truth disparity on the target domain are used as a supervision to\njointly fine-tune the pre-trained stereo matching model. These proposed methods\nare extensively evaluated on SceneFlow, KITTI, Middbury 2014 and ETH3D. The\nSR-Stereo achieves competitive disparity estimation performance and\nstate-of-the-art cross-domain generalisation performance. Meanwhile, the\nproposed DAPE significantly improves the disparity estimation performance of\nfine-tuned models, especially in the textureless and detail regions.\n","authors":["Weiqing Xiao","Wei Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.06953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07115v2","updated":"2024-06-13T13:08:24Z","published":"2023-09-13T17:45:41Z","title":"Getting More for Less: Using Weak Labels and AV-Mixup for Robust\n  Audio-Visual Speaker Verification","summary":"  Distance Metric Learning (DML) has typically dominated the audio-visual\nspeaker verification problem space, owing to strong performance in new and\nunseen classes. In our work, we explored multitask learning techniques to\nfurther enhance DML, and show that an auxiliary task with even weak labels can\nincrease the quality of the learned speaker representation without increasing\nmodel complexity during inference. We also extend the Generalized End-to-End\nLoss (GE2E) to multimodal inputs and demonstrate that it can achieve\ncompetitive performance in an audio-visual space. Finally, we introduce\nAV-Mixup, a multimodal augmentation technique during training time that has\nshown to reduce speaker overfit. Our network achieves state of the art\nperformance for speaker verification, reporting 0.244%, 0.252%, 0.441% Equal\nError Rate (EER) on the VoxCeleb1-O/E/H test sets, which is to our knowledge,\nthe best published results on VoxCeleb1-E and VoxCeleb1-H.\n","authors":["Anith Selvakumar","Homa Fashandi"],"pdf_url":"https://arxiv.org/pdf/2309.07115v2.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.09070v1","updated":"2024-06-13T12:55:10Z","published":"2024-06-13T12:55:10Z","title":"EquiPrompt: Debiasing Diffusion Models via Iterative Bootstrapping in\n  Chain of Thoughts","summary":"  In the domain of text-to-image generative models, the inadvertent propagation\nof biases inherent in training datasets poses significant ethical challenges,\nparticularly in the generation of socially sensitive content. This paper\nintroduces EquiPrompt, a novel method employing Chain of Thought (CoT)\nreasoning to reduce biases in text-to-image generative models. EquiPrompt uses\niterative bootstrapping and bias-aware exemplar selection to balance creativity\nand ethical responsibility. It integrates iterative reasoning refinement with\ncontrolled evaluation techniques, addressing zero-shot CoT issues in sensitive\ncontexts. Experiments on several generation tasks show EquiPrompt effectively\nlowers bias while maintaining generative quality, advancing ethical AI and\nsocially responsible creative processes.Code will be publically available.\n","authors":["Zahraa Al Sahili","Ioannis Patras","Matthew Purver"],"pdf_url":"https://arxiv.org/pdf/2406.09070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09067v1","updated":"2024-06-13T12:54:20Z","published":"2024-06-13T12:54:20Z","title":"How structured are the representations in transformer-based vision\n  encoders? An analysis of multi-object representations in vision-language\n  models","summary":"  Forming and using symbol-like structured representations for reasoning has\nbeen considered essential for generalising over novel inputs. The primary tool\nthat allows generalisation outside training data distribution is the ability to\nabstract away irrelevant information into a compact form relevant to the task.\nAn extreme form of such abstract representations is symbols. Humans make use of\nsymbols to bind information while abstracting away irrelevant parts to utilise\nthe information consistently and meaningfully. This work estimates the state of\nsuch structured representations in vision encoders. Specifically, we evaluate\nimage encoders in large vision-language pre-trained models to address the\nquestion of which desirable properties their representations lack by applying\nthe criteria of symbolic structured reasoning described for LLMs to the image\nmodels. We test the representation space of image encoders like VIT, BLIP,\nCLIP, and FLAVA to characterise the distribution of the object representations\nin these models. In particular, we create decoding tasks using multi-object\nscenes from the COCO dataset, relating the token space to its input content for\nvarious objects in the scene. We use these tasks to characterise the network's\ntoken and layer-wise information modelling. Our analysis highlights that the\nCLS token, used for the downstream task, only focuses on a few objects\nnecessary for the trained downstream task. Still, other individual objects are\nwell-modelled separately by the tokens in the network originating from those\nobjects. We further observed a widespread distribution of scene information.\nThis demonstrates that information is far more entangled in tokens than optimal\nfor representing objects similar to symbols. Given these symbolic properties,\nwe show the network dynamics that cause failure modes of these models on basic\ndownstream tasks in a multi-object scene.\n","authors":["Tarun Khajuria","Braian Olmiro Dias","Jaan Aru"],"pdf_url":"https://arxiv.org/pdf/2406.09067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04426v2","updated":"2024-06-13T12:54:10Z","published":"2024-06-06T18:12:04Z","title":"DeTra: A Unified Model for Object Detection and Trajectory Forecasting","summary":"  The tasks of object detection and trajectory forecasting play a crucial role\nin understanding the scene for autonomous driving. These tasks are typically\nexecuted in a cascading manner, making them prone to compounding errors.\nFurthermore, there is usually a very thin interface between the two tasks,\ncreating a lossy information bottleneck. To address these challenges, our\napproach formulates the union of the two tasks as a trajectory refinement\nproblem, where the first pose is the detection (current time), and the\nsubsequent poses are the waypoints of the multiple forecasts (future time). To\ntackle this unified task, we design a refinement transformer that infers the\npresence, pose, and multi-modal future behaviors of objects directly from LiDAR\npoint clouds and high-definition maps. We call this model DeTra, short for\nobject Detection and Trajectory forecasting. In our experiments, we observe\nthat \\ourmodel{} outperforms the state-of-the-art on Argoverse 2 Sensor and\nWaymo Open Dataset by a large margin, across a broad range of metrics. Last but\nnot least, we perform extensive ablation studies that show the value of\nrefinement for this task, that every proposed component contributes positively\nto its performance, and that key design choices were made.\n","authors":["Sergio Casas","Ben Agro","Jiageng Mao","Thomas Gilles","Alexander Cui","Thomas Li","Raquel Urtasun"],"pdf_url":"https://arxiv.org/pdf/2406.04426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09040v1","updated":"2024-06-13T12:23:35Z","published":"2024-06-13T12:23:35Z","title":"FacEnhance: Facial Expression Enhancing with Recurrent DDPMs","summary":"  Facial expressions, vital in non-verbal human communication, have found\napplications in various computer vision fields like virtual reality, gaming,\nand emotional AI assistants. Despite advancements, many facial expression\ngeneration models encounter challenges such as low resolution (e.g., 32x32 or\n64x64 pixels), poor quality, and the absence of background details. In this\npaper, we introduce FacEnhance, a novel diffusion-based approach addressing\nconstraints in existing low-resolution facial expression generation models.\nFacEnhance enhances low-resolution facial expression videos (64x64 pixels) to\nhigher resolutions (192x192 pixels), incorporating background details and\nimproving overall quality. Leveraging conditional denoising within a diffusion\nframework, guided by a background-free low-resolution video and a single\nneutral expression high-resolution image, FacEnhance generates a video\nincorporating the facial expression from the low-resolution video performed by\nthe individual with background from the neutral image. By complementing\nlightweight low-resolution models, FacEnhance strikes a balance between\ncomputational efficiency and desirable image resolution and quality. Extensive\nexperiments on the MUG facial expression database demonstrate the efficacy of\nFacEnhance in enhancing low-resolution model outputs to state-of-the-art\nquality while preserving content and identity consistency. FacEnhance\nrepresents significant progress towards resource-efficient, high-fidelity\nfacial expression generation, Renewing outdated low-resolution methods to\nup-to-date standards.\n","authors":["Hamza Bouzid","Lahoucine Ballihi"],"pdf_url":"https://arxiv.org/pdf/2406.09040v1.pdf","comment":"submitted to Multimedia Tools and Applications"},{"id":"http://arxiv.org/abs/2406.09026v1","updated":"2024-06-13T12:01:28Z","published":"2024-06-13T12:01:28Z","title":"Steganalysis on Digital Watermarking: Is Your Defense Truly Impervious?","summary":"  Digital watermarking techniques are crucial for copyright protection and\nsource identification of images, especially in the era of generative AI models.\nHowever, many existing watermarking methods, particularly content-agnostic\napproaches that embed fixed patterns regardless of image content, are\nvulnerable to steganalysis attacks that can extract and remove the watermark\nwith minimal perceptual distortion. In this work, we categorize watermarking\nalgorithms into content-adaptive and content-agnostic ones, and demonstrate how\naveraging a collection of watermarked images could reveal the underlying\nwatermark pattern. We then leverage this extracted pattern for effective\nwatermark removal under both graybox and blackbox settings, even when the\ncollection contains multiple watermark patterns. For some algorithms like\nTree-Ring watermarks, the extracted pattern can also forge convincing\nwatermarks on clean images. Our quantitative and qualitative evaluations across\ntwelve watermarking methods highlight the threat posed by steganalysis to\ncontent-agnostic watermarks and the importance of designing watermarking\ntechniques resilient to such analytical attacks. We propose security guidelines\ncalling for using content-adaptive watermarking strategies and performing\nsecurity evaluation against steganalysis. We also suggest multi-key assignments\nas potential mitigations against steganalysis vulnerabilities.\n","authors":["Pei Yang","Hai Ci","Yiren Song","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2406.09026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07273v2","updated":"2024-06-13T11:59:15Z","published":"2023-12-12T13:52:55Z","title":"Benchmarking Pretrained Vision Embeddings for Near- and Duplicate\n  Detection in Medical Images","summary":"  Near- and duplicate image detection is a critical concern in the field of\nmedical imaging. Medical datasets often contain similar or duplicate images\nfrom various sources, which can lead to significant performance issues and\nevaluation biases, especially in machine learning tasks due to data leakage\nbetween training and testing subsets. In this paper, we present an approach for\nidentifying near- and duplicate 3D medical images leveraging publicly available\n2D computer vision embeddings. We assessed our approach by comparing embeddings\nextracted from two state-of-the-art self-supervised pretrained models and two\ndifferent vector index structures for similarity retrieval. We generate an\nexperimental benchmark based on the publicly available Medical Segmentation\nDecathlon dataset. The proposed method yields promising results for near- and\nduplicate image detection achieving a mean sensitivity and specificity of\n0.9645 and 0.8559, respectively.\n","authors":["Tuan Truong","Farnaz Khun Jush","Matthias Lenga"],"pdf_url":"https://arxiv.org/pdf/2312.07273v2.pdf","comment":"Accepted to International Symposium on Biomedical Imaging 2024,\n  Athens, Greece"},{"id":"http://arxiv.org/abs/2406.09017v1","updated":"2024-06-13T11:40:26Z","published":"2024-06-13T11:40:26Z","title":"A PCA based Keypoint Tracking Approach to Automated Facial Expressions\n  Encoding","summary":"  The Facial Action Coding System (FACS) for studying facial expressions is\nmanual and requires significant effort and expertise. This paper explores the\nuse of automated techniques to generate Action Units (AUs) for studying facial\nexpressions. We propose an unsupervised approach based on Principal Component\nAnalysis (PCA) and facial keypoint tracking to generate data-driven AUs called\nPCA AUs using the publicly available DISFA dataset. The PCA AUs comply with the\ndirection of facial muscle movements and are capable of explaining over 92.83\npercent of the variance in other public test datasets (BP4D-Spontaneous and\nCK+), indicating their capability to generalize facial expressions. The PCA AUs\nare also comparable to a keypoint-based equivalence of FACS AUs in terms of\nvariance explained on the test datasets. In conclusion, our research\ndemonstrates the potential of automated techniques to be an alternative to\nmanual FACS labeling which could lead to efficient real-time analysis of facial\nexpressions in psychology and related fields. To promote further research, we\nhave made code repository publicly available.\n","authors":["Shivansh Chandra Tripathi","Rahul Garg"],"pdf_url":"https://arxiv.org/pdf/2406.09017v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in [LNCS,volume 14301], and is available online at\n  https://doi.org/10.1007/978-3-031-45170-6_85"},{"id":"http://arxiv.org/abs/2406.09016v1","updated":"2024-06-13T11:40:06Z","published":"2024-06-13T11:40:06Z","title":"Cross-Modal Learning for Anomaly Detection in Fused Magnesium Smelting\n  Process: Methodology and Benchmark","summary":"  Fused Magnesium Furnace (FMF) is a crucial industrial equipment in the\nproduction of magnesia, and anomaly detection plays a pivotal role in ensuring\nits efficient, stable, and secure operation. Existing anomaly detection methods\nprimarily focus on analyzing dominant anomalies using the process variables\n(such as arc current) or constructing neural networks based on abnormal visual\nfeatures, while overlooking the intrinsic correlation of cross-modal\ninformation. This paper proposes a cross-modal Transformer (dubbed FmFormer),\ndesigned to facilitate anomaly detection in fused magnesium smelting processes\nby exploring the correlation between visual features (video) and process\nvariables (current). Our approach introduces a novel tokenization paradigm to\neffectively bridge the substantial dimensionality gap between the 3D video\nmodality and the 1D current modality in a multiscale manner, enabling a\nhierarchical reconstruction of pixel-level anomaly detection. Subsequently, the\nFmFormer leverages self-attention to learn internal features within each\nmodality and bidirectional cross-attention to capture correlations across\nmodalities. To validate the effectiveness of the proposed method, we also\npresent a pioneering cross-modal benchmark of the fused magnesium smelting\nprocess, featuring synchronously acquired video and current data for over 2.2\nmillion samples. Leveraging cross-modal learning, the proposed FmFormer\nachieves state-of-the-art performance in detecting anomalies, particularly\nunder extreme interferences such as current fluctuations and visual occlusion\ncaused by heavy water mist. The presented methodology and benchmark may be\napplicable to other industrial applications with some amendments. The benchmark\nwill be released at https://github.com/GaochangWu/FMF-Benchmark.\n","authors":["Gaochang Wu","Yapeng Zhang","Lan Deng","Jingxin Zhang","Tianyou Chai"],"pdf_url":"https://arxiv.org/pdf/2406.09016v1.pdf","comment":"14 pages, 6 figures, 5 tables. Submitted to IEEE"},{"id":"http://arxiv.org/abs/2406.09015v1","updated":"2024-06-13T11:39:02Z","published":"2024-06-13T11:39:02Z","title":"AMSA-UNet: An Asymmetric Multiple Scales U-net Based on Self-attention\n  for Deblurring","summary":"  The traditional ingle-scale U-Net often leads to the loss of spatial\ninformation during deblurring, which affects the deblurring accracy.\nAdditionally, due to the convolutional method's limitation in capturing\nlong-range dependencies, the quality of the recovered image is degraded. To\naddress the above problems, an asymmetric multiple scales U-net based on\nself-attention (AMSA-UNet) is proposed to improve the accuracy and\ncomputational complexity. By introducing a multiple-scales U shape\narchitecture, the network can focus on blurry regions at the global level and\nbetter recover image details at the local level. In order to overcome the\nlimitations of traditional convolutional methods in capturing the long-range\ndependencies of information, a self-attention mechanism is introduced into the\ndecoder part of the backbone network, which significantly increases the model's\nreceptive field, enabling it to pay more attention to semantic information of\nthe image, thereby producing more accurate and visually pleasing deblurred\nimages. What's more, a frequency domain-based computation method was introduced\nto reduces the computation amount. The experimental results demonstrate that\nthe proposed method exhibits significant improvements in both accuracy and\nspeed compared to eight excellent methods\n","authors":["Yingying Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09015v1.pdf","comment":"15pages, 6figures"},{"id":"http://arxiv.org/abs/2405.05945v3","updated":"2024-06-13T11:13:39Z","published":"2024-05-09T17:35:16Z","title":"Lumina-T2X: Transforming Text into Any Modality, Resolution, and\n  Duration via Flow-based Large Diffusion Transformers","summary":"  Sora unveils the potential of scaling Diffusion Transformer for generating\nphotorealistic images and videos at arbitrary resolutions, aspect ratios, and\ndurations, yet it still lacks sufficient implementation details. In this\ntechnical report, we introduce the Lumina-T2X family - a series of Flow-based\nLarge Diffusion Transformers (Flag-DiT) equipped with zero-initialized\nattention, as a unified framework designed to transform noise into images,\nvideos, multi-view 3D objects, and audio clips conditioned on text\ninstructions. By tokenizing the latent spatial-temporal space and incorporating\nlearnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2X\nseamlessly unifies the representations of different modalities across various\nspatial-temporal resolutions. This unified approach enables training within a\nsingle framework for different modalities and allows for flexible generation of\nmultimodal data at any resolution, aspect ratio, and length during inference.\nAdvanced techniques like RoPE, RMSNorm, and flow matching enhance the\nstability, flexibility, and scalability of Flag-DiT, enabling models of\nLumina-T2X to scale up to 7 billion parameters and extend the context window to\n128K tokens. This is particularly beneficial for creating ultra-high-definition\nimages with our Lumina-T2I model and long 720p videos with our Lumina-T2V\nmodel. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT,\nrequires only 35% of the training computational costs of a\n600-million-parameter naive DiT. Our further comprehensive analysis underscores\nLumina-T2X's preliminary capability in resolution extrapolation,\nhigh-resolution editing, generating consistent 3D views, and synthesizing\nvideos with seamless transitions. We expect that the open-sourcing of\nLumina-T2X will further foster creativity, transparency, and diversity in the\ngenerative AI community.\n","authors":["Peng Gao","Le Zhuo","Dongyang Liu","Ruoyi Du","Xu Luo","Longtian Qiu","Yuhang Zhang","Chen Lin","Rongjie Huang","Shijie Geng","Renrui Zhang","Junlin Xi","Wenqi Shao","Zhengkai Jiang","Tianshuo Yang","Weicai Ye","He Tong","Jingwen He","Yu Qiao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2405.05945v3.pdf","comment":"Technical Report; Code at: https://github.com/Alpha-VLLM/Lumina-T2X"},{"id":"http://arxiv.org/abs/2406.09003v1","updated":"2024-06-13T11:12:46Z","published":"2024-06-13T11:12:46Z","title":"Enhancing Cross-Modal Fine-Tuning with Gradually Intermediate Modality\n  Generation","summary":"  Large-scale pretrained models have proven immensely valuable in handling\ndata-intensive modalities like text and image. However, fine-tuning these\nmodels for certain specialized modalities, such as protein sequence and cosmic\nray, poses challenges due to the significant modality discrepancy and scarcity\nof labeled data. In this paper, we propose an end-to-end method, PaRe, to\nenhance cross-modal fine-tuning, aiming to transfer a large-scale pretrained\nmodel to various target modalities. PaRe employs a gating mechanism to select\nkey patches from both source and target data. Through a modality-agnostic Patch\nReplacement scheme, these patches are preserved and combined to construct\ndata-rich intermediate modalities ranging from easy to hard. By gradually\nintermediate modality generation, we can not only effectively bridge the\nmodality gap to enhance stability and transferability of cross-modal\nfine-tuning, but also address the challenge of limited data in the target\nmodality by leveraging enriched intermediate modality data. Compared with\nhand-designed, general-purpose, task-specific, and state-of-the-art cross-modal\nfine-tuning approaches, PaRe demonstrates superior performance across three\nchallenging benchmarks, encompassing more than ten modalities.\n","authors":["Lincan Cai","Shuang Li","Wenxuan Ma","Jingxuan Kang","Binhui Xie","Zixun Sun","Chengwei Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.09003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14126v2","updated":"2024-06-13T10:57:58Z","published":"2023-07-26T11:45:39Z","title":"Multi-modal Learning with Missing Modality via Shared-Specific Feature\n  Modelling","summary":"  The missing modality issue is critical but non-trivial to be solved by\nmulti-modal models. Current methods aiming to handle the missing modality\nproblem in multi-modal tasks, either deal with missing modalities only during\nevaluation or train separate models to handle specific missing modality\nsettings. In addition, these models are designed for specific tasks, so for\nexample, classification models are not easily adapted to segmentation tasks and\nvice versa. In this paper, we propose the Shared-Specific Feature Modelling\n(ShaSpec) method that is considerably simpler and more effective than competing\napproaches that address the issues above. ShaSpec is designed to take advantage\nof all available input modalities during training and evaluation by learning\nshared and specific features to better represent the input data. This is\nachieved from a strategy that relies on auxiliary tasks based on distribution\nalignment and domain classification, in addition to a residual feature fusion\nprocedure. Also, the design simplicity of ShaSpec enables its easy adaptation\nto multiple tasks, such as classification and segmentation. Experiments are\nconducted on both medical image segmentation and computer vision\nclassification, with results indicating that ShaSpec outperforms competing\nmethods by a large margin. For instance, on BraTS2018, ShaSpec improves the\nSOTA by more than 3% for enhancing tumour, 5% for tumour core and 3% for whole\ntumour. The code repository address is https://github.com/billhhh/ShaSpec/.\n","authors":["Hu Wang","Yuanhong Chen","Congbo Ma","Jodie Avery","Louise Hull","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2307.14126v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08997v1","updated":"2024-06-13T10:57:24Z","published":"2024-06-13T10:57:24Z","title":"Adaptive Temporal Motion Guided Graph Convolution Network for\n  Micro-expression Recognition","summary":"  Micro-expressions serve as essential cues for understanding individuals'\ngenuine emotional states. Recognizing micro-expressions attracts increasing\nresearch attention due to its various applications in fields such as business\nnegotiation and psychotherapy. However, the intricate and transient nature of\nmicro-expressions poses a significant challenge to their accurate recognition.\nMost existing works either neglect temporal dependencies or suffer from\nredundancy issues in clip-level recognition. In this work, we propose a novel\nframework for micro-expression recognition, named the Adaptive Temporal Motion\nGuided Graph Convolution Network (ATM-GCN). Our framework excels at capturing\ntemporal dependencies between frames across the entire clip, thereby enhancing\nmicro-expression recognition at the clip level. Specifically, the integration\nof Adaptive Temporal Motion layers empowers our method to aggregate global and\nlocal motion features inherent in micro-expressions. Experimental results\ndemonstrate that ATM-GCN not only surpasses existing state-of-the-art methods,\nparticularly on the Composite dataset, but also achieves superior performance\non the latest micro-expression dataset CAS(ME)$^3$.\n","authors":["Fengyuan Zhang","Zhaopei Huang","Xinjie Zhang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2406.08997v1.pdf","comment":"Accepted by ICME 2024"},{"id":"http://arxiv.org/abs/2403.07350v2","updated":"2024-06-13T10:47:48Z","published":"2024-03-12T06:16:33Z","title":"VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark","summary":"  Recently, knowledge editing on large language models (LLMs) has received\nconsiderable attention. Compared to this, editing Large Vision-Language Models\n(LVLMs) faces extra challenges from diverse data modalities and complicated\nmodel components, and data for LVLMs editing are limited. The existing LVLM\nediting benchmark, which comprises three metrics (Reliability, Locality, and\nGenerality), falls short in the quality of synthesized evaluation images and\ncannot assess whether models apply edited knowledge in relevant content.\nTherefore, we employ more reliable data collection methods to construct a new\nLarge $\\textbf{V}$ision-$\\textbf{L}$anguage Model $\\textbf{K}$nowledge\n$\\textbf{E}$diting $\\textbf{B}$enchmark, $\\textbf{VLKEB}$, and extend the\nPortability metric for more comprehensive evaluation. Leveraging a multi-modal\nknowledge graph, our image data are bound with knowledge entities. This can be\nfurther used to extract entity-related knowledge, which constitutes the base of\nediting data. We conduct experiments of different editing methods on five\nLVLMs, and thoroughly analyze how do they impact the models. The results reveal\nstrengths and deficiencies of these methods and hopefully provide insights for\nfuture research. The codes and dataset are available at:\n$\\href{https://github.com/VLKEB/VLKEB}{\\text{https://github.com/VLKEB/VLKEB}}$.\n","authors":["Han Huang","Haitian Zhong","Tao Yu","Qiang Liu","Shu Wu","Liang Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2403.07350v2.pdf","comment":"9+11 pages (main+appendix), 7 figures, 13 tables.\n  $\\href{https://github.com/VLKEB/VLKEB}{\\text{get code and data}}$"},{"id":"http://arxiv.org/abs/2303.09199v3","updated":"2024-06-13T10:43:49Z","published":"2023-03-16T10:17:33Z","title":"A Generative Model for Digital Camera Noise Synthesis","summary":"  Noise synthesis is a challenging low-level vision task aiming to generate\nrealistic noise given a clean image along with the camera settings. To this\nend, we propose an effective generative model which utilizes clean features as\nguidance followed by noise injections into the network. Specifically, our\ngenerator follows a UNet-like structure with skip connections but without\ndownsampling and upsampling layers. Firstly, we extract deep features from a\nclean image as the guidance and concatenate a Gaussian noise map to the\ntransition point between the encoder and decoder as the noise source. Secondly,\nwe propose noise synthesis blocks in the decoder in each of which we inject\nGaussian noise to model the noise characteristics. Thirdly, we propose to\nutilize an additional Style Loss and demonstrate that this allows better noise\ncharacteristics supervision in the generator. Through a number of new\nexperiments, we evaluate the temporal variance and the spatial correlation of\nthe generated noise which we hope can provide meaningful insights for future\nworks. Finally, we show that our proposed approach outperforms existing methods\nfor synthesizing camera noise.\n","authors":["Mingyang Song","Yang Zhang","Tunç O. Aydın","Elham Amin Mansour","Christopher Schroers"],"pdf_url":"https://arxiv.org/pdf/2303.09199v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06579v2","updated":"2024-06-13T10:29:45Z","published":"2024-06-04T13:52:54Z","title":"From Redundancy to Relevance: Enhancing Explainability in Multimodal\n  Large Language Models","summary":"  Recently, multimodal large language models have exploded with an endless\nvariety, most of the popular Large Vision Language Models (LVLMs) depend on\nsequential visual representation, where images are converted into hundreds or\nthousands of tokens before being input into the Large Language Model (LLM)\nalong with language prompts. The black-box design hinders the interpretability\nof visual-language models, especially regarding more complex reasoning tasks.\nTo explore the interaction process between image and text in complex reasoning\ntasks, we introduce the information flow method to visualize the interaction\nmechanism. By analyzing the dynamic flow of the information flow, we find that\nthe information flow appears to converge in the shallow layer. Further\ninvestigation revealed a redundancy of the image token in the shallow layer.\nConsequently, a truncation strategy was introduced to aggregate image tokens\nwithin these shallow layers. This approach has been validated through\nexperiments across multiple models, yielding consistent improvements.\n","authors":["Xiaofeng Zhang","Chen Shen","Xiaosong Yuan","Shaotian Yan","Liang Xie","Wenxiao Wang","Chaochen Gu","Hao Tang","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2406.06579v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06784v2","updated":"2024-06-13T10:09:51Z","published":"2024-02-09T21:17:31Z","title":"Transfer learning with generative models for object detection on limited\n  datasets","summary":"  The availability of data is limited in some fields, especially for object\ndetection tasks, where it is necessary to have correctly labeled bounding boxes\naround each object. A notable example of such data scarcity is found in the\ndomain of marine biology, where it is useful to develop methods to\nautomatically detect submarine species for environmental monitoring. To address\nthis data limitation, the state-of-the-art machine learning strategies employ\ntwo main approaches. The first involves pretraining models on existing datasets\nbefore generalizing to the specific domain of interest. The second strategy is\nto create synthetic datasets specifically tailored to the target domain using\nmethods like copy-paste techniques or ad-hoc simulators. The first strategy\noften faces a significant domain shift, while the second demands custom\nsolutions crafted for the specific task. In response to these challenges, here\nwe propose a transfer learning framework that is valid for a generic scenario.\nIn this framework, generated images help to improve the performances of an\nobject detector in a few-real data regime. This is achieved through a\ndiffusion-based generative model that was pretrained on large generic datasets.\nWith respect to the state-of-the-art, we find that it is not necessary to fine\ntune the generative model on the specific domain of interest. We believe that\nthis is an important advance because it mitigates the labor-intensive task of\nmanual labeling the images in object detection tasks. We validate our approach\nfocusing on fishes in an underwater environment, and on the more common domain\nof cars in an urban setting. Our method achieves detection performance\ncomparable to models trained on thousands of images, using only a few hundreds\nof input data. Our results pave the way for new generative AI-based protocols\nfor machine learning applications in various domains.\n","authors":["Matteo Paiano","Stefano Martina","Carlotta Giannelli","Filippo Caruso"],"pdf_url":"https://arxiv.org/pdf/2402.06784v2.pdf","comment":"28 pages, 16 figures, 1 table"},{"id":"http://arxiv.org/abs/2406.08960v1","updated":"2024-06-13T09:49:31Z","published":"2024-06-13T09:49:31Z","title":"AirPlanes: Accurate Plane Estimation via 3D-Consistent Embeddings","summary":"  Extracting planes from a 3D scene is useful for downstream tasks in robotics\nand augmented reality. In this paper we tackle the problem of estimating the\nplanar surfaces in a scene from posed images. Our first finding is that a\nsurprisingly competitive baseline results from combining popular clustering\nalgorithms with recent improvements in 3D geometry estimation. However, such\npurely geometric methods are understandably oblivious to plane semantics, which\nare crucial to discerning distinct planes. To overcome this limitation, we\npropose a method that predicts multi-view consistent plane embeddings that\ncomplement geometry when clustering points into planes. We show through\nextensive evaluation on the ScanNetV2 dataset that our new method outperforms\nexisting approaches and our strong geometric baseline for the task of plane\nestimation.\n","authors":["Jamie Watson","Filippo Aleotti","Mohamed Sayed","Zawar Qureshi","Oisin Mac Aodha","Gabriel Brostow","Michael Firman","Sara Vicente"],"pdf_url":"https://arxiv.org/pdf/2406.08960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07471v3","updated":"2024-06-13T09:46:33Z","published":"2024-06-11T17:18:11Z","title":"OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow\n  Understanding","summary":"  Surgical scene perception via videos are critical for advancing robotic\nsurgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology.\nHowever, the scarcity of diverse and richly annotated video datasets has\nhindered the development of intelligent systems for surgical workflow analysis.\nExisting datasets for surgical workflow analysis, which typically face\nchallenges such as small scale, a lack of diversity in surgery and phase\ncategories, and the absence of time-localized annotations, limit the\nrequirements for action understanding and model generalization validation in\ncomplex and diverse real-world surgical scenarios. To address this gap, we\nintroduce OphNet, a large-scale, expert-annotated video benchmark for\nophthalmic surgical workflow understanding. OphNet features: 1) A diverse\ncollection of 2,278 surgical videos spanning 66 types of cataract, glaucoma,\nand corneal surgeries, with detailed annotations for 102 unique surgical phases\nand 150 granular operations; 2) It offers sequential and hierarchical\nannotations for each surgery, phase, and operation, enabling comprehensive\nunderstanding and improved interpretability; 3) Moreover, OphNet provides\ntime-localized annotations, facilitating temporal localization and prediction\ntasks within surgical workflows. With approximately 205 hours of surgical\nvideos, OphNet is about 20 times larger than the largest existing surgical\nworkflow analysis benchmark. Our dataset and code have been made available at:\n\\url{https://github.com/minghu0830/OphNet-benchmark}.\n","authors":["Ming Hu","Peng Xia","Lin Wang","Siyuan Yan","Feilong Tang","Zhongxing Xu","Yimin Luo","Kaimin Song","Jurgen Leitner","Xuelian Cheng","Jun Cheng","Chi Liu","Kaijing Zhou","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2406.07471v3.pdf","comment":"Version 1"},{"id":"http://arxiv.org/abs/2403.03173v8","updated":"2024-06-13T09:41:55Z","published":"2024-03-05T18:08:29Z","title":"Solving the Clustering Reasoning Problems by Modeling a\n  Deep-Learning-Based Probabilistic Model","summary":"  Visual abstract reasoning problems pose significant challenges to the\nperception and cognition abilities of artificial intelligence algorithms,\ndemanding deeper pattern recognition and inductive reasoning beyond mere\nidentification of explicit image features. Research advancements in this field\noften provide insights and technical support for other similar domains. In this\nstudy, we introduce PMoC, a deep-learning-based probabilistic model, achieving\nhigh reasoning accuracy in the Bongard-Logo, which stands as one of the most\nchallenging clustering reasoning tasks. PMoC is a novel approach for\nconstructing probabilistic models based on deep learning, which is distinctly\ndifferent from previous techniques. PMoC revitalizes the probabilistic\napproach, which has been relatively weak in visual abstract reasoning. As a\nbonus, we also designed Pose-Transformer for complex visual abstract reasoning\ntasks. Inspired by capsule networks, it focuses on positional relationships in\nimage data, boosting accuracy when combined with PMoC. Our Pose-Transformer\neffectively addresses reasoning difficulties associated with changes in the\nposition of entities, outperforming previous models on RAVEN dataset, and the\nPGM dataset. RAVEN and PGM represent two significant progressive pattern\nreasoning problems. Finally, considering the deployment difficulties of\nPose-Transformer, we introduced Straw-Pose-Transformer, a lightweight version.\nThis study contributes to enhancing the capabilities of artificial intelligence\nin abstract reasoning, cognitive pattern, and probabilistic modeling of complex\nsystems.\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.03173v8.pdf","comment":"14 pages, 17 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.08407v2","updated":"2024-06-13T09:37:50Z","published":"2024-06-12T16:54:54Z","title":"MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation\n  in Videos","summary":"  Multimodal Language Language Models (MLLMs) demonstrate the emerging\nabilities of \"world models\" -- interpreting and reasoning about complex\nreal-world dynamics. To assess these abilities, we posit videos are the ideal\nmedium, as they encapsulate rich representations of real-world dynamics and\ncausalities. To this end, we introduce MMWorld, a new benchmark for\nmulti-discipline, multi-faceted multimodal video understanding. MMWorld\ndistinguishes itself from previous video understanding benchmarks with two\nunique advantages: (1) multi-discipline, covering various disciplines that\noften require domain expertise for comprehensive understanding; (2)\nmulti-faceted reasoning, including explanation, counterfactual thinking, future\nprediction, etc. MMWorld consists of a human-annotated dataset to evaluate\nMLLMs with questions about the whole videos and a synthetic dataset to analyze\nMLLMs within a single modality of perception. Together, MMWorld encompasses\n1,910 videos across seven broad disciplines and 69 subdisciplines, complete\nwith 6,627 question-answer pairs and associated captions. The evaluation\nincludes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld\n(e.g., GPT-4V performs the best with only 52.3\\% accuracy), showing large room\nfor improvement. Further ablation studies reveal other interesting findings\nsuch as models' different skill sets from humans. We hope MMWorld can serve as\nan essential step towards world model evaluation in videos.\n","authors":["Xuehai He","Weixi Feng","Kaizhi Zheng","Yujie Lu","Wanrong Zhu","Jiachen Li","Yue Fan","Jianfeng Wang","Linjie Li","Zhengyuan Yang","Kevin Lin","William Yang Wang","Lijuan Wang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08953v1","updated":"2024-06-13T09:32:40Z","published":"2024-06-13T09:32:40Z","title":"Preserving Identity with Variational Score for General-purpose 3D\n  Editing","summary":"  We present Piva (Preserving Identity with Variational Score Distillation), a\nnovel optimization-based method for editing images and 3D models based on\ndiffusion models. Specifically, our approach is inspired by the recently\nproposed method for 2D image editing - Delta Denoising Score (DDS). We pinpoint\nthe limitations in DDS for 2D and 3D editing, which causes detail loss and\nover-saturation. To address this, we propose an additional score distillation\nterm that enforces identity preservation. This results in a more stable editing\nprocess, gradually optimizing NeRF models to match target prompts while\nretaining crucial input characteristics. We demonstrate the effectiveness of\nour approach in zero-shot image and neural field editing. Our method\nsuccessfully alters visual attributes, adds both subtle and substantial\nstructural elements, translates shapes, and achieves competitive results on\nstandard 2D and 3D editing benchmarks. Additionally, our method imposes no\nconstraints like masking or pre-training, making it compatible with a wide\nrange of pre-trained diffusion models. This allows for versatile editing\nwithout needing neural field-to-mesh conversion, offering a more user-friendly\nexperience.\n","authors":["Duong H. Le","Tuan Pham","Aniruddha Kembhavi","Stephan Mandt","Wei-Chiu Ma","Jiasen Lu"],"pdf_url":"https://arxiv.org/pdf/2406.08953v1.pdf","comment":"22 pages, 14 figures"},{"id":"http://arxiv.org/abs/2404.05641v2","updated":"2024-06-13T09:28:06Z","published":"2024-04-08T16:21:22Z","title":"3D-COCO: extension of MS-COCO dataset for image detection and 3D\n  reconstruction modules","summary":"  We introduce 3D-COCO, an extension of the original MS-COCO dataset providing\n3D models and 2D-3D alignment annotations. 3D-COCO was designed to achieve\ncomputer vision tasks such as 3D reconstruction or image detection configurable\nwith textual, 2D image, and 3D CAD model queries. We complete the existing\nMS-COCO dataset with 28K 3D models collected on ShapeNet and Objaverse. By\nusing an IoU-based method, we match each MS-COCO annotation with the best 3D\nmodels to provide a 2D-3D alignment. The open-source nature of 3D-COCO is a\npremiere that should pave the way for new research on 3D-related topics. The\ndataset and its source codes is available at\nhttps://kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/\n","authors":["Maxence Bideaux","Alice Phe","Mohamed Chaouch","Bertrand Luvison","Quoc-Cuong Pham"],"pdf_url":"https://arxiv.org/pdf/2404.05641v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12639v2","updated":"2024-06-13T09:12:48Z","published":"2023-11-21T14:39:18Z","title":"KNVQA: A Benchmark for evaluation knowledge-based VQA","summary":"  Within the multimodal field, large vision-language models (LVLMs) have made\nsignificant progress due to their strong perception and reasoning capabilities\nin the visual and language systems. However, LVLMs are still plagued by the two\ncritical issues of object hallucination and factual accuracy, which limit the\npracticality of LVLMs in different scenarios. Furthermore, previous evaluation\nmethods focus more on the comprehension and reasoning of language content but\nlack a comprehensive evaluation of multimodal interactions, thereby resulting\nin potential limitations. To this end, we propose a novel KNVQA-Eval, which is\ndevoted to knowledge-based VQA task evaluation to reflect the factuality of\nmultimodal LVLMs. To ensure the robustness and scalability of the evaluation,\nwe develop a new KNVQA dataset by incorporating human judgment and perception,\naiming to evaluate the accuracy of standard answers relative to AI-generated\nanswers in knowledge-based VQA. This work not only comprehensively evaluates\nthe contextual information of LVLMs using reliable human annotations, but also\nfurther analyzes the fine-grained capabilities of current methods to reveal\npotential avenues for subsequent optimization of LVLMs-based estimators. Our\nproposed VQA-Eval and corresponding dataset KNVQA will facilitate the\ndevelopment of automatic evaluation tools with the advantages of low cost,\nprivacy protection, and reproducibility. Our code will be released upon\npublication.\n","authors":["Sirui Cheng","Siyu Zhang","Jiayi Wu","Muchen Lan"],"pdf_url":"https://arxiv.org/pdf/2311.12639v2.pdf","comment":"There was a little error in the method section of the paper"},{"id":"http://arxiv.org/abs/2406.08943v1","updated":"2024-06-13T09:12:26Z","published":"2024-06-13T09:12:26Z","title":"Neural NeRF Compression","summary":"  Neural Radiance Fields (NeRFs) have emerged as powerful tools for capturing\ndetailed 3D scenes through continuous volumetric representations. Recent NeRFs\nutilize feature grids to improve rendering quality and speed; however, these\nrepresentations introduce significant storage overhead. This paper presents a\nnovel method for efficiently compressing a grid-based NeRF model, addressing\nthe storage overhead concern. Our approach is based on the non-linear transform\ncoding paradigm, employing neural compression for compressing the model's\nfeature grids. Due to the lack of training data involving many i.i.d scenes, we\ndesign an encoder-free, end-to-end optimized approach for individual scenes,\nusing lightweight decoders. To leverage the spatial inhomogeneity of the latent\nfeature grids, we introduce an importance-weighted rate-distortion objective\nand a sparse entropy model employing a masking mechanism. Our experimental\nresults validate that our proposed method surpasses existing works in terms of\ngrid-based NeRF compression efficacy and reconstruction quality.\n","authors":["Tuan Pham","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2406.08943v1.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2303.09068v2","updated":"2024-06-13T09:00:29Z","published":"2023-03-16T04:02:17Z","title":"Vortex Feature Positioning: Bridging Tabular IIoT Data and Image-Based\n  Deep Learning","summary":"  Tabular data from IIoT devices are typically analyzed using decision\ntree-based machine learning techniques, which struggle with high-dimensional\nand numeric data. To overcome these limitations, techniques converting tabular\ndata into images have been developed, leveraging the strengths of image-based\ndeep learning approaches such as Convolutional Neural Networks. These methods\ncluster similar features into distinct image areas with fixed sizes, regardless\nof the number of features, resembling actual photographs. However, this\nincreases the possibility of overfitting, as similar features, when selected\ncarefully in a tabular format, are often discarded to prevent this issue.\nAdditionally, fixed image sizes can lead to wasted pixels with fewer features,\nresulting in computational inefficiency. We introduce Vortex Feature\nPositioning (VFP) to address these issues. VFP arranges features based on their\ncorrelation, spacing similar ones in a vortex pattern from the image center,\nwith the image size determined by the attribute count. VFP outperforms\ntraditional machine learning methods and existing conversion techniques in\ntests across seven datasets with varying real-valued attributes.\n","authors":["Jong-Ik Park","Sihoon Seong","JunKyu Lee","Cheol-Ho Hong"],"pdf_url":"https://arxiv.org/pdf/2303.09068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08929v1","updated":"2024-06-13T08:58:45Z","published":"2024-06-13T08:58:45Z","title":"Step-by-Step Diffusion: An Elementary Tutorial","summary":"  We present an accessible first course on diffusion models and flow matching\nfor machine learning, aimed at a technical audience with no diffusion\nexperience. We try to simplify the mathematical details as much as possible\n(sometimes heuristically), while retaining enough precision to derive correct\nalgorithms.\n","authors":["Preetum Nakkiran","Arwen Bradley","Hattie Zhou","Madhu Advani"],"pdf_url":"https://arxiv.org/pdf/2406.08929v1.pdf","comment":"35 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.08928v1","updated":"2024-06-13T08:51:57Z","published":"2024-06-13T08:51:57Z","title":"Multiple Prior Representation Learning for Self-Supervised Monocular\n  Depth Estimation via Hybrid Transformer","summary":"  Self-supervised monocular depth estimation aims to infer depth information\nwithout relying on labeled data. However, the lack of labeled information poses\na significant challenge to the model's representation, limiting its ability to\ncapture the intricate details of the scene accurately. Prior information can\npotentially mitigate this issue, enhancing the model's understanding of scene\nstructure and texture. Nevertheless, solely relying on a single type of prior\ninformation often falls short when dealing with complex scenes, necessitating\nimprovements in generalization performance. To address these challenges, we\nintroduce a novel self-supervised monocular depth estimation model that\nleverages multiple priors to bolster representation capabilities across\nspatial, context, and semantic dimensions. Specifically, we employ a hybrid\ntransformer and a lightweight pose network to obtain long-range spatial priors\nin the spatial dimension. Then, the context prior attention is designed to\nimprove generalization, particularly in complex structures or untextured areas.\nIn addition, semantic priors are introduced by leveraging semantic boundary\nloss, and semantic prior attention is supplemented, further refining the\nsemantic features extracted by the decoder. Experiments on three diverse\ndatasets demonstrate the effectiveness of the proposed model. It integrates\nmultiple priors to comprehensively enhance the representation ability,\nimproving the accuracy and reliability of depth estimation. Codes are available\nat: \\url{https://github.com/MVME-HBUT/MPRLNet}\n","authors":["Guodong Sun","Junjie Liu","Mingxuan Liu","Moyun Liu","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08928v1.pdf","comment":"28 pages, 12 figures"},{"id":"http://arxiv.org/abs/2406.08924v1","updated":"2024-06-13T08:44:12Z","published":"2024-06-13T08:44:12Z","title":"Learning Images Across Scales Using Adversarial Training","summary":"  The real world exhibits rich structure and detail across many scales of\nobservation. It is difficult, however, to capture and represent a broad\nspectrum of scales using ordinary images. We devise a novel paradigm for\nlearning a representation that captures an orders-of-magnitude variety of\nscales from an unstructured collection of ordinary images. We treat this\ncollection as a distribution of scale-space slices to be learned using\nadversarial training, and additionally enforce coherency across slices. Our\napproach relies on a multiscale generator with carefully injected procedural\nfrequency content, which allows to interactively explore the emerging\ncontinuous scale space. Training across vastly different scales poses\nchallenges regarding stability, which we tackle using a supervision scheme that\ninvolves careful sampling of scales. We show that our generator can be used as\na multiscale generative model, and for reconstructions of scale spaces from\nunstructured patches. Significantly outperforming the state of the art, we\ndemonstrate zoom-in factors of up to 256x at high quality and scale\nconsistency.\n","authors":["Krzysztof Wolski","Adarsh Djeacoumar","Alireza Javanmardi","Hans-Peter Seidel","Christian Theobalt","Guillaume Cordonnier","Karol Myszkowski","George Drettakis","Xingang Pan","Thomas Leimkühler"],"pdf_url":"https://arxiv.org/pdf/2406.08924v1.pdf","comment":"SIGGRAPH 2024; project page: https://scalespacegan.mpi-inf.mpg.de/"},{"id":"http://arxiv.org/abs/2406.08079v2","updated":"2024-06-13T08:29:43Z","published":"2024-06-12T11:02:15Z","title":"A$^{2}$-MAE: A spatial-temporal-spectral unified remote sensing\n  pre-training method based on anchor-aware masked autoencoder","summary":"  Vast amounts of remote sensing (RS) data provide Earth observations across\nmultiple dimensions, encompassing critical spatial, temporal, and spectral\ninformation which is essential for addressing global-scale challenges such as\nland use monitoring, disaster prevention, and environmental change mitigation.\nDespite various pre-training methods tailored to the characteristics of RS\ndata, a key limitation persists: the inability to effectively integrate\nspatial, temporal, and spectral information within a single unified model. To\nunlock the potential of RS data, we construct a Spatial-Temporal-Spectral\nStructured Dataset (STSSD) characterized by the incorporation of multiple RS\nsources, diverse coverage, unified locations within image sets, and\nheterogeneity within images. Building upon this structured dataset, we propose\nan Anchor-Aware Masked AutoEncoder method (A$^{2}$-MAE), leveraging intrinsic\ncomplementary information from the different kinds of images and\ngeo-information to reconstruct the masked patches during the pre-training\nphase. A$^{2}$-MAE integrates an anchor-aware masking strategy and a geographic\nencoding module to comprehensively exploit the properties of RS images.\nSpecifically, the proposed anchor-aware masking strategy dynamically adapts the\nmasking process based on the meta-information of a pre-selected anchor image,\nthereby facilitating the training on images captured by diverse types of RS\nsources within one model. Furthermore, we propose a geographic encoding method\nto leverage accurate spatial patterns, enhancing the model generalization\ncapabilities for downstream applications that are generally location-related.\nExtensive experiments demonstrate our method achieves comprehensive\nimprovements across various downstream tasks compared with existing RS\npre-training methods, including image classification, semantic segmentation,\nand change detection tasks.\n","authors":["Lixian Zhang","Yi Zhao","Runmin Dong","Jinxiao Zhang","Shuai Yuan","Shilei Cao","Mengxuan Chen","Juepeng Zheng","Weijia Li","Wei Liu","Litong Feng","Haohuan Fu"],"pdf_url":"https://arxiv.org/pdf/2406.08079v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08909v1","updated":"2024-06-13T08:12:48Z","published":"2024-06-13T08:12:48Z","title":"A Label-Free and Non-Monotonic Metric for Evaluating Denoising in Event\n  Cameras","summary":"  Event cameras are renowned for their high efficiency due to outputting a\nsparse, asynchronous stream of events. However, they are plagued by noisy\nevents, especially in low light conditions. Denoising is an essential task for\nevent cameras, but evaluating denoising performance is challenging.\nLabel-dependent denoising metrics involve artificially adding noise to clean\nsequences, complicating evaluations. Moreover, the majority of these metrics\nare monotonic, which can inflate scores by removing substantial noise and valid\nevents. To overcome these limitations, we propose the first label-free and\nnon-monotonic evaluation metric, the area of the continuous contrast curve\n(AOCC), which utilizes the area enclosed by event frame contrast curves across\ndifferent time intervals. This metric is inspired by how events capture the\nedge contours of scenes or objects with high temporal resolution. An effective\ndenoising method removes noise without eliminating these edge-contour events,\nthus preserving the contrast of event frames. Consequently, contrast across\nvarious time ranges serves as a metric to assess denoising effectiveness. As\nthe time interval lengthens, the curve will initially rise and then fall. The\nproposed metric is validated through both theoretical and experimental\nevidence.\n","authors":["Chenyang Shi","Shasha Guo","Boyi Wei","Hanxiao Liu","Yibo Zhang","Ningfang Song","Jing Jin"],"pdf_url":"https://arxiv.org/pdf/2406.08909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08907v1","updated":"2024-06-13T08:06:57Z","published":"2024-06-13T08:06:57Z","title":"Dual Attribute-Spatial Relation Alignment for 3D Visual Grounding","summary":"  3D visual grounding is an emerging research area dedicated to making\nconnections between the 3D physical world and natural language, which is\ncrucial for achieving embodied intelligence. In this paper, we propose DASANet,\na Dual Attribute-Spatial relation Alignment Network that separately models and\naligns object attributes and spatial relation features between language and 3D\nvision modalities. We decompose both the language and 3D point cloud input into\ntwo separate parts and design a dual-branch attention module to separately\nmodel the decomposed inputs while preserving global context in\nattribute-spatial feature fusion by cross attentions. Our DASANet achieves the\nhighest grounding accuracy 65.1% on the Nr3D dataset, 1.3% higher than the best\ncompetitor. Besides, the visualization of the two branches proves that our\nmethod is efficient and highly interpretable.\n","authors":["Yue Xu","Kaizhi Yang","Jiebo Luo","Xuejin Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08898v1","updated":"2024-06-13T07:51:08Z","published":"2024-06-13T07:51:08Z","title":"Computer Vision Approaches for Automated Bee Counting Application","summary":"  Many application from the bee colony health state monitoring could be\nefficiently solved using a computer vision techniques. One of such challenges\nis an efficient way for counting the number of incoming and outcoming bees,\nwhich could be used to further analyse many trends, such as the bee colony\nhealth state, blooming periods, or for investigating the effects of\nagricultural spraying. In this paper, we compare three methods for the\nautomated bee counting over two own datasets. The best performing method is\nbased on the ResNet-50 convolutional neural network classifier, which achieved\naccuracy of 87% over the BUT1 dataset and the accuracy of 93% over the BUT2\ndataset.\n","authors":["Simon Bilik","Ilona Janakova","Adam Ligocki","Dominik Ficek","Karel Horak"],"pdf_url":"https://arxiv.org/pdf/2406.08898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08896v1","updated":"2024-06-13T07:50:15Z","published":"2024-06-13T07:50:15Z","title":"Blind Super-Resolution via Meta-learning and Markov Chain Monte Carlo\n  Simulation","summary":"  Learning-based approaches have witnessed great successes in blind single\nimage super-resolution (SISR) tasks, however, handcrafted kernel priors and\nlearning based kernel priors are typically required. In this paper, we propose\na Meta-learning and Markov Chain Monte Carlo (MCMC) based SISR approach to\nlearn kernel priors from organized randomness. In concrete, a lightweight\nnetwork is adopted as kernel generator, and is optimized via learning from the\nMCMC simulation on random Gaussian distributions. This procedure provides an\napproximation for the rational blur kernel, and introduces a network-level\nLangevin dynamics into SISR optimization processes, which contributes to\npreventing bad local optimal solutions for kernel estimation. Meanwhile, a\nmeta-learning-based alternating optimization procedure is proposed to optimize\nthe kernel generator and image restorer, respectively. In contrast to the\nconventional alternating minimization strategy, a meta-learning-based framework\nis applied to learn an adaptive optimization strategy, which is less-greedy and\nresults in better convergence performance. These two procedures are iteratively\nprocessed in a plug-and-play fashion, for the first time, realizing a\nlearning-based but plug-and-play blind SISR solution in unsupervised inference.\nExtensive simulations demonstrate the superior performance and generalization\nability of the proposed approach when comparing with state-of-the-arts on\nsynthesis and real-world datasets. The code is available at\nhttps://github.com/XYLGroup/MLMC.\n","authors":["Jingyuan Xia","Zhixiong Yang","Shengxi Li","Shuanghui Zhang","Yaowen Fu","Deniz Gündüz","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2406.08896v1.pdf","comment":"This paper has been accepted for publication in IEEE Transactions on\n  Pattern Analysis and Machine Intelligence (2024)"},{"id":"http://arxiv.org/abs/2406.08894v1","updated":"2024-06-13T07:46:17Z","published":"2024-06-13T07:46:17Z","title":"OpenMaterial: A Comprehensive Dataset of Complex Materials for 3D\n  Reconstruction","summary":"  Recent advances in deep learning such as neural radiance fields and implicit\nneural representations have significantly propelled the field of 3D\nreconstruction. However, accurately reconstructing objects with complex optical\nproperties, such as metals and glass, remains a formidable challenge due to\ntheir unique specular and light-transmission characteristics. To facilitate the\ndevelopment of solutions to these challenges, we introduce the OpenMaterial\ndataset, comprising 1001 objects made of 295 distinct materials-including\nconductors, dielectrics, plastics, and their roughened variants- and captured\nunder 723 diverse lighting conditions. To this end, we utilized physics-based\nrendering with laboratory-measured Indices of Refraction (IOR) and generated\nhigh-fidelity multiview images that closely replicate real-world objects.\nOpenMaterial provides comprehensive annotations, including 3D shape, material\ntype, camera pose, depth, and object mask. It stands as the first large-scale\ndataset enabling quantitative evaluations of existing algorithms on objects\nwith diverse and challenging materials, thereby paving the way for the\ndevelopment of 3D reconstruction algorithms capable of handling complex\nmaterial properties.\n","authors":["Zheng Dang","Jialu Huang","Fei Wang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2406.08894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08884v1","updated":"2024-06-13T07:37:16Z","published":"2024-06-13T07:37:16Z","title":"The Penalized Inverse Probability Measure for Conformal Classification","summary":"  The deployment of safe and trustworthy machine learning systems, and\nparticularly complex black box neural networks, in real-world applications\nrequires reliable and certified guarantees on their performance. The conformal\nprediction framework offers such formal guarantees by transforming any point\ninto a set predictor with valid, finite-set, guarantees on the coverage of the\ntrue at a chosen level of confidence. Central to this methodology is the notion\nof the nonconformity score function that assigns to each example a measure of\n''strangeness'' in comparison with the previously seen observations. While the\ncoverage guarantees are maintained regardless of the nonconformity measure, the\npoint predictor and the dataset, previous research has shown that the\nperformance of a conformal model, as measured by its efficiency (the average\nsize of the predicted sets) and its informativeness (the proportion of\nprediction sets that are singletons), is influenced by the choice of the\nnonconformity score function. The current work introduces the Penalized Inverse\nProbability (PIP) nonconformity score, and its regularized version RePIP, that\nallow the joint optimization of both efficiency and informativeness. Through\ntoy examples and empirical results on the task of crop and weed image\nclassification in agricultural robotics, the current work shows how PIP-based\nconformal classifiers exhibit precisely the desired behavior in comparison with\nother nonconformity measures and strike a good balance between informativeness\nand efficiency.\n","authors":["Paul Melki","Lionel Bombrun","Boubacar Diallo","Jérôme Dias","Jean-Pierre da Costa"],"pdf_url":"https://arxiv.org/pdf/2406.08884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08877v1","updated":"2024-06-13T07:28:45Z","published":"2024-06-13T07:28:45Z","title":"EgoExo-Fitness: Towards Egocentric and Exocentric Full-Body Action\n  Understanding","summary":"  We present EgoExo-Fitness, a new full-body action understanding dataset,\nfeaturing fitness sequence videos recorded from synchronized egocentric and\nfixed exocentric (third-person) cameras. Compared with existing full-body\naction understanding datasets, EgoExo-Fitness not only contains videos from\nfirst-person perspectives, but also provides rich annotations. Specifically,\ntwo-level temporal boundaries are provided to localize single action videos\nalong with sub-steps of each action. More importantly, EgoExo-Fitness\nintroduces innovative annotations for interpretable action judgement--including\ntechnical keypoint verification, natural language comments on action execution,\nand action quality scores. Combining all of these, EgoExo-Fitness provides new\nresources to study egocentric and exocentric full-body action understanding\nacross dimensions of \"what\", \"when\", and \"how well\". To facilitate research on\negocentric and exocentric full-body action understanding, we construct\nbenchmarks on a suite of tasks (i.e., action classification, action\nlocalization, cross-view sequence verification, cross-view skill determination,\nand a newly proposed task of guidance-based execution verification), together\nwith detailed analysis. Code and data will be available at\nhttps://github.com/iSEE-Laboratory/EgoExo-Fitness/tree/main.\n","authors":["Yuan-Ming Li","Wei-Jin Huang","An-Lan Wang","Ling-An Zeng","Jing-Ke Meng","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.08877v1.pdf","comment":"33 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.08866v1","updated":"2024-06-13T07:09:41Z","published":"2024-06-13T07:09:41Z","title":"Zoom and Shift are All You Need","summary":"  Feature alignment serves as the primary mechanism for fusing multimodal data.\nWe put forth a feature alignment approach that achieves full integration of\nmultimodal information. This is accomplished via an alternating process of\nshifting and expanding feature representations across modalities to obtain a\nconsistent unified representation in a joint feature space. The proposed\ntechnique can reliably capture high-level interplay between features\noriginating from distinct modalities. Consequently, substantial gains in\nmultimodal learning performance are attained. Additionally, we demonstrate the\nsuperiority of our approach over other prevalent multimodal fusion schemes on a\nrange of tasks. Extensive experimental evaluation conducted on multimodal\ndatasets comprising time series, image, and text demonstrates that our method\nachieves state-of-the-art results.\n","authors":["Jiahao Qin"],"pdf_url":"https://arxiv.org/pdf/2406.08866v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.08863v1","updated":"2024-06-13T06:56:49Z","published":"2024-06-13T06:56:49Z","title":"Self-supervised Graph Neural Network for Mechanical CAD Retrieval","summary":"  CAD (Computer-Aided Design) plays a crucial role in mechanical industry,\nwhere large numbers of similar-shaped CAD parts are often created. Efficiently\nreusing these parts is key to reducing design and production costs for\nenterprises. Retrieval systems are vital for achieving CAD reuse, but the\ncomplex shapes of CAD models are difficult to accurately describe using text or\nkeywords, making traditional retrieval methods ineffective. While existing\nrepresentation learning approaches have been developed for CAD, manually\nlabeling similar samples in these methods is expensive. Additionally, CAD\nmodels' unique parameterized data structure presents challenges for applying\nexisting 3D shape representation learning techniques directly. In this work, we\npropose GC-CAD, a self-supervised contrastive graph neural network-based method\nfor mechanical CAD retrieval that directly models parameterized CAD raw files.\nGC-CAD consists of two key modules: structure-aware representation learning and\ncontrastive graph learning framework. The method leverages graph neural\nnetworks to extract both geometric and topological information from CAD models,\ngenerating feature representations. We then introduce a simple yet effective\ncontrastive graph learning framework approach, enabling the model to train\nwithout manual labels and generate retrieval-ready representations.\nExperimental results on four datasets including human evaluation demonstrate\nthat the proposed method achieves significant accuracy improvements and up to\n100 times efficiency improvement over the baseline methods.\n","authors":["Yuhan Quan","Huan ZHao","Jinfeng Yi","Yuqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08859v1","updated":"2024-06-13T06:48:25Z","published":"2024-06-13T06:48:25Z","title":"Fusion of regional and sparse attention in Vision Transformers","summary":"  Modern vision transformers leverage visually inspired local interaction\nbetween pixels through attention computed within window or grid regions, in\ncontrast to the global attention employed in the original ViT. Regional\nattention restricts pixel interactions within specific regions, while sparse\nattention disperses them across sparse grids. These differing approaches pose a\nchallenge between maintaining hierarchical relationships vs. capturing a global\ncontext. In this study, drawing inspiration from atrous convolution, we propose\nAtrous Attention, a blend of regional and sparse attention that dynamically\nintegrates both local and global information while preserving hierarchical\nstructures. Based on this, we introduce a versatile, hybrid vision transformer\nbackbone called ACC-ViT, tailored for standard vision tasks. Our compact model\nachieves approximately 84% accuracy on ImageNet-1K with fewer than 28.5 million\nparameters, outperforming the state-of-the-art MaxViT by 0.42% while requiring\n8.4% fewer parameters.\n","authors":["Nabil Ibtehaz","Ning Yan","Masood Mortazavi","Daisuke Kihara"],"pdf_url":"https://arxiv.org/pdf/2406.08859v1.pdf","comment":"Accepted as a Workshop Paper at T4V@CVPR2024. arXiv admin note:\n  substantial text overlap with arXiv:2403.04200"},{"id":"http://arxiv.org/abs/2403.03790v2","updated":"2024-06-13T06:46:14Z","published":"2024-03-06T15:35:53Z","title":"Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection\n  from Remote Sensing Imagery","summary":"  Ship detection needs to identify ship locations from remote sensing (RS)\nscenes. Due to different imaging payloads, various appearances of ships, and\ncomplicated background interference from the bird's eye view, it is difficult\nto set up a unified paradigm for achieving multi-source ship detection. To\naddress this challenge, in this article, leveraging the large language models\n(LLMs)'s powerful generalization ability, a unified visual-language model\ncalled Popeye is proposed for multi-source ship detection from RS imagery.\nSpecifically, to bridge the interpretation gap between the multi-source images\nfor ship detection, a novel unified labeling paradigm is designed to integrate\ndifferent visual modalities and the various ship detection ways, i.e.,\nhorizontal bounding box (HBB) and oriented bounding box (OBB). Subsequently,\nthe hybrid experts encoder is designed to refine multi-scale visual features,\nthereby enhancing visual perception. Then, a visual-language alignment method\nis developed for Popeye to enhance interactive comprehension ability between\nvisual and language content. Furthermore, an instruction adaption mechanism is\nproposed for transferring the pre-trained visual-language knowledge from the\nnature scene into the RS domain for multi-source ship detection. In addition,\nthe segment anything model (SAM) is also seamlessly integrated into the\nproposed Popeye to achieve pixel-level ship segmentation without additional\ntraining costs. Finally, extensive experiments are conducted on the newly\nconstructed ship instruction dataset named MMShip, and the results indicate\nthat the proposed Popeye outperforms current specialist, open-vocabulary, and\nother visual-language models for zero-shot multi-source ship detection.\n","authors":["Wei Zhang","Miaoxin Cai","Tong Zhang","Guoqiang Lei","Yin Zhuang","Xuerui Mao"],"pdf_url":"https://arxiv.org/pdf/2403.03790v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08858v1","updated":"2024-06-13T06:44:46Z","published":"2024-06-13T06:44:46Z","title":"OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body\n  Teleoperation and Learning","summary":"  We present OmniH2O (Omni Human-to-Humanoid), a learning-based system for\nwhole-body humanoid teleoperation and autonomy. Using kinematic pose as a\nuniversal control interface, OmniH2O enables various ways for a human to\ncontrol a full-sized humanoid with dexterous hands, including using real-time\nteleoperation through VR headset, verbal instruction, and RGB camera. OmniH2O\nalso enables full autonomy by learning from teleoperated demonstrations or\nintegrating with frontier models such as GPT-4. OmniH2O demonstrates\nversatility and dexterity in various real-world whole-body tasks through\nteleoperation or autonomy, such as playing multiple sports, moving and\nmanipulating objects, and interacting with humans. We develop an RL-based\nsim-to-real pipeline, which involves large-scale retargeting and augmentation\nof human motion datasets, learning a real-world deployable policy with sparse\nsensor input by imitating a privileged teacher policy, and reward designs to\nenhance robustness and stability. We release the first humanoid whole-body\ncontrol dataset, OmniH2O-6, containing six everyday tasks, and demonstrate\nhumanoid whole-body skill learning from teleoperated datasets.\n","authors":["Tairan He","Zhengyi Luo","Xialin He","Wenli Xiao","Chong Zhang","Weinan Zhang","Kris Kitani","Changliu Liu","Guanya Shi"],"pdf_url":"https://arxiv.org/pdf/2406.08858v1.pdf","comment":"Project page: https://omni.human2humanoid.com/"},{"id":"http://arxiv.org/abs/2311.17117v3","updated":"2024-06-13T06:37:20Z","published":"2023-11-28T12:27:15Z","title":"Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for\n  Character Animation","summary":"  Character Animation aims to generating character videos from still images\nthrough driving signals. Currently, diffusion models have become the mainstream\nin visual generation research, owing to their robust generative capabilities.\nHowever, challenges persist in the realm of image-to-video, especially in\ncharacter animation, where temporally maintaining consistency with detailed\ninformation from character remains a formidable problem. In this paper, we\nleverage the power of diffusion models and propose a novel framework tailored\nfor character animation. To preserve consistency of intricate appearance\nfeatures from reference image, we design ReferenceNet to merge detail features\nvia spatial attention. To ensure controllability and continuity, we introduce\nan efficient pose guider to direct character's movements and employ an\neffective temporal modeling approach to ensure smooth inter-frame transitions\nbetween video frames. By expanding the training data, our approach can animate\narbitrary characters, yielding superior results in character animation compared\nto other image-to-video methods. Furthermore, we evaluate our method on\nbenchmarks for fashion video and human dance synthesis, achieving\nstate-of-the-art results.\n","authors":["Li Hu","Xin Gao","Peng Zhang","Ke Sun","Bang Zhang","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2311.17117v3.pdf","comment":"Page: https://humanaigc.github.io/animate-anyone/"},{"id":"http://arxiv.org/abs/2405.20343v2","updated":"2024-06-13T06:36:38Z","published":"2024-05-30T17:59:54Z","title":"Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single\n  Image","summary":"  In this work, we introduce Unique3D, a novel image-to-3D framework for\nefficiently generating high-quality 3D meshes from single-view images,\nfeaturing state-of-the-art generation fidelity and strong generalizability.\nPrevious methods based on Score Distillation Sampling (SDS) can produce\ndiversified 3D results by distilling 3D knowledge from large 2D diffusion\nmodels, but they usually suffer from long per-case optimization time with\ninconsistent issues. Recent works address the problem and generate better 3D\nresults either by finetuning a multi-view diffusion model or training a fast\nfeed-forward model. However, they still lack intricate textures and complex\ngeometries due to inconsistency and limited generated resolution. To\nsimultaneously achieve high fidelity, consistency, and efficiency in single\nimage-to-3D, we propose a novel framework Unique3D that includes a multi-view\ndiffusion model with a corresponding normal diffusion model to generate\nmulti-view images with their normal maps, a multi-level upscale process to\nprogressively improve the resolution of generated orthographic multi-views, as\nwell as an instant and consistent mesh reconstruction algorithm called ISOMER,\nwhich fully integrates the color and geometric priors into mesh results.\nExtensive experiments demonstrate that our Unique3D significantly outperforms\nother image-to-3D baselines in terms of geometric and textural details.\n","authors":["Kailu Wu","Fangfu Liu","Zhihan Cai","Runjie Yan","Hanyang Wang","Yating Hu","Yueqi Duan","Kaisheng Ma"],"pdf_url":"https://arxiv.org/pdf/2405.20343v2.pdf","comment":"Project page: https://wukailu.github.io/Unique3D"},{"id":"http://arxiv.org/abs/2406.08850v1","updated":"2024-06-13T06:27:13Z","published":"2024-06-13T06:27:13Z","title":"COVE: Unleashing the Diffusion Feature Correspondence for Consistent\n  Video Editing","summary":"  Video editing is an emerging task, in which most current methods adopt the\npre-trained text-to-image (T2I) diffusion model to edit the source video in a\nzero-shot manner. Despite extensive efforts, maintaining the temporal\nconsistency of edited videos remains challenging due to the lack of temporal\nconstraints in the regular T2I diffusion model. To address this issue, we\npropose COrrespondence-guided Video Editing (COVE), leveraging the inherent\ndiffusion feature correspondence to achieve high-quality and consistent video\nediting. Specifically, we propose an efficient sliding-window-based strategy to\ncalculate the similarity among tokens in the diffusion features of source\nvideos, identifying the tokens with high correspondence across frames. During\nthe inversion and denoising process, we sample the tokens in noisy latent based\non the correspondence and then perform self-attention within them. To save GPU\nmemory usage and accelerate the editing process, we further introduce the\ntemporal-dimensional token merging strategy, which can effectively reduce\nredundancy. COVE can be seamlessly integrated into the pre-trained T2I\ndiffusion model without the need for extra training or optimization. Extensive\nexperiment results demonstrate that COVE achieves the start-of-the-art\nperformance in various video editing scenarios, outperforming existing methods\nboth quantitatively and qualitatively. The code will be release at\nhttps://github.com/wangjiangshan0725/COVE\n","authors":["Jiangshan Wang","Yue Ma","Jiayi Guo","Yicheng Xiao","Gao Huang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2406.08850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08845v1","updated":"2024-06-13T06:09:22Z","published":"2024-06-13T06:09:22Z","title":"Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing\n  Reliability,Reproducibility, and Practicality","summary":"  Recent text-to-video (T2V) technology advancements, as demonstrated by models\nsuch as Gen2, Pika, and Sora, have significantly broadened its applicability\nand popularity. Despite these strides, evaluating these models poses\nsubstantial challenges. Primarily, due to the limitations inherent in automatic\nmetrics, manual evaluation is often considered a superior method for assessing\nT2V generation. However, existing manual evaluation protocols face\nreproducibility, reliability, and practicality issues. To address these\nchallenges, this paper introduces the Text-to-Video Human Evaluation (T2VHE)\nprotocol, a comprehensive and standardized protocol for T2V models. The T2VHE\nprotocol includes well-defined metrics, thorough annotator training, and an\neffective dynamic evaluation module. Experimental results demonstrate that this\nprotocol not only ensures high-quality annotations but can also reduce\nevaluation costs by nearly 50%. We will open-source the entire setup of the\nT2VHE protocol, including the complete protocol workflow, the dynamic\nevaluation component details, and the annotation interface code. This will help\ncommunities establish more sophisticated human assessment protocols.\n","authors":["Tianle Zhang","Langtian Ma","Yuchen Yan","Yuchen Zhang","Kai Wang","Yue Yang","Ziyao Guo","Wenqi Shao","Yang You","Yu Qiao","Ping Luo","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08840v1","updated":"2024-06-13T06:04:34Z","published":"2024-06-13T06:04:34Z","title":"Conceptual Learning via Embedding Approximations for Reinforcing\n  Interpretability and Transparency","summary":"  Concept bottleneck models (CBMs) have emerged as critical tools in domains\nwhere interpretability is paramount. These models rely on predefined textual\ndescriptions, referred to as concepts, to inform their decision-making process\nand offer more accurate reasoning. As a result, the selection of concepts used\nin the model is of utmost significance. This study proposes\n\\underline{\\textbf{C}}onceptual \\underline{\\textbf{L}}earning via\n\\underline{\\textbf{E}}mbedding \\underline{\\textbf{A}}pproximations for\n\\underline{\\textbf{R}}einforcing Interpretability and Transparency, abbreviated\nas CLEAR, a framework for constructing a CBM for image classification. Using\nscore matching and Langevin sampling, we approximate the embedding of concepts\nwithin the latent space of a vision-language model (VLM) by learning the scores\nassociated with the joint distribution of images and concepts. A concept\nselection process is then employed to optimize the similarity between the\nlearned embeddings and the predefined ones. The derived bottleneck offers\ninsights into the CBM's decision-making process, enabling more comprehensive\ninterpretations. Our approach was evaluated through extensive experiments and\nachieved state-of-the-art performance on various benchmarks. The code for our\nexperiments is available at https://github.com/clearProject/CLEAR/tree/main\n","authors":["Maor Dikter","Tsachi Blau","Chaim Baskin"],"pdf_url":"https://arxiv.org/pdf/2406.08840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08839v1","updated":"2024-06-13T06:04:19Z","published":"2024-06-13T06:04:19Z","title":"NeRF Director: Revisiting View Selection in Neural Volume Rendering","summary":"  Neural Rendering representations have significantly contributed to the field\nof 3D computer vision. Given their potential, considerable efforts have been\ninvested to improve their performance. Nonetheless, the essential question of\nselecting training views is yet to be thoroughly investigated. This key aspect\nplays a vital role in achieving high-quality results and aligns with the\nwell-known tenet of deep learning: \"garbage in, garbage out\". In this paper, we\nfirst illustrate the importance of view selection by demonstrating how a simple\nrotation of the test views within the most pervasive NeRF dataset can lead to\nconsequential shifts in the performance rankings of state-of-the-art\ntechniques. To address this challenge, we introduce a unified framework for\nview selection methods and devise a thorough benchmark to assess its impact.\nSignificant improvements can be achieved without leveraging error or\nuncertainty estimation but focusing on uniform view coverage of the\nreconstructed object, resulting in a training-free approach. Using this\ntechnique, we show that high-quality renderings can be achieved faster by using\nfewer views. We conduct extensive experiments on both synthetic datasets and\nrealistic data to demonstrate the effectiveness of our proposed method compared\nwith random, conventional error-based, and uncertainty-guided view selection.\n","authors":["Wenhui Xiao","Rodrigo Santa Cruz","David Ahmedt-Aristizabal","Olivier Salvado","Clinton Fookes","Leo Lebrat"],"pdf_url":"https://arxiv.org/pdf/2406.08839v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2406.08837v1","updated":"2024-06-13T06:00:28Z","published":"2024-06-13T06:00:28Z","title":"Research on Deep Learning Model of Feature Extraction Based on\n  Convolutional Neural Network","summary":"  Neural networks with relatively shallow layers and simple structures may have\nlimited ability in accurately identifying pneumonia. In addition, deep neural\nnetworks also have a large demand for computing resources, which may cause\nconvolutional neural networks to be unable to be implemented on terminals.\nTherefore, this paper will carry out the optimal classification of\nconvolutional neural networks. Firstly, according to the characteristics of\npneumonia images, AlexNet and InceptionV3 were selected to obtain better image\nrecognition results. Combining the features of medical images, the forward\nneural network with deeper and more complex structure is learned. Finally,\nknowledge extraction technology is used to extract the obtained data into the\nAlexNet model to achieve the purpose of improving computing efficiency and\nreducing computing costs. The results showed that the prediction accuracy,\nspecificity, and sensitivity of the trained AlexNet model increased by 4.25\npercentage points, 7.85 percentage points, and 2.32 percentage points,\nrespectively. The graphics processing usage has decreased by 51% compared to\nthe InceptionV3 mode.\n","authors":["Houze Liu","Iris Li","Yaxin Liang","Dan Sun","Yining Yang","Haowei Yang"],"pdf_url":"https://arxiv.org/pdf/2406.08837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18820v2","updated":"2024-06-13T05:41:27Z","published":"2024-04-29T16:02:38Z","title":"Towards Extreme Image Compression with Latent Feature Guidance and\n  Diffusion Prior","summary":"  Image compression at extremely low bitrates (below 0.1 bits per pixel (bpp))\nis a significant challenge due to substantial information loss. In this work,\nwe propose a novel two-stage extreme image compression framework that exploits\nthe powerful generative capability of pre-trained diffusion models to achieve\nrealistic image reconstruction at extremely low bitrates. In the first stage,\nwe treat the latent representation of images in the diffusion space as\nguidance, employing a VAE-based compression approach to compress images and\ninitially decode the compressed information into content variables. The second\nstage leverages pre-trained stable diffusion to reconstruct images under the\nguidance of content variables. Specifically, we introduce a small control\nmodule to inject content information while keeping the stable diffusion model\nfixed to maintain its generative capability. Furthermore, we design a space\nalignment loss to force the content variables to align with the diffusion space\nand provide the necessary constraints for optimization. Extensive experiments\ndemonstrate that our method significantly outperforms state-of-the-art\napproaches in terms of visual performance at extremely low bitrates.\n","authors":["Zhiyuan Li","Yanhui Zhou","Hao Wei","Chenyang Ge","Jingwen Jiang"],"pdf_url":"https://arxiv.org/pdf/2404.18820v2.pdf","comment":"Submitted to IEEE TCSVT"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.09355v1","updated":"2024-06-13T17:40:56Z","published":"2024-06-13T17:40:56Z","title":"Can't Hide Behind the API: Stealing Black-Box Commercial Embedding\n  Models","summary":"  Embedding models that generate representation vectors from natural language\ntext are widely used, reflect substantial investments, and carry significant\ncommercial value. Companies such as OpenAI and Cohere have developed competing\nembedding models accessed through APIs that require users to pay for usage. In\nthis architecture, the models are \"hidden\" behind APIs, but this does not mean\nthat they are \"well guarded\". We present, to our knowledge, the first effort to\n\"steal\" these models for retrieval by training local models on text-embedding\npairs obtained from the commercial APIs. Our experiments show using standard\nbenchmarks that it is possible to efficiently replicate the retrieval\neffectiveness of the commercial embedding models using an attack that costs\nonly around $200 to train (presumably) smaller models with fewer dimensions.\nOur findings raise important considerations for deploying commercial embedding\nmodels and suggest measures to mitigate the risk of model theft.\n","authors":["Manveer Singh Tamber","Jasper Xian","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2406.09355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09323v1","updated":"2024-06-13T17:01:28Z","published":"2024-06-13T17:01:28Z","title":"Master of Disaster: A Disaster-Related Event Monitoring System From News\n  Streams","summary":"  The need for a disaster-related event monitoring system has arisen due to the\nsocietal and economic impact caused by the increasing number of severe disaster\nevents. An event monitoring system should be able to extract event-related\ninformation from texts, and discriminates event instances. We demonstrate our\nopen-source event monitoring system, namely, Master of Disaster (MoD), which\nreceives news streams, extracts event information, links extracted information\nto a knowledge graph (KG), in this case Wikidata, and discriminates event\ninstances visually. The goal of event visualization is to group event mentions\nreferring to the same real-world event instance so that event instance\ndiscrimination can be achieved by visual screening.\n","authors":["Junbo Huang","Ricardo Usbeck"],"pdf_url":"https://arxiv.org/pdf/2406.09323v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.09320v1","updated":"2024-06-13T16:58:02Z","published":"2024-06-13T16:58:02Z","title":"Khmer Semantic Search Engine: Digital Information Access and Document\n  Retrieval","summary":"  The search engine process is crucial for document content retrieval. For\nKhmer documents, a tool is needed to extract essential keywords. Despite the\ndaily generation of significant Khmer content, Cambodians struggle to find\nnecessary documents due to the lack of an effective semantic searching tool.\nEven Google does not deliver high accuracy for Khmer content. Semantic search\nengines improve search results by employing advanced algorithms to understand\nvarious content types. With the rise in Khmer digital content such as reports,\narticles, and social media feedback enhanced search capabilities are essential.\nThis research proposes the first Khmer Semantic Search Engine (KSE), designed\nto improve traditional Khmer search methods. Utilizing semantic matching\ntechniques and formally annotated semantic content, our tool extracts\nmeaningful keywords from user queries performs precise matching, and provides\nthe best matching offline documents and online URL documents. We propose two\nsemantic search frameworks based on keyword extraction and semantic search\nmatching. Additionally, we developed tools for data preparation, including\ndocument addition and manual keyword extraction. To evaluate performance, we\ncreated a ground truth dataset and discussed issues related to searching and\nsemantic search. Our findings show how understanding search term semantics can\nlead to more accurate results.\n","authors":["Nimol Thuon"],"pdf_url":"https://arxiv.org/pdf/2406.09320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.10718v3","updated":"2024-06-13T15:55:33Z","published":"2022-10-19T16:53:08Z","title":"Whole Page Unbiased Learning to Rank","summary":"  The page presentation biases in the information retrieval system, especially\non the click behavior, is a well-known challenge that hinders improving ranking\nmodels' performance with implicit user feedback. Unbiased Learning to\nRank~(ULTR) algorithms are then proposed to learn an unbiased ranking model\nwith biased click data. However, most existing algorithms are specifically\ndesigned to mitigate position-related bias, e.g., trust bias, without\nconsidering biases induced by other features in search result page\npresentation(SERP), e.g. attractive bias induced by the multimedia.\nUnfortunately, those biases widely exist in industrial systems and may lead to\nan unsatisfactory search experience. Therefore, we introduce a new problem,\ni.e., whole-page Unbiased Learning to Rank(WP-ULTR), aiming to handle biases\ninduced by whole-page SERP features simultaneously. It presents tremendous\nchallenges: (1) a suitable user behavior model (user behavior hypothesis) can\nbe hard to find; and (2) complex biases cannot be handled by existing\nalgorithms. To address the above challenges, we propose a Bias Agnostic\nwhole-page unbiased Learning to rank algorithm, named BAL, to automatically\nfind the user behavior model with causal discovery and mitigate the biases\ninduced by multiple SERP features with no specific design. Experimental results\non a real-world dataset verify the effectiveness of the BAL.\n","authors":["Haitao Mao","Lixin Zou","Yujia Zheng","Jiliang Tang","Xiaokai Chu","Jiashu Zhao","Qian Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2210.10718v3.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.09215v1","updated":"2024-06-13T15:16:11Z","published":"2024-06-13T15:16:11Z","title":"On Softmax Direct Preference Optimization for Recommendation","summary":"  Recommender systems aim to predict personalized rankings based on user\npreference data. With the rise of Language Models (LMs), LM-based recommenders\nhave been widely explored due to their extensive world knowledge and powerful\nreasoning abilities. Most of the LM-based recommenders convert historical\ninteractions into language prompts, pairing with a positive item as the target\nresponse and fine-tuning LM with a language modeling loss. However, the current\nobjective fails to fully leverage preference data and is not optimized for\npersonalized ranking tasks, which hinders the performance of LM-based\nrecommenders. Inspired by the current advancement of Direct Preference\nOptimization (DPO) in human preference alignment and the success of softmax\nloss in recommendations, we propose Softmax-DPO (\\textbf{S-DPO}) to instill\nranking information into the LM to help LM-based recommenders distinguish\npreferred items from negatives, rather than solely focusing on positives.\nSpecifically, we incorporate multiple negatives in user preference data and\ndevise an alternative version of DPO loss tailored for LM-based recommenders,\nconnected to softmax sampling strategies. Theoretically, we bridge S-DPO with\nthe softmax loss over negative sampling and find that it has a side effect of\nmining hard negatives, which assures its exceptional capabilities in\nrecommendation tasks. Empirically, extensive experiments conducted on three\nreal-world datasets demonstrate the superiority of S-DPO to effectively model\nuser preference and further boost recommendation performance while mitigating\nthe data likelihood decline issue of DPO. Our codes are available at\nhttps://github.com/chenyuxin1999/S-DPO.\n","authors":["Yuxin Chen","Junfei Tan","An Zhang","Zhengyi Yang","Leheng Sheng","Enzhi Zhang","Xiang Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2406.09215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09188v1","updated":"2024-06-13T14:49:28Z","published":"2024-06-13T14:49:28Z","title":"Reducing Task Discrepancy of Text Encoders for Zero-Shot Composed Image\n  Retrieval","summary":"  Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nreference image and conditioning text, enabling controllable searches. Due to\nthe expensive dataset construction cost for CIR triplets, a zero-shot (ZS) CIR\nsetting has been actively studied to eliminate the need for human-collected\ntriplet datasets. The mainstream of ZS-CIR employs an efficient projection\nmodule that projects a CLIP image embedding to the CLIP text token embedding\nspace, while fixing the CLIP encoders. Using the projected image embedding,\nthese methods generate image-text composed features by using the pre-trained\ntext encoder. However, their CLIP image and text encoders suffer from the task\ndiscrepancy between the pre-training task (text $\\leftrightarrow$ image) and\nthe target CIR task (image + text $\\leftrightarrow$ image). Conceptually, we\nneed expensive triplet samples to reduce the discrepancy, but we use cheap text\ntriplets instead and update the text encoder. To that end, we introduce the\nReducing Task Discrepancy of text encoders for Composed Image Retrieval (RTD),\na plug-and-play training scheme for the text encoder that enhances its\ncapability using a novel target-anchored text contrastive learning. We also\npropose two additional techniques to improve the proposed learning scheme: a\nhard negatives-based refined batch sampling strategy and a sophisticated\nconcatenation scheme. Integrating RTD into the state-of-the-art\nprojection-based ZS-CIR methods significantly improves performance across\nvarious datasets and backbones, demonstrating its efficiency and\ngeneralizability.\n","authors":["Jaeseok Byun","Seokhyeon Jeong","Wonjae Kim","Sanghyuk Chun","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2406.09188v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2402.03915v2","updated":"2024-06-13T12:02:23Z","published":"2024-02-06T11:31:04Z","title":"Learning Metrics that Maximise Power for Accelerated A/B-Tests","summary":"  Online controlled experiments are a crucial tool to allow for confident\ndecision-making in technology companies. A North Star metric is defined (such\nas long-term revenue or user retention), and system variants that statistically\nsignificantly improve on this metric in an A/B-test can be considered superior.\nNorth Star metrics are typically delayed and insensitive. As a result, the cost\nof experimentation is high: experiments need to run for a long time, and even\nthen, type-II errors (i.e. false negatives) are prevalent.\n  We propose to tackle this by learning metrics from short-term signals that\ndirectly maximise the statistical power they harness with respect to the North\nStar. We show that existing approaches are prone to overfitting, in that higher\naverage metric sensitivity does not imply improved type-II errors, and propose\nto instead minimise the $p$-values a metric would have produced on a log of\npast experiments. We collect such datasets from two social media applications\nwith over 160 million Monthly Active Users each, totalling over 153 A/B-pairs.\nEmpirical results show that we are able to increase statistical power by up to\n78% when using our learnt metrics stand-alone, and by up to 210% when used in\ntandem with the North Star. Alternatively, we can obtain constant statistical\npower at a sample size that is down to 12% of what the North Star requires,\nsignificantly reducing the cost of experimentation.\n","authors":["Olivier Jeunen","Aleksei Ustimenko"],"pdf_url":"https://arxiv.org/pdf/2402.03915v2.pdf","comment":"To appear in the Applied Data Science track at the ACM SIGKDD\n  Conference on Knowledge Discovery and Data Mining (KDD '24)"},{"id":"http://arxiv.org/abs/2406.09021v1","updated":"2024-06-13T11:55:40Z","published":"2024-06-13T11:55:40Z","title":"Contextual Distillation Model for Diversified Recommendation","summary":"  The diversity of recommendation is equally crucial as accuracy in improving\nuser experience. Existing studies, e.g., Determinantal Point Process (DPP) and\nMaximal Marginal Relevance (MMR), employ a greedy paradigm to iteratively\nselect items that optimize both accuracy and diversity. However, prior methods\ntypically exhibit quadratic complexity, limiting their applications to the\nre-ranking stage and are not applicable to other recommendation stages with a\nlarger pool of candidate items, such as the pre-ranking and ranking stages. In\nthis paper, we propose Contextual Distillation Model (CDM), an efficient\nrecommendation model that addresses diversification, suitable for the\ndeployment in all stages of industrial recommendation pipelines. Specifically,\nCDM utilizes the candidate items in the same user request as context to enhance\nthe diversification of the results. We propose a contrastive context encoder\nthat employs attention mechanisms to model both positive and negative contexts.\nFor the training of CDM, we compare each target item with its context embedding\nand utilize the knowledge distillation framework to learn the win probability\nof each target item under the MMR algorithm, where the teacher is derived from\nMMR outputs. During inference, ranking is performed through a linear\ncombination of the recommendation and student model scores, ensuring both\ndiversity and efficiency. We perform offline evaluations on two industrial\ndatasets and conduct online A/B test of CDM on the short-video platform\nKuaiShou. The considerable enhancements observed in both recommendation quality\nand diversity, as shown by metrics, provide strong superiority for the\neffectiveness of CDM.\n","authors":["Fan Li","Xu Si","Shisong Tang","Dingmin Wang","Kunyan Han","Bing Han","Guorui Zhou","Yang Song","Hechang Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09021v1.pdf","comment":"accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2204.00815v2","updated":"2024-06-13T09:13:30Z","published":"2022-04-02T08:47:20Z","title":"Unbiased Top-k Learning to Rank with Causal Likelihood Decomposition","summary":"  Unbiased learning to rank has been proposed to alleviate the biases in the\nsearch ranking, making it possible to train ranking models with user\ninteraction data. In real applications, search engines are designed to display\nonly the most relevant k documents from the retrieved candidate set. The rest\ncandidates are discarded. As a consequence, position bias and sample selection\nbias usually occur simultaneously. Existing unbiased learning to rank\napproaches either focus on one type of bias (e.g., position bias) or mitigate\nthe position bias and sample selection bias with separate components,\noverlooking their associations. In this study, we first analyze the mechanisms\nand associations of position bias and sample selection bias from the viewpoint\nof a causal graph. Based on the analysis, we propose Causal Likelihood\nDecomposition (CLD), a unified approach to simultaneously mitigating these two\nbiases in top-k learning to rank. By decomposing the log-likelihood of the\nbiased data as an unbiased term that only related to relevance, plus other\nterms related to biases, CLD successfully detaches the relevance from position\nbias and sample selection bias. An unbiased ranking model can be obtained from\nthe unbiased term, via maximizing the whole likelihood. An extension to the\npairwise neural ranking is also developed. Advantages of CLD include\ntheoretical soundness and a unified framework for pointwise and pairwise\nunbiased top-k learning to rank. Extensive experimental results verified that\nCLD, including its pairwise neural extension, outperformed the baselines by\nmitigating both the position bias and the sample selection bias. Empirical\nstudies also showed that CLD is robust to the variation of bias severity and\nthe click noise.\n","authors":["Haiyuan Zhao","Jun Xu","Xiao Zhang","Guohao Cai","Zhenhua Dong","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2204.00815v2.pdf","comment":"Accepted by SIGIR-AP 2023"},{"id":"http://arxiv.org/abs/2406.07932v2","updated":"2024-06-13T09:08:32Z","published":"2024-06-12T06:55:35Z","title":"Counteracting Duration Bias in Video Recommendation via Counterfactual\n  Watch Time","summary":"  In video recommendation, an ongoing effort is to satisfy users' personalized\ninformation needs by leveraging their logged watch time. However, watch time\nprediction suffers from duration bias, hindering its ability to reflect users'\ninterests accurately. Existing label-correction approaches attempt to uncover\nuser interests through grouping and normalizing observed watch time according\nto video duration. Although effective to some extent, we found that these\napproaches regard completely played records (i.e., a user watches the entire\nvideo) as equally high interest, which deviates from what we observed on real\ndatasets: users have varied explicit feedback proportion when completely\nplaying videos. In this paper, we introduce the counterfactual watch time(CWT),\nthe potential watch time a user would spend on the video if its duration is\nsufficiently long. Analysis shows that the duration bias is caused by the\ntruncation of CWT due to the video duration limitation, which usually occurs on\nthose completely played records. Besides, a Counterfactual Watch Model (CWM) is\nproposed, revealing that CWT equals the time users get the maximum benefit from\nvideo recommender systems. Moreover, a cost-based transform function is defined\nto transform the CWT into the estimation of user interest, and the model can be\nlearned by optimizing a counterfactual likelihood function defined over\nobserved user watch times. Extensive experiments on three real video\nrecommendation datasets and online A/B testing demonstrated that CWM\neffectively enhanced video recommendation accuracy and counteracted the\nduration bias.\n","authors":["Haiyuan Zhao","Guohao Cai","Jieming Zhu","Zhenhua Dong","Jun Xu","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2406.07932v2.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2406.08105v2","updated":"2024-06-13T08:53:56Z","published":"2024-06-12T11:34:19Z","title":"Prediction of the Realisation of an Information Need: An EEG Study","summary":"  One of the foundational goals of Information Retrieval (IR) is to satisfy\nsearchers' Information Needs (IN). Understanding how INs physically manifest\nhas long been a complex and elusive process. However, recent studies utilising\nElectroencephalography (EEG) data have provided real-time insights into the\nneural processes associated with INs. Unfortunately, they have yet to\ndemonstrate how this insight can practically benefit the search experience. As\nsuch, within this study, we explore the ability to predict the realisation of\nIN within EEG data across 14 subjects whilst partaking in a Question-Answering\n(Q/A) task. Furthermore, we investigate the combinations of EEG features that\nyield optimal predictive performance, as well as identify regions within the\nQ/A queries where a subject's realisation of IN is more pronounced. The\nfindings from this work demonstrate that EEG data is sufficient for the\nreal-time prediction of the realisation of an IN across all subjects with an\naccuracy of 73.5% (SD 2.6%) and on a per-subject basis with an accuracy of\n90.1% (SD 22.1%). This work helps to close the gap by bridging theoretical\nneuroscientific advancements with tangible improvements in information\nretrieval practices, paving the way for real-time prediction of the realisation\nof IN.\n","authors":["Niall McGuire","Dr Yashar Moshfeghi"],"pdf_url":"https://arxiv.org/pdf/2406.08105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19046v3","updated":"2024-06-13T07:57:49Z","published":"2024-05-29T12:43:39Z","title":"Continual Collaborative Distillation for Recommender System","summary":"  Knowledge distillation (KD) has emerged as a promising technique for\naddressing the computational challenges associated with deploying large-scale\nrecommender systems. KD transfers the knowledge of a massive teacher system to\na compact student model, to reduce the huge computational burdens for inference\nwhile retaining high accuracy. The existing KD studies primarily focus on\none-time distillation in static environments, leaving a substantial gap in\ntheir applicability to real-world scenarios dealing with continuously incoming\nusers, items, and their interactions. In this work, we delve into a systematic\napproach to operating the teacher-student KD in a non-stationary data stream.\nOur goal is to enable efficient deployment through a compact student, which\npreserves the high performance of the massive teacher, while effectively\nadapting to continuously incoming data. We propose Continual Collaborative\nDistillation (CCD) framework, where both the teacher and the student\ncontinually and collaboratively evolve along the data stream. CCD facilitates\nthe student in effectively adapting to new data, while also enabling the\nteacher to fully leverage accumulated knowledge. We validate the effectiveness\nof CCD through extensive quantitative, ablative, and exploratory experiments on\ntwo real-world datasets. We expect this research direction to contribute to\nnarrowing the gap between existing KD studies and practical applications,\nthereby enhancing the applicability of KD in real-world systems.\n","authors":["Gyuseok Lee","SeongKu Kang","Wonbin Kweon","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2405.19046v3.pdf","comment":"Accepted by KDD 2024 research track. 9 main pages + 1 appendix page,\n  5 figures"},{"id":"http://arxiv.org/abs/2404.15678v4","updated":"2024-06-13T07:53:06Z","published":"2024-04-24T06:16:09Z","title":"Retrieval and Distill: A Temporal Data Shift-Free Paradigm for Online\n  Recommendation System","summary":"  Current recommendation systems are significantly affected by a serious issue\nof temporal data shift, which is the inconsistency between the distribution of\nhistorical data and that of online data. Most existing models focus on\nutilizing updated data, overlooking the transferable, temporal data shift-free\ninformation that can be learned from shifting data. We propose the Temporal\nInvariance of Association theorem, which suggests that given a fixed search\nspace, the relationship between the data and the data in the search space keeps\ninvariant over time. Leveraging this principle, we designed a retrieval-based\nrecommendation system framework that can train a data shift-free relevance\nnetwork using shifting data, significantly enhancing the predictive performance\nof the original model in the recommendation system. However, retrieval-based\nrecommendation models face substantial inference time costs when deployed\nonline. To address this, we further designed a distill framework that can\ndistill information from the relevance network into a parameterized module\nusing shifting data. The distilled model can be deployed online alongside the\noriginal model, with only a minimal increase in inference time. Extensive\nexperiments on multiple real datasets demonstrate that our framework\nsignificantly improves the performance of the original model by utilizing\nshifting data.\n","authors":["Lei Zheng","Ning Li","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.15678v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08891v1","updated":"2024-06-13T07:44:21Z","published":"2024-06-13T07:44:21Z","title":"Robust Information Retrieval","summary":"  Beyond effectiveness, the robustness of an information retrieval (IR) system\nis increasingly attracting attention. When deployed, a critical technology such\nas IR should not only deliver strong performance on average but also have the\nability to handle a variety of exceptional situations. In recent years,\nresearch into the robustness of IR has seen significant growth, with numerous\nresearchers offering extensive analyses and proposing myriad strategies to\naddress robustness challenges. In this tutorial, we first provide background\ninformation covering the basics and a taxonomy of robustness in IR. Then, we\nexamine adversarial robustness and out-of-distribution (OOD) robustness within\nIR-specific contexts, extensively reviewing recent progress in methods to\nenhance robustness. The tutorial concludes with a discussion on the robustness\nof IR in the context of large language models (LLMs), highlighting ongoing\nchallenges and promising directions for future research. This tutorial aims to\ngenerate broader attention to robustness issues in IR, facilitate an\nunderstanding of the relevant literature, and lower the barrier to entry for\ninterested researchers and practitioners.\n","authors":["Yu-An Liu","Ruqing Zhang","Jiafeng Guo","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2406.08891v1.pdf","comment":"accepted by SIGIR2024 Tutorial"},{"id":"http://arxiv.org/abs/2404.12008v3","updated":"2024-06-13T07:31:09Z","published":"2024-04-18T08:59:32Z","title":"How Do Recommendation Models Amplify Popularity Bias? An Analysis from\n  the Spectral Perspective","summary":"  Recommendation Systems (RS) are often plagued by popularity bias. When\ntraining a recommendation model on a typically long-tailed dataset, the model\ntends to not only inherit this bias but often exacerbate it, resulting in\nover-representation of popular items in the recommendation lists. This study\nconducts comprehensive empirical and theoretical analyses to expose the root\ncauses of this phenomenon, yielding two core insights: 1) Item popularity is\nmemorized in the principal spectrum of the score matrix predicted by the\nrecommendation model; 2) The dimension collapse phenomenon amplifies the\nrelative prominence of the principal spectrum, thereby intensifying the\npopularity bias. Building on these insights, we propose a novel debiasing\nstrategy that leverages a spectral norm regularizer to penalize the magnitude\nof the principal singular value. We have developed an efficient algorithm to\nexpedite the calculation of the spectral norm by exploiting the spectral\nproperty of the score matrix. Extensive experiments across seven real-world\ndatasets and three testing paradigms have been conducted to validate the\nsuperiority of the proposed method.\n","authors":["Siyi Lin","Chongming Gao","Jiawei Chen","Sheng Zhou","Binbin Hu","Yan Feng","Chun Chen","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12008v3.pdf","comment":"23 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.08863v1","updated":"2024-06-13T06:56:49Z","published":"2024-06-13T06:56:49Z","title":"Self-supervised Graph Neural Network for Mechanical CAD Retrieval","summary":"  CAD (Computer-Aided Design) plays a crucial role in mechanical industry,\nwhere large numbers of similar-shaped CAD parts are often created. Efficiently\nreusing these parts is key to reducing design and production costs for\nenterprises. Retrieval systems are vital for achieving CAD reuse, but the\ncomplex shapes of CAD models are difficult to accurately describe using text or\nkeywords, making traditional retrieval methods ineffective. While existing\nrepresentation learning approaches have been developed for CAD, manually\nlabeling similar samples in these methods is expensive. Additionally, CAD\nmodels' unique parameterized data structure presents challenges for applying\nexisting 3D shape representation learning techniques directly. In this work, we\npropose GC-CAD, a self-supervised contrastive graph neural network-based method\nfor mechanical CAD retrieval that directly models parameterized CAD raw files.\nGC-CAD consists of two key modules: structure-aware representation learning and\ncontrastive graph learning framework. The method leverages graph neural\nnetworks to extract both geometric and topological information from CAD models,\ngenerating feature representations. We then introduce a simple yet effective\ncontrastive graph learning framework approach, enabling the model to train\nwithout manual labels and generate retrieval-ready representations.\nExperimental results on four datasets including human evaluation demonstrate\nthat the proposed method achieves significant accuracy improvements and up to\n100 times efficiency improvement over the baseline methods.\n","authors":["Yuhan Quan","Huan ZHao","Jinfeng Yi","Yuqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09295v2","updated":"2024-06-13T05:59:13Z","published":"2024-03-14T11:35:08Z","title":"Seed-based information retrieval in networks of research publications:\n  Evaluation of direct citations, bibliographic coupling, co-citations and\n  PubMed related article score","summary":"  In this contribution, we deal with seed-based information retrieval in\nnetworks of research publications. Using systematic reviews as a baseline, and\npublication data from the NIH Open Citation Collection, we compare the\nperformance of the three citation-based approaches direct citation,\nco-citation, and bibliographic coupling with respect to recall and precision\nmeasures. In addition, we include the PubMed Related Article score as well as\ncombined approaches in the comparison. We also provide a fairly comprehensive\nreview of earlier research in which citation relations have been used for\ninformation retrieval purposes. The results show an advantage for co-citation\nover bibliographic coupling and direct citation. However, combining the three\napproaches outperforms the exclusive use of co-citation in the study. The\nresults further indicate, in line with previous research, that combining\ncitation-based approaches with textual approaches enhances the performance of\nseed-based information retrieval. The results from the study may guide\napproaches combining citation-based and textual approaches in their choice of\ncitation similarity measures. We suggest that future research use more\nstructured approaches to evaluate methods for seed-based retrieval of\npublications, including comparative approaches as well as the elaboration of\ncommon data sets and baselines for evaluation.\n","authors":["Peter Sjögårde","Per Ahlgren"],"pdf_url":"https://arxiv.org/pdf/2403.09295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08827v1","updated":"2024-06-13T05:37:54Z","published":"2024-06-13T05:37:54Z","title":"How Powerful is Graph Filtering for Recommendation","summary":"  It has been shown that the effectiveness of graph convolutional network (GCN)\nfor recommendation is attributed to the spectral graph filtering. Most\nGCN-based methods consist of a graph filter or followed by a low-rank mapping\noptimized based on supervised training. However, we show two limitations\nsuppressing the power of graph filtering: (1) Lack of generality. Due to the\nvaried noise distribution, graph filters fail to denoise sparse data where\nnoise is scattered across all frequencies, while supervised training results in\nworse performance on dense data where noise is concentrated in middle\nfrequencies that can be removed by graph filters without training. (2) Lack of\nexpressive power. We theoretically show that linear GCN (LGCN) that is\neffective on collaborative filtering (CF) cannot generate arbitrary embeddings,\nimplying the possibility that optimal data representation might be unreachable.\n  To tackle the first limitation, we show close relation between noise\ndistribution and the sharpness of spectrum where a sharper spectral\ndistribution is more desirable causing data noise to be separable from\nimportant features without training. Based on this observation, we propose a\ngeneralized graph normalization G^2N to adjust the sharpness of spectral\ndistribution in order to redistribute data noise to assure that it can be\nremoved by graph filtering without training. As for the second limitation, we\npropose an individualized graph filter (IGF) adapting to the different\nconfidence levels of the user preference that interactions can reflect, which\nis proved to be able to generate arbitrary embeddings. By simplifying LGCN, we\nfurther propose a simplified graph filtering (SGFCF) which only requires the\ntop-K singular values for recommendation. Finally, experimental results on four\ndatasets with different density settings demonstrate the effectiveness and\nefficiency of our proposed methods.\n","authors":["Shaowen Peng","Xin Liu","Kazunari Sugiyama","Tsunenori Mine"],"pdf_url":"https://arxiv.org/pdf/2406.08827v1.pdf","comment":"Accepted to KDD'24"},{"id":"http://arxiv.org/abs/2406.08804v1","updated":"2024-06-13T04:39:16Z","published":"2024-06-13T04:39:16Z","title":"DIET: Customized Slimming for Incompatible Networks in Sequential\n  Recommendation","summary":"  Due to the continuously improving capabilities of mobile edges, recommender\nsystems start to deploy models on edges to alleviate network congestion caused\nby frequent mobile requests. Several studies have leveraged the proximity of\nedge-side to real-time data, fine-tuning them to create edge-specific models.\nDespite their significant progress, these methods require substantial on-edge\ncomputational resources and frequent network transfers to keep the model up to\ndate. The former may disrupt other processes on the edge to acquire\ncomputational resources, while the latter consumes network bandwidth, leading\nto a decrease in user satisfaction. In response to these challenges, we propose\na customizeD slImming framework for incompatiblE neTworks(DIET). DIET deploys\nthe same generic backbone (potentially incompatible for a specific edge) to all\ndevices. To minimize frequent bandwidth usage and storage consumption in\npersonalization, DIET tailors specific subnets for each edge based on its past\ninteractions, learning to generate slimming subnets(diets) within incompatible\nnetworks for efficient transfer. It also takes the inter-layer relationships\ninto account, empirically reducing inference time while obtaining more suitable\ndiets. We further explore the repeated modules within networks and propose a\nmore storage-efficient framework, DIETING, which utilizes a single layer of\nparameters to represent the entire network, achieving comparably excellent\nperformance. The experiments across four state-of-the-art datasets and two\nwidely used models demonstrate the superior accuracy in recommendation and\nefficiency in transmission and storage of our framework.\n","authors":["Kairui Fu","Shengyu Zhang","Zheqi Lv","Jingyuan Chen","Jiwei Li"],"pdf_url":"https://arxiv.org/pdf/2406.08804v1.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2406.09621v1","updated":"2024-06-13T23:08:06Z","published":"2024-06-13T23:08:06Z","title":"Enhancing Knowledge Retrieval with In-Context Learning and Semantic\n  Search through Generative AI","summary":"  Retrieving and extracting knowledge from extensive research documents and\nlarge databases presents significant challenges for researchers, students, and\nprofessionals in today's information-rich era. Existing retrieval systems,\nwhich rely on general-purpose Large Language Models (LLMs), often fail to\nprovide accurate responses to domain-specific inquiries. Additionally, the high\ncost of pretraining or fine-tuning LLMs for specific domains limits their\nwidespread adoption. To address these limitations, we propose a novel\nmethodology that combines the generative capabilities of LLMs with the fast and\naccurate retrieval capabilities of vector databases. This advanced retrieval\nsystem can efficiently handle both tabular and non-tabular data, understand\nnatural language user queries, and retrieve relevant information without\nfine-tuning. The developed model, Generative Text Retrieval (GTR), is adaptable\nto both unstructured and structured data with minor refinement. GTR was\nevaluated on both manually annotated and public datasets, achieving over 90%\naccuracy and delivering truthful outputs in 87% of cases. Our model achieved\nstate-of-the-art performance with a Rouge-L F1 score of 0.98 on the MSMARCO\ndataset. The refined model, Generative Tabular Text Retrieval (GTR-T),\ndemonstrated its efficiency in large database querying, achieving an Execution\nAccuracy (EX) of 0.82 and an Exact-Set-Match (EM) accuracy of 0.60 on the\nSpider dataset, using an open-source LLM. These efforts leverage Generative AI\nand In-Context Learning to enhance human-text interaction and make advanced AI\ncapabilities more accessible. By integrating robust retrieval systems with\npowerful LLMs, our approach aims to democratize access to sophisticated AI\ntools, improving the efficiency, accuracy, and scalability of AI-driven\ninformation retrieval and database querying.\n","authors":["Mohammed-Khalil Ghali","Abdelrahman Farrag","Daehan Won","Yu Jin"],"pdf_url":"https://arxiv.org/pdf/2406.09621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09618v1","updated":"2024-06-13T22:55:22Z","published":"2024-06-13T22:55:22Z","title":"Multi-Modal Retrieval For Large Language Model Based Speech Recognition","summary":"  Retrieval is a widely adopted approach for improving language models\nleveraging external information. As the field moves towards multi-modal large\nlanguage models, it is important to extend the pure text based methods to\nincorporate other modalities in retrieval as well for applications across the\nwide spectrum of machine learning tasks and data types. In this work, we\npropose multi-modal retrieval with two approaches: kNN-LM and cross-attention\ntechniques. We demonstrate the effectiveness of our retrieval approaches\nempirically by applying them to automatic speech recognition tasks with access\nto external information. Under this setting, we show that speech-based\nmulti-modal retrieval outperforms text based retrieval, and yields up to 50 %\nimprovement in word error rate over the multi-modal language model baseline.\nFurthermore, we achieve state-of-the-art recognition results on the\nSpoken-Squad question answering dataset.\n","authors":["Jari Kolehmainen","Aditya Gourav","Prashanth Gurunath Shivakumar","Yile Gu","Ankur Gandhe","Ariya Rastrow","Grant Strimel","Ivan Bulyko"],"pdf_url":"https://arxiv.org/pdf/2406.09618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09520v1","updated":"2024-06-13T18:16:27Z","published":"2024-06-13T18:16:27Z","title":"A Systematic Review of Generative AI for Teaching and Learning Practice","summary":"  The use of generative artificial intelligence (GenAI) in academia is a\nsubjective and hotly debated topic. Currently, there are no agreed guidelines\ntowards the usage of GenAI systems in higher education (HE) and, thus, it is\nstill unclear how to make effective use of the technology for teaching and\nlearning practice. This paper provides an overview of the current state of\nresearch on GenAI for teaching and learning in HE. To this end, this study\nconducted a systematic review of relevant studies indexed by Scopus, using the\npreferred reporting items for systematic reviews and meta-analyses (PRISMA)\nguidelines. The search criteria revealed a total of 625 research papers, of\nwhich 355 met the final inclusion criteria. The findings from the review showed\nthe current state and the future trends in documents, citations, document\nsources/authors, keywords, and co-authorship. The research gaps identified\nsuggest that while some authors have looked at understanding the detection of\nAI-generated text, it may be beneficial to understand how GenAI can be\nincorporated into supporting the educational curriculum for assessments,\nteaching, and learning delivery. Furthermore, there is a need for additional\ninterdisciplinary, multidimensional studies in HE through collaboration. This\nwill strengthen the awareness and understanding of students, tutors, and other\nstakeholders, which will be instrumental in formulating guidelines, frameworks,\nand policies for GenAI usage.\n","authors":["Bayode Ogunleye","Kudirat Ibilola Zakariyyah","Oluwaseun Ajao","Olakunle Olayinka","Hemlata Sharma"],"pdf_url":"https://arxiv.org/pdf/2406.09520v1.pdf","comment":"20 pages, 10 figures, article published in Education Sciences"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2406.09415v1","updated":"2024-06-13T17:59:58Z","published":"2024-06-13T17:59:58Z","title":"An Image is Worth More Than 16x16 Patches: Exploring Transformers on\n  Individual Pixels","summary":"  This work does not introduce a new method. Instead, we present an interesting\nfinding that questions the necessity of the inductive bias -- locality in\nmodern computer vision architectures. Concretely, we find that vanilla\nTransformers can operate by directly treating each individual pixel as a token\nand achieve highly performant results. This is substantially different from the\npopular design in Vision Transformer, which maintains the inductive bias from\nConvNets towards local neighborhoods (e.g. by treating each 16x16 patch as a\ntoken). We mainly showcase the effectiveness of pixels-as-tokens across three\nwell-studied tasks in computer vision: supervised learning for object\nclassification, self-supervised learning via masked autoencoding, and image\ngeneration with diffusion models. Although directly operating on individual\npixels is less computationally practical, we believe the community must be\naware of this surprising piece of knowledge when devising the next generation\nof neural architectures for computer vision.\n","authors":["Duy-Kien Nguyen","Mahmoud Assran","Unnat Jain","Martin R. Oswald","Cees G. M. Snoek","Xinlei Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09415v1.pdf","comment":"Technical report, 23 pages"},{"id":"http://arxiv.org/abs/2406.09417v1","updated":"2024-06-13T17:59:58Z","published":"2024-06-13T17:59:58Z","title":"Rethinking Score Distillation as a Bridge Between Image Distributions","summary":"  Score distillation sampling (SDS) has proven to be an important tool,\nenabling the use of large-scale diffusion priors for tasks operating in\ndata-poor domains. Unfortunately, SDS has a number of characteristic artifacts\nthat limit its usefulness in general-purpose applications. In this paper, we\nmake progress toward understanding the behavior of SDS and its variants by\nviewing them as solving an optimal-cost transport path from a source\ndistribution to a target distribution. Under this new interpretation, these\nmethods seek to transport corrupted images (source) to the natural image\ndistribution (target). We argue that current methods' characteristic artifacts\nare caused by (1) linear approximation of the optimal path and (2) poor\nestimates of the source distribution. We show that calibrating the text\nconditioning of the source distribution can produce high-quality generation and\ntranslation results with little extra overhead. Our method can be easily\napplied across many domains, matching or beating the performance of specialized\nmethods. We demonstrate its utility in text-to-2D, text-based NeRF\noptimization, translating paintings to real images, optical illusion\ngeneration, and 3D sketch-to-real. We compare our method to existing approaches\nfor score distillation sampling and show that it can produce high-frequency\ndetails with realistic colors.\n","authors":["David McAllister","Songwei Ge","Jia-Bin Huang","David W. Jacobs","Alexei A. Efros","Aleksander Holynski","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2406.09417v1.pdf","comment":"Project webpage: https://sds-bridge.github.io/"},{"id":"http://arxiv.org/abs/2406.09413v1","updated":"2024-06-13T17:59:56Z","published":"2024-06-13T17:59:56Z","title":"Interpreting the Weight Space of Customized Diffusion Models","summary":"  We investigate the space of weights spanned by a large collection of\ncustomized diffusion models. We populate this space by creating a dataset of\nover 60,000 models, each of which is a base model fine-tuned to insert a\ndifferent person's visual identity. We model the underlying manifold of these\nweights as a subspace, which we term weights2weights. We demonstrate three\nimmediate applications of this space -- sampling, editing, and inversion.\nFirst, as each point in the space corresponds to an identity, sampling a set of\nweights from it results in a model encoding a novel identity. Next, we find\nlinear directions in this space corresponding to semantic edits of the identity\n(e.g., adding a beard). These edits persist in appearance across generated\nsamples. Finally, we show that inverting a single image into this space\nreconstructs a realistic identity, even if the input image is out of\ndistribution (e.g., a painting). Our results indicate that the weight space of\nfine-tuned diffusion models behaves as an interpretable latent space of\nidentities.\n","authors":["Amil Dravid","Yossi Gandelsman","Kuan-Chieh Wang","Rameen Abdal","Gordon Wetzstein","Alexei A. Efros","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2406.09413v1.pdf","comment":"Project Page: https://snap-research.github.io/weights2weights"},{"id":"http://arxiv.org/abs/2406.09412v1","updated":"2024-06-13T17:59:53Z","published":"2024-06-13T17:59:53Z","title":"Explore the Limits of Omni-modal Pretraining at Scale","summary":"  We propose to build omni-modal intelligence, which is capable of\nunderstanding any modality and learning universal representations. In specific,\nwe propose a scalable pretraining paradigm, named Multimodal Context (MiCo),\nwhich can scale up the numbers of modalities and amount of data, together with\nthe model parameters, in the pretraining process. With MiCo, the pretrained\nmodels show significant emergent abilities in multimodal learning, which are\nevaluated on the following tasks: i) single-modality perception benchmarks of\n10 different modalities, ii) 25 cross-modality understanding tasks of\nretrieval, question-answering, captioning, and iii) 18 multimodal large\nlanguage model benchmarks. Our models establish 37 new records for\nstate-of-the-art performance. We hope that our research could contribute to the\ndevelopment of omni-modal intelligence. Code and Models are at\nhttps://github.com/invictus717/MiCo\n","authors":["Yiyuan Zhang","Handong Li","Jing Liu","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2406.09412v1.pdf","comment":"Project Website: https://invictus717.github.io/MiCo/"},{"id":"http://arxiv.org/abs/2406.09408v1","updated":"2024-06-13T17:59:44Z","published":"2024-06-13T17:59:44Z","title":"Data Attribution for Text-to-Image Models by Unlearning Synthesized\n  Images","summary":"  The goal of data attribution for text-to-image models is to identify the\ntraining images that most influence the generation of a new image. We can\ndefine \"influence\" by saying that, for a given output, if a model is retrained\nfrom scratch without that output's most influential images, the model should\nthen fail to generate that output image. Unfortunately, directly searching for\nthese influential images is computationally infeasible, since it would require\nrepeatedly retraining from scratch. We propose a new approach that efficiently\nidentifies highly-influential images. Specifically, we simulate unlearning the\nsynthesized image, proposing a method to increase the training loss on the\noutput image, without catastrophic forgetting of other, unrelated concepts.\nThen, we find training images that are forgotten by proxy, identifying ones\nwith significant loss deviations after the unlearning process, and label these\nas influential. We evaluate our method with a computationally intensive but\n\"gold-standard\" retraining from scratch and demonstrate our method's advantages\nover previous methods.\n","authors":["Sheng-Yu Wang","Aaron Hertzmann","Alexei A. Efros","Jun-Yan Zhu","Richard Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.09408v1.pdf","comment":"Project page: https://peterwang512.github.io/AttributeByUnlearning\n  Code: https://github.com/PeterWang512/AttributeByUnlearning"},{"id":"http://arxiv.org/abs/2406.09406v1","updated":"2024-06-13T17:59:42Z","published":"2024-06-13T17:59:42Z","title":"4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities","summary":"  Current multimodal and multitask foundation models like 4M or UnifiedIO show\npromising results, but in practice their out-of-the-box abilities to accept\ndiverse inputs and perform diverse tasks are limited by the (usually rather\nsmall) number of modalities and tasks they are trained on. In this paper, we\nexpand upon the capabilities of them by training a single model on tens of\nhighly diverse modalities and by performing co-training on large-scale\nmultimodal datasets and text corpora. This includes training on several\nsemantic and geometric modalities, feature maps from recent state of the art\nmodels like DINOv2 and ImageBind, pseudo labels of specialist models like SAM\nand 4DHumans, and a range of new modalities that allow for novel ways to\ninteract with the model and steer the generation, for example image metadata or\ncolor palettes. A crucial step in this process is performing discrete\ntokenization on various modalities, whether they are image-like, neural network\nfeature maps, vectors, structured data like instance segmentation or human\nposes, or data that can be represented as text. Through this, we expand on the\nout-of-the-box capabilities of multimodal models and specifically show the\npossibility of training one model to solve at least 3x more tasks/modalities\nthan existing ones and doing so without a loss in performance. This enables\nmore fine-grained and controllable multimodal generation capabilities and\nallows us to study the distillation of models trained on diverse data and\nobjectives into a unified model. We successfully scale the training to a three\nbillion parameter model using tens of modalities and different datasets. The\nresulting models and training code are open sourced at 4m.epfl.ch.\n","authors":["Roman Bachmann","Oğuzhan Fatih Kar","David Mizrahi","Ali Garjani","Mingfei Gao","David Griffiths","Jiaming Hu","Afshin Dehghan","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2406.09406v1.pdf","comment":"Project page at 4m.epfl.ch"},{"id":"http://arxiv.org/abs/2406.09405v1","updated":"2024-06-13T17:59:35Z","published":"2024-06-13T17:59:35Z","title":"Why Warmup the Learning Rate? Underlying Mechanisms and Improvements","summary":"  It is common in deep learning to warm up the learning rate $\\eta$, often by a\nlinear schedule between $\\eta_{\\text{init}} = 0$ and a predetermined target\n$\\eta_{\\text{trgt}}$. In this paper, we show through systematic experiments\nusing SGD and Adam that the overwhelming benefit of warmup arises from allowing\nthe network to tolerate larger $\\eta_{\\text{trgt}}$ by forcing the network to\nmore well-conditioned areas of the loss landscape. The ability to handle larger\n$\\eta_{\\text{trgt}}$ makes hyperparameter tuning more robust while improving\nthe final performance. We uncover different regimes of operation during the\nwarmup period, depending on whether training starts off in a progressive\nsharpening or sharpness reduction phase, which in turn depends on the\ninitialization and parameterization. Using these insights, we show how\n$\\eta_{\\text{init}}$ can be properly chosen by utilizing the loss catapult\nmechanism, which saves on the number of warmup steps, in some cases completely\neliminating the need for warmup. We also suggest an initialization for the\nvariance in Adam which provides benefits similar to warmup.\n","authors":["Dayal Singh Kalra","Maissam Barkeshli"],"pdf_url":"https://arxiv.org/pdf/2406.09405v1.pdf","comment":"11+22 pages, 7+24 figures"},{"id":"http://arxiv.org/abs/2406.09404v1","updated":"2024-06-13T17:59:32Z","published":"2024-06-13T17:59:32Z","title":"ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene\n  Editing","summary":"  This paper proposes ConsistDreamer - a novel framework that lifts 2D\ndiffusion models with 3D awareness and 3D consistency, thus enabling\nhigh-fidelity instruction-guided scene editing. To overcome the fundamental\nlimitation of missing 3D consistency in 2D diffusion models, our key insight is\nto introduce three synergetic strategies that augment the input of the 2D\ndiffusion model to become 3D-aware and to explicitly enforce 3D consistency\nduring the training process. Specifically, we design surrounding views as\ncontext-rich input for the 2D diffusion model, and generate 3D-consistent,\nstructured noise instead of image-independent noise. Moreover, we introduce\nself-supervised consistency-enforcing training within the per-scene editing\nprocedure. Extensive evaluation shows that our ConsistDreamer achieves\nstate-of-the-art performance for instruction-guided scene editing across\nvarious scenes and editing instructions, particularly in complicated\nlarge-scale indoor scenes from ScanNet++, with significantly improved sharpness\nand fine-grained textures. Notably, ConsistDreamer stands as the first work\ncapable of successfully editing complex (e.g., plaid/checkered) patterns. Our\nproject page is at immortalco.github.io/ConsistDreamer.\n","authors":["Jun-Kun Chen","Samuel Rota Bulò","Norman Müller","Lorenzo Porzi","Peter Kontschieder","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09404v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.09402v1","updated":"2024-06-13T17:59:30Z","published":"2024-06-13T17:59:30Z","title":"Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D\n  Diffusion","summary":"  This paper proposes Instruct 4D-to-4D that achieves 4D awareness and\nspatial-temporal consistency for 2D diffusion models to generate high-quality\ninstruction-guided dynamic scene editing results. Traditional applications of\n2D diffusion models in dynamic scene editing often result in inconsistency,\nprimarily due to their inherent frame-by-frame editing methodology. Addressing\nthe complexities of extending instruction-guided editing to 4D, our key insight\nis to treat a 4D scene as a pseudo-3D scene, decoupled into two sub-problems:\nachieving temporal consistency in video editing and applying these edits to the\npseudo-3D scene. Following this, we first enhance the Instruct-Pix2Pix (IP2P)\nmodel with an anchor-aware attention module for batch processing and consistent\nediting. Additionally, we integrate optical flow-guided appearance propagation\nin a sliding window fashion for more precise frame-to-frame editing and\nincorporate depth-based projection to manage the extensive data of pseudo-3D\nscenes, followed by iterative editing to achieve convergence. We extensively\nevaluate our approach in various scenes and editing instructions, and\ndemonstrate that it achieves spatially and temporally consistent editing\nresults, with significantly enhanced detail and sharpness over the prior art.\nNotably, Instruct 4D-to-4D is general and applicable to both monocular and\nchallenging multi-camera scenes. Code and more results are available at\nimmortalco.github.io/Instruct-4D-to-4D.\n","authors":["Linzhan Mou","Jun-Kun Chen","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09402v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.09400v1","updated":"2024-06-13T17:59:29Z","published":"2024-06-13T17:59:29Z","title":"Yo'LLaVA: Your Personalized Language and Vision Assistant","summary":"  Large Multimodal Models (LMMs) have shown remarkable capabilities across a\nvariety of tasks (e.g., image captioning, visual question answering). While\nbroad, their knowledge remains generic (e.g., recognizing a dog), and they are\nunable to handle personalized subjects (e.g., recognizing a user's pet dog).\nHuman reasoning, in contrast, typically operates within the context of specific\nsubjects in our surroundings. For example, one might ask, \"What should I buy\nfor my dog's birthday?\"; as opposed to a generic inquiry about \"What should I\nbuy for a dog's birthday?\". Similarly, when looking at a friend's image, the\ninterest lies in seeing their activities (e.g., \"my friend is holding a cat\"),\nrather than merely observing generic human actions (e.g., \"a man is holding a\ncat\"). In this paper, we introduce the novel task of personalizing LMMs, so\nthat they can have conversations about a specific subject. We propose Yo'LLaVA,\nwhich learns to embed a personalized subject into a set of latent tokens given\na handful of example images of the subject. Our qualitative and quantitative\nanalyses reveal that Yo'LLaVA can learn the concept more efficiently using\nfewer tokens and more effectively encode the visual attributes compared to\nstrong prompting baselines (e.g., LLaVA).\n","authors":["Thao Nguyen","Haotian Liu","Yuheng Li","Mu Cai","Utkarsh Ojha","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2406.09400v1.pdf","comment":"Project page: https://thaoshibe.github.io/YoLLaVA"},{"id":"http://arxiv.org/abs/2406.09393v1","updated":"2024-06-13T17:59:09Z","published":"2024-06-13T17:59:09Z","title":"Improving Autoregressive Training with Dynamic Oracles","summary":"  Many tasks within NLP can be framed as sequential decision problems, ranging\nfrom sequence tagging to text generation. However, for many tasks, the standard\ntraining methods, including maximum likelihood (teacher forcing) and scheduled\nsampling, suffer from exposure bias and a mismatch between metrics employed\nduring training and inference. DAgger provides a solution to mitigate these\nproblems, yet it requires a metric-specific dynamic oracle algorithm, which\ndoes not exist for many common metrics like span-based F1, ROUGE, and BLEU. In\nthis paper, we develop these novel dynamic oracles and show they maintain\nDAgger's no-regret guarantee for decomposable metrics like span-based F1. We\nevaluate the algorithm's performance on named entity recognition (NER), text\nsummarization, and machine translation (MT). While DAgger with dynamic oracle\nyields less favorable results in our MT experiments, it outperforms the\nbaseline techniques in NER and text summarization.\n","authors":["Jianing Yang","Harshine Visvanathan","Yilin Wang","Xinyi Hu","Matthew Gormley"],"pdf_url":"https://arxiv.org/pdf/2406.09393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09391v1","updated":"2024-06-13T17:59:06Z","published":"2024-06-13T17:59:06Z","title":"A More Practical Approach to Machine Unlearning","summary":"  Machine learning models often incorporate vast amounts of data, raising\nsignificant privacy concerns. Machine unlearning, the ability to remove the\ninfluence of specific data points from a trained model, addresses these\nconcerns. This paper explores practical methods for implementing machine\nunlearning, focusing on a first-epoch gradient-ascent approach.\n  Key findings include: 1. Single vs. Multi-Epoch Unlearning: First-epoch\ngradient unlearning is more effective than multi-epoch gradients. 2.\nLayer-Based Unlearning: The embedding layer in GPT-2 is crucial for effective\nunlearning. Gradients from the output layers (11 and 12) have no impact.\nEfficient unlearning can be achieved using only the embedding layer, halving\nspace complexity. 3. Influence Functions & Scoring: Techniques like Hessian\nVector Product and the dot product of activations and tensors are used for\nquantifying unlearning. 4. Gradient Ascent Considerations: Calibration is\nnecessary to avoid overexposing the model to specific data points during\nunlearning, which could prematurely terminate the process. 5. Fuzzy Matching\nvs. Iterative Unlearning: Fuzzy matching techniques shift the model to a new\noptimum, while iterative unlearning provides a more complete modality.\n  Our empirical evaluation confirms that first-epoch gradient ascent for\nmachine unlearning is more effective than whole-model gradient ascent. These\nresults highlight the potential of machine unlearning for enhancing data\nprivacy and compliance with regulations such as GDPR and CCPA. The study\nunderscores the importance of formal methods to comprehensively evaluate the\nunlearning process.\n","authors":["David Zagardo"],"pdf_url":"https://arxiv.org/pdf/2406.09391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09390v1","updated":"2024-06-13T17:59:05Z","published":"2024-06-13T17:59:05Z","title":"LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities\n  of Living","summary":"  Large Language Vision Models (LLVMs) have demonstrated effectiveness in\nprocessing internet videos, yet they struggle with the visually perplexing\ndynamics present in Activities of Daily Living (ADL) due to limited pertinent\ndatasets and models tailored to relevant cues. To this end, we propose a\nframework for curating ADL multiview datasets to fine-tune LLVMs, resulting in\nthe creation of ADL-X, comprising 100K RGB video-instruction pairs, language\ndescriptions, 3D skeletons, and action-conditioned object trajectories. We\nintroduce LLAVIDAL, an LLVM capable of incorporating 3D poses and relevant\nobject trajectories to understand the intricate spatiotemporal relationships\nwithin ADLs. Furthermore, we present a novel benchmark, ADLMCQ, for quantifying\nLLVM effectiveness in ADL scenarios. When trained on ADL-X, LLAVIDAL\nconsistently achieves state-of-the-art performance across all ADL evaluation\nmetrics. Qualitative analysis reveals LLAVIDAL's temporal reasoning\ncapabilities in understanding ADL. The link to the dataset is provided at:\nhttps://adl-x.github.io/\n","authors":["Rajatsubhra Chakraborty","Arkaprava Sinha","Dominick Reilly","Manish Kumar Govind","Pu Wang","Francois Bremond","Srijan Das"],"pdf_url":"https://arxiv.org/pdf/2406.09390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09388v1","updated":"2024-06-13T17:58:39Z","published":"2024-06-13T17:58:39Z","title":"Exploring the Spectrum of Visio-Linguistic Compositionality and\n  Recognition","summary":"  Vision and language models (VLMs) such as CLIP have showcased remarkable\nzero-shot recognition abilities yet face challenges in visio-linguistic\ncompositionality, particularly in linguistic comprehension and fine-grained\nimage-text alignment. This paper explores the intricate relationship between\ncompositionality and recognition -- two pivotal aspects of VLM capability. We\nconduct a comprehensive evaluation of existing VLMs, covering both pre-training\napproaches aimed at recognition and the fine-tuning methods designed to improve\ncompositionality. Our evaluation employs 12 benchmarks for compositionality,\nalong with 21 zero-shot classification and two retrieval benchmarks for\nrecognition. In our analysis from 274 CLIP model checkpoints, we reveal\npatterns and trade-offs that emerge between compositional understanding and\nrecognition accuracy. Ultimately, this necessitates strategic efforts towards\ndeveloping models that improve both capabilities, as well as the meticulous\nformulation of benchmarks for compositionality. We open our evaluation\nframework at https://github.com/ytaek-oh/vl_compo.\n","authors":["Youngtaek Oh","Pyunghwan Ahn","Jinhyung Kim","Gwangmo Song","Soonyoung Lee","In So Kweon","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2406.09388v1.pdf","comment":"Accepted to CVPRW 2024 on 'What is Next in Multimodal Foundation\n  Models?'. Code: https://github.com/ytaek-oh/vl_compo"},{"id":"http://arxiv.org/abs/2406.09384v1","updated":"2024-06-13T17:57:10Z","published":"2024-06-13T17:57:10Z","title":"Reflecting on the State of Rehearsal-free Continual Learning with\n  Pretrained Models","summary":"  With the advent and recent ubiquity of foundation models, continual learning\n(CL) has recently shifted from continual training from scratch to the continual\nadaptation of pretrained models, seeing particular success on rehearsal-free CL\nbenchmarks (RFCL). To achieve this, most proposed methods adapt and restructure\nparameter-efficient finetuning techniques (PEFT) to suit the continual nature\nof the problem. Based most often on input-conditional query-mechanisms or\nregularizations on top of prompt- or adapter-based PEFT, these PEFT-style RFCL\n(P-RFCL) approaches report peak performances; often convincingly outperforming\nexisting CL techniques. However, on the other end, critical studies have\nrecently highlighted competitive results by training on just the first task or\nvia simple non-parametric baselines. Consequently, questions arise about the\nrelationship between methodological choices in P-RFCL and their reported high\nbenchmark scores. In this work, we tackle these questions to better understand\nthe true drivers behind strong P-RFCL performances, their placement w.r.t.\nrecent first-task adaptation studies, and their relation to preceding CL\nstandards such as EWC or SI. In particular, we show: (1) P-RFCL techniques\nrelying on input-conditional query mechanisms work not because, but rather\ndespite them by collapsing towards standard PEFT shortcut solutions. (2)\nIndeed, we show how most often, P-RFCL techniques can be matched by a simple\nand lightweight PEFT baseline. (3) Using this baseline, we identify the\nimplicit bound on tunable parameters when deriving RFCL approaches from PEFT\nmethods as a potential denominator behind P-RFCL efficacy. Finally, we (4)\nbetter disentangle continual versus first-task adaptation, and (5) motivate\nstandard RFCL techniques s.a. EWC or SI in light of recent P-RFCL methods.\n","authors":["Lukas Thede","Karsten Roth","Olivier J. Hénaff","Matthias Bethge","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2406.09384v1.pdf","comment":"3rd Conference on Lifelong Learning Agents (CoLLAs) 2024"},{"id":"http://arxiv.org/abs/2405.15891v2","updated":"2024-06-13T17:56:53Z","published":"2024-05-24T19:22:09Z","title":"Score Distillation via Reparametrized DDIM","summary":"  While 2D diffusion models generate realistic, high-detail images, 3D shape\ngeneration methods like Score Distillation Sampling (SDS) built on these 2D\ndiffusion models produce cartoon-like, over-smoothed shapes. To help explain\nthis discrepancy, we show that the image guidance used in Score Distillation\ncan be understood as the velocity field of a 2D denoising generative process,\nup to the choice of a noise term. In particular, after a change of variables,\nSDS resembles a high-variance version of Denoising Diffusion Implicit Models\n(DDIM) with a differently-sampled noise term: SDS introduces noise i.i.d.\nrandomly at each step, while DDIM infers it from the previous noise\npredictions. This excessive variance can lead to over-smoothing and unrealistic\noutputs. We show that a better noise approximation can be recovered by\ninverting DDIM in each SDS update step. This modification makes SDS's\ngenerative process for 2D images almost identical to DDIM. In 3D, it removes\nover-smoothing, preserves higher-frequency detail, and brings the generation\nquality closer to that of 2D samplers. Experimentally, our method achieves\nbetter or similar 3D generation quality compared to other state-of-the-art\nScore Distillation methods, all without training additional neural networks or\nmulti-view supervision, and providing useful insights into relationship between\n2D and 3D asset generation with diffusion models.\n","authors":["Artem Lukoianov","Haitz Sáez de Ocáriz Borde","Kristjan Greenewald","Vitor Campagnolo Guizilini","Timur Bagautdinov","Vincent Sitzmann","Justin Solomon"],"pdf_url":"https://arxiv.org/pdf/2405.15891v2.pdf","comment":"Preprint. 25 pages, 26 figures. Revision : added missed comparisons,\n  fixed typos, fixed PDF compatibility issues"},{"id":"http://arxiv.org/abs/2406.09375v1","updated":"2024-06-13T17:53:47Z","published":"2024-06-13T17:53:47Z","title":"Learning conditional distributions on continuous spaces","summary":"  We investigate sample-based learning of conditional distributions on\nmulti-dimensional unit boxes, allowing for different dimensions of the feature\nand target spaces. Our approach involves clustering data near varying query\npoints in the feature space to create empirical measures in the target space.\nWe employ two distinct clustering schemes: one based on a fixed-radius ball and\nthe other on nearest neighbors. We establish upper bounds for the convergence\nrates of both methods and, from these bounds, deduce optimal configurations for\nthe radius and the number of neighbors. We propose to incorporate the nearest\nneighbors method into neural network training, as our empirical analysis\nindicates it has better performance in practice. For efficiency, our training\nprocess utilizes approximate nearest neighbors search with random binary space\npartitioning. Additionally, we employ the Sinkhorn algorithm and a\nsparsity-enforced transport plan. Our empirical findings demonstrate that, with\na suitably designed structure, the neural network has the ability to adapt to a\nsuitable level of Lipschitz continuity locally. For reproducibility, our code\nis available at \\url{https://github.com/zcheng-a/LCD_kNN}.\n","authors":["Cyril Bénézet","Ziteng Cheng","Sebastian Jaimungal"],"pdf_url":"https://arxiv.org/pdf/2406.09375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04382v2","updated":"2024-06-13T17:53:01Z","published":"2024-06-06T04:05:23Z","title":"Improving the Fairness of Deep-Learning, Short-term Crime Prediction\n  with Under-reporting-aware Models","summary":"  Deep learning crime predictive tools use past crime data and additional\nbehavioral datasets to forecast future crimes. Nevertheless, these tools have\nbeen shown to suffer from unfair predictions across minority racial and ethnic\ngroups. Current approaches to address this unfairness generally propose either\npre-processing methods that mitigate the bias in the training datasets by\napplying corrections to crime counts based on domain knowledge or in-processing\nmethods that are implemented as fairness regularizers to optimize for both\naccuracy and fairness. In this paper, we propose a novel deep learning\narchitecture that combines the power of these two approaches to increase\nprediction fairness. Our results show that the proposed model improves the\nfairness of crime predictions when compared to models with in-processing\nde-biasing approaches and with models without any type of bias correction,\nalbeit at the cost of reducing accuracy.\n","authors":["Jiahui Wu","Vanessa Frias-Martinez"],"pdf_url":"https://arxiv.org/pdf/2406.04382v2.pdf","comment":"25 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.09373v1","updated":"2024-06-13T17:51:10Z","published":"2024-06-13T17:51:10Z","title":"Efficient Discrepancy Testing for Learning with Distribution Shift","summary":"  A fundamental notion of distance between train and test distributions from\nthe field of domain adaptation is discrepancy distance. While in general hard\nto compute, here we provide the first set of provably efficient algorithms for\ntesting localized discrepancy distance, where discrepancy is computed with\nrespect to a fixed output classifier. These results imply a broad set of new,\nefficient learning algorithms in the recently introduced model of Testable\nLearning with Distribution Shift (TDS learning) due to Klivans et al. (2023).\n  Our approach generalizes and improves all prior work on TDS learning: (1) we\nobtain universal learners that succeed simultaneously for large classes of test\ndistributions, (2) achieve near-optimal error rates, and (3) give exponential\nimprovements for constant depth circuits. Our methods further extend to\nsemi-parametric settings and imply the first positive results for\nlow-dimensional convex sets. Additionally, we separate learning and testing\nphases and obtain algorithms that run in fully polynomial time at test time.\n","authors":["Gautam Chandrasekaran","Adam R. Klivans","Vasilis Kontonis","Konstantinos Stavropoulos","Arsen Vasilyan"],"pdf_url":"https://arxiv.org/pdf/2406.09373v1.pdf","comment":"45 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.09371v1","updated":"2024-06-13T17:51:00Z","published":"2024-06-13T17:51:00Z","title":"LRM-Zero: Training Large Reconstruction Models with Synthesized Data","summary":"  We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on\nsynthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The\ncore of LRM-Zero is our procedural 3D dataset, Zeroverse, which is\nautomatically synthesized from simple primitive shapes with random texturing\nand augmentations (e.g., height fields, boolean differences, and wireframes).\nUnlike previous 3D datasets (e.g., Objaverse) which are often captured or\ncrafted by humans to approximate real 3D data, Zeroverse completely ignores\nrealistic global semantics but is rich in complex geometric and texture details\nthat are locally similar to or even more intricate than real objects. We\ndemonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse,\ncan achieve high visual quality in the reconstruction of real-world objects,\ncompetitive with models trained on Objaverse. We also analyze several critical\ndesign choices of Zeroverse that contribute to LRM-Zero's capability and\ntraining stability. Our work demonstrates that 3D reconstruction, one of the\ncore tasks in 3D vision, can potentially be addressed without the semantics of\nreal-world objects. The Zeroverse's procedural synthesis code and interactive\nvisualization are available at: https://desaixie.github.io/lrm-zero/.\n","authors":["Desai Xie","Sai Bi","Zhixin Shu","Kai Zhang","Zexiang Xu","Yi Zhou","Sören Pirk","Arie Kaufman","Xin Sun","Hao Tan"],"pdf_url":"https://arxiv.org/pdf/2406.09371v1.pdf","comment":"23 pages, 8 figures. Our code and interactive visualization are\n  available at: https://desaixie.github.io/lrm-zero/"},{"id":"http://arxiv.org/abs/2406.09370v1","updated":"2024-06-13T17:50:51Z","published":"2024-06-13T17:50:51Z","title":"Data-dependent and Oracle Bounds on Forgetting in Continual Learning","summary":"  In continual learning, knowledge must be preserved and re-used between tasks,\nmaintaining good transfer to future tasks and minimizing forgetting of\npreviously learned ones. While several practical algorithms have been devised\nfor this setting, there have been few theoretical works aiming to quantify and\nbound the degree of Forgetting in general settings. We provide both\ndata-dependent and oracle upper bounds that apply regardless of model and\nalgorithm choice, as well as bounds for Gibbs posteriors. We derive an\nalgorithm inspired by our bounds and demonstrate empirically that our approach\nyields improved forward and backward transfer.\n","authors":["Lior Friedman","Ron Meir"],"pdf_url":"https://arxiv.org/pdf/2406.09370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09366v1","updated":"2024-06-13T17:49:56Z","published":"2024-06-13T17:49:56Z","title":"Towards an Improved Understanding and Utilization of Maximum Manifold\n  Capacity Representations","summary":"  Maximum Manifold Capacity Representations (MMCR) is a recent multi-view\nself-supervised learning (MVSSL) method that matches or surpasses other leading\nMVSSL methods. MMCR is intriguing because it does not fit neatly into any of\nthe commonplace MVSSL lineages, instead originating from a statistical\nmechanical perspective on the linear separability of data manifolds. In this\npaper, we seek to improve our understanding and our utilization of MMCR. To\nbetter understand MMCR, we leverage tools from high dimensional probability to\ndemonstrate that MMCR incentivizes alignment and uniformity of learned\nembeddings. We then leverage tools from information theory to show that such\nembeddings maximize a well-known lower bound on mutual information between\nviews, thereby connecting the geometric perspective of MMCR to the\ninformation-theoretic perspective commonly discussed in MVSSL. To better\nutilize MMCR, we mathematically predict and experimentally confirm\nnon-monotonic changes in the pretraining loss akin to double descent but with\nrespect to atypical hyperparameters. We also discover compute scaling laws that\nenable predicting the pretraining loss as a function of gradients steps, batch\nsize, embedding dimension and number of views. We then show that MMCR,\noriginally applied to image data, is performant on multimodal image-text data.\nBy more deeply understanding the theoretical and empirical behavior of MMCR,\nour work reveals insights on improving MVSSL methods.\n","authors":["Rylan Schaeffer","Victor Lecomte","Dhruv Bhandarkar Pai","Andres Carranza","Berivan Isik","Alyssa Unell","Mikail Khona","Thomas Yerxa","Yann LeCun","SueYeon Chung","Andrey Gromov","Ravid Shwartz-Ziv","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2406.09366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09363v1","updated":"2024-06-13T17:49:10Z","published":"2024-06-13T17:49:10Z","title":"ElicitationGPT: Text Elicitation Mechanisms via Language Models","summary":"  Scoring rules evaluate probabilistic forecasts of an unknown state against\nthe realized state and are a fundamental building block in the incentivized\nelicitation of information and the training of machine learning models. This\npaper develops mechanisms for scoring elicited text against ground truth text\nusing domain-knowledge-free queries to a large language model (specifically\nChatGPT) and empirically evaluates their alignment with human preferences. The\nempirical evaluation is conducted on peer reviews from a peer-grading dataset\nand in comparison to manual instructor scores for the peer reviews.\n","authors":["Yifan Wu","Jason Hartline"],"pdf_url":"https://arxiv.org/pdf/2406.09363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05689v2","updated":"2024-06-13T17:43:45Z","published":"2024-02-08T14:07:20Z","title":"Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of\n  Average-Reward Restless Bandits","summary":"  We consider the infinite-horizon, average-reward restless bandit problem in\ndiscrete time. We propose a new class of policies that are designed to drive a\nprogressively larger subset of arms toward the optimal distribution. We show\nthat our policies are asymptotically optimal with an $O(1/\\sqrt{N})$ optimality\ngap for an $N$-armed problem, provided that the single-armed MDP is unichain\nand aperiodic under the optimal single-armed policy. Our approach departs from\nmost existing work that focuses on index or priority policies, which rely on\nthe Uniform Global Attractor Property (UGAP) to guarantee convergence to the\noptimum, or a recently developed simulation-based policy, which requires a\nSynchronization Assumption (SA).\n","authors":["Yige Hong","Qiaomin Xie","Yudong Chen","Weina Wang"],"pdf_url":"https://arxiv.org/pdf/2402.05689v2.pdf","comment":"49 pages, 3 figures. This version adds details on the unichain\n  condition, stationary distribution, and long-run time average"},{"id":"http://arxiv.org/abs/2406.09358v1","updated":"2024-06-13T17:43:41Z","published":"2024-06-13T17:43:41Z","title":"Understanding Hallucinations in Diffusion Models through Mode\n  Interpolation","summary":"  Colloquially speaking, image generation models based upon diffusion processes\nare frequently said to exhibit \"hallucinations,\" samples that could never occur\nin the training data. But where do such hallucinations come from? In this\npaper, we study a particular failure mode in diffusion models, which we term\nmode interpolation. Specifically, we find that diffusion models smoothly\n\"interpolate\" between nearby data modes in the training set, to generate\nsamples that are completely outside the support of the original training\ndistribution; this phenomenon leads diffusion models to generate artifacts that\nnever existed in real data (i.e., hallucinations). We systematically study the\nreasons for, and the manifestation of this phenomenon. Through experiments on\n1D and 2D Gaussians, we show how a discontinuous loss landscape in the\ndiffusion model's decoder leads to a region where any smooth approximation will\ncause such hallucinations. Through experiments on artificial datasets with\nvarious shapes, we show how hallucination leads to the generation of\ncombinations of shapes that never existed. Finally, we show that diffusion\nmodels in fact know when they go out of support and hallucinate. This is\ncaptured by the high variance in the trajectory of the generated sample towards\nthe final few backward sampling process. Using a simple metric to capture this\nvariance, we can remove over 95% of hallucinations at generation time while\nretaining 96% of in-support samples. We conclude our exploration by showing the\nimplications of such hallucination (and its removal) on the collapse (and\nstabilization) of recursive training on synthetic data with experiments on\nMNIST and 2D Gaussians dataset. We release our code at\nhttps://github.com/locuslab/diffusion-model-hallucination.\n","authors":["Sumukh K Aithal","Pratyush Maini","Zachary C. Lipton","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2406.09358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09357v1","updated":"2024-06-13T17:42:57Z","published":"2024-06-13T17:42:57Z","title":"Advancing Graph Generation through Beta Diffusion","summary":"  Diffusion models have demonstrated effectiveness in generating natural images\nand have been extended to generate diverse data types, including graphs. This\nnew generation of diffusion-based graph generative models has demonstrated\nsignificant performance improvements over methods that rely on variational\nautoencoders or generative adversarial networks. It's important to recognize,\nhowever, that most of these models employ Gaussian or categorical diffusion\nprocesses, which can struggle with sparse and long-tailed data distributions.\nIn our work, we introduce Graph Beta Diffusion (GBD), a diffusion-based\ngenerative model particularly adept at capturing diverse graph structures. GBD\nutilizes a beta diffusion process, tailored for the sparse and range-bounded\ncharacteristics of graph adjacency matrices. Furthermore, we have developed a\nmodulation technique that enhances the realism of the generated graphs by\nstabilizing the generation of critical graph structures, while preserving\nflexibility elsewhere. The outstanding performance of GBD across three general\ngraph benchmarks and two biochemical graph benchmarks highlights its capability\nto effectively capture the complexities of real-world graph data. The code will\nbe made available at https://github.com/YH-UtMSB/Graph_Beta_Diffusion\n","authors":["Yilin He","Xinyang Liu","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.09357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09353v1","updated":"2024-06-13T17:40:15Z","published":"2024-06-13T17:40:15Z","title":"Enhancing Domain Adaptation through Prompt Gradient Alignment","summary":"  Prior Unsupervised Domain Adaptation (UDA) methods often aim to train a\ndomain-invariant feature extractor, which may hinder the model from learning\nsufficiently discriminative features. To tackle this, a line of works based on\nprompt learning leverages the power of large-scale pre-trained vision-language\nmodels to learn both domain-invariant and specific features through a set of\ndomain-agnostic and domain-specific learnable prompts. Those studies typically\nenforce invariant constraints on representation, output, or prompt space to\nlearn such prompts. Differently, we cast UDA as a multiple-objective\noptimization problem in which each objective is represented by a domain loss.\nUnder this new framework, we propose aligning per-objective gradients to foster\nconsensus between them. Additionally, to prevent potential overfitting when\nfine-tuning this deep learning architecture, we penalize the norm of these\ngradients. To achieve these goals, we devise a practical gradient update\nprocedure that can work under both single-source and multi-source UDA.\nEmpirically, our method consistently surpasses other prompt-based baselines by\na large margin on different UDA benchmarks\n","authors":["Hoang Phan","Lam Tran","Quyen Tran","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2406.09353v1.pdf","comment":"26 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.09351v1","updated":"2024-06-13T17:38:26Z","published":"2024-06-13T17:38:26Z","title":"On the Expressibility of the Reconstructional Color Refinement","summary":"  One of the most basic facts related to the famous Ulam reconstruction\nconjecture is that the connectedness of a graph can be determined by the deck\nof its vertex-deleted subgraphs, which are considered up to isomorphism. We\nstrengthen this result by proving that connectedness can still be determined\nwhen the subgraphs in the deck are given up to equivalence under the color\nrefinement isomorphism test. Consequently, this implies that connectedness is\nrecognizable by Reconstruction Graph Neural Networks, a recently introduced GNN\narchitecture inspired by the reconstruction conjecture (Cotta, Morris, Ribeiro\n2021).\n","authors":["V. Arvind","Johannes Köbler","Oleg Verbitsky"],"pdf_url":"https://arxiv.org/pdf/2406.09351v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.08474v2","updated":"2024-06-13T17:38:12Z","published":"2024-06-12T17:57:06Z","title":"Real2Code: Reconstruct Articulated Objects via Code Generation","summary":"  We present Real2Code, a novel approach to reconstructing articulated objects\nvia code generation. Given visual observations of an object, we first\nreconstruct its part geometry using an image segmentation model and a shape\ncompletion model. We then represent the object parts with oriented bounding\nboxes, which are input to a fine-tuned large language model (LLM) to predict\njoint articulation as code. By leveraging pre-trained vision and language\nmodels, our approach scales elegantly with the number of articulated parts, and\ngeneralizes from synthetic training data to real world objects in unstructured\nenvironments. Experimental results demonstrate that Real2Code significantly\noutperforms previous state-of-the-art in reconstruction accuracy, and is the\nfirst approach to extrapolate beyond objects' structural complexity in the\ntraining set, and reconstructs objects with up to 10 articulated parts. When\nincorporated with a stereo reconstruction model, Real2Code also generalizes to\nreal world objects from a handful of multi-view RGB images, without the need\nfor depth or camera information.\n","authors":["Zhao Mandi","Yijia Weng","Dominik Bauer","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2406.08474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09347v1","updated":"2024-06-13T17:31:30Z","published":"2024-06-13T17:31:30Z","title":"Separations in the Representational Capabilities of Transformers and\n  Recurrent Architectures","summary":"  Transformer architectures have been widely adopted in foundation models. Due\nto their high inference costs, there is renewed interest in exploring the\npotential of efficient recurrent architectures (RNNs). In this paper, we\nanalyze the differences in the representational capabilities of Transformers\nand RNNs across several tasks of practical relevance, including index lookup,\nnearest neighbor, recognizing bounded Dyck languages, and string equality. For\nthe tasks considered, our results show separations based on the size of the\nmodel required for different architectures. For example, we show that a\none-layer Transformer of logarithmic width can perform index lookup, whereas an\nRNN requires a hidden state of linear size. Conversely, while constant-size\nRNNs can recognize bounded Dyck languages, we show that one-layer Transformers\nrequire a linear size for this task. Furthermore, we show that two-layer\nTransformers of logarithmic size can perform decision tasks such as string\nequality or disjointness, whereas both one-layer Transformers and recurrent\nmodels require linear size for these tasks. We also show that a log-size\ntwo-layer Transformer can implement the nearest neighbor algorithm in its\nforward pass; on the other hand recurrent models require linear size. Our\nconstructions are based on the existence of $N$ nearly orthogonal vectors in\n$O(\\log N)$ dimensional space and our lower bounds are based on reductions from\ncommunication complexity problems. We supplement our theoretical results with\nexperiments that highlight the differences in the performance of these\narchitectures on practical-size sequences.\n","authors":["Satwik Bhattamishra","Michael Hahn","Phil Blunsom","Varun Kanade"],"pdf_url":"https://arxiv.org/pdf/2406.09347v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.09346v1","updated":"2024-06-13T17:31:02Z","published":"2024-06-13T17:31:02Z","title":"Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking\n  Scores","summary":"  In this study, we present ScoreFormer, a novel graph transformer model\ndesigned to accurately predict molecular docking scores, thereby optimizing\nhigh-throughput virtual screening (HTVS) in drug discovery. The architecture\nintegrates Principal Neighborhood Aggregation (PNA) and Learnable Random Walk\nPositional Encodings (LRWPE), enhancing the model's ability to understand\ncomplex molecular structures and their relationship with their respective\ndocking scores. This approach significantly surpasses traditional HTVS methods\nand recent Graph Neural Network (GNN) models in both recovery and efficiency\ndue to a wider coverage of the chemical space and enhanced performance. Our\nresults demonstrate that ScoreFormer achieves competitive performance in\ndocking score prediction and offers a substantial 1.65-fold reduction in\ninference time compared to existing models. We evaluated ScoreFormer across\nmultiple datasets under various conditions, confirming its robustness and\nreliability in identifying potential drug candidates rapidly.\n","authors":["Álvaro Ciudad","Adrián Morales-Pastor","Laura Malo","Isaac Filella-Mercè","Victor Guallar","Alexis Molina"],"pdf_url":"https://arxiv.org/pdf/2406.09346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07769v2","updated":"2024-06-13T17:21:26Z","published":"2024-06-11T23:23:54Z","title":"Personalized Product Assortment with Real-time 3D Perception and\n  Bayesian Payoff Estimation","summary":"  Product assortment selection is a critical challenge facing physical\nretailers. Effectively aligning inventory with the preferences of shoppers can\nincrease sales and decrease out-of-stocks. However, in real-world settings the\nproblem is challenging due to the combinatorial explosion of product assortment\npossibilities. Consumer preferences are typically heterogeneous across space\nand time, making inventory-preference alignment challenging. Additionally,\nexisting strategies rely on syndicated data, which tends to be aggregated, low\nresolution, and suffer from high latency. To solve these challenges, we\nintroduce a real-time recommendation system, which we call EdgeRec3D. Our\nsystem utilizes recent advances in 3D computer vision for perception and\nautomatic, fine grained sales estimation. These perceptual components run on\nthe edge of the network and facilitate real-time reward signals. Additionally,\nwe develop a Bayesian payoff model to account for noisy estimates from 3D LIDAR\ndata. We rely on spatial clustering to allow the system to adapt to\nheterogeneous consumer preferences, and a graph-based candidate generation\nalgorithm to address the combinatorial search problem. We test our system in\nreal-world stores across two, 6-8 week A/B tests with beverage products and\ndemonstrate a 35% and 27% increase in sales respectively. Finally, we monitor\nthe deployed system for a period of 28 weeks with an observational study and\nshow a 9.4% increase in sales.\n","authors":["Porter Jenkins","Michael Selander","J. Stockton Jenkins","Andrew Merrill","Kyle Armstrong"],"pdf_url":"https://arxiv.org/pdf/2406.07769v2.pdf","comment":"Accepted to KDD 2024"},{"id":"http://arxiv.org/abs/2406.09338v1","updated":"2024-06-13T17:19:43Z","published":"2024-06-13T17:19:43Z","title":"Learning the Influence Graph of a High-Dimensional Markov Process with\n  Memory","summary":"  Motivated by multiple applications in social networks, nervous systems, and\nfinancial risk analysis, we consider the problem of learning the underlying\n(directed) influence graph or causal graph of a high-dimensional multivariate\ndiscrete-time Markov process with memory. At any discrete time instant, each\nobserved variable of the multivariate process is a binary string of random\nlength, which is parameterized by an unobservable or hidden [0,1]-valued\nscalar. The hidden scalars corresponding to the variables evolve according to\ndiscrete-time linear stochastic dynamics dictated by the underlying influence\ngraph whose nodes are the variables. We extend an existing algorithm for\nlearning i.i.d. graphical models to this Markovian setting with memory and\nprove that it can learn the influence graph based on the binary observations\nusing logarithmic (in number of variables or nodes) samples when the degree of\nthe influence graph is bounded. The crucial analytical contribution of this\nwork is the derivation of the sample complexity result by upper and lower\nbounding the rate of convergence of the observed Markov process with memory to\nits stationary distribution in terms of the parameters of the influence graph.\n","authors":["Smita Bagewadi","Avhishek Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2406.09338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09335v1","updated":"2024-06-13T17:17:31Z","published":"2024-06-13T17:17:31Z","title":"Instance-level quantitative saliency in multiple sclerosis lesion\n  segmentation","summary":"  In recent years, explainable methods for artificial intelligence (XAI) have\ntried to reveal and describe models' decision mechanisms in the case of\nclassification tasks. However, XAI for semantic segmentation and in particular\nfor single instances has been little studied to date. Understanding the process\nunderlying automatic segmentation of single instances is crucial to reveal what\ninformation was used to detect and segment a given object of interest. In this\nstudy, we proposed two instance-level explanation maps for semantic\nsegmentation based on SmoothGrad and Grad-CAM++ methods. Then, we investigated\ntheir relevance for the detection and segmentation of white matter lesions\n(WML), a magnetic resonance imaging (MRI) biomarker in multiple sclerosis (MS).\n687 patients diagnosed with MS for a total of 4043 FLAIR and MPRAGE MRI scans\nwere collected at the University Hospital of Basel, Switzerland. Data were\nrandomly split into training, validation and test sets to train a 3D U-Net for\nMS lesion segmentation. We observed 3050 true positive (TP), 1818 false\npositive (FP), and 789 false negative (FN) cases. We generated instance-level\nexplanation maps for semantic segmentation, by developing two XAI methods based\non SmoothGrad and Grad-CAM++. We investigated: 1) the distribution of gradients\nin saliency maps with respect to both input MRI sequences; 2) the model's\nresponse in the case of synthetic lesions; 3) the amount of perilesional tissue\nneeded by the model to segment a lesion. Saliency maps (based on SmoothGrad) in\nFLAIR showed positive values inside a lesion and negative in its neighborhood.\nPeak values of saliency maps generated for these four groups of volumes\npresented distributions that differ significantly from one another, suggesting\na quantitative nature of the proposed saliency. Contextual information of 7mm\naround the lesion border was required for their segmentation.\n","authors":["Federico Spagnolo","Nataliia Molchanova","Roger Schaer","Meritxell Bach Cuadra","Mario Ocampo Pineda","Lester Melie-Garcia","Cristina Granziera","Vincent Andrearczyk","Adrien Depeursinge"],"pdf_url":"https://arxiv.org/pdf/2406.09335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09329v1","updated":"2024-06-13T17:07:49Z","published":"2024-06-13T17:07:49Z","title":"Is Value Learning Really the Main Bottleneck in Offline RL?","summary":"  While imitation learning requires access to high-quality data, offline\nreinforcement learning (RL) should, in principle, perform similarly or better\nwith substantially lower data quality by using a value function. However,\ncurrent results indicate that offline RL often performs worse than imitation\nlearning, and it is often unclear what holds back the performance of offline\nRL. Motivated by this observation, we aim to understand the bottlenecks in\ncurrent offline RL algorithms. While poor performance of offline RL is\ntypically attributed to an imperfect value function, we ask: is the main\nbottleneck of offline RL indeed in learning the value function, or something\nelse? To answer this question, we perform a systematic empirical study of (1)\nvalue learning, (2) policy extraction, and (3) policy generalization in offline\nRL problems, analyzing how these components affect performance. We make two\nsurprising observations. First, we find that the choice of a policy extraction\nalgorithm significantly affects the performance and scalability of offline RL,\noften more so than the value learning objective. For instance, we show that\ncommon value-weighted behavioral cloning objectives (e.g., AWR) do not fully\nleverage the learned value function, and switching to behavior-constrained\npolicy gradient objectives (e.g., DDPG+BC) often leads to substantial\nimprovements in performance and scalability. Second, we find that a big barrier\nto improving offline RL performance is often imperfect policy generalization on\ntest-time states out of the support of the training data, rather than policy\nlearning on in-distribution states. We then show that the use of suboptimal but\nhigh-coverage data or test-time policy training techniques can address this\ngeneralization issue in practice. Specifically, we propose two simple test-time\npolicy improvement methods and show that these methods lead to better\nperformance.\n","authors":["Seohong Park","Kevin Frans","Sergey Levine","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2406.09329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04539v2","updated":"2024-06-13T17:05:54Z","published":"2023-06-07T15:44:53Z","title":"Multimodal Learning Without Labeled Multimodal Data: Guarantees and\n  Applications","summary":"  In many machine learning systems that jointly learn from multiple modalities,\na core research question is to understand the nature of multimodal\ninteractions: how modalities combine to provide new task-relevant information\nthat was not present in either alone. We study this challenge of interaction\nquantification in a semi-supervised setting with only labeled unimodal data and\nnaturally co-occurring multimodal data (e.g., unlabeled images and captions,\nvideo and corresponding audio) but when labeling them is time-consuming. Using\na precise information-theoretic definition of interactions, our key\ncontribution is the derivation of lower and upper bounds to quantify the amount\nof multimodal interactions in this semi-supervised setting. We propose two\nlower bounds: one based on the shared information between modalities and the\nother based on disagreement between separately trained unimodal classifiers,\nand derive an upper bound through connections to approximate algorithms for\nmin-entropy couplings. We validate these estimated bounds and show how they\naccurately track true interactions. Finally, we show how these theoretical\nresults can be used to estimate multimodal model performance, guide data\ncollection, and select appropriate multimodal models for various tasks.\n","authors":["Paul Pu Liang","Chun Kai Ling","Yun Cheng","Alex Obolenskiy","Yudong Liu","Rohan Pandey","Alex Wilf","Louis-Philippe Morency","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2306.04539v2.pdf","comment":"ICLR 2024, Code available at: https://github.com/pliang279/PID"},{"id":"http://arxiv.org/abs/2302.10796v2","updated":"2024-06-13T17:00:41Z","published":"2023-02-21T16:23:11Z","title":"Provably Efficient Exploration in Quantum Reinforcement Learning with\n  Logarithmic Worst-Case Regret","summary":"  While quantum reinforcement learning (RL) has attracted a surge of attention\nrecently, its theoretical understanding is limited. In particular, it remains\nelusive how to design provably efficient quantum RL algorithms that can address\nthe exploration-exploitation trade-off. To this end, we propose a novel\nUCRL-style algorithm that takes advantage of quantum computing for tabular\nMarkov decision processes (MDPs) with $S$ states, $A$ actions, and horizon $H$,\nand establish an $\\mathcal{O}(\\mathrm{poly}(S, A, H, \\log T))$ worst-case\nregret for it, where $T$ is the number of episodes. Furthermore, we extend our\nresults to quantum RL with linear function approximation, which is capable of\nhandling problems with large state spaces. Specifically, we develop a quantum\nalgorithm based on value target regression (VTR) for linear mixture MDPs with\n$d$-dimensional linear representation and prove that it enjoys\n$\\mathcal{O}(\\mathrm{poly}(d, H, \\log T))$ regret. Our algorithms are variants\nof UCRL/UCRL-VTR algorithms in classical RL, which also leverage a novel\ncombination of lazy updating mechanisms and quantum estimation subroutines.\nThis is the key to breaking the $\\Omega(\\sqrt{T})$-regret barrier in classical\nRL. To the best of our knowledge, this is the first work studying the online\nexploration in quantum RL with provable logarithmic worst-case regret.\n","authors":["Han Zhong","Jiachen Hu","Yecheng Xue","Tongyang Li","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2302.10796v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2406.09322v1","updated":"2024-06-13T17:00:30Z","published":"2024-06-13T17:00:30Z","title":"Active Inference Meeting Energy-Efficient Control of Parallel and\n  Identical Machines","summary":"  We investigate the application of active inference in developing\nenergy-efficient control agents for manufacturing systems. Active inference,\nrooted in neuroscience, provides a unified probabilistic framework integrating\nperception, learning, and action, with inherent uncertainty quantification\nelements. Our study explores deep active inference, an emerging field that\ncombines deep learning with the active inference decision-making framework.\nLeveraging a deep active inference agent, we focus on controlling parallel and\nidentical machine workstations to enhance energy efficiency. We address\nchallenges posed by the problem's stochastic nature and delayed policy response\nby introducing tailored enhancements to existing agent architectures.\nSpecifically, we introduce multi-step transition and hybrid horizon methods to\nmitigate the need for complex planning. Our experimental results demonstrate\nthe effectiveness of these enhancements and highlight the potential of the\nactive inference-based approach.\n","authors":["Yavar Taheri Yeganeh","Mohsen Jafari","Andrea Matta"],"pdf_url":"https://arxiv.org/pdf/2406.09322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14014v3","updated":"2024-06-13T16:51:50Z","published":"2024-05-22T21:48:17Z","title":"RadarOcc: Robust 3D Occupancy Prediction with 4D Imaging Radar","summary":"  3D occupancy-based perception pipeline has significantly advanced autonomous\ndriving by capturing detailed scene descriptions and demonstrating strong\ngeneralizability across various object categories and shapes. Current methods\npredominantly rely on LiDAR or camera inputs for 3D occupancy prediction. These\nmethods are susceptible to adverse weather conditions, limiting the all-weather\ndeployment of self-driving cars. To improve perception robustness, we leverage\nthe recent advances in automotive radars and introduce a novel approach that\nutilizes 4D imaging radar sensors for 3D occupancy prediction. Our method,\nRadarOcc, circumvents the limitations of sparse radar point clouds by directly\nprocessing the 4D radar tensor, thus preserving essential scene details.\nRadarOcc innovatively addresses the challenges associated with the voluminous\nand noisy 4D radar data by employing Doppler bins descriptors, sidelobe-aware\nspatial sparsification, and range-wise self-attention mechanisms. To minimize\nthe interpolation errors associated with direct coordinate transformations, we\nalso devise a spherical-based feature encoding followed by\nspherical-to-Cartesian feature aggregation. We benchmark various baseline\nmethods based on distinct modalities on the public K-Radar dataset. The results\ndemonstrate RadarOcc's state-of-the-art performance in radar-based 3D occupancy\nprediction and promising results even when compared with LiDAR- or camera-based\nmethods. Additionally, we present qualitative evidence of the superior\nperformance of 4D radar in adverse weather conditions and explore the impact of\nkey pipeline components through ablation studies.\n","authors":["Fangqiang Ding","Xiangyu Wen","Lawrence Zhu","Yiming Li","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2405.14014v3.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.09315v1","updated":"2024-06-13T16:51:33Z","published":"2024-06-13T16:51:33Z","title":"Vertical LoRA: Dense Expectation-Maximization Interpretation of\n  Transformers","summary":"  In this paper, we show how Transformers can be interpreted as dense\nExpectation-Maximization algorithms performed on Bayesian Nets. Based on the\nabove interpretation, we propose a new model design paradigm, namely Vertical\nLoRA (VLoRA), which reduces the parameter count dramatically while preserving\nperformance. In VLoRA, a model consists of layers, each of which recursively\nlearns an increment based on the previous layer. We then apply LoRA\ndecomposition to the increments. VLoRA works on the base model, which is\northogonal to LoRA, meaning they can be used together. We do experiments on\nvarious tasks and models. The results show that 1) with VLoRA, the Transformer\nmodel parameter count can be reduced dramatically and 2) the performance of the\noriginal model is preserved. The source code is available at\n\\url{https://github.com/neverUseThisName/vlora}\n","authors":["Zhuolin Fu"],"pdf_url":"https://arxiv.org/pdf/2406.09315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09871v3","updated":"2024-06-13T16:51:26Z","published":"2024-03-14T21:01:06Z","title":"ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric\n  Thermal Images","summary":"  In this work, we present ThermoHands, a new benchmark for thermal image-based\negocentric 3D hand pose estimation, aimed at overcoming challenges like varying\nlighting conditions and obstructions (e.g., handwear). The benchmark includes a\nmulti-view and multi-spectral dataset collected from 28 subjects performing\nhand-object and hand-virtual interactions under diverse scenarios, accurately\nannotated with 3D hand poses through an automated process. We introduce a new\nbaseline method, TherFormer, utilizing dual transformer modules for effective\negocentric 3D hand pose estimation in thermal imagery. Our experimental results\nhighlight TherFormer's leading performance and affirm thermal imaging's\neffectiveness in enabling robust 3D hand pose estimation in adverse conditions.\n","authors":["Fangqiang Ding","Lawrence Zhu","Xiangyu Wen","Gaowen Liu","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2403.09871v3.pdf","comment":"15 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.09310v1","updated":"2024-06-13T16:44:58Z","published":"2024-06-13T16:44:58Z","title":"Neural networks in non-metric spaces","summary":"  Leveraging the infinite dimensional neural network architecture we proposed\nin arXiv:2109.13512v4 and which can process inputs from Fr\\'echet spaces, and\nusing the universal approximation property shown therein, we now largely extend\nthe scope of this architecture by proving several universal approximation\ntheorems for a vast class of input and output spaces. More precisely, the input\nspace $\\mathfrak X$ is allowed to be a general topological space satisfying\nonly a mild condition (\"quasi-Polish\"), and the output space can be either\nanother quasi-Polish space $\\mathfrak Y$ or a topological vector space $E$.\nSimilarly to arXiv:2109.13512v4, we show furthermore that our neural network\narchitectures can be projected down to \"finite dimensional\" subspaces with any\ndesirable accuracy, thus obtaining approximating networks that are easy to\nimplement and allow for fast computation and fitting. The resulting neural\nnetwork architecture is therefore applicable for prediction tasks based on\nfunctional data. To the best of our knowledge, this is the first result which\ndeals with such a wide class of input/output spaces and simultaneously\nguarantees the numerical feasibility of the ensuing architectures. Finally, we\nprove an obstruction result which indicates that the category of quasi-Polish\nspaces is in a certain sense the correct category to work with if one aims at\nconstructing approximating architectures on infinite-dimensional spaces\n$\\mathfrak X$ which, at the same time, have sufficient expressive power to\napproximate continuous functions on $\\mathfrak X$, are specified by a finite\nnumber of parameters only and are \"stable\" with respect to these parameters.\n","authors":["Luca Galimberti"],"pdf_url":"https://arxiv.org/pdf/2406.09310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09308v1","updated":"2024-06-13T16:42:06Z","published":"2024-06-13T16:42:06Z","title":"Transformers meet Neural Algorithmic Reasoners","summary":"  Transformers have revolutionized machine learning with their simple yet\neffective architecture. Pre-training Transformers on massive text datasets from\nthe Internet has led to unmatched generalization for natural language\nunderstanding (NLU) tasks. However, such language models remain fragile when\ntasked with algorithmic forms of reasoning, where computations must be precise\nand robust. To address this limitation, we propose a novel approach that\ncombines the Transformer's language understanding with the robustness of graph\nneural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs\nproved effective as generic solvers for algorithmic tasks, when specified in\ngraph form. To make their embeddings accessible to a Transformer, we propose a\nhybrid architecture with a two-phase training procedure, allowing the tokens in\nthe language model to cross-attend to the node embeddings from the NAR. We\nevaluate our resulting TransNAR model on CLRS-Text, the text-based version of\nthe CLRS-30 benchmark, and demonstrate significant gains over Transformer-only\nmodels for algorithmic reasoning, both in and out of distribution.\n","authors":["Wilfried Bounsi","Borja Ibarz","Andrew Dudzik","Jessica B. Hamrick","Larisa Markeeva","Alex Vitvitskyi","Razvan Pascanu","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2406.09308v1.pdf","comment":"To appear at CVPR 2024 Multimodal Algorithmic Reasoning (MAR)\n  Workshop. 10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.09307v1","updated":"2024-06-13T16:41:30Z","published":"2024-06-13T16:41:30Z","title":"A tutorial on fairness in machine learning in healthcare","summary":"  OBJECTIVE: Ensuring that machine learning (ML) algorithms are safe and\neffective within all patient groups, and do not disadvantage particular\npatients, is essential to clinical decision making and preventing the\nreinforcement of existing healthcare inequities. The objective of this tutorial\nis to introduce the medical informatics community to the common notions of\nfairness within ML, focusing on clinical applications and implementation in\npractice.\n  TARGET AUDIENCE: As gaps in fairness arise in a variety of healthcare\napplications, this tutorial is designed to provide an understanding of\nfairness, without assuming prior knowledge, to researchers and clinicians who\nmake use of modern clinical data.\n  SCOPE: We describe the fundamental concepts and methods used to define\nfairness in ML, including an overview of why models in healthcare may be\nunfair, a summary and comparison of the metrics used to quantify fairness, and\na discussion of some ongoing research. We illustrate some of the fairness\nmethods introduced through a case study of mortality prediction in a publicly\navailable electronic health record dataset. Finally, we provide a user-friendly\nR package for comprehensive group fairness evaluation, enabling researchers and\nclinicians to assess fairness in their own ML work.\n","authors":["Jianhui Gao","Benson Chou","Zachary R. McCaw","Hilary Thurston","Paul Varghese","Chuan Hong","Jessica Gronsbell"],"pdf_url":"https://arxiv.org/pdf/2406.09307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.08269v3","updated":"2024-06-13T16:34:45Z","published":"2022-06-16T16:06:47Z","title":"Learning with little mixing","summary":"  We study square loss in a realizable time-series framework with martingale\ndifference noise. Our main result is a fast rate excess risk bound which shows\nthat whenever a trajectory hypercontractivity condition holds, the risk of the\nleast-squares estimator on dependent data matches the iid rate order-wise after\na burn-in time. In comparison, many existing results in learning from dependent\ndata have rates where the effective sample size is deflated by a factor of the\nmixing-time of the underlying process, even after the burn-in time.\nFurthermore, our results allow the covariate process to exhibit long range\ncorrelations which are substantially weaker than geometric ergodicity. We call\nthis phenomenon learning with little mixing, and present several examples for\nwhen it occurs: bounded function classes for which the $L^2$ and\n$L^{2+\\epsilon}$ norms are equivalent, ergodic finite state Markov chains,\nvarious parametric models, and a broad family of infinite dimensional\n$\\ell^2(\\mathbb{N})$ ellipsoids. By instantiating our main result to system\nidentification of nonlinear dynamics with generalized linear model transitions,\nwe obtain a nearly minimax optimal excess risk bound after only a polynomial\nburn-in time.\n","authors":["Ingvar Ziemann","Stephen Tu"],"pdf_url":"https://arxiv.org/pdf/2206.08269v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09297v1","updated":"2024-06-13T16:33:44Z","published":"2024-06-13T16:33:44Z","title":"MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding","summary":"  Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv\n","authors":["Zayd Muhammad Kawakibi Zuhri","Muhammad Farid Adilazuarda","Ayu Purwarianti","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2406.09297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09294v1","updated":"2024-06-13T16:30:03Z","published":"2024-06-13T16:30:03Z","title":"You Don't Need Data-Augmentation in Self-Supervised Learning","summary":"  Self-Supervised learning (SSL) with Joint-Embedding Architectures (JEA) has\nled to outstanding performances. All instantiations of this paradigm were\ntrained using strong and well-established hand-crafted data augmentations,\nleading to the general belief that they are required for the proper training\nand performance of such models. On the other hand, generative\nreconstruction-based models such as BEIT and MAE or Joint-Embedding Predictive\nArchitectures such as I-JEPA have shown strong performance without using data\naugmentations except masking. In this work, we challenge the importance of\ninvariance and data-augmentation in JEAs at scale. By running a case-study on a\nrecent SSL foundation model - DINOv2 - we show that strong image\nrepresentations can be obtained with JEAs and only cropping without resizing\nprovided the training data is large enough, reaching state-of-the-art results\nand using the least amount of augmentation in the literature. Through this\nstudy, we also discuss the impact of compute constraints on the outcomes of\nexperimental deep learning research, showing that they can lead to very\ndifferent conclusions.\n","authors":["Théo Moutakanni","Maxime Oquab","Marc Szafraniec","Maria Vakalopoulou","Piotr Bojanowski"],"pdf_url":"https://arxiv.org/pdf/2406.09294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09292v1","updated":"2024-06-13T16:29:18Z","published":"2024-06-13T16:29:18Z","title":"Neural Assets: 3D-Aware Multi-Object Scene Synthesis with Image\n  Diffusion Models","summary":"  We address the problem of multi-object 3D pose control in image diffusion\nmodels. Instead of conditioning on a sequence of text tokens, we propose to use\na set of per-object representations, Neural Assets, to control the 3D pose of\nindividual objects in a scene. Neural Assets are obtained by pooling visual\nrepresentations of objects from a reference image, such as a frame in a video,\nand are trained to reconstruct the respective objects in a different image,\ne.g., a later frame in the video. Importantly, we encode object visuals from\nthe reference image while conditioning on object poses from the target frame.\nThis enables learning disentangled appearance and pose features. Combining\nvisual and 3D pose representations in a sequence-of-tokens format allows us to\nkeep the text-to-image architecture of existing models, with Neural Assets in\nplace of text tokens. By fine-tuning a pre-trained text-to-image diffusion\nmodel with this information, our approach enables fine-grained 3D pose and\nplacement control of individual objects in a scene. We further demonstrate that\nNeural Assets can be transferred and recomposed across different scenes. Our\nmodel achieves state-of-the-art multi-object editing results on both synthetic\n3D scene datasets, as well as two real-world video datasets (Objectron, Waymo\nOpen).\n","authors":["Ziyi Wu","Yulia Rubanova","Rishabh Kabra","Drew A. Hudson","Igor Gilitschenski","Yusuf Aytar","Sjoerd van Steenkiste","Kelsey R. Allen","Thomas Kipf"],"pdf_url":"https://arxiv.org/pdf/2406.09292v1.pdf","comment":"Additional details and video results are available at\n  https://neural-assets-paper.github.io/"},{"id":"http://arxiv.org/abs/2406.09291v1","updated":"2024-06-13T16:29:06Z","published":"2024-06-13T16:29:06Z","title":"A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products\n  and Graph Coarsening","summary":"  Subgraph Graph Neural Networks (Subgraph GNNs) enhance the expressivity of\nmessage-passing GNNs by representing graphs as sets of subgraphs. They have\nshown impressive performance on several tasks, but their complexity limits\napplications to larger graphs. Previous approaches suggested processing only\nsubsets of subgraphs, selected either randomly or via learnable sampling.\nHowever, they make suboptimal subgraph selections or can only cope with very\nsmall subset sizes, inevitably incurring performance degradation. This paper\nintroduces a new Subgraph GNNs framework to address these issues. We employ a\ngraph coarsening function to cluster nodes into super-nodes with induced\nconnectivity. The product between the coarsened and the original graph reveals\nan implicit structure whereby subgraphs are associated with specific sets of\nnodes. By running generalized message-passing on such graph product, our method\neffectively implements an efficient, yet powerful Subgraph GNN. Controlling the\ncoarsening function enables meaningful selection of any number of subgraphs\nwhile, contrary to previous methods, being fully compatible with standard\ntraining techniques. Notably, we discover that the resulting node feature\ntensor exhibits new, unexplored permutation symmetries. We leverage this\nstructure, characterize the associated linear equivariant layers and\nincorporate them into the layers of our Subgraph GNN architecture. Extensive\nexperiments on multiple graph learning benchmarks demonstrate that our method\nis significantly more flexible than previous approaches, as it can seamlessly\nhandle any number of subgraphs, while consistently outperforming baseline\napproaches.\n","authors":["Guy Bar-Shalom","Yam Eitan","Fabrizio Frasca","Haggai Maron"],"pdf_url":"https://arxiv.org/pdf/2406.09291v1.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2406.09289v1","updated":"2024-06-13T16:26:47Z","published":"2024-06-13T16:26:47Z","title":"Understanding Jailbreak Success: A Study of Latent Space Dynamics in\n  Large Language Models","summary":"  Conversational Large Language Models are trained to refuse to answer harmful\nquestions. However, emergent jailbreaking techniques can still elicit unsafe\noutputs, presenting an ongoing challenge for model alignment. To better\nunderstand how different jailbreak types circumvent safeguards, this paper\nanalyses model activations on different jailbreak inputs. We find that it is\npossible to extract a jailbreak vector from a single class of jailbreaks that\nworks to mitigate jailbreak effectiveness from other classes. This may indicate\nthat different kinds of effective jailbreaks operate via similar internal\nmechanisms. We investigate a potential common mechanism of harmfulness feature\nsuppression, and provide evidence for its existence by looking at the\nharmfulness vector component. These findings offer actionable insights for\ndeveloping more robust jailbreak countermeasures and lay the groundwork for a\ndeeper, mechanistic understanding of jailbreak dynamics in language models.\n","authors":["Sarah Ball","Frauke Kreuter","Nina Rimsky"],"pdf_url":"https://arxiv.org/pdf/2406.09289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09288v1","updated":"2024-06-13T16:26:37Z","published":"2024-06-13T16:26:37Z","title":"Zero-Shot Learning Over Large Output Spaces : Utilizing Indirect\n  Knowledge Extraction from Large Language Models","summary":"  Extreme Multi-label Learning (XMC) is a task that allocates the most relevant\nlabels for an instance from a predefined label set. Extreme Zero-shot XMC\n(EZ-XMC) is a special setting of XMC wherein no supervision is provided; only\nthe instances (raw text of the document) and the predetermined label set are\ngiven. The scenario is designed to address cold-start problems in\ncategorization and recommendation. Traditional state-of-the-art methods extract\npseudo labels from the document title or segments. These labels from the\ndocument are used to train a zero-shot bi-encoder model. The main issue with\nthese generated labels is their misalignment with the tagging task. In this\nwork, we propose a framework to train a small bi-encoder model via the feedback\nfrom the large language model (LLM), the bi-encoder model encodes the document\nand labels into embeddings for retrieval. Our approach leverages the zero-shot\nability of LLM to assess the correlation between labels and the document\ninstead of using the low-quality labels extracted from the document itself. Our\nmethod also guarantees fast inference without the involvement of LLM. The\nperformance of our approach outperforms the SOTA methods on various datasets\nwhile retaining a similar training time for large datasets.\n","authors":["Jinbin Zhang","Nasib Ullah","Rohit Babbar"],"pdf_url":"https://arxiv.org/pdf/2406.09288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07460v3","updated":"2024-06-13T16:22:04Z","published":"2024-05-13T04:35:14Z","title":"HoneyBee: A Scalable Modular Framework for Creating Multimodal Oncology\n  Datasets with Foundational Embedding Models","summary":"  Developing accurate machine learning models for oncology requires\nlarge-scale, high-quality multimodal datasets. However, creating such datasets\nremains challenging due to the complexity and heterogeneity of medical data. To\naddress this challenge, we introduce HoneyBee, a scalable modular framework for\nbuilding multimodal oncology datasets that leverages foundation models to\ngenerate representative embeddings. HoneyBee integrates various data\nmodalities, including clinical diagnostic and pathology imaging data, medical\nnotes, reports, records, and molecular data. It employs data preprocessing\ntechniques and foundation models to generate embeddings that capture the\nessential features and relationships within the raw medical data. The generated\nembeddings are stored in a structured format using Hugging Face datasets and\nPyTorch dataloaders for accessibility. Vector databases enable efficient\nquerying and retrieval for machine learning applications. We demonstrate the\neffectiveness of HoneyBee through experiments assessing the quality and\nrepresentativeness of these embeddings. The framework is designed to be\nextensible to other medical domains and aims to accelerate oncology research by\nproviding high-quality, machine learning-ready datasets. HoneyBee is an ongoing\nopen-source effort, and the code, datasets, and models are available at the\nproject repository.\n","authors":["Aakash Tripathi","Asim Waqas","Yasin Yilmaz","Ghulam Rasool"],"pdf_url":"https://arxiv.org/pdf/2405.07460v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00811v2","updated":"2024-06-13T16:20:59Z","published":"2024-02-01T17:46:19Z","title":"An Analysis of the Variance of Diffusion-based Speech Enhancement","summary":"  Diffusion models proved to be powerful models for generative speech\nenhancement. In recent SGMSE+ approaches, training involves a stochastic\ndifferential equation for the diffusion process, adding both Gaussian and\nenvironmental noise to the clean speech signal gradually. The speech\nenhancement performance varies depending on the choice of the stochastic\ndifferential equation that controls the evolution of the mean and the variance\nalong the diffusion processes when adding environmental and Gaussian noise. In\nthis work, we highlight that the scale of the variance is a dominant parameter\nfor speech enhancement performance and show that it controls the tradeoff\nbetween noise attenuation and speech distortions. More concretely, we show that\na larger variance increases the noise attenuation and allows for reducing the\ncomputational footprint, as fewer function evaluations for generating the\nestimate are required\n","authors":["Bunlong Lay","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2402.00811v2.pdf","comment":"5 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2406.09277v1","updated":"2024-06-13T16:15:53Z","published":"2024-06-13T16:15:53Z","title":"End-to-end Streaming model for Low-Latency Speech Anonymization","summary":"  Speaker anonymization aims to conceal cues to speaker identity while\npreserving linguistic content. Current machine learning based approaches\nrequire substantial computational resources, hindering real-time streaming\napplications. To address these concerns, we propose a streaming model that\nachieves speaker anonymization with low latency. The system is trained in an\nend-to-end autoencoder fashion using a lightweight content encoder that\nextracts HuBERT-like information, a pretrained speaker encoder that extract\nspeaker identity, and a variance encoder that injects pitch and energy\ninformation. These three disentangled representations are fed to a decoder that\nresynthesizes the speech signal. We present evaluation results from two\nimplementations of our system, a full model that achieves a latency of 230ms,\nand a lite version (0.1x in size) that further reduces latency to 66ms while\nmaintaining state-of-the-art performance in naturalness, intelligibility, and\nprivacy preservation.\n","authors":["Waris Quamer","Ricardo Gutierrez-Osuna"],"pdf_url":"https://arxiv.org/pdf/2406.09277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09263v1","updated":"2024-06-13T16:03:15Z","published":"2024-06-13T16:03:15Z","title":"Generative Inverse Design of Crystal Structures via Diffusion Models\n  with Transformers","summary":"  Recent advances in deep learning have enabled the generation of realistic\ndata by training generative models on large datasets of text, images, and\naudio. While these models have demonstrated exceptional performance in\ngenerating novel and plausible data, it remains an open question whether they\ncan effectively accelerate scientific discovery through the data generation and\ndrive significant advancements across various scientific fields. In particular,\nthe discovery of new inorganic materials with promising properties poses a\ncritical challenge, both scientifically and for industrial applications.\nHowever, unlike textual or image data, materials, or more specifically crystal\nstructures, consist of multiple types of variables - including lattice vectors,\natom positions, and atomic species. This complexity in data give rise to a\nvariety of approaches for representing and generating such data. Consequently,\nthe design choices of generative models for crystal structures remain an open\nquestion. In this study, we explore a new type of diffusion model for the\ngenerative inverse design of crystal structures, with a backbone based on a\nTransformer architecture. We demonstrate our models are superior to previous\nmethods in their versatility for generating crystal structures with desired\nproperties. Furthermore, our empirical results suggest that the optimal\nconditioning methods vary depending on the dataset.\n","authors":["Izumi Takahara","Kiyou Shibata","Teruyasu Mizoguchi"],"pdf_url":"https://arxiv.org/pdf/2406.09263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09262v1","updated":"2024-06-13T16:02:03Z","published":"2024-06-13T16:02:03Z","title":"Flexible Heteroscedastic Count Regression with Deep Double Poisson\n  Networks","summary":"  Neural networks that can produce accurate, input-conditional uncertainty\nrepresentations are critical for real-world applications. Recent progress on\nheteroscedastic continuous regression has shown great promise for calibrated\nuncertainty quantification on complex tasks, like image regression. However,\nwhen these methods are applied to discrete regression tasks, such as crowd\ncounting, ratings prediction, or inventory estimation, they tend to produce\npredictive distributions with numerous pathologies. We propose to address these\nissues by training a neural network to output the parameters of a Double\nPoisson distribution, which we call the Deep Double Poisson Network (DDPN). In\ncontrast to existing methods that are trained to minimize Gaussian negative log\nlikelihood (NLL), DDPNs produce a proper probability mass function over\ndiscrete output. Additionally, DDPNs naturally model under-, over-, and\nequi-dispersion, unlike networks trained with the more rigid Poisson and\nNegative Binomial parameterizations. We show DDPNs 1) vastly outperform\nexisting discrete models; 2) meet or exceed the accuracy and flexibility of\nnetworks trained with Gaussian NLL; 3) produce proper predictive distributions\nover discrete counts; and 4) exhibit superior out-of-distribution detection.\nDDPNs can easily be applied to a variety of count regression datasets including\ntabular, image, point cloud, and text data.\n","authors":["Spencer Young","Porter Jenkins","Lonchao Da","Jeff Dotson","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2406.09262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04575v2","updated":"2024-06-13T16:00:06Z","published":"2024-04-06T09:55:03Z","title":"To Cool or not to Cool? Temperature Network Meets Large Foundation\n  Models via DRO","summary":"  The temperature parameter plays a profound role during training and/or\ninference with large foundation models (LFMs) such as large language models\n(LLMs) and CLIP models. Particularly, it adjusts the logits in the softmax\nfunction in LLMs, which is crucial for next token generation, and it scales the\nsimilarities in the contrastive loss for training CLIP models. A significant\nquestion remains: Is it viable to learn a neural network to predict a\npersonalized temperature of any input data for enhancing LFMs\"? In this paper,\nwe present a principled framework for learning a small yet generalizable\ntemperature prediction network (TempNet) to improve LFMs. Our solution is\ncomposed of a novel learning framework with a robust loss underpinned by\nconstrained distributionally robust optimization (DRO), and a properly designed\nTempNet with theoretical inspiration. TempNet can be trained together with a\nlarge foundation model from scratch or learned separately given a pretrained\nfoundation model. It is not only useful for predicting personalized temperature\nto promote the training of LFMs but also generalizable and transferable to new\ntasks. Our experiments on LLMs and CLIP models demonstrate that TempNet greatly\nimproves the performance of existing solutions or models, e.g. Table 1. The\ncode to reproduce the experimental results in this paper can be found at\nhttps://github.com/zhqiu/TempNet.\n","authors":["Zi-Hao Qiu","Siqi Guo","Mao Xu","Tuo Zhao","Lijun Zhang","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2404.04575v2.pdf","comment":"41 pages, 10 figures, accepted by ICML 24"},{"id":"http://arxiv.org/abs/2406.09257v1","updated":"2024-06-13T15:58:37Z","published":"2024-06-13T15:58:37Z","title":"Assessing Model Generalization in Vicinity","summary":"  This paper evaluates the generalization ability of classification models on\nout-of-distribution test sets without depending on ground truth labels. Common\napproaches often calculate an unsupervised metric related to a specific model\nproperty, like confidence or invariance, which correlates with\nout-of-distribution accuracy. However, these metrics are typically computed for\neach test sample individually, leading to potential issues caused by spurious\nmodel responses, such as overly high or low confidence. To tackle this\nchallenge, we propose incorporating responses from neighboring test samples\ninto the correctness assessment of each individual sample. In essence, if a\nmodel consistently demonstrates high correctness scores for nearby samples, it\nincreases the likelihood of correctly predicting the target sample, and vice\nversa. The resulting scores are then averaged across all test samples to\nprovide a holistic indication of model accuracy. Developed under the vicinal\nrisk formulation, this approach, named vicinal risk proxy (VRP), computes\naccuracy without relying on labels. We show that applying the VRP method to\nexisting generalization indicators, such as average confidence and effective\ninvariance, consistently improves over these baselines both methodologically\nand experimentally. This yields a stronger correlation with model accuracy,\nespecially on challenging out-of-distribution test sets.\n","authors":["Yuchi Liu","Yifan Sun","Jingdong Wang","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.09257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09253v1","updated":"2024-06-13T15:56:55Z","published":"2024-06-13T15:56:55Z","title":"Deep Sketched Output Kernel Regression for Structured Prediction","summary":"  By leveraging the kernel trick in the output space, kernel-induced losses\nprovide a principled way to define structured output prediction tasks for a\nwide variety of output modalities. In particular, they have been successfully\nused in the context of surrogate non-parametric regression, where the kernel\ntrick is typically exploited in the input space as well. However, when inputs\nare images or texts, more expressive models such as deep neural networks seem\nmore suited than non-parametric methods. In this work, we tackle the question\nof how to train neural networks to solve structured output prediction tasks,\nwhile still benefiting from the versatility and relevance of kernel-induced\nlosses. We design a novel family of deep neural architectures, whose last layer\npredicts in a data-dependent finite-dimensional subspace of the\ninfinite-dimensional output feature space deriving from the kernel-induced\nloss. This subspace is chosen as the span of the eigenfunctions of a\nrandomly-approximated version of the empirical kernel covariance operator.\nInterestingly, this approach unlocks the use of gradient descent algorithms\n(and consequently of any neural architecture) for structured prediction.\nExperiments on synthetic tasks as well as real-world supervised graph\nprediction problems show the relevance of our method.\n","authors":["Tamim El Ahmad","Junjie Yang","Pierre Laforgue","Florence d'Alché-Buc"],"pdf_url":"https://arxiv.org/pdf/2406.09253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09250v1","updated":"2024-06-13T15:55:04Z","published":"2024-06-13T15:55:04Z","title":"MirrorCheck: Efficient Adversarial Defense for Vision-Language Models","summary":"  Vision-Language Models (VLMs) are becoming increasingly vulnerable to\nadversarial attacks as various novel attack strategies are being proposed\nagainst these models. While existing defenses excel in unimodal contexts, they\ncurrently fall short in safeguarding VLMs against adversarial threats. To\nmitigate this vulnerability, we propose a novel, yet elegantly simple approach\nfor detecting adversarial samples in VLMs. Our method leverages Text-to-Image\n(T2I) models to generate images based on captions produced by target VLMs.\nSubsequently, we calculate the similarities of the embeddings of both input and\ngenerated images in the feature space to identify adversarial samples.\nEmpirical evaluations conducted on different datasets validate the efficacy of\nour approach, outperforming baseline methods adapted from image classification\ndomains. Furthermore, we extend our methodology to classification tasks,\nshowcasing its adaptability and model-agnostic nature. Theoretical analyses and\nempirical findings also show the resilience of our approach against adaptive\nattacks, positioning it as an excellent defense mechanism for real-world\ndeployment against adversarial threats.\n","authors":["Samar Fares","Klea Ziu","Toluwani Aremu","Nikita Durasov","Martin Takáč","Pascal Fua","Karthik Nandakumar","Ivan Laptev"],"pdf_url":"https://arxiv.org/pdf/2406.09250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09246v1","updated":"2024-06-13T15:46:55Z","published":"2024-06-13T15:46:55Z","title":"OpenVLA: An Open-Source Vision-Language-Action Model","summary":"  Large policies pretrained on a combination of Internet-scale vision-language\ndata and diverse robot demonstrations have the potential to change how we teach\nrobots new skills: rather than training new behaviors from scratch, we can\nfine-tune such vision-language-action (VLA) models to obtain robust,\ngeneralizable policies for visuomotor control. Yet, widespread adoption of VLAs\nfor robotics has been challenging as 1) existing VLAs are largely closed and\ninaccessible to the public, and 2) prior work fails to explore methods for\nefficiently fine-tuning VLAs for new tasks, a key component for adoption.\nAddressing these challenges, we introduce OpenVLA, a 7B-parameter open-source\nVLA trained on a diverse collection of 970k real-world robot demonstrations.\nOpenVLA builds on a Llama 2 language model combined with a visual encoder that\nfuses pretrained features from DINOv2 and SigLIP. As a product of the added\ndata diversity and new model components, OpenVLA demonstrates strong results\nfor generalist manipulation, outperforming closed models such as RT-2-X (55B)\nby 16.5% in absolute task success rate across 29 tasks and multiple robot\nembodiments, with 7x fewer parameters. We further show that we can effectively\nfine-tune OpenVLA for new settings, with especially strong generalization\nresults in multi-task environments involving multiple objects and strong\nlanguage grounding abilities, and outperform expressive from-scratch imitation\nlearning methods such as Diffusion Policy by 20.4%. We also explore compute\nefficiency; as a separate contribution, we show that OpenVLA can be fine-tuned\non consumer GPUs via modern low-rank adaptation methods and served efficiently\nvia quantization without a hit to downstream success rate. Finally, we release\nmodel checkpoints, fine-tuning notebooks, and our PyTorch codebase with\nbuilt-in support for training VLAs at scale on Open X-Embodiment datasets.\n","authors":["Moo Jin Kim","Karl Pertsch","Siddharth Karamcheti","Ted Xiao","Ashwin Balakrishna","Suraj Nair","Rafael Rafailov","Ethan Foster","Grace Lam","Pannag Sanketi","Quan Vuong","Thomas Kollar","Benjamin Burchfiel","Russ Tedrake","Dorsa Sadigh","Sergey Levine","Percy Liang","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2406.09246v1.pdf","comment":"Website: https://openvla.github.io/"},{"id":"http://arxiv.org/abs/2406.09241v1","updated":"2024-06-13T15:44:23Z","published":"2024-06-13T15:44:23Z","title":"What is the long-run distribution of stochastic gradient descent? A\n  large deviations analysis","summary":"  In this paper, we examine the long-run distribution of stochastic gradient\ndescent (SGD) in general, non-convex problems. Specifically, we seek to\nunderstand which regions of the problem's state space are more likely to be\nvisited by SGD, and by how much. Using an approach based on the theory of large\ndeviations and randomly perturbed dynamical systems, we show that the long-run\ndistribution of SGD resembles the Boltzmann-Gibbs distribution of equilibrium\nthermodynamics with temperature equal to the method's step-size and energy\nlevels determined by the problem's objective and the statistics of the noise.\nIn particular, we show that, in the long run, (a) the problem's critical region\nis visited exponentially more often than any non-critical region; (b) the\niterates of SGD are exponentially concentrated around the problem's minimum\nenergy state (which does not always coincide with the global minimum of the\nobjective); (c) all other connected components of critical points are visited\nwith frequency that is exponentially proportional to their energy level; and,\nfinally (d) any component of local maximizers or saddle points is \"dominated\"\nby a component of local minimizers which is visited exponentially more often.\n","authors":["Waïss Azizian","Franck Iutzeler","Jérôme Malick","Panayotis Mertikopoulos"],"pdf_url":"https://arxiv.org/pdf/2406.09241v1.pdf","comment":"70 pages, 3 figures; to be published in the proceedings of ICML 2024"},{"id":"http://arxiv.org/abs/2406.06664v2","updated":"2024-06-13T15:39:03Z","published":"2024-06-10T15:39:04Z","title":"ASTRA: Aligning Speech and Text Representations for Asr without Sampling","summary":"  This paper introduces ASTRA, a novel method for improving Automatic Speech\nRecognition (ASR) through text injection.Unlike prevailing techniques, ASTRA\neliminates the need for sampling to match sequence lengths between speech and\ntext modalities. Instead, it leverages the inherent alignments learned within\nCTC/RNNT models. This approach offers the following two advantages, namely,\navoiding potential misalignment between speech and text features that could\narise from upsampling and eliminating the need for models to accurately predict\nduration of sub-word tokens. This novel formulation of modality (length)\nmatching as a weighted RNNT objective matches the performance of the\nstate-of-the-art duration-based methods on the FLEURS benchmark, while opening\nup other avenues of research in speech processing.\n","authors":["Neeraj Gaur","Rohan Agrawal","Gary Wang","Parisa Haghani","Andrew Rosenberg","Bhuvana Ramabhadran"],"pdf_url":"https://arxiv.org/pdf/2406.06664v2.pdf","comment":"To be published in Interspeech 2024"},{"id":"http://arxiv.org/abs/2403.06009v2","updated":"2024-06-13T15:31:28Z","published":"2024-03-09T21:07:16Z","title":"Detectors for Safe and Reliable LLMs: Implementations, Uses, and\n  Limitations","summary":"  Large language models (LLMs) are susceptible to a variety of risks, from\nnon-faithful output to biased and toxic generations. Due to several limiting\nfactors surrounding LLMs (training cost, API access, data availability, etc.),\nit may not always be feasible to impose direct safety constraints on a deployed\nmodel. Therefore, an efficient and reliable alternative is required. To this\nend, we present our ongoing efforts to create and deploy a library of\ndetectors: compact and easy-to-build classification models that provide labels\nfor various harms. In addition to the detectors themselves, we discuss a wide\nrange of uses for these detector models - from acting as guardrails to enabling\neffective AI governance. We also deep dive into inherent challenges in their\ndevelopment and discuss future work aimed at making the detectors more reliable\nand broadening their scope.\n","authors":["Swapnaja Achintalwar","Adriana Alvarado Garcia","Ateret Anaby-Tavor","Ioana Baldini","Sara E. Berger","Bishwaranjan Bhattacharjee","Djallel Bouneffouf","Subhajit Chaudhury","Pin-Yu Chen","Lamogha Chiazor","Elizabeth M. Daly","Kirushikesh DB","Rogério Abreu de Paula","Pierre Dognin","Eitan Farchi","Soumya Ghosh","Michael Hind","Raya Horesh","George Kour","Ja Young Lee","Nishtha Madaan","Sameep Mehta","Erik Miehling","Keerthiram Murugesan","Manish Nagireddy","Inkit Padhi","David Piorkowski","Ambrish Rawat","Orna Raz","Prasanna Sattigeri","Hendrik Strobelt","Sarathkrishna Swaminathan","Christoph Tillmann","Aashka Trivedi","Kush R. Varshney","Dennis Wei","Shalisha Witherspooon","Marcel Zalmanovici"],"pdf_url":"https://arxiv.org/pdf/2403.06009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02409v2","updated":"2024-06-13T15:19:24Z","published":"2023-10-03T20:07:06Z","title":"Dodo: Dynamic Contextual Compression for Decoder-only LMs","summary":"  Transformer-based language models (LMs) are inefficient in long contexts. We\npropose Dodo, a solution for context compression. Instead of one vector per\ntoken in a standard transformer model, Dodo represents text with a dynamic\nnumber of hidden states at each layer, reducing the cost of self-attention to a\nfraction of typical time and space. Moreover, off-the-shelf models such as\nLLaMA can be adapted to Dodo by efficient parameter tuning methods such as\nLoRA. In use, Dodo can act as either an autoregressive LM or a context\ncompressor for downstream tasks. We demonstrate through experiments in language\nmodeling, question answering, and summarization that Dodo retains capabilities\nin these tasks, while drastically reducing the overhead during decoding. For\nexample, in the autoencoding task, Dodo shrinks context at a 20x compression\nratio with a BLEU score of 98% for reconstruction, achieving nearly lossless\nencoding.\n","authors":["Guanghui Qin","Corby Rosset","Ethan C. Chau","Nikhil Rao","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2310.02409v2.pdf","comment":"ACL 2024 camera-ready. 15 pages and 7 figures"},{"id":"http://arxiv.org/abs/2310.02897v2","updated":"2024-06-13T15:13:09Z","published":"2023-10-04T15:36:33Z","title":"How Much Training Data is Memorized in Overparameterized Autoencoders?\n  An Inverse Problem Perspective on Memorization Evaluation","summary":"  Overparameterized autoencoder models often memorize their training data. For\nimage data, memorization is often examined by using the trained autoencoder to\nrecover missing regions in its training images (that were used only in their\ncomplete forms in the training). In this paper, we propose an inverse problem\nperspective for the study of memorization. Given a degraded training image, we\ndefine the recovery of the original training image as an inverse problem and\nformulate it as an optimization task. In our inverse problem, we use the\ntrained autoencoder to implicitly define a regularizer for the particular\ntraining dataset that we aim to retrieve from. We develop the intricate\noptimization task into a practical method that iteratively applies the trained\nautoencoder and relatively simple computations that estimate and address the\nunknown degradation operator. We evaluate our method for blind inpainting where\nthe goal is to recover training images from degradation of many missing pixels\nin an unknown pattern. We examine various deep autoencoder architectures, such\nas fully connected and U-Net (with various nonlinearities and at diverse train\nloss values), and show that our method significantly outperforms previous\nmemorization-evaluation methods that recover training data from autoencoders.\nImportantly, our method greatly improves the recovery performance also in\nsettings that were previously considered highly challenging, and even\nimpractical, for such recovery and memorization evaluation.\n","authors":["Koren Abitbul","Yehuda Dar"],"pdf_url":"https://arxiv.org/pdf/2310.02897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02939v4","updated":"2024-06-13T15:09:18Z","published":"2023-06-05T15:03:01Z","title":"Improved Stability and Generalization Guarantees of the Decentralized\n  SGD Algorithm","summary":"  This paper presents a new generalization error analysis for Decentralized\nStochastic Gradient Descent (D-SGD) based on algorithmic stability. The\nobtained results overhaul a series of recent works that suggested an increased\ninstability due to decentralization and a detrimental impact of\npoorly-connected communication graphs on generalization. On the contrary, we\nshow, for convex, strongly convex and non-convex functions, that D-SGD can\nalways recover generalization bounds analogous to those of classical SGD,\nsuggesting that the choice of graph does not matter. We then argue that this\nresult is coming from a worst-case analysis, and we provide a refined\noptimization-dependent generalization bound for general convex functions. This\nnew bound reveals that the choice of graph can in fact improve the worst-case\nbound in certain regimes, and that surprisingly, a poorly-connected graph can\neven be beneficial for generalization.\n","authors":["Batiste Le Bars","Aurélien Bellet","Marc Tommasi","Kevin Scaman","Giovanni Neglia"],"pdf_url":"https://arxiv.org/pdf/2306.02939v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09207v1","updated":"2024-06-13T15:08:44Z","published":"2024-06-13T15:08:44Z","title":"Investigating potential causes of Sepsis with Bayesian network structure\n  learning","summary":"  Sepsis is a life-threatening and serious global health issue. This study\ncombines knowledge with available hospital data to investigate the potential\ncauses of Sepsis that can be affected by policy decisions. We investigate the\nunderlying causal structure of this problem by combining clinical expertise\nwith score-based, constraint-based, and hybrid structure learning algorithms. A\nnovel approach to model averaging and knowledge-based constraints was\nimplemented to arrive at a consensus structure for causal inference. The\nstructure learning process highlighted the importance of exploring data-driven\napproaches alongside clinical expertise. This includes discovering unexpected,\nalthough reasonable, relationships from a clinical perspective. Hypothetical\ninterventions on Chronic Obstructive Pulmonary Disease, Alcohol dependence, and\nDiabetes suggest that the presence of any of these risk factors in patients\nincreases the likelihood of Sepsis. This finding, alongside measuring the\neffect of these risk factors on Sepsis, has potential policy implications.\nRecognising the importance of prediction in improving Sepsis related health\noutcomes, the model built is also assessed in its ability to predict Sepsis.\nThe predictions generated by the consensus model were assessed for their\naccuracy, sensitivity, and specificity. These three indicators all had results\naround 70%, and the AUC was 80%, which means the causal structure of the model\nis reasonably accurate given that the models were trained on data available for\ncommissioning purposes only.\n","authors":["Bruno Petrungaro","Neville K. Kitson","Anthony C. Constantinou"],"pdf_url":"https://arxiv.org/pdf/2406.09207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09206v1","updated":"2024-06-13T15:06:11Z","published":"2024-06-13T15:06:11Z","title":"Self-Training for Sample-Efficient Active Learning for Text\n  Classification with Pre-Trained Language Models","summary":"  Active learning is an iterative labeling process that is used to obtain a\nsmall labeled subset, despite the absence of labeled data, thereby enabling to\ntrain a model for supervised tasks such as text classification. While active\nlearning has made considerable progress in recent years due to improvements\nprovided by pre-trained language models, there is untapped potential in the\noften neglected unlabeled portion of the data, although it is available in\nconsiderably larger quantities than the usually small set of labeled data. Here\nwe investigate how self-training, a semi-supervised approach where a model is\nused to obtain pseudo-labels from the unlabeled data, can be used to improve\nthe efficiency of active learning for text classification. Starting with an\nextensive reproduction of four previous self-training approaches, some of which\nare evaluated for the first time in the context of active learning or natural\nlanguage processing, we devise HAST, a new and effective self-training\nstrategy, which is evaluated on four text classification benchmarks, on which\nit outperforms the reproduced self-training approaches and reaches\nclassification results comparable to previous experiments for three out of four\ndatasets, using only 25% of the data.\n","authors":["Christopher Schröder","Gerhard Heyer"],"pdf_url":"https://arxiv.org/pdf/2406.09206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11432v2","updated":"2024-06-13T15:03:40Z","published":"2024-03-18T02:59:13Z","title":"Demystifying the Physics of Deep Reinforcement Learning-Based Autonomous\n  Vehicle Decision-Making","summary":"  With the advent of universal function approximators in the domain of\nreinforcement learning, the number of practical applications leveraging deep\nreinforcement learning (DRL) has exploded. Decision-making in autonomous\nvehicles (AVs) has emerged as a chief application among them, taking the sensor\ndata or the higher-order kinematic variables as the input and providing a\ndiscrete choice or continuous control output. There has been a continuous\neffort to understand the black-box nature of the DRL models, but so far, there\nhasn't been any discussion (to the best of authors' knowledge) about how the\nmodels learn the physical process. This presents an overwhelming limitation\nthat restricts the real-world deployment of DRL in AVs. Therefore, in this\nresearch work, we try to decode the knowledge learnt by the attention-based DRL\nframework about the physical process. We use a continuous proximal policy\noptimization-based DRL algorithm as the baseline model and add a multi-head\nattention framework in an open-source AV simulation environment. We provide\nsome analytical techniques for discussing the interpretability of the trained\nmodels in terms of explainability and causality for spatial and temporal\ncorrelations. We show that the weights in the first head encode the positions\nof the neighboring vehicles while the second head focuses on the leader vehicle\nexclusively. Also, the ego vehicle's action is causally dependent on the\nvehicles in the target lane spatially and temporally. Through these findings,\nwe reliably show that these techniques can help practitioners decipher the\nresults of the DRL algorithms.\n","authors":["Hanxi Wan","Pei Li","Arpan Kusari"],"pdf_url":"https://arxiv.org/pdf/2403.11432v2.pdf","comment":"Submitted for peer-review"},{"id":"http://arxiv.org/abs/2406.09199v1","updated":"2024-06-13T14:56:52Z","published":"2024-06-13T14:56:52Z","title":"Precise analysis of ridge interpolators under heavy correlations -- a\n  Random Duality Theory view","summary":"  We consider fully row/column-correlated linear regression models and study\nseveral classical estimators (including minimum norm interpolators (GLS),\nordinary least squares (LS), and ridge regressors). We show that \\emph{Random\nDuality Theory} (RDT) can be utilized to obtain precise closed form\ncharacterizations of all estimators related optimizing quantities of interest,\nincluding the \\emph{prediction risk} (testing or generalization error). On a\nqualitative level out results recover the risk's well known non-monotonic\n(so-called double-descent) behavior as the number of features/sample size ratio\nincreases. On a quantitative level, our closed form results show how the risk\nexplicitly depends on all key model parameters, including the problem\ndimensions and covariance matrices. Moreover, a special case of our results,\nobtained when intra-sample (or time-series) correlations are not present,\nprecisely match the corresponding ones obtained via spectral methods in\n[6,16,17,24].\n","authors":["Mihailo Stojnic"],"pdf_url":"https://arxiv.org/pdf/2406.09199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09196v1","updated":"2024-06-13T14:55:11Z","published":"2024-06-13T14:55:11Z","title":"Adaptive Slot Attention: Object Discovery with Dynamic Slot Number","summary":"  Object-centric learning (OCL) extracts the representation of objects with\nslots, offering an exceptional blend of flexibility and interpretability for\nabstracting low-level perceptual features. A widely adopted method within OCL\nis slot attention, which utilizes attention mechanisms to iteratively refine\nslot representations. However, a major drawback of most object-centric models,\nincluding slot attention, is their reliance on predefining the number of slots.\nThis not only necessitates prior knowledge of the dataset but also overlooks\nthe inherent variability in the number of objects present in each instance. To\novercome this fundamental limitation, we present a novel complexity-aware\nobject auto-encoder framework. Within this framework, we introduce an adaptive\nslot attention (AdaSlot) mechanism that dynamically determines the optimal\nnumber of slots based on the content of the data. This is achieved by proposing\na discrete slot sampling module that is responsible for selecting an\nappropriate number of slots from a candidate list. Furthermore, we introduce a\nmasked slot decoder that suppresses unselected slots during the decoding\nprocess. Our framework, tested extensively on object discovery tasks with\nvarious datasets, shows performance matching or exceeding top fixed-slot\nmodels. Moreover, our analysis substantiates that our method exhibits the\ncapability to dynamically adapt the slot number according to each instance's\ncomplexity, offering the potential for further exploration in slot attention\nresearch. Project will be available at https://kfan21.github.io/AdaSlot/\n","authors":["Ke Fan","Zechen Bai","Tianjun Xiao","Tong He","Max Horn","Yanwei Fu","Francesco Locatello","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.09196v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2302.05534v3","updated":"2024-06-13T14:54:41Z","published":"2023-02-10T22:25:42Z","title":"Robust Knowledge Transfer in Tiered Reinforcement Learning","summary":"  In this paper, we study the Tiered Reinforcement Learning setting, a parallel\ntransfer learning framework, where the goal is to transfer knowledge from the\nlow-tier (source) task to the high-tier (target) task to reduce the exploration\nrisk of the latter while solving the two tasks in parallel. Unlike previous\nwork, we do not assume the low-tier and high-tier tasks share the same dynamics\nor reward functions, and focus on robust knowledge transfer without prior\nknowledge on the task similarity. We identify a natural and necessary condition\ncalled the ``Optimal Value Dominance'' for our objective. Under this condition,\nwe propose novel online learning algorithms such that, for the high-tier task,\nit can achieve constant regret on partial states depending on the task\nsimilarity and retain near-optimal regret when the two tasks are dissimilar,\nwhile for the low-tier task, it can keep near-optimal without making sacrifice.\nMoreover, we further study the setting with multiple low-tier tasks, and\npropose a novel transfer source selection mechanism, which can ensemble the\ninformation from all low-tier tasks and allow provable benefits on a much\nlarger state-action space.\n","authors":["Jiawei Huang","Niao He"],"pdf_url":"https://arxiv.org/pdf/2302.05534v3.pdf","comment":"47 Pages; 1 Figure; NeurIPS 2023"},{"id":"http://arxiv.org/abs/2406.09194v1","updated":"2024-06-13T14:54:30Z","published":"2024-06-13T14:54:30Z","title":"Bengining overfitting in Fixed Dimension via Physics-Informed Learning\n  with Smooth Iductive Bias","summary":"  Recent advances in machine learning theory showed that interpolation to noisy\nsamples using over-parameterized machine learning algorithms always leads to\ninconsistency. However, this work surprisingly discovers that interpolated\nmachine learning can exhibit benign overfitting and consistency when using\nphysics-informed learning for supervised tasks governed by partial differential\nequations (PDEs) describing laws of physics. An analysis provides an asymptotic\nSobolev norm learning curve for kernel ridge(less) regression addressing linear\ninverse problems involving elliptic PDEs. The results reveal that the PDE\noperators can stabilize variance and lead to benign overfitting for\nfixed-dimensional problems, contrasting standard regression settings. The\nimpact of various inductive biases introduced by minimizing different Sobolev\nnorms as implicit regularization is also examined. Notably, the convergence\nrate is independent of the specific (smooth) inductive bias for both ridge and\nridgeless regression. For regularized least squares estimators, all (smooth\nenough) inductive biases can achieve optimal convergence rates when the\nregularization parameter is properly chosen. The smoothness requirement\nrecovers a condition previously found in the Bayesian setting and extends\nconclusions to minimum norm interpolation estimators.\n","authors":["Honam Wong","Wendao Wu","Fanghui Liu","Yiping Lu"],"pdf_url":"https://arxiv.org/pdf/2406.09194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09548v2","updated":"2024-06-13T14:50:49Z","published":"2023-09-18T07:51:09Z","title":"Non-Intrusive Speech Intelligibility Prediction for Hearing Aids using\n  Whisper and Metadata","summary":"  Automated speech intelligibility assessment is pivotal for hearing aid (HA)\ndevelopment. In this paper, we present three novel methods to improve\nintelligibility prediction accuracy and introduce MBI-Net+, an enhanced version\nof MBI-Net, the top-performing system in the 1st Clarity Prediction Challenge.\nMBI-Net+ leverages Whisper's embeddings to create cross-domain acoustic\nfeatures and includes metadata from speech signals by using a classifier that\ndistinguishes different enhancement methods. Furthermore, MBI-Net+ integrates\nthe hearing-aid speech perception index (HASPI) as a supplementary metric into\nthe objective function to further boost prediction performance. Experimental\nresults demonstrate that MBI-Net+ surpasses several intrusive baseline systems\nand MBI-Net on the Clarity Prediction Challenge 2023 dataset, validating the\neffectiveness of incorporating Whisper embeddings, speech metadata, and related\ncomplementary metrics to improve prediction performance for HA.\n","authors":["Ryandhimas E. Zezario","Fei Chen","Chiou-Shann Fuh","Hsin-Min Wang","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2309.09548v2.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.09187v1","updated":"2024-06-13T14:49:26Z","published":"2024-06-13T14:49:26Z","title":"GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled\n  Reasoning","summary":"  The rapid advancement of large language models (LLMs) has catalyzed the\ndeployment of LLM-powered agents across numerous applications, raising new\nconcerns regarding their safety and trustworthiness. Existing methods for\nenhancing the safety of LLMs are not directly transferable to LLM-powered\nagents due to their diverse objectives and output modalities. In this paper, we\npropose GuardAgent, the first LLM agent as a guardrail to other LLM agents.\nSpecifically, GuardAgent oversees a target LLM agent by checking whether its\ninputs/outputs satisfy a set of given guard requests defined by the users.\nGuardAgent comprises two steps: 1) creating a task plan by analyzing the\nprovided guard requests, and 2) generating guardrail code based on the task\nplan and executing the code by calling APIs or using external engines. In both\nsteps, an LLM is utilized as the core reasoning component, supplemented by\nin-context demonstrations retrieved from a memory module. Such\nknowledge-enabled reasoning allows GuardAgent to understand various textual\nguard requests and accurately \"translate\" them into executable code that\nprovides reliable guardrails. Furthermore, GuardAgent is equipped with an\nextendable toolbox containing functions and APIs and requires no additional LLM\ntraining, which underscores its generalization capabilities and low operational\noverhead. Additionally, we propose two novel benchmarks: an EICU-AC benchmark\nfor assessing privacy-related access control for healthcare agents and a\nMind2Web-SC benchmark for safety evaluation for web agents. We show the\neffectiveness of GuardAgent on these two benchmarks with 98.7% and 90.0%\naccuracy in moderating invalid inputs and outputs for the two types of agents,\nrespectively. We also show that GuardAgent is able to define novel functions in\nadaption to emergent LLM agents and guard requests, which underscores its\nstrong generalization capabilities.\n","authors":["Zhen Xiang","Linzhi Zheng","Yanjie Li","Junyuan Hong","Qinbin Li","Han Xie","Jiawei Zhang","Zidi Xiong","Chulin Xie","Carl Yang","Dawn Song","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2406.09187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09183v1","updated":"2024-06-13T14:46:08Z","published":"2024-06-13T14:46:08Z","title":"Ridge interpolators in correlated factor regression models -- exact risk\n  analysis","summary":"  We consider correlated \\emph{factor} regression models (FRM) and analyze the\nperformance of classical ridge interpolators. Utilizing powerful \\emph{Random\nDuality Theory} (RDT) mathematical engine, we obtain \\emph{precise} closed form\ncharacterizations of the underlying optimization problems and all associated\noptimizing quantities. In particular, we provide \\emph{excess prediction risk}\ncharacterizations that clearly show the dependence on all key model parameters,\ncovariance matrices, loadings, and dimensions. As a function of the\nover-parametrization ratio, the generalized least squares (GLS) risk also\nexhibits the well known \\emph{double-descent} (non-monotonic) behavior.\nSimilarly to the classical linear regression models (LRM), we demonstrate that\nsuch FRM phenomenon can be smoothened out by the optimally tuned ridge\nregularization. The theoretical results are supplemented by numerical\nsimulations and an excellent agrement between the two is observed. Moreover, we\nnote that ``ridge smootenhing'' is often of limited effect already for\nover-parametrization ratios above $5$ and of virtually no effect for those\nabove $10$. This solidifies the notion that one of the recently most popular\nneural networks paradigms -- \\emph{zero-training (interpolating) generalizes\nwell} -- enjoys wider applicability, including the one within the FRM\nestimation/prediction context.\n","authors":["Mihailo Stojnic"],"pdf_url":"https://arxiv.org/pdf/2406.09183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04056v2","updated":"2024-06-13T14:45:36Z","published":"2024-01-08T17:55:02Z","title":"A Minimaximalist Approach to Reinforcement Learning from Human Feedback","summary":"  We present Self-Play Preference Optimization (SPO), an algorithm for\nreinforcement learning from human feedback. Our approach is minimalist in that\nit does not require training a reward model nor unstable adversarial training\nand is therefore rather simple to implement. Our approach is maximalist in that\nit provably handles non-Markovian, intransitive, and stochastic preferences\nwhile being robust to the compounding errors that plague offline approaches to\nsequential prediction. To achieve the preceding qualities, we build upon the\nconcept of a Minimax Winner (MW), a notion of preference aggregation from the\nsocial choice theory literature that frames learning from preferences as a\nzero-sum game between two policies. By leveraging the symmetry of this game, we\nprove that rather than using the traditional technique of dueling two policies\nto compute the MW, we can simply have a single agent play against itself while\nmaintaining strong convergence guarantees. Practically, this corresponds to\nsampling multiple trajectories from a policy, asking a preference or teacher\nmodel to compare them, and then using the proportion of wins as the reward for\na particular trajectory. We demonstrate that on a suite of continuous control\ntasks, we are able to learn significantly more efficiently than reward-model\nbased approaches while maintaining robustness to the intransitive and\nstochastic preferences that frequently occur in practice when aggregating human\njudgments.\n","authors":["Gokul Swamy","Christoph Dann","Rahul Kidambi","Zhiwei Steven Wu","Alekh Agarwal"],"pdf_url":"https://arxiv.org/pdf/2401.04056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09182v1","updated":"2024-06-13T14:45:35Z","published":"2024-06-13T14:45:35Z","title":"Federated Contrastive Learning for Personalized Semantic Communication","summary":"  In this letter, we design a federated contrastive learning (FedCL) framework\naimed at supporting personalized semantic communication. Our FedCL enables\ncollaborative training of local semantic encoders across multiple clients and a\nglobal semantic decoder owned by the base station. This framework supports\nheterogeneous semantic encoders since it does not require client-side model\naggregation. Furthermore, to tackle the semantic imbalance issue arising from\nheterogeneous datasets across distributed clients, we employ contrastive\nlearning to train a semantic centroid generator (SCG). This generator obtains\nrepresentative global semantic centroids that exhibit intra-semantic\ncompactness and inter-semantic separability. Consequently, it provides superior\nsupervision for learning discriminative local semantic features. Additionally,\nwe conduct theoretical analysis to quantify the convergence performance of\nFedCL. Simulation results verify the superiority of the proposed FedCL\nframework compared to other distributed learning benchmarks in terms of task\nperformance and robustness under different numbers of clients and channel\nconditions, especially in low signal-to-noise ratio and highly heterogeneous\ndata scenarios.\n","authors":["Yining Wang","Wanli Ni","Wenqiang Yi","Xiaodong Xu","Ping Zhang","Arumugam Nallanathan"],"pdf_url":"https://arxiv.org/pdf/2406.09182v1.pdf","comment":"IEEE Communications Letters"},{"id":"http://arxiv.org/abs/2406.09180v1","updated":"2024-06-13T14:42:17Z","published":"2024-06-13T14:42:17Z","title":"Detection-Rate-Emphasized Multi-objective Evolutionary Feature Selection\n  for Network Intrusion Detection","summary":"  Network intrusion detection is one of the most important issues in the field\nof cyber security, and various machine learning techniques have been applied to\nbuild intrusion detection systems. However, since the number of features to\ndescribe the network connections is often large, where some features are\nredundant or noisy, feature selection is necessary in such scenarios, which can\nboth improve the efficiency and accuracy. Recently, some researchers focus on\nusing multi-objective evolutionary algorithms (MOEAs) to select features. But\nusually, they only consider the number of features and classification accuracy\nas the objectives, resulting in unsatisfactory performance on a critical\nmetric, detection rate. This will lead to the missing of many real attacks and\nbring huge losses to the network system. In this paper, we propose DR-MOFS to\nmodel the feature selection problem in network intrusion detection as a\nthree-objective optimization problem, where the number of features, accuracy\nand detection rate are optimized simultaneously, and use MOEAs to solve it.\nExperiments on two popular network intrusion detection datasets NSL-KDD and\nUNSW-NB15 show that in most cases the proposed method can outperform previous\nmethods, i.e., lead to fewer features, higher accuracy and detection rate.\n","authors":["Zi-Hang Cheng","Haopu Shang","Chao Qian"],"pdf_url":"https://arxiv.org/pdf/2406.09180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09179v1","updated":"2024-06-13T14:41:00Z","published":"2024-06-13T14:41:00Z","title":"Unlearning with Control: Assessing Real-world Utility for Large Language\n  Model Unlearning","summary":"  The compelling goal of eradicating undesirable data behaviors, while\npreserving usual model functioning, underscores the significance of machine\nunlearning within the domain of large language models (LLMs). Recent research\nhas begun to approach LLM unlearning via gradient ascent (GA) -- increasing the\nprediction risk for those training strings targeted to be unlearned, thereby\nerasing their parameterized responses. Despite their simplicity and efficiency,\nwe suggest that GA-based methods face the propensity towards excessive\nunlearning, resulting in various undesirable model behaviors, such as\ncatastrophic forgetting, that diminish their practical utility. In this paper,\nwe suggest a set of metrics that can capture multiple facets of real-world\nutility and propose several controlling methods that can regulate the extent of\nexcessive unlearning. Accordingly, we suggest a general framework to better\nreflect the practical efficacy of various unlearning methods -- we begin by\ncontrolling the unlearning procedures/unlearned models such that no excessive\nunlearning occurs and follow by the evaluation for unlearning efficacy. Our\nexperimental analysis on established benchmarks revealed that GA-based methods\nare far from perfect in practice, as strong unlearning is at the high cost of\nhindering the model utility. We conclude that there is still a long way towards\npractical and effective LLM unlearning, and more efforts are required in this\nfield.\n","authors":["Qizhou Wang","Bo Han","Puning Yang","Jianing Zhu","Tongliang Liu","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2406.09179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09177v1","updated":"2024-06-13T14:39:40Z","published":"2024-06-13T14:39:40Z","title":"Scalable and Flexible Causal Discovery with an Efficient Test for\n  Adjacency","summary":"  To make accurate predictions, understand mechanisms, and design interventions\nin systems of many variables, we wish to learn causal graphs from large scale\ndata. Unfortunately the space of all possible causal graphs is enormous so\nscalably and accurately searching for the best fit to the data is a challenge.\nIn principle we could substantially decrease the search space, or learn the\ngraph entirely, by testing the conditional independence of variables. However,\ndeciding if two variables are adjacent in a causal graph may require an\nexponential number of tests. Here we build a scalable and flexible method to\nevaluate if two variables are adjacent in a causal graph, the Differentiable\nAdjacency Test (DAT). DAT replaces an exponential number of tests with a\nprovably equivalent relaxed problem. It then solves this problem by training\ntwo neural networks. We build a graph learning method based on DAT, DAT-Graph,\nthat can also learn from data with interventions. DAT-Graph can learn graphs of\n1000 variables with state of the art accuracy. Using the graph learned by\nDAT-Graph, we also build models that make much more accurate predictions of the\neffects of interventions on large scale RNA sequencing data.\n","authors":["Alan Nawzad Amin","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2406.09177v1.pdf","comment":"ICML 2024; Code at https://github.com/AlanNawzadAmin/DAT-graph"},{"id":"http://arxiv.org/abs/2402.07586v3","updated":"2024-06-13T14:37:15Z","published":"2024-02-12T11:35:25Z","title":"Unveiling Group-Specific Distributed Concept Drift: A Fairness\n  Imperative in Federated Learning","summary":"  In the evolving field of machine learning, ensuring fairness has become a\ncritical concern, prompting the development of algorithms designed to mitigate\ndiscriminatory outcomes in decision-making processes. However, achieving\nfairness in the presence of group-specific concept drift remains an unexplored\nfrontier, and our research represents pioneering efforts in this regard.\nGroup-specific concept drift refers to situations where one group experiences\nconcept drift over time while another does not, leading to a decrease in\nfairness even if accuracy remains fairly stable. Within the framework of\nfederated learning, where clients collaboratively train models, its distributed\nnature further amplifies these challenges since each client can experience\ngroup-specific concept drift independently while still sharing the same\nunderlying concept, creating a complex and dynamic environment for maintaining\nfairness. One of the significant contributions of our research is the\nformalization and introduction of the problem of group-specific concept drift\nand its distributed counterpart, shedding light on its critical importance in\nthe realm of fairness. In addition, leveraging insights from prior research, we\nadapt an existing distributed concept drift adaptation algorithm to tackle\ngroup-specific distributed concept drift which utilizes a multi-model approach,\na local group-specific drift detection mechanism, and continuous clustering of\nmodels over time. The findings from our experiments highlight the importance of\naddressing group-specific concept drift and its distributed counterpart to\nadvance fairness in machine learning.\n","authors":["Teresa Salazar","João Gama","Helder Araújo","Pedro Henriques Abreu"],"pdf_url":"https://arxiv.org/pdf/2402.07586v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09173v1","updated":"2024-06-13T14:35:11Z","published":"2024-06-13T14:35:11Z","title":"Potion: Towards Poison Unlearning","summary":"  Adversarial attacks by malicious actors on machine learning systems, such as\nintroducing poison triggers into training datasets, pose significant risks. The\nchallenge in resolving such an attack arises in practice when only a subset of\nthe poisoned data can be identified. This necessitates the development of\nmethods to remove, i.e. unlearn, poison triggers from already trained models\nwith only a subset of the poison data available. The requirements for this task\nsignificantly deviate from privacy-focused unlearning where all of the data to\nbe forgotten by the model is known. Previous work has shown that the\nundiscovered poisoned samples lead to a failure of established unlearning\nmethods, with only one method, Selective Synaptic Dampening (SSD), showing\nlimited success. Even full retraining, after the removal of the identified\npoison, cannot address this challenge as the undiscovered poison samples lead\nto a reintroduction of the poison trigger in the model. Our work addresses two\nkey challenges to advance the state of the art in poison unlearning. First, we\nintroduce a novel outlier-resistant method, based on SSD, that significantly\nimproves model protection and unlearning performance. Second, we introduce\nPoison Trigger Neutralisation (PTN) search, a fast, parallelisable,\nhyperparameter search that utilises the characteristic \"unlearning versus model\nprotection\" trade-off to find suitable hyperparameters in settings where the\nforget set size is unknown and the retain set is contaminated. We benchmark our\ncontributions using ResNet-9 on CIFAR10 and WideResNet-28x10 on CIFAR100.\nExperimental results show that our method heals 93.72% of poison compared to\nSSD with 83.41% and full retraining with 40.68%. We achieve this while also\nlowering the average model accuracy drop caused by unlearning from 5.68% (SSD)\nto 1.41% (ours).\n","authors":["Stefan Schoepf","Jack Foster","Alexandra Brintrup"],"pdf_url":"https://arxiv.org/pdf/2406.09173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09172v1","updated":"2024-06-13T14:32:43Z","published":"2024-06-13T14:32:43Z","title":"Generative vs. Discriminative modeling under the lens of uncertainty\n  quantification","summary":"  Learning a parametric model from a given dataset indeed enables to capture\nintrinsic dependencies between random variables via a parametric conditional\nprobability distribution and in turn predict the value of a label variable\ngiven observed variables. In this paper, we undertake a comparative analysis of\ngenerative and discriminative approaches which differ in their construction and\nthe structure of the underlying inference problem. Our objective is to compare\nthe ability of both approaches to leverage information from various sources in\nan epistemic uncertainty aware inference via the posterior predictive\ndistribution. We assess the role of a prior distribution, explicit in the\ngenerative case and implicit in the discriminative case, leading to a\ndiscussion about discriminative models suffering from imbalanced dataset. We\nnext examine the double role played by the observed variables in the generative\ncase, and discuss the compatibility of both approaches with semi-supervised\nlearning. We also provide with practical insights and we examine how the\nmodeling choice impacts the sampling from the posterior predictive\ndistribution. With regard to this, we propose a general sampling scheme\nenabling supervised learning for both approaches, as well as semi-supervised\nlearning when compatible with the considered modeling approach. Throughout this\npaper, we illustrate our arguments and conclusions using the example of affine\nregression, and validate our comparative analysis through classification\nsimulations using neural network based models.\n","authors":["Elouan Argouarc'h","François Desbouvries","Eric Barat","Eiji Kawasaki"],"pdf_url":"https://arxiv.org/pdf/2406.09172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09168v1","updated":"2024-06-13T14:30:35Z","published":"2024-06-13T14:30:35Z","title":"SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image\n  Super-Resolution","summary":"  Confocal fluorescence microscopy is one of the most accessible and widely\nused imaging techniques for the study of biological processes. Scanning\nconfocal microscopy allows the capture of high-quality images from 3D samples,\nyet suffers from well-known limitations such as photobleaching and\nphototoxicity of specimens caused by intense light exposure, which limits its\nuse in some applications, especially for living cells. Cellular damage can be\nalleviated by changing imaging parameters to reduce light exposure, often at\nthe expense of image quality. Machine/deep learning methods for single-image\nsuper-resolution (SISR) can be applied to restore image quality by upscaling\nlower-resolution (LR) images to produce high-resolution images (HR). These SISR\nmethods have been successfully applied to photo-realistic images due partly to\nthe abundance of publicly available data. In contrast, the lack of publicly\navailable data partly limits their application and success in scanning confocal\nmicroscopy. In this paper, we introduce a large scanning confocal microscopy\ndataset named SR-CACO-2 that is comprised of low- and high-resolution image\npairs marked for three different fluorescent markers. It allows the evaluation\nof performance of SISR methods on three different upscaling levels (X2, X4,\nX8). SR-CACO-2 contains the human epithelial cell line Caco-2 (ATCC HTB-37),\nand it is composed of 22 tiles that have been translated in the form of 9,937\nimage patches for experiments with SISR methods. Given the new SR-CACO-2\ndataset, we also provide benchmarking results for 15 state-of-the-art methods\nthat are representative of the main SISR families. Results show that these\nmethods have limited success in producing high-resolution textures, indicating\nthat SR-CACO-2 represents a challenging problem. Our dataset, code and\npretrained weights are available: https://github.com/sbelharbi/sr-caco-2.\n","authors":["Soufiane Belharbi","Mara KM Whitford","Phuong Hoang","Shakeeb Murtaza","Luke McCaffrey","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2406.09168v1.pdf","comment":"23 pages, 13 figures"},{"id":"http://arxiv.org/abs/2406.09156v1","updated":"2024-06-13T14:18:56Z","published":"2024-06-13T14:18:56Z","title":"Towards Multilingual Audio-Visual Question Answering","summary":"  In this paper, we work towards extending Audio-Visual Question Answering\n(AVQA) to multilingual settings. Existing AVQA research has predominantly\nrevolved around English and replicating it for addressing AVQA in other\nlanguages requires a substantial allocation of resources. As a scalable\nsolution, we leverage machine translation and present two multilingual AVQA\ndatasets for eight languages created from existing benchmark AVQA datasets.\nThis prevents extra human annotation efforts of collecting questions and\nanswers manually. To this end, we propose, MERA framework, by leveraging\nstate-of-the-art (SOTA) video, audio, and textual foundation models for AVQA in\nmultiple languages. We introduce a suite of models namely MERA-L, MERA-C,\nMERA-T with varied model architectures to benchmark the proposed datasets. We\nbelieve our work will open new research directions and act as a reference\nbenchmark for future works in multilingual AVQA.\n","authors":["Orchid Chetia Phukan","Priyabrata Mallick","Swarup Ranjan Behera","Aalekhya Satya Narayani","Arun Balaji Buduru","Rajesh Sharma"],"pdf_url":"https://arxiv.org/pdf/2406.09156v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.09155v1","updated":"2024-06-13T14:18:13Z","published":"2024-06-13T14:18:13Z","title":"DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities,\nrevolutionizing the integration of AI in daily life applications. However, they\nare prone to hallucinations, generating claims that contradict established\nfacts, deviating from prompts, and producing inconsistent responses when the\nsame prompt is presented multiple times. Addressing these issues is challenging\ndue to the lack of comprehensive and easily assessable benchmark datasets. Most\nexisting datasets are small and rely on multiple-choice questions, which are\ninadequate for evaluating the generative prowess of LLMs. To measure\nhallucination in LLMs, this paper introduces a comprehensive benchmark dataset\ncomprising over 75,000 prompts across eight domains. These prompts are designed\nto elicit definitive, concise, and informative answers. The dataset is divided\ninto two segments: one publicly available for testing and assessing LLM\nperformance and a hidden segment for benchmarking various LLMs. In our\nexperiments, we tested six LLMs-GPT-3.5, LLama 2, LLama 3, Gemini, Mixtral, and\nZephyr-revealing that overall factual hallucination ranges from 59% to 82% on\nthe public dataset and 57% to 76% in the hidden benchmark. Prompt misalignment\nhallucination ranges from 6% to 95% in the public dataset and 17% to 94% in the\nhidden counterpart. Average consistency ranges from 21% to 61% and 22% to 63%,\nrespectively. Domain-wise analysis shows that LLM performance significantly\ndeteriorates when asked for specific numeric information while performing\nmoderately with person, location, and date queries. Our dataset demonstrates\nits efficacy and serves as a comprehensive benchmark for LLM performance\nevaluation. Our dataset and LLMs responses are available at\n\\href{https://github.com/ashikiut/DefAn}{https://github.com/ashikiut/DefAn}.\n","authors":["A B M Ashikur Rahman","Saeed Anwar","Muhammad Usman","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2406.09155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09152v1","updated":"2024-06-13T14:16:50Z","published":"2024-06-13T14:16:50Z","title":"EncCluster: Scalable Functional Encryption in Federated Learning through\n  Weight Clustering and Probabilistic Filters","summary":"  Federated Learning (FL) enables model training across decentralized devices\nby communicating solely local model updates to an aggregation server. Although\nsuch limited data sharing makes FL more secure than centralized approached, FL\nremains vulnerable to inference attacks during model update transmissions.\nExisting secure aggregation approaches rely on differential privacy or\ncryptographic schemes like Functional Encryption (FE) to safeguard individual\nclient data. However, such strategies can reduce performance or introduce\nunacceptable computational and communication overheads on clients running on\nedge devices with limited resources. In this work, we present EncCluster, a\nnovel method that integrates model compression through weight clustering with\nrecent decentralized FE and privacy-enhancing data encoding using probabilistic\nfilters to deliver strong privacy guarantees in FL without affecting model\nperformance or adding unnecessary burdens to clients. We performed a\ncomprehensive evaluation, spanning various datasets and architectures, to\ndemonstrate EncCluster's scalability across encryption levels. Our findings\nreveal that EncCluster significantly reduces communication costs - below even\nconventional FedAvg - and accelerates encryption by more than four times over\nall baselines; at the same time, it maintains high model accuracy and enhanced\nprivacy assurances.\n","authors":["Vasileios Tsouvalas","Samaneh Mohammadi","Ali Balador","Tanir Ozcelebi","Francesco Flammini","Nirvana Meratnia"],"pdf_url":"https://arxiv.org/pdf/2406.09152v1.pdf","comment":"21 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.09147v1","updated":"2024-06-13T14:14:27Z","published":"2024-06-13T14:14:27Z","title":"Weakly-supervised anomaly detection for multimodal data distributions","summary":"  Weakly-supervised anomaly detection can outperform existing unsupervised\nmethods with the assistance of a very small number of labeled anomalies, which\nattracts increasing attention from researchers. However, existing\nweakly-supervised anomaly detection methods are limited as these methods do not\nfactor in the multimodel nature of the real-world data distribution. To\nmitigate this, we propose the Weakly-supervised Variational-mixture-model-based\nAnomaly Detector (WVAD). WVAD excels in multimodal datasets. It consists of two\ncomponents: a deep variational mixture model, and an anomaly score estimator.\nThe deep variational mixture model captures various features of the data from\ndifferent clusters, then these features are delivered to the anomaly score\nestimator to assess the anomaly levels. Experimental results on three\nreal-world datasets demonstrate WVAD's superiority.\n","authors":["Xu Tan","Junqi Chen","Sylwan Rahardja","Jiawei Yang","Susanto Rahardja"],"pdf_url":"https://arxiv.org/pdf/2406.09147v1.pdf","comment":"5 pages, 3 figures. Accepted by 2024 IEEE International Conference on\n  Signal Processing, Communications and Computing (ICSPCC)"},{"id":"http://arxiv.org/abs/2406.09143v1","updated":"2024-06-13T14:11:19Z","published":"2024-06-13T14:11:19Z","title":"Generative AI-based Prompt Evolution Engineering Design Optimization\n  With Vision-Language Model","summary":"  Engineering design optimization requires an efficient combination of a 3D\nshape representation, an optimization algorithm, and a design performance\nevaluation method, which is often computationally expensive. We present a\nprompt evolution design optimization (PEDO) framework contextualized in a\nvehicle design scenario that leverages a vision-language model for penalizing\nimpractical car designs synthesized by a generative model. The backbone of our\nframework is an evolutionary strategy coupled with an optimization objective\nfunction that comprises a physics-based solver and a vision-language model for\npractical or functional guidance in the generated car designs. In the prompt\nevolutionary search, the optimizer iteratively generates a population of text\nprompts, which embed user specifications on the aerodynamic performance and\nvisual preferences of the 3D car designs. Then, in addition to the\ncomputational fluid dynamics simulations, the pre-trained vision-language model\nis used to penalize impractical designs and, thus, foster the evolutionary\nalgorithm to seek more viable designs. Our investigations on a car design\noptimization problem show a wide spread of potential car designs generated at\nthe early phase of the search, which indicates a good diversity of designs in\nthe initial populations, and an increase of over 20\\% in the probability of\ngenerating practical designs compared to a baseline framework without using a\nvision-language model. Visual inspection of the designs against the performance\nresults demonstrates prompt evolution as a very promising paradigm for finding\nnovel designs with good optimization performance while providing ease of use in\nspecifying design specifications and preferences via a natural language\ninterface.\n","authors":["Melvin Wong","Thiago Rios","Stefan Menzel","Yew Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2406.09143v1.pdf","comment":"Accepted and to be published in IEEE Congress on Evolutionary\n  Computation 2024"},{"id":"http://arxiv.org/abs/2406.09141v1","updated":"2024-06-13T14:10:57Z","published":"2024-06-13T14:10:57Z","title":"Optimal Control of Agent-Based Dynamics under Deep Galerkin Feedback\n  Laws","summary":"  Ever since the concepts of dynamic programming were introduced, one of the\nmost difficult challenges has been to adequately address high-dimensional\ncontrol problems. With growing dimensionality, the utilisation of Deep Neural\nNetworks promises to circumvent the issue of an otherwise exponentially\nincreasing complexity. The paper specifically investigates the sampling issues\nthe Deep Galerkin Method is subjected to. It proposes a drift relaxation-based\nsampling approach to alleviate the symptoms of high-variance policy\napproximations. This is validated on mean-field control problems; namely, the\nvariations of the opinion dynamics presented by the Sznajd and the\nHegselmann-Krause model. The resulting policies induce a significant cost\nreduction over manually optimised control functions and show improvements on\nthe Linear-Quadratic Regulator problem over the Deep FBSDE approach.\n","authors":["Frederik Kelbel"],"pdf_url":"https://arxiv.org/pdf/2406.09141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09137v1","updated":"2024-06-13T14:07:15Z","published":"2024-06-13T14:07:15Z","title":"Dynamic Correlation Clustering in Sublinear Update Time","summary":"  We study the classic problem of correlation clustering in dynamic node\nstreams. In this setting, nodes are either added or randomly deleted over time,\nand each node pair is connected by a positive or negative edge. The objective\nis to continuously find a partition which minimizes the sum of positive edges\ncrossing clusters and negative edges within clusters. We present an algorithm\nthat maintains an $O(1)$-approximation with $O$(polylog $n$) amortized update\ntime. Prior to our work, Behnezhad, Charikar, Ma, and L. Tan achieved a\n$5$-approximation with $O(1)$ expected update time in edge streams which\ntranslates in node streams to an $O(D)$-update time where $D$ is the maximum\npossible degree. Finally we complement our theoretical analysis with\nexperiments on real world data.\n","authors":["Vincent Cohen-Addad","Silvio Lattanzi","Andreas Maggiori","Nikos Parotsidis"],"pdf_url":"https://arxiv.org/pdf/2406.09137v1.pdf","comment":"ICML'24 (spotlight)"},{"id":"http://arxiv.org/abs/2406.09136v1","updated":"2024-06-13T14:07:02Z","published":"2024-06-13T14:07:02Z","title":"Chain of Preference Optimization: Improving Chain-of-Thought Reasoning\n  in LLMs","summary":"  The recent development of chain-of-thought (CoT) decoding has enabled large\nlanguage models (LLMs) to generate explicit logical reasoning paths for complex\nproblem-solving. However, research indicates that these paths are not always\ndeliberate and optimal. The tree-of-thought (ToT) method employs tree-searching\nto extensively explore the reasoning space and find better reasoning paths that\nCoT decoding might overlook. This deliberation, however, comes at the cost of\nsignificantly increased inference complexity. In this work, we demonstrate that\nfine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to\nachieve similar or better performance, thereby avoiding the substantial\ninference burden. This is achieved through Chain of Preference Optimization\n(CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths\nwith those of ToT using the inherent preference information in the tree-search\nprocess. Extensive experimental results show that CPO significantly improves\nLLM performance in solving a variety of complex problems, including question\nanswering, fact verification, and arithmetic reasoning, demonstrating its\neffectiveness. Our code is available at https://github.com/sail-sg/CPO.\n","authors":["Xuan Zhang","Chao Du","Tianyu Pang","Qian Liu","Wei Gao","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2406.09136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09132v1","updated":"2024-06-13T14:04:34Z","published":"2024-06-13T14:04:34Z","title":"Jacobian-Enhanced Neural Networks","summary":"  Jacobian-Enhanced Neural Networks (JENN) are densely connected multi-layer\nperceptrons, whose training process is modified to predict partial derivatives\naccurately. Their main benefit is better accuracy with fewer training points\ncompared to standard neural networks. These attributes are particularly\ndesirable in the field of computer-aided design, where there is often the need\nto replace computationally expensive, physics-based models with fast running\napproximations, known as surrogate models or meta-models. Since a surrogate\nemulates the original model accurately in near-real time, it yields a speed\nbenefit that can be used to carry out orders of magnitude more function calls\nquickly. However, in the special case of gradient-enhanced methods, there is\nthe additional value proposition that partial derivatives are accurate, which\nis a critical property for one important use-case: surrogate-based\noptimization. This work derives the complete theory and exemplifies its\nsuperiority over standard neural nets for surrogate-based optimization.\n","authors":["Steven H. Berguin"],"pdf_url":"https://arxiv.org/pdf/2406.09132v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.09131v1","updated":"2024-06-13T14:02:18Z","published":"2024-06-13T14:02:18Z","title":"OLGA: One-cLass Graph Autoencoder","summary":"  One-class learning (OCL) comprises a set of techniques applied when\nreal-world problems have a single class of interest. The usual procedure for\nOCL is learning a hypersphere that comprises instances of this class and,\nideally, repels unseen instances from any other classes. Besides, several OCL\nalgorithms for graphs have been proposed since graph representation learning\nhas succeeded in various fields. These methods may use a two-step strategy,\ninitially representing the graph and, in a second step, classifying its nodes.\nOn the other hand, end-to-end methods learn the node representations while\nclassifying the nodes in one learning process. We highlight three main gaps in\nthe literature on OCL for graphs: (i) non-customized representations for OCL;\n(ii) the lack of constraints on hypersphere parameters learning; and (iii) the\nmethods' lack of interpretability and visualization. We propose One-cLass Graph\nAutoencoder (OLGA). OLGA is end-to-end and learns the representations for the\ngraph nodes while encapsulating the interest instances by combining two loss\nfunctions. We propose a new hypersphere loss function to encapsulate the\ninterest instances. OLGA combines this new hypersphere loss with the graph\nautoencoder reconstruction loss to improve model learning. OLGA achieved\nstate-of-the-art results and outperformed six other methods with a\nstatistically significant difference from five methods. Moreover, OLGA learns\nlow-dimensional representations maintaining the classification performance with\nan interpretable model representation learning and results.\n","authors":["M. P. S. Gôlo","J. G. B. M. Junior","D. F. Silva","R. M. Marcacini"],"pdf_url":"https://arxiv.org/pdf/2406.09131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09130v1","updated":"2024-06-13T14:01:34Z","published":"2024-06-13T14:01:34Z","title":"Time-Series Forecasting for Out-of-Distribution Generalization Using\n  Invariant Learning","summary":"  Time-series forecasting (TSF) finds broad applications in real-world\nscenarios. Due to the dynamic nature of time-series data, it is crucial to\nequip TSF models with out-of-distribution (OOD) generalization abilities, as\nhistorical training data and future test data can have different distributions.\nIn this paper, we aim to alleviate the inherent OOD problem in TSF via\ninvariant learning. We identify fundamental challenges of invariant learning\nfor TSF. First, the target variables in TSF may not be sufficiently determined\nby the input due to unobserved core variables in TSF, breaking the conventional\nassumption of invariant learning. Second, time-series datasets lack adequate\nenvironment labels, while existing environmental inference methods are not\nsuitable for TSF.\n  To address these challenges, we propose FOIL, a model-agnostic framework that\nenables timeseries Forecasting for Out-of-distribution generalization via\nInvariant Learning. FOIL employs a novel surrogate loss to mitigate the impact\nof unobserved variables. Further, FOIL implements a joint optimization by\nalternately inferring environments effectively with a multi-head network while\npreserving the temporal adjacency structure, and learning invariant\nrepresentations across inferred environments for OOD generalized TSF. We\ndemonstrate that the proposed FOIL significantly improves the performance of\nvarious TSF models, achieving gains of up to 85%.\n","authors":["Haoxin Liu","Harshavardhan Kamarthi","Lingkai Kong","Zhiyuan Zhao","Chao Zhang","B. Aditya Prakash"],"pdf_url":"https://arxiv.org/pdf/2406.09130v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2302.03438v2","updated":"2024-06-13T13:49:44Z","published":"2023-02-07T12:46:54Z","title":"Uncoupled Learning of Differential Stackelberg Equilibria with\n  Commitments","summary":"  In multi-agent problems requiring a high degree of cooperation, success often\ndepends on the ability of the agents to adapt to each other's behavior. A\nnatural solution concept in such settings is the Stackelberg equilibrium, in\nwhich the ``leader'' agent selects the strategy that maximizes its own payoff\ngiven that the ``follower'' agent will choose their best response to this\nstrategy. Recent work has extended this solution concept to two-player\ndifferentiable games, such as those arising from multi-agent deep reinforcement\nlearning, in the form of the \\textit{differential} Stackelberg equilibrium.\nWhile this previous work has presented learning dynamics which converge to such\nequilibria, these dynamics are ``coupled'' in the sense that the learning\nupdates for the leader's strategy require some information about the follower's\npayoff function. As such, these methods cannot be applied to truly\ndecentralised multi-agent settings, particularly ad hoc cooperation, where each\nagent only has access to its own payoff function. In this work we present\n``uncoupled'' learning dynamics based on zeroth-order gradient estimators, in\nwhich each agent's strategy update depends only on their observations of the\nother's behavior. We analyze the convergence of these dynamics in general-sum\ngames, and prove that they converge to differential Stackelberg equilibria\nunder the same conditions as previous coupled methods. Furthermore, we present\nan online mechanism by which symmetric learners can negotiate leader-follower\nroles. We conclude with a discussion of the implications of our work for\nmulti-agent reinforcement learning and ad hoc collaboration more generally.\n","authors":["Robert Loftin","Mustafa Mert Çelikok","Herke van Hoof","Samuel Kaski","Frans A. Oliehoek"],"pdf_url":"https://arxiv.org/pdf/2302.03438v2.pdf","comment":"International Conference on Autonomous Agents and Multi-Agent Systems\n  (AAMAS) 2024"},{"id":"http://arxiv.org/abs/2406.09116v1","updated":"2024-06-13T13:43:59Z","published":"2024-06-13T13:43:59Z","title":"Injective Flows for parametric hypersurfaces","summary":"  Normalizing Flows (NFs) are powerful and efficient models for density\nestimation. When modeling densities on manifolds, NFs can be generalized to\ninjective flows but the Jacobian determinant becomes computationally\nprohibitive. Current approaches either consider bounds on the log-likelihood or\nrely on some approximations of the Jacobian determinant. In contrast, we\npropose injective flows for parametric hypersurfaces and show that for such\nmanifolds we can compute the Jacobian determinant exactly and efficiently, with\nthe same cost as NFs. Furthermore, we show that for the subclass of star-like\nmanifolds we can extend the proposed framework to always allow for a Cartesian\nrepresentation of the density. We showcase the relevance of modeling densities\non hypersurfaces in two settings. Firstly, we introduce a novel Objective\nBayesian approach to penalized likelihood models by interpreting level-sets of\nthe penalty as star-like manifolds. Secondly, we consider Bayesian mixture\nmodels and introduce a general method for variational inference by defining the\nposterior of mixture weights on the probability simplex.\n","authors":["Marcello Massimo Negri","Jonathan Aellen","Volker Roth"],"pdf_url":"https://arxiv.org/pdf/2406.09116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20233v3","updated":"2024-06-13T13:43:42Z","published":"2024-03-29T15:22:03Z","title":"Functional Bilevel Optimization for Machine Learning","summary":"  In this paper, we introduce a new functional point of view on bilevel\noptimization problems for machine learning, where the inner objective is\nminimized over a function space. These types of problems are most often solved\nby using methods developed in the parametric setting, where the inner objective\nis strongly convex with respect to the parameters of the prediction function.\nThe functional point of view does not rely on this assumption and notably\nallows using over-parameterized neural networks as the inner prediction\nfunction. We propose scalable and efficient algorithms for the functional\nbilevel optimization problem and illustrate the benefits of our approach on\ninstrumental regression and reinforcement learning tasks.\n","authors":["Ieva Petrulionyte","Julien Mairal","Michael Arbel"],"pdf_url":"https://arxiv.org/pdf/2403.20233v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09112v1","updated":"2024-06-13T13:43:01Z","published":"2024-06-13T13:43:01Z","title":"Large-Scale Evaluation of Open-Set Image Classification Techniques","summary":"  The goal for classification is to correctly assign labels to unseen samples.\nHowever, most methods misclassify samples with unseen labels and assign them to\none of the known classes. Open-Set Classification (OSC) algorithms aim to\nmaximize both closed and open-set recognition capabilities. Recent studies\nshowed the utility of such algorithms on small-scale data sets, but limited\nexperimentation makes it difficult to assess their performances in real-world\nproblems. Here, we provide a comprehensive comparison of various OSC\nalgorithms, including training-based (SoftMax, Garbage, EOS) and\npost-processing methods (Maximum SoftMax Scores, Maximum Logit Scores, OpenMax,\nEVM, PROSER), the latter are applied on features from the former. We perform\nour evaluation on three large-scale protocols that mimic real-world challenges,\nwhere we train on known and negative open-set samples, and test on known and\nunknown instances. Our results show that EOS helps to improve performance of\nalmost all post-processing algorithms. Particularly, OpenMax and PROSER are\nable to exploit better-trained networks, demonstrating the utility of hybrid\nmodels. However, while most algorithms work well on negative test samples --\nsamples of open-set classes seen during training -- they tend to perform poorly\nwhen tested on samples of previously unseen unknown classes, especially in\nchallenging conditions.\n","authors":["Halil Bisgin","Andres Palechor","Mike Suter","Manuel Günther"],"pdf_url":"https://arxiv.org/pdf/2406.09112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09105v1","updated":"2024-06-13T13:31:49Z","published":"2024-06-13T13:31:49Z","title":"INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance\n  in Insurance","summary":"  Large Vision-Language Models (LVLMs) have demonstrated outstanding\nperformance in various general multimodal applications such as image\nrecognition and visual reasoning, and have also shown promising potential in\nspecialized domains. However, the application potential of LVLMs in the\ninsurance domain-characterized by rich application scenarios and abundant\nmultimodal data-has not been effectively explored. There is no systematic\nreview of multimodal tasks in the insurance domain, nor a benchmark\nspecifically designed to evaluate the capabilities of LVLMs in insurance. This\ngap hinders the development of LVLMs within the insurance domain. In this\npaper, we systematically review and distill multimodal tasks for four\nrepresentative types of insurance: auto insurance, property insurance, health\ninsurance, and agricultural insurance. We propose INS-MMBench, the first\ncomprehensive LVLMs benchmark tailored for the insurance domain. INS-MMBench\ncomprises a total of 2.2K thoroughly designed multiple-choice questions,\ncovering 12 meta-tasks and 22 fundamental tasks. Furthermore, we evaluate\nmultiple representative LVLMs, including closed-source models such as GPT-4o\nand open-source models like BLIP-2. This evaluation not only validates the\neffectiveness of our benchmark but also provides an in-depth performance\nanalysis of current LVLMs on various multimodal tasks in the insurance domain.\nWe hope that INS-MMBench will facilitate the further application of LVLMs in\nthe insurance domain and inspire interdisciplinary development. Our dataset and\nevaluation code are available at https://github.com/FDU-INS/INS-MMBench.\n","authors":["Chenwei Lin","Hanjia Lyu","Xian Xu","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2406.09105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03679v3","updated":"2024-06-13T13:31:05Z","published":"2024-06-06T01:49:29Z","title":"On the Effects of Data Scale on Computer Control Agents","summary":"  Autonomous agents that control computer interfaces to accomplish human tasks\nare emerging. Leveraging LLMs to power such agents has been of special\ninterest, but unless fine-tuned on human-collected task demonstrations,\nperformance is still relatively low. In this work we study whether fine-tuning\nalone is a viable approach for building real-world computer control agents. In\nparticularly, we investigate how performance measured on both high and\nlow-level tasks in domain and out of domain scales as more training data is\ncollected. To this end we collect and release a new dataset, AndroidControl,\nconsisting of 15,283 demonstrations of everyday tasks with Android apps.\nCompared to existing datasets, each AndroidControl task instance includes both\nhigh and low-level human-generated instructions, allowing us to explore the\nlevel of task complexity an agent can handle. Moreover, AndroidControl is the\nmost diverse computer control dataset to date, including 15,283 unique tasks\nover 833 Android apps, thus allowing us to conduct in-depth analysis of the\nmodel performance in and out of the domain of the training data. Using the\ndataset, we find that when tested in domain fine-tuned models outperform zero\nand few-shot baselines and scale in such a way that robust performance might\nfeasibly be obtained simply by collecting more data. Out of domain, performance\nscales significantly more slowly and suggests that in particular for high-level\ntasks, fine-tuning on more data alone may be insufficient for achieving robust\nout-of-domain performance.\n","authors":["Wei Li","William Bishop","Alice Li","Chris Rawles","Folawiyo Campbell-Ajala","Divya Tyamagundlu","Oriana Riva"],"pdf_url":"https://arxiv.org/pdf/2406.03679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09089v1","updated":"2024-06-13T13:15:40Z","published":"2024-06-13T13:15:40Z","title":"DiffPoGAN: Diffusion Policies with Generative Adversarial Networks for\n  Offline Reinforcement Learning","summary":"  Offline reinforcement learning (RL) can learn optimal policies from\npre-collected offline datasets without interacting with the environment, but\nthe sampled actions of the agent cannot often cover the action distribution\nunder a given state, resulting in the extrapolation error issue. Recent works\naddress this issue by employing generative adversarial networks (GANs).\nHowever, these methods often suffer from insufficient constraints on policy\nexploration and inaccurate representation of behavior policies. Moreover, the\ngenerator in GANs fails in fooling the discriminator while maximizing the\nexpected returns of a policy. Inspired by the diffusion, a generative model\nwith powerful feature expressiveness, we propose a new offline RL method named\nDiffusion Policies with Generative Adversarial Networks (DiffPoGAN). In this\napproach, the diffusion serves as the policy generator to generate diverse\ndistributions of actions, and a regularization method based on maximum\nlikelihood estimation (MLE) is developed to generate data that approximate the\ndistribution of behavior policies. Besides, we introduce an additional\nregularization term based on the discriminator output to effectively constrain\npolicy exploration for policy improvement. Comprehensive experiments are\nconducted on the datasets for deep data-driven reinforcement learning (D4RL),\nand experimental results show that DiffPoGAN outperforms state-of-the-art\nmethods in offline RL.\n","authors":["Xuemin Hu","Shen Li","Yingfen Xu","Bo Tang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07115v2","updated":"2024-06-13T13:08:24Z","published":"2023-09-13T17:45:41Z","title":"Getting More for Less: Using Weak Labels and AV-Mixup for Robust\n  Audio-Visual Speaker Verification","summary":"  Distance Metric Learning (DML) has typically dominated the audio-visual\nspeaker verification problem space, owing to strong performance in new and\nunseen classes. In our work, we explored multitask learning techniques to\nfurther enhance DML, and show that an auxiliary task with even weak labels can\nincrease the quality of the learned speaker representation without increasing\nmodel complexity during inference. We also extend the Generalized End-to-End\nLoss (GE2E) to multimodal inputs and demonstrate that it can achieve\ncompetitive performance in an audio-visual space. Finally, we introduce\nAV-Mixup, a multimodal augmentation technique during training time that has\nshown to reduce speaker overfit. Our network achieves state of the art\nperformance for speaker verification, reporting 0.244%, 0.252%, 0.441% Equal\nError Rate (EER) on the VoxCeleb1-O/E/H test sets, which is to our knowledge,\nthe best published results on VoxCeleb1-E and VoxCeleb1-H.\n","authors":["Anith Selvakumar","Homa Fashandi"],"pdf_url":"https://arxiv.org/pdf/2309.07115v2.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.09084v1","updated":"2024-06-13T13:07:52Z","published":"2024-06-13T13:07:52Z","title":"Operator-informed score matching for Markov diffusion models","summary":"  Diffusion models are typically trained using score matching, yet score\nmatching is agnostic to the particular forward process that defines the model.\nThis paper argues that Markov diffusion models enjoy an advantage over other\ntypes of diffusion model, as their associated operators can be exploited to\nimprove the training process. In particular, (i) there exists an explicit\nformal solution to the forward process as a sequence of time-dependent kernel\nmean embeddings; and (ii) the derivation of score-matching and related\nestimators can be streamlined. Building upon (i), we propose Riemannian\ndiffusion kernel smoothing, which ameliorates the need for neural score\napproximation, at least in the low-dimensional context; Building upon (ii), we\npropose operator-informed score matching, a variance reduction technique that\nis straightforward to implement in both low- and high-dimensional diffusion\nmodeling and is demonstrated to improve score matching in an empirical\nproof-of-concept.\n","authors":["Zheyang Shen","Chris J. Oates"],"pdf_url":"https://arxiv.org/pdf/2406.09084v1.pdf","comment":"Preprint; 19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.10036v3","updated":"2024-06-13T13:04:41Z","published":"2024-02-15T15:59:59Z","title":"Predictive Linear Online Tracking for Unknown Targets","summary":"  In this paper, we study the problem of online tracking in linear control\nsystems, where the objective is to follow a moving target. Unlike classical\ntracking control, the target is unknown, non-stationary, and its state is\nrevealed sequentially, thus, fitting the framework of online non-stochastic\ncontrol. We consider the case of quadratic costs and propose a new algorithm,\ncalled predictive linear online tracking (PLOT). The algorithm uses recursive\nleast squares with exponential forgetting to learn a time-varying dynamic model\nof the target. The learned model is used in the optimal policy under the\nframework of receding horizon control. We show the dynamic regret of PLOT\nscales with $\\mathcal{O}(\\sqrt{TV_T})$, where $V_T$ is the total variation of\nthe target dynamics and $T$ is the time horizon. Unlike prior work, our\ntheoretical results hold for non-stationary targets. We implement PLOT on a\nreal quadrotor and provide open-source software, thus, showcasing one of the\nfirst successful applications of online control methods on real hardware.\n","authors":["Anastasios Tsiamis","Aren Karapetyan","Yueshan Li","Efe C. Balta","John Lygeros"],"pdf_url":"https://arxiv.org/pdf/2402.10036v3.pdf","comment":"ICML 2024 (spotlight)"},{"id":"http://arxiv.org/abs/2406.09079v1","updated":"2024-06-13T13:03:37Z","published":"2024-06-13T13:03:37Z","title":"Latent Assistance Networks: Rediscovering Hyperbolic Tangents in RL","summary":"  Activation functions are one of the key components of a neural network. The\nmost commonly used activation functions can be classed into the category of\ncontinuously differentiable (e.g. tanh) and linear-unit functions (e.g. ReLU),\nboth having their own strengths and drawbacks with respect to downstream\nperformance and representation capacity through learning (e.g. measured by the\nnumber of dead neurons and the effective rank). In reinforcement learning, the\nperformance of continuously differentiable activations often falls short as\ncompared to linear-unit functions. From the perspective of the activations in\nthe last hidden layer, this paper provides insights regarding this\nsub-optimality and explores how activation functions influence the occurrence\nof dead neurons and the magnitude of the effective rank. Additionally, a novel\nneural architecture is proposed that leverages the product of independent\nactivation values. In the Atari domain, we show faster learning, a reduction in\ndead neurons and increased effective rank.\n","authors":["Jacob E. Kooi","Mark Hoogendoorn","Vincent François-Lavet"],"pdf_url":"https://arxiv.org/pdf/2406.09079v1.pdf","comment":"22 pages, 17 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.08316v2","updated":"2024-06-13T12:59:06Z","published":"2024-06-12T15:16:40Z","title":"Is Programming by Example solved by LLMs?","summary":"  Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n`solved' PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short.\n","authors":["Wen-Ding Li","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2406.08316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09073v1","updated":"2024-06-13T12:58:00Z","published":"2024-06-13T12:58:00Z","title":"Are we making progress in unlearning? Findings from the first NeurIPS\n  unlearning competition","summary":"  We present the findings of the first NeurIPS competition on unlearning, which\nsought to stimulate the development of novel algorithms and initiate\ndiscussions on formal and robust evaluation methodologies. The competition was\nhighly successful: nearly 1,200 teams from across the world participated, and a\nwealth of novel, imaginative solutions with different characteristics were\ncontributed. In this paper, we analyze top solutions and delve into discussions\non benchmarking unlearning, which itself is a research problem. The evaluation\nmethodology we developed for the competition measures forgetting quality\naccording to a formal notion of unlearning, while incorporating model utility\nfor a holistic evaluation. We analyze the effectiveness of different\ninstantiations of this evaluation framework vis-a-vis the associated compute\ncost, and discuss implications for standardizing evaluation. We find that the\nranking of leading methods remains stable under several variations of this\nframework, pointing to avenues for reducing the cost of evaluation. Overall,\nour findings indicate progress in unlearning, with top-performing competition\nentries surpassing existing algorithms under our evaluation framework. We\nanalyze trade-offs made by different algorithms and strengths or weaknesses in\nterms of generalizability to new datasets, paving the way for advancing both\nbenchmarking and algorithm development in this important area.\n","authors":["Eleni Triantafillou","Peter Kairouz","Fabian Pedregosa","Jamie Hayes","Meghdad Kurmanji","Kairan Zhao","Vincent Dumoulin","Julio Jacques Junior","Ioannis Mitliagkas","Jun Wan","Lisheng Sun Hosoya","Sergio Escalera","Gintare Karolina Dziugaite","Peter Triantafillou","Isabelle Guyon"],"pdf_url":"https://arxiv.org/pdf/2406.09073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09070v1","updated":"2024-06-13T12:55:10Z","published":"2024-06-13T12:55:10Z","title":"EquiPrompt: Debiasing Diffusion Models via Iterative Bootstrapping in\n  Chain of Thoughts","summary":"  In the domain of text-to-image generative models, the inadvertent propagation\nof biases inherent in training datasets poses significant ethical challenges,\nparticularly in the generation of socially sensitive content. This paper\nintroduces EquiPrompt, a novel method employing Chain of Thought (CoT)\nreasoning to reduce biases in text-to-image generative models. EquiPrompt uses\niterative bootstrapping and bias-aware exemplar selection to balance creativity\nand ethical responsibility. It integrates iterative reasoning refinement with\ncontrolled evaluation techniques, addressing zero-shot CoT issues in sensitive\ncontexts. Experiments on several generation tasks show EquiPrompt effectively\nlowers bias while maintaining generative quality, advancing ethical AI and\nsocially responsible creative processes.Code will be publically available.\n","authors":["Zahraa Al Sahili","Ioannis Patras","Matthew Purver"],"pdf_url":"https://arxiv.org/pdf/2406.09070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09069v1","updated":"2024-06-13T12:54:53Z","published":"2024-06-13T12:54:53Z","title":"On the Robustness of Global Feature Effect Explanations","summary":"  We study the robustness of global post-hoc explanations for predictive models\ntrained on tabular data. Effects of predictor features in black-box supervised\nlearning are an essential diagnostic tool for model debugging and scientific\ndiscovery in applied sciences. However, how vulnerable they are to data and\nmodel perturbations remains an open research question. We introduce several\ntheoretical bounds for evaluating the robustness of partial dependence plots\nand accumulated local effects. Our experimental results with synthetic and\nreal-world datasets quantify the gap between the best and worst-case scenarios\nof (mis)interpreting machine learning predictions globally.\n","authors":["Hubert Baniecki","Giuseppe Casalicchio","Bernd Bischl","Przemyslaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2406.09069v1.pdf","comment":"Accepted at ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2406.09068v1","updated":"2024-06-13T12:54:29Z","published":"2024-06-13T12:54:29Z","title":"Dispelling the Mirage of Progress in Offline MARL through Standardised\n  Baselines and Evaluation","summary":"  Offline multi-agent reinforcement learning (MARL) is an emerging field with\ngreat promise for real-world applications. Unfortunately, the current state of\nresearch in offline MARL is plagued by inconsistencies in baselines and\nevaluation protocols, which ultimately makes it difficult to accurately assess\nprogress, trust newly proposed innovations, and allow researchers to easily\nbuild upon prior work. In this paper, we firstly identify significant\nshortcomings in existing methodologies for measuring the performance of novel\nalgorithms through a representative study of published offline MARL work.\nSecondly, by directly comparing to this prior work, we demonstrate that simple,\nwell-implemented baselines can achieve state-of-the-art (SOTA) results across a\nwide range of tasks. Specifically, we show that on 35 out of 47 datasets used\nin prior work (almost 75% of cases), we match or surpass the performance of the\ncurrent purported SOTA. Strikingly, our baselines often substantially\noutperform these more sophisticated algorithms. Finally, we correct for the\nshortcomings highlighted from this prior work by introducing a straightforward\nstandardised methodology for evaluation and by providing our baseline\nimplementations with statistically robust results across several scenarios,\nuseful for comparisons in future work. Our proposal includes simple and\nsensible steps that are easy to adopt, which in combination with solid\nbaselines and comparative results, could substantially improve the overall\nrigour of empirical science in offline MARL moving forward.\n","authors":["Claude Formanek","Callum Rhys Tilbury","Louise Beyers","Jonathan Shock","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2406.09068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04426v2","updated":"2024-06-13T12:54:10Z","published":"2024-06-06T18:12:04Z","title":"DeTra: A Unified Model for Object Detection and Trajectory Forecasting","summary":"  The tasks of object detection and trajectory forecasting play a crucial role\nin understanding the scene for autonomous driving. These tasks are typically\nexecuted in a cascading manner, making them prone to compounding errors.\nFurthermore, there is usually a very thin interface between the two tasks,\ncreating a lossy information bottleneck. To address these challenges, our\napproach formulates the union of the two tasks as a trajectory refinement\nproblem, where the first pose is the detection (current time), and the\nsubsequent poses are the waypoints of the multiple forecasts (future time). To\ntackle this unified task, we design a refinement transformer that infers the\npresence, pose, and multi-modal future behaviors of objects directly from LiDAR\npoint clouds and high-definition maps. We call this model DeTra, short for\nobject Detection and Trajectory forecasting. In our experiments, we observe\nthat \\ourmodel{} outperforms the state-of-the-art on Argoverse 2 Sensor and\nWaymo Open Dataset by a large margin, across a broad range of metrics. Last but\nnot least, we perform extensive ablation studies that show the value of\nrefinement for this task, that every proposed component contributes positively\nto its performance, and that key design choices were made.\n","authors":["Sergio Casas","Ben Agro","Jiageng Mao","Thomas Gilles","Alexander Cui","Thomas Li","Raquel Urtasun"],"pdf_url":"https://arxiv.org/pdf/2406.04426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09062v1","updated":"2024-06-13T12:51:22Z","published":"2024-06-13T12:51:22Z","title":"State-Space Modeling in Long Sequence Processing: A Survey on Recurrence\n  in the Transformer Era","summary":"  Effectively learning from sequential data is a longstanding goal of\nArtificial Intelligence, especially in the case of long sequences. From the\ndawn of Machine Learning, several researchers engaged in the search of\nalgorithms and architectures capable of processing sequences of patterns,\nretaining information about the past inputs while still leveraging the upcoming\ndata, without losing precious long-term dependencies and correlations. While\nsuch an ultimate goal is inspired by the human hallmark of continuous real-time\nprocessing of sensory information, several solutions simplified the learning\nparadigm by artificially limiting the processed context or dealing with\nsequences of limited length, given in advance. These solutions were further\nemphasized by the large ubiquity of Transformers, that have initially shaded\nthe role of Recurrent Neural Nets. However, recurrent networks are facing a\nstrong recent revival due to the growing popularity of (deep) State-Space\nmodels and novel instances of large-context Transformers, which are both based\non recurrent computations to go beyond several limits of currently ubiquitous\ntechnologies. In fact, the fast development of Large Language Models enhanced\nthe interest in efficient solutions to process data over time. This survey\nprovides an in-depth summary of the latest approaches that are based on\nrecurrent models for sequential data processing. A complete taxonomy over the\nlatest trends in architectural and algorithmic solutions is reported and\ndiscussed, guiding researchers in this appealing research field. The emerging\npicture suggests that there is room for thinking of novel routes, constituted\nby learning algorithms which depart from the standard Backpropagation Through\nTime, towards a more realistic scenario where patterns are effectively\nprocessed online, leveraging local-forward computations, opening to further\nresearch on this topic.\n","authors":["Matteo Tiezzi","Michele Casoni","Alessandro Betti","Marco Gori","Stefano Melacci"],"pdf_url":"https://arxiv.org/pdf/2406.09062v1.pdf","comment":"Currently under review"},{"id":"http://arxiv.org/abs/2406.09041v1","updated":"2024-06-13T12:27:55Z","published":"2024-06-13T12:27:55Z","title":"ME-Switch: A Memory-Efficient Expert Switching Framework for Large\n  Language Models","summary":"  The typical process for developing LLMs involves pre-training a general\nfoundation model on massive data, followed by fine-tuning on task-specific data\nto create specialized experts. Serving these experts poses challenges, as\nloading all experts onto devices is impractical, and frequent switching between\nexperts in response to user requests incurs substantial I/O costs, increasing\nlatency and expenses. Previous approaches decompose expert weights into\npre-trained model weights and residual delta weights, then quantize the delta\nweights to reduce model size. However, these methods often lead to significant\nquantization errors at extremely low bitwidths and assume the appropriate model\nfor a user request is known in advance, which is not practical. To address\nthese issues, we introduce ME-Switch, a memory-efficient expert switching\nframework for LLM serving. ME-Switch uses mixed-precision quantization,\nselectively quantizing non-salient input channels of delta weights to extremely\nlow bits while keeping salient ones intact, significantly reducing storage\ndemands while maintaining performance. Additionally, we develop a routing\nmethod that efficiently directs user queries to the most suitable expert by\ntransforming the model selection problem into a domain classification problem.\nExtensive experiments show ME-Switch's promising memory efficiency and routing\nperformance. For example, when serving three models from the Mistral-7B family,\nME-Switch reduces model size by 1.74x while maintaining nearly lossless\nperformance on instruction, mathematical reasoning, and code generation tasks.\nFurthermore, ME-Switch can efficiently serve 16 models from the Mistral-7B\nfamily on a single NVIDIA A100 GPU.\n","authors":["Jing Liu","Ruihao Gong","Mingyang Zhang","Yefei He","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2406.09041v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2406.09038v1","updated":"2024-06-13T12:22:08Z","published":"2024-06-13T12:22:08Z","title":"CGP++ : A Modern C++ Implementation of Cartesian Genetic Programming","summary":"  The reference implementation of Cartesian Genetic Programming (CGP) was\nwritten in the C programming language. C inherently follows a procedural\nprogramming paradigm, which entails challenges in providing a reusable and\nscalable implementation model for complex structures and methods. Moreover, due\nto the limiting factors of C, the reference implementation of CGP does not\nprovide a generic framework and is therefore restricted to a set of predefined\nevaluation types. Besides the reference implementation, we also observe that\nother existing implementations are limited with respect to the features\nprovided. In this work, we therefore propose the first version of a modern C++\nimplementation of CGP that pursues object-oriented design and generic\nprogramming paradigm to provide an efficient implementation model that can\nfacilitate the discovery of new problem domains and the implementation of\ncomplex advanced methods that have been proposed for CGP over time. With the\nproposal of our new implementation, we aim to generally promote\ninterpretability, accessibility and reproducibility in the field of CGP.\n","authors":["Roman Kalkreuth","Thomas Baeck"],"pdf_url":"https://arxiv.org/pdf/2406.09038v1.pdf","comment":"Accepted as a full paper in the BBSR track at the Genetic and\n  Evolutionary Computation Conference (GECCO'24), July 14-18, 2024, Melbourne,\n  Australia"},{"id":"http://arxiv.org/abs/2406.09031v1","updated":"2024-06-13T12:04:40Z","published":"2024-06-13T12:04:40Z","title":"A Comprehensive Graph Pooling Benchmark: Effectiveness, Robustness and\n  Generalizability","summary":"  Graph pooling has gained attention for its ability to obtain effective node\nand graph representations for various downstream tasks. Despite the recent\nsurge in graph pooling approaches, there is a lack of standardized experimental\nsettings and fair benchmarks to evaluate their performance. To address this\nissue, we have constructed a comprehensive benchmark that includes 15 graph\npooling methods and 21 different graph datasets. This benchmark systematically\nassesses the performance of graph pooling methods in three dimensions, i.e.,\neffectiveness, robustness, and generalizability. We first evaluate the\nperformance of these graph pooling approaches across different tasks including\ngraph classification, graph regression and node classification. Then, we\ninvestigate their performance under potential noise attacks and\nout-of-distribution shifts in real-world scenarios. We also involve detailed\nefficiency analysis and parameter analysis. Extensive experiments validate the\nstrong capability and applicability of graph pooling approaches in various\nscenarios, which can provide valuable insights and guidance for deep geometric\nlearning research. The source code of our benchmark is available at\nhttps://github.com/goose315/Graph_Pooling_Benchmark.\n","authors":["Pengyun Wang","Junyu Luo","Yanxin Shen","Siyu Heng","Xiao Luo"],"pdf_url":"https://arxiv.org/pdf/2406.09031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09030v1","updated":"2024-06-13T12:03:40Z","published":"2024-06-13T12:03:40Z","title":"CUER: Corrected Uniform Experience Replay for Off-Policy Continuous Deep\n  Reinforcement Learning Algorithms","summary":"  The utilization of the experience replay mechanism enables agents to\neffectively leverage their experiences on several occasions. In previous\nstudies, the sampling probability of the transitions was modified based on\ntheir relative significance. The process of reassigning sample probabilities\nfor every transition in the replay buffer after each iteration is considered\nextremely inefficient. Hence, in order to enhance computing efficiency,\nexperience replay prioritization algorithms reassess the importance of a\ntransition as it is sampled. However, the relative importance of the\ntransitions undergoes dynamic adjustments when the agent's policy and value\nfunction are iteratively updated. Furthermore, experience replay is a mechanism\nthat retains the transitions generated by the agent's past policies, which\ncould potentially diverge significantly from the agent's most recent policy. An\nincreased deviation from the agent's most recent policy results in a greater\nfrequency of off-policy updates, which has a negative impact on the agent's\nperformance. In this paper, we develop a novel algorithm, Corrected Uniform\nExperience Replay (CUER), which stochastically samples the stored experience\nwhile considering the fairness among all other experiences without ignoring the\ndynamic nature of the transition importance by making sampled state\ndistribution more on-policy. CUER provides promising improvements for\noff-policy continuous control algorithms in terms of sample efficiency, final\nperformance, and stability of the policy during the training.\n","authors":["Arda Sarp Yenicesu","Furkan B. Mutlu","Suleyman S. Kozat","Ozgur S. Oguz"],"pdf_url":"https://arxiv.org/pdf/2406.09030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09028v1","updated":"2024-06-13T12:02:51Z","published":"2024-06-13T12:02:51Z","title":"From Biased to Unbiased Dynamics: An Infinitesimal Generator Approach","summary":"  We investigate learning the eigenfunctions of evolution operators for\ntime-reversal invariant stochastic processes, a prime example being the\nLangevin equation used in molecular dynamics. Many physical or chemical\nprocesses described by this equation involve transitions between metastable\nstates separated by high potential barriers that can hardly be crossed during a\nsimulation. To overcome this bottleneck, data are collected via biased\nsimulations that explore the state space more rapidly. We propose a framework\nfor learning from biased simulations rooted in the infinitesimal generator of\nthe process and the associated resolvent operator. We contrast our approach to\nmore common ones based on the transfer operator, showing that it can provably\nlearn the spectral properties of the unbiased system from biased data. In\nexperiments, we highlight the advantages of our method over transfer operator\napproaches and recent developments based on generator learning, demonstrating\nits effectiveness in estimating eigenfunctions and eigenvalues. Importantly, we\nshow that even with datasets containing only a few relevant transitions due to\nsub-optimal biasing, our approach recovers relevant information about the\ntransition mechanism.\n","authors":["Timothée Devergne","Vladimir Kostic","Michele Parrinello","Massimiliano Pontil"],"pdf_url":"https://arxiv.org/pdf/2406.09028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03915v2","updated":"2024-06-13T12:02:23Z","published":"2024-02-06T11:31:04Z","title":"Learning Metrics that Maximise Power for Accelerated A/B-Tests","summary":"  Online controlled experiments are a crucial tool to allow for confident\ndecision-making in technology companies. A North Star metric is defined (such\nas long-term revenue or user retention), and system variants that statistically\nsignificantly improve on this metric in an A/B-test can be considered superior.\nNorth Star metrics are typically delayed and insensitive. As a result, the cost\nof experimentation is high: experiments need to run for a long time, and even\nthen, type-II errors (i.e. false negatives) are prevalent.\n  We propose to tackle this by learning metrics from short-term signals that\ndirectly maximise the statistical power they harness with respect to the North\nStar. We show that existing approaches are prone to overfitting, in that higher\naverage metric sensitivity does not imply improved type-II errors, and propose\nto instead minimise the $p$-values a metric would have produced on a log of\npast experiments. We collect such datasets from two social media applications\nwith over 160 million Monthly Active Users each, totalling over 153 A/B-pairs.\nEmpirical results show that we are able to increase statistical power by up to\n78% when using our learnt metrics stand-alone, and by up to 210% when used in\ntandem with the North Star. Alternatively, we can obtain constant statistical\npower at a sample size that is down to 12% of what the North Star requires,\nsignificantly reducing the cost of experimentation.\n","authors":["Olivier Jeunen","Aleksei Ustimenko"],"pdf_url":"https://arxiv.org/pdf/2402.03915v2.pdf","comment":"To appear in the Applied Data Science track at the ACM SIGKDD\n  Conference on Knowledge Discovery and Data Mining (KDD '24)"},{"id":"http://arxiv.org/abs/2406.09023v1","updated":"2024-06-13T11:56:20Z","published":"2024-06-13T11:56:20Z","title":"Schur's Positive-Definite Network: Deep Learning in the SPD cone with\n  structure","summary":"  Estimating matrices in the symmetric positive-definite (SPD) cone is of\ninterest for many applications ranging from computer vision to graph learning.\nWhile there exist various convex optimization-based estimators, they remain\nlimited in expressivity due to their model-based approach. The success of deep\nlearning has thus led many to use neural networks to learn to estimate SPD\nmatrices in a data-driven fashion. For learning structured outputs, one\npromising strategy involves architectures designed by unrolling iterative\nalgorithms, which potentially benefit from inductive bias properties. However,\ndesigning correct unrolled architectures for SPD learning is difficult: they\neither do not guarantee that their output has all the desired properties, rely\non heavy computations, or are overly restrained to specific matrices which\nhinders their expressivity. In this paper, we propose a novel and generic\nlearning module with guaranteed SPD outputs called SpodNet, that also enables\nlearning a larger class of functions than existing approaches. Notably, it\nsolves the challenging task of learning jointly SPD and sparse matrices. Our\nexperiments demonstrate the versatility of SpodNet layers.\n","authors":["Can Pouliquen","Mathurin Massias","Titouan Vayer"],"pdf_url":"https://arxiv.org/pdf/2406.09023v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.03099v3","updated":"2024-06-13T11:56:04Z","published":"2023-11-06T13:43:07Z","title":"Language Models are Super Mario: Absorbing Abilities from Homologous\n  Models as a Free Lunch","summary":"  In this paper, we unveil that Language Models (LMs) can acquire new\ncapabilities by assimilating parameters from homologous models without\nretraining or GPUs. We first introduce DARE to set most delta parameters (i.e.,\nthe disparity between fine-tuned and pre-trained parameters) to zeros without\naffecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly\nDrops delta parameters with a ratio $p$ And REscales the remaining ones by $1 /\n(1 - p)$ to approximate the original embeddings. Then, we use DARE as a\nversatile plug-in to sparsify delta parameters of multiple SFT homologous\nmodels for mitigating parameter interference and merge them into a single model\nby parameter fusing. We experiment with encoder- and decoder-based LMs, showing\nthat: (1) SFT delta parameter value ranges are typically small (within 0.002)\nwith extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of\nthem; (2) DARE can merge multiple task-specific LMs into one LM with diverse\ncapabilities. Notably, this phenomenon is more pronounced in large-scale LMs,\nwhere the merged LM reveals the potential to surpass the performance of any\nsource LM, providing a new discovery. We also utilize DARE to create a merged\nLM that ranks first among models with 7 billion parameters on the Open LLM\nLeaderboard.\n","authors":["Le Yu","Bowen Yu","Haiyang Yu","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2311.03099v3.pdf","comment":"Accepted at ICML 2024"},{"id":"http://arxiv.org/abs/2406.09014v1","updated":"2024-06-13T11:38:58Z","published":"2024-06-13T11:38:58Z","title":"Deep learning empowered sensor fusion to improve infant movement\n  classification","summary":"  There is a recent boom in the development of AI solutions to facilitate and\nenhance diagnostic procedures for established clinical tools. To assess the\nintegrity of the developing nervous system, the Prechtl general movement\nassessment (GMA) is recognized for its clinical value in the diagnosis of\nneurological impairments in early infancy. GMA has been increasingly augmented\nthrough machine learning approaches intending to scale-up its application,\ncircumvent costs in the training of human assessors and further standardize\nclassification of spontaneous motor patterns. Available deep learning tools,\nall of which are based on single sensor modalities, are however still\nconsiderably inferior to that of well-trained human assessors. These approaches\nare hardly comparable as all models are designed, trained and evaluated on\nproprietary/ silo-data sets. We propose a sensor fusion approach for assessing\nfidgety movements (FMs) comparing three different sensor modalities (pressure,\ninertial, and visual sensors). Various combinations and two sensor fusion\napproaches (late and early fusion) for infant movement classification were\ntested to evaluate whether a multi-sensor system outperforms single modality\nassessments. The performance of the three-sensor fusion (classification\naccuracy of 94.5\\%) was significantly higher than that of any single modality\nevaluated, suggesting the sensor fusion approach is a promising avenue for\nautomated classification of infant motor patterns. The development of a robust\nsensor fusion system may significantly enhance AI-based early recognition of\nneurofunctions, ultimately facilitating early implementation of automated\ndetection of neurodevelopmental conditions.\n","authors":["Tomas Kulvicius","Dajie Zhang","Luise Poustka","Sven Bölte","Lennart Jahn","Sarah Flügge","Marc Kraft","Markus Zweckstetter","Karin Nielsen-Saines","Florentin Wörgötter","Peter B Marschik"],"pdf_url":"https://arxiv.org/pdf/2406.09014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12701v4","updated":"2024-06-13T11:33:26Z","published":"2023-09-22T08:18:08Z","title":"Interpretable Decision Tree Search as a Markov Decision Process","summary":"  Finding an optimal decision tree for a supervised learning task is a\nchallenging combinatorial problem to solve at scale. It was recently proposed\nto frame the problem as a Markov Decision Problem (MDP) and use deep\nreinforcement learning to tackle scaling. Unfortunately, these methods are not\ncompetitive with the current branch-and-bound state-of-the-art. We propose\ninstead to scale the resolution of such MDPs using an information-theoretic\ntests generating function that heuristically, and dynamically for every state,\nlimits the set of admissible test actions to a few good candidates. As a\nsolver, we show empirically that our algorithm is at the very least competitive\nwith branch-and-bound alternatives. As a machine learning tool, a key advantage\nof our approach is to solve for multiple complexity-performance trade-offs at\nvirtually no additional cost. With such a set of solutions, a user can then\nselect the tree that generalizes best and which has the interpretability level\nthat best suits their needs, which no current branch-and-bound method allows.\n","authors":["Hector Kohler","Riad Akrour","Philippe Preux"],"pdf_url":"https://arxiv.org/pdf/2309.12701v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09009v1","updated":"2024-06-13T11:29:21Z","published":"2024-06-13T11:29:21Z","title":"Fredformer: Frequency Debiased Transformer for Time Series Forecasting","summary":"  The Transformer model has shown leading performance in time series\nforecasting. Nevertheless, in some complex scenarios, it tends to learn\nlow-frequency features in the data and overlook high-frequency features,\nshowing a frequency bias. This bias prevents the model from accurately\ncapturing important high-frequency data features. In this paper, we undertook\nempirical analyses to understand this bias and discovered that frequency bias\nresults from the model disproportionately focusing on frequency features with\nhigher energy. Based on our analysis, we formulate this bias and propose\nFredformer, a Transformer-based framework designed to mitigate frequency bias\nby learning features equally across different frequency bands. This approach\nprevents the model from overlooking lower amplitude features important for\naccurate forecasting. Extensive experiments show the effectiveness of our\nproposed approach, which can outperform other baselines in different real-world\ntime-series datasets. Furthermore, we introduce a lightweight variant of the\nFredformer with an attention matrix approximation, which achieves comparable\nperformance but with much fewer parameters and lower computation costs. The\ncode is available at: https://github.com/chenzRG/Fredformer\n","authors":["Xihao Piao","Zheng Chen","Taichi Murayama","Yasuko Matsubara","Yasushi Sakurai"],"pdf_url":"https://arxiv.org/pdf/2406.09009v1.pdf","comment":"This paper has been accepted by SIGKDD2024"},{"id":"http://arxiv.org/abs/2406.07213v2","updated":"2024-06-13T11:20:07Z","published":"2024-06-11T12:42:41Z","title":"Semantic-Aware Spectrum Sharing in Internet of Vehicles Based on Deep\n  Reinforcement Learning","summary":"  This work aims to investigate semantic communication in high-speed mobile\nInternet of vehicles (IoV) environments, with a focus on the spectrum sharing\nbetween vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I)\ncommunications. We specifically address spectrum scarcity and network traffic\nand then propose a semantic-aware spectrum sharing algorithm (SSS) based on the\ndeep reinforcement learning (DRL) soft actor-critic (SAC) approach. Firstly, we\ndelve into the extraction of semantic information. Secondly, we redefine\nmetrics for semantic information in V2V and V2I spectrum sharing in IoV\nenvironments, introducing high-speed semantic spectrum efficiency (HSSE) and\nsemantic transmission rate (HSR). Finally, we employ the SAC algorithm for\ndecision optimization in V2V and V2I spectrum sharing based on semantic\ninformation. This optimization encompasses the optimal link of V2V and V2I\nsharing strategies, the transmission power for vehicles sending semantic\ninformation and the length of transmitted semantic symbols, aiming at\nmaximizing HSSE of V2I and enhancing success rate of effective semantic\ninformation transmission (SRS) of V2V. Experimental results demonstrate that\nthe SSS algorithm outperforms other baseline algorithms, including other\ntraditional-communication-based spectrum sharing algorithms and spectrum\nsharing algorithm using other reinforcement learning approaches. The SSS\nalgorithm exhibits a 15% increase in HSSE and approximately a 7% increase in\nSRS.\n","authors":["Zhiyu Shao","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Jiangzhou Wang","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2406.07213v2.pdf","comment":"This paper has been submitted to IEEE Journal. The source code has\n  been released at:\n  https://github.com/qiongwu86/Semantic-Aware-Spectrum-Sharing-in-Internet-of-Vehicles-Based-on-Deep-Reinforcement-Learning"},{"id":"http://arxiv.org/abs/2406.09003v1","updated":"2024-06-13T11:12:46Z","published":"2024-06-13T11:12:46Z","title":"Enhancing Cross-Modal Fine-Tuning with Gradually Intermediate Modality\n  Generation","summary":"  Large-scale pretrained models have proven immensely valuable in handling\ndata-intensive modalities like text and image. However, fine-tuning these\nmodels for certain specialized modalities, such as protein sequence and cosmic\nray, poses challenges due to the significant modality discrepancy and scarcity\nof labeled data. In this paper, we propose an end-to-end method, PaRe, to\nenhance cross-modal fine-tuning, aiming to transfer a large-scale pretrained\nmodel to various target modalities. PaRe employs a gating mechanism to select\nkey patches from both source and target data. Through a modality-agnostic Patch\nReplacement scheme, these patches are preserved and combined to construct\ndata-rich intermediate modalities ranging from easy to hard. By gradually\nintermediate modality generation, we can not only effectively bridge the\nmodality gap to enhance stability and transferability of cross-modal\nfine-tuning, but also address the challenge of limited data in the target\nmodality by leveraging enriched intermediate modality data. Compared with\nhand-designed, general-purpose, task-specific, and state-of-the-art cross-modal\nfine-tuning approaches, PaRe demonstrates superior performance across three\nchallenging benchmarks, encompassing more than ten modalities.\n","authors":["Lincan Cai","Shuang Li","Wenxuan Ma","Jingxuan Kang","Binhui Xie","Zixun Sun","Chengwei Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.09003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03071v2","updated":"2024-06-13T11:06:45Z","published":"2024-03-05T15:59:54Z","title":"On a Neural Implementation of Brenier's Polar Factorization","summary":"  In 1991, Brenier proved a theorem that generalizes the polar decomposition\nfor square matrices -- factored as PSD $\\times$ unitary -- to any vector field\n$F:\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$. The theorem, known as the polar\nfactorization theorem, states that any field $F$ can be recovered as the\ncomposition of the gradient of a convex function $u$ with a measure-preserving\nmap $M$, namely $F=\\nabla u \\circ M$. We propose a practical implementation of\nthis far-reaching theoretical result, and explore possible uses within machine\nlearning. The theorem is closely related to optimal transport (OT) theory, and\nwe borrow from recent advances in the field of neural optimal transport to\nparameterize the potential $u$ as an input convex neural network. The map $M$\ncan be either evaluated pointwise using $u^*$, the convex conjugate of $u$,\nthrough the identity $M=\\nabla u^* \\circ F$, or learned as an auxiliary\nnetwork. Because $M$ is, in general, not injective, we consider the additional\ntask of estimating the ill-posed inverse map that can approximate the pre-image\nmeasure $M^{-1}$ using a stochastic generator. We illustrate possible\napplications of Brenier's polar factorization to non-convex optimization\nproblems, as well as sampling of densities that are not log-concave.\n","authors":["Nina Vesseron","Marco Cuturi"],"pdf_url":"https://arxiv.org/pdf/2403.03071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08993v1","updated":"2024-06-13T10:53:33Z","published":"2024-06-13T10:53:33Z","title":"Classic GNNs are Strong Baselines: Reassessing GNNs for Node\n  Classification","summary":"  Graph Transformers (GTs) have recently emerged as popular alternatives to\ntraditional message-passing Graph Neural Networks (GNNs), due to their\ntheoretically superior expressiveness and impressive performance reported on\nstandard node classification benchmarks, often significantly outperforming\nGNNs. In this paper, we conduct a thorough empirical analysis to reevaluate the\nperformance of three classic GNN models (GCN, GAT, and GraphSAGE) against GTs.\nOur findings suggest that the previously reported superiority of GTs may have\nbeen overstated due to suboptimal hyperparameter configurations in GNNs.\nRemarkably, with slight hyperparameter tuning, these classic GNN models achieve\nstate-of-the-art performance, matching or even exceeding that of recent GTs\nacross 17 out of the 18 diverse datasets examined. Additionally, we conduct\ndetailed ablation studies to investigate the influence of various GNN\nconfigurations, such as normalization, dropout, residual connections, network\ndepth, and jumping knowledge mode, on node classification performance. Our\nstudy aims to promote a higher standard of empirical rigor in the field of\ngraph machine learning, encouraging more accurate comparisons and evaluations\nof model capabilities.\n","authors":["Yuankai Luo","Lei Shi","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2406.08993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11963v3","updated":"2024-06-13T10:48:08Z","published":"2024-01-22T14:06:37Z","title":"Bridging Evolutionary Algorithms and Reinforcement Learning: A\n  Comprehensive Survey on Hybrid Algorithms","summary":"  Evolutionary Reinforcement Learning (ERL), which integrates Evolutionary\nAlgorithms (EAs) and Reinforcement Learning (RL) for optimization, has\ndemonstrated remarkable performance advancements. By fusing both approaches,\nERL has emerged as a promising research direction. This survey offers a\ncomprehensive overview of the diverse research branches in ERL. Specifically,\nwe systematically summarize recent advancements in related algorithms and\nidentify three primary research directions: EA-assisted Optimization of RL,\nRL-assisted Optimization of EA, and synergistic optimization of EA and RL.\nFollowing that, we conduct an in-depth analysis of each research direction,\norganizing multiple research branches. We elucidate the problems that each\nbranch aims to tackle and how the integration of EAs and RL addresses these\nchallenges. In conclusion, we discuss potential challenges and prospective\nfuture research directions across various research directions. To facilitate\nresearchers in delving into ERL, we organize the algorithms and codes involved\non https://github.com/yeshenpy/Awesome-Evolutionary-Reinforcement-Learning.\n","authors":["Pengyi Li","Jianye Hao","Hongyao Tang","Xian Fu","Yan Zheng","Ke Tang"],"pdf_url":"https://arxiv.org/pdf/2401.11963v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08990v1","updated":"2024-06-13T10:38:38Z","published":"2024-06-13T10:38:38Z","title":"BTS: Building Timeseries Dataset: Empowering Large-Scale Building\n  Analytics","summary":"  Buildings play a crucial role in human well-being, influencing occupant\ncomfort, health, and safety. Additionally, they contribute significantly to\nglobal energy consumption, accounting for one-third of total energy usage, and\ncarbon emissions. Optimizing building performance presents a vital opportunity\nto combat climate change and promote human flourishing. However, research in\nbuilding analytics has been hampered by the lack of accessible, available, and\ncomprehensive real-world datasets on multiple building operations. In this\npaper, we introduce the Building TimeSeries (BTS) dataset. Our dataset covers\nthree buildings over a three-year period, comprising more than ten thousand\ntimeseries data points with hundreds of unique ontologies. Moreover, the\nmetadata is standardized using the Brick schema. To demonstrate the utility of\nthis dataset, we performed benchmarks on two tasks: timeseries ontology\nclassification and zero-shot forecasting. These tasks represent an essential\ninitial step in addressing challenges related to interoperability in building\nanalytics. Access to the dataset and the code used for benchmarking are\navailable here: https://github.com/cruiseresearchgroup/DIEF_BTS .\n","authors":["Arian Prabowo","Xiachong Lin","Imran Razzak","Hao Xue","Emily W. Yap","Matthew Amos","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2406.08990v1.pdf","comment":"21 pages, 2 figures, 9 tables, under review"},{"id":"http://arxiv.org/abs/2406.07875v2","updated":"2024-06-13T10:29:16Z","published":"2024-06-12T05:08:51Z","title":"Carbon Market Simulation with Adaptive Mechanism Design","summary":"  A carbon market is a market-based tool that incentivizes economic agents to\nalign individual profits with the global utility, i.e., reducing carbon\nemissions to tackle climate change. Cap and trade stands as a critical\nprinciple based on allocating and trading carbon allowances (carbon emission\ncredit), enabling economic agents to follow planned emissions and penalizing\nexcess emissions. A central authority is responsible for introducing and\nallocating those allowances in cap and trade. However, the complexity of carbon\nmarket dynamics makes accurate simulation intractable, which in turn hinders\nthe design of effective allocation strategies. To address this, we propose an\nadaptive mechanism design framework, simulating the market using hierarchical,\nmodel-free multi-agent reinforcement learning (MARL). Government agents\nallocate carbon credits, while enterprises engage in economic activities and\ncarbon trading. This framework illustrates agents' behavior comprehensively.\nNumerical results show MARL enables government agents to balance productivity,\nequality, and carbon emissions. Our project is available at\nhttps://github.com/xwanghan/Carbon-Simulator.\n","authors":["Han Wang","Wenhao Li","Hongyuan Zha","Baoxiang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07875v2.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.08980v1","updated":"2024-06-13T10:23:52Z","published":"2024-06-13T10:23:52Z","title":"From Theory to Therapy: Reframing SBDD Model Evaluation via Practical\n  Metrics","summary":"  Recent advancements in structure-based drug design (SBDD) have significantly\nenhanced the efficiency and precision of drug discovery by generating molecules\ntailored to bind specific protein pockets. Despite these technological strides,\ntheir practical application in real-world drug development remains challenging\ndue to the complexities of synthesizing and testing these molecules. The\nreliability of the Vina docking score, the current standard for assessing\nbinding abilities, is increasingly questioned due to its susceptibility to\noverfitting. To address these limitations, we propose a comprehensive\nevaluation framework that includes assessing the similarity of generated\nmolecules to known active compounds, introducing a virtual screening-based\nmetric for practical deployment capabilities, and re-evaluating binding\naffinity more rigorously. Our experiments reveal that while current SBDD models\nachieve high Vina scores, they fall short in practical usability metrics,\nhighlighting a significant gap between theoretical predictions and real-world\napplicability. Our proposed metrics and dataset aim to bridge this gap,\nenhancing the practical applicability of future SBDD models and aligning them\nmore closely with the needs of pharmaceutical research and development.\n","authors":["Bowen Gao","Haichuan Tan","Yanwen Huang","Minsi Ren","Xiao Huang","Wei-Ying Ma","Ya-Qin Zhang","Yanyan Lan"],"pdf_url":"https://arxiv.org/pdf/2406.08980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06784v2","updated":"2024-06-13T10:09:51Z","published":"2024-02-09T21:17:31Z","title":"Transfer learning with generative models for object detection on limited\n  datasets","summary":"  The availability of data is limited in some fields, especially for object\ndetection tasks, where it is necessary to have correctly labeled bounding boxes\naround each object. A notable example of such data scarcity is found in the\ndomain of marine biology, where it is useful to develop methods to\nautomatically detect submarine species for environmental monitoring. To address\nthis data limitation, the state-of-the-art machine learning strategies employ\ntwo main approaches. The first involves pretraining models on existing datasets\nbefore generalizing to the specific domain of interest. The second strategy is\nto create synthetic datasets specifically tailored to the target domain using\nmethods like copy-paste techniques or ad-hoc simulators. The first strategy\noften faces a significant domain shift, while the second demands custom\nsolutions crafted for the specific task. In response to these challenges, here\nwe propose a transfer learning framework that is valid for a generic scenario.\nIn this framework, generated images help to improve the performances of an\nobject detector in a few-real data regime. This is achieved through a\ndiffusion-based generative model that was pretrained on large generic datasets.\nWith respect to the state-of-the-art, we find that it is not necessary to fine\ntune the generative model on the specific domain of interest. We believe that\nthis is an important advance because it mitigates the labor-intensive task of\nmanual labeling the images in object detection tasks. We validate our approach\nfocusing on fishes in an underwater environment, and on the more common domain\nof cars in an urban setting. Our method achieves detection performance\ncomparable to models trained on thousands of images, using only a few hundreds\nof input data. Our results pave the way for new generative AI-based protocols\nfor machine learning applications in various domains.\n","authors":["Matteo Paiano","Stefano Martina","Carlotta Giannelli","Filippo Caruso"],"pdf_url":"https://arxiv.org/pdf/2402.06784v2.pdf","comment":"28 pages, 16 figures, 1 table"},{"id":"http://arxiv.org/abs/2406.08973v1","updated":"2024-06-13T10:04:17Z","published":"2024-06-13T10:04:17Z","title":"XLand-100B: A Large-Scale Multi-Task Dataset for In-Context\n  Reinforcement Learning","summary":"  Following the success of the in-context learning paradigm in large-scale\nlanguage and computer vision models, the recently emerging field of in-context\nreinforcement learning is experiencing a rapid growth. However, its development\nhas been held back by the lack of challenging benchmarks, as all the\nexperiments have been carried out in simple environments and on small-scale\ndatasets. We present \\textbf{XLand-100B}, a large-scale dataset for in-context\nreinforcement learning based on the XLand-MiniGrid environment, as a first step\nto alleviate this problem. It contains complete learning histories for nearly\n$30,000$ different tasks, covering $100$B transitions and $2.5$B episodes. It\ntook $50,000$ GPU hours to collect the dataset, which is beyond the reach of\nmost academic labs. Along with the dataset, we provide the utilities to\nreproduce or expand it even further. With this substantial effort, we aim to\ndemocratize research in the rapidly growing field of in-context reinforcement\nlearning and provide a solid foundation for further scaling. The code is\nopen-source and available under Apache 2.0 licence at\nhttps://github.com/dunno-lab/xland-minigrid-datasets.\n","authors":["Alexander Nikulin","Ilya Zisman","Alexey Zemtsov","Viacheslav Sinii","Vladislav Kurenkov","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2406.08973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08966v1","updated":"2024-06-13T09:52:44Z","published":"2024-06-13T09:52:44Z","title":"Separation Power of Equivariant Neural Networks","summary":"  The separation power of a machine learning model refers to its capacity to\ndistinguish distinct inputs, and it is often employed as a proxy for its\nexpressivity. In this paper, we propose a theoretical framework to investigate\nthe separation power of equivariant neural networks with point-wise\nactivations. Using the proposed framework, we can derive an explicit\ndescription of inputs indistinguishable by a family of neural networks with\ngiven architecture, demonstrating that it remains unaffected by the choice of\nnon-polynomial activation function employed. We are able to understand the role\nplayed by activation functions in separability. Indeed, we show that all\nnon-polynomial activations, such as ReLU and sigmoid, are equivalent in terms\nof expressivity, and that they reach maximum discrimination capacity. We\ndemonstrate how assessing the separation power of an equivariant neural network\ncan be simplified to evaluating the separation power of minimal\nrepresentations. We conclude by illustrating how these minimal components form\na hierarchy in separation power.\n","authors":["Marco Pacini","Xiaowen Dong","Bruno Lepri","Gabriele Santin"],"pdf_url":"https://arxiv.org/pdf/2406.08966v1.pdf","comment":"9 pages of main text, 2 figures"},{"id":"http://arxiv.org/abs/2406.08961v1","updated":"2024-06-13T09:49:58Z","published":"2024-06-13T09:49:58Z","title":"SIU: A Million-Scale Structural Small Molecule-Protein Interaction\n  Dataset for Unbiased Bioactivity Prediction","summary":"  Small molecules play a pivotal role in modern medicine, and scrutinizing\ntheir interactions with protein targets is essential for the discovery and\ndevelopment of novel, life-saving therapeutics. The term \"bioactivity\"\nencompasses various biological effects resulting from these interactions,\nincluding both binding and functional responses. The magnitude of bioactivity\ndictates the therapeutic or toxic pharmacological outcomes of small molecules,\nrendering accurate bioactivity prediction crucial for the development of safe\nand effective drugs. However, existing structural datasets of small\nmolecule-protein interactions are often limited in scale and lack\nsystematically organized bioactivity labels, thereby impeding our understanding\nof these interactions and precise bioactivity prediction. In this study, we\nintroduce a comprehensive dataset of small molecule-protein interactions,\nconsisting of over a million binding structures, each annotated with real\nbiological activity labels. This dataset is designed to facilitate unbiased\nbioactivity prediction. We evaluated several classical models on this dataset,\nand the results demonstrate that the task of unbiased bioactivity prediction is\nchallenging yet essential.\n","authors":["Yanwen Huang","Bowen Gao","Yinjun Jia","Hongbo Ma","Wei-Ying Ma","Ya-Qin Zhang","Yanyan Lan"],"pdf_url":"https://arxiv.org/pdf/2406.08961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08958v1","updated":"2024-06-13T09:36:27Z","published":"2024-06-13T09:36:27Z","title":"An Unsupervised Approach to Achieve Supervised-Level Explainability in\n  Healthcare Records","summary":"  Electronic healthcare records are vital for patient safety as they document\nconditions, plans, and procedures in both free text and medical codes. Language\nmodels have significantly enhanced the processing of such records, streamlining\nworkflows and reducing manual data entry, thereby saving healthcare providers\nsignificant resources. However, the black-box nature of these models often\nleaves healthcare professionals hesitant to trust them. State-of-the-art\nexplainability methods increase model transparency but rely on human-annotated\nevidence spans, which are costly. In this study, we propose an approach to\nproduce plausible and faithful explanations without needing such annotations.\nWe demonstrate on the automated medical coding task that adversarial robustness\ntraining improves explanation plausibility and introduce AttInGrad, a new\nexplanation method superior to previous ones. By combining both contributions\nin a fully unsupervised setup, we produce explanations of comparable quality,\nor better, to that of a supervised approach. We release our code and model\nweights.\n","authors":["Joakim Edin","Maria Maistro","Lars Maaløe","Lasse Borgholt","Jakob D. Havtorn","Tuukka Ruotsalo"],"pdf_url":"https://arxiv.org/pdf/2406.08958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08953v1","updated":"2024-06-13T09:32:40Z","published":"2024-06-13T09:32:40Z","title":"Preserving Identity with Variational Score for General-purpose 3D\n  Editing","summary":"  We present Piva (Preserving Identity with Variational Score Distillation), a\nnovel optimization-based method for editing images and 3D models based on\ndiffusion models. Specifically, our approach is inspired by the recently\nproposed method for 2D image editing - Delta Denoising Score (DDS). We pinpoint\nthe limitations in DDS for 2D and 3D editing, which causes detail loss and\nover-saturation. To address this, we propose an additional score distillation\nterm that enforces identity preservation. This results in a more stable editing\nprocess, gradually optimizing NeRF models to match target prompts while\nretaining crucial input characteristics. We demonstrate the effectiveness of\nour approach in zero-shot image and neural field editing. Our method\nsuccessfully alters visual attributes, adds both subtle and substantial\nstructural elements, translates shapes, and achieves competitive results on\nstandard 2D and 3D editing benchmarks. Additionally, our method imposes no\nconstraints like masking or pre-training, making it compatible with a wide\nrange of pre-trained diffusion models. This allows for versatile editing\nwithout needing neural field-to-mesh conversion, offering a more user-friendly\nexperience.\n","authors":["Duong H. Le","Tuan Pham","Aniruddha Kembhavi","Stephan Mandt","Wei-Chiu Ma","Jiasen Lu"],"pdf_url":"https://arxiv.org/pdf/2406.08953v1.pdf","comment":"22 pages, 14 figures"},{"id":"http://arxiv.org/abs/2402.04655v3","updated":"2024-06-13T09:30:36Z","published":"2024-02-07T08:42:48Z","title":"Open-Vocabulary Calibration for Fine-tuned CLIP","summary":"  Vision-language models (VLMs) have emerged as formidable tools, showing their\nstrong capability in handling various open-vocabulary tasks in image\nrecognition, text-driven visual content generation, and visual chatbots, to\nname a few. In recent years, considerable efforts and resources have been\ndevoted to adaptation methods for improving downstream performance of VLMs,\nparticularly on parameter-efficient fine-tuning methods like prompt learning.\nHowever, a crucial aspect that has been largely overlooked is the confidence\ncalibration problem in fine-tuned VLMs, which could greatly reduce reliability\nwhen deploying such models in the real world. This paper bridges the gap by\nsystematically investigating the confidence calibration problem in the context\nof prompt learning and reveals that existing calibration methods are\ninsufficient to address the problem, especially in the open-vocabulary setting.\nTo solve the problem, we present a simple and effective approach called\nDistance-Aware Calibration (DAC), which is based on scaling the temperature\nusing as guidance the distance between predicted text labels and base classes.\nThe experiments with 7 distinct prompt learning methods applied across 11\ndiverse downstream datasets demonstrate the effectiveness of DAC, which\nachieves high efficacy without sacrificing the inference speed. Our code is\navailable at [this https\nURL](https://github.com/ml-stat-Sustech/CLIP_Calibration).\n","authors":["Shuoyuan Wang","Jindong Wang","Guoqing Wang","Bob Zhang","Kaiyang Zhou","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2402.04655v3.pdf","comment":"Accepted by ICML 2024"},{"id":"http://arxiv.org/abs/2310.01959v3","updated":"2024-06-13T09:22:57Z","published":"2023-10-03T11:10:21Z","title":"Beyond Labeling Oracles: What does it mean to steal ML models?","summary":"  Model extraction attacks are designed to steal trained models with only query\naccess, as is often provided through APIs that ML-as-a-Service providers offer.\nMachine Learning (ML) models are expensive to train, in part because data is\nhard to obtain, and a primary incentive for model extraction is to acquire a\nmodel while incurring less cost than training from scratch. Literature on model\nextraction commonly claims or presumes that the attacker is able to save on\nboth data acquisition and labeling costs. We thoroughly evaluate this\nassumption and find that the attacker often does not. This is because current\nattacks implicitly rely on the adversary being able to sample from the victim\nmodel's data distribution. We thoroughly research factors influencing the\nsuccess of model extraction. We discover that prior knowledge of the attacker,\ni.e., access to in-distribution data, dominates other factors like the attack\npolicy the adversary follows to choose which queries to make to the victim\nmodel API. Our findings urge the community to redefine the adversarial goals of\nME attacks as current evaluation methods misinterpret the ME performance.\n","authors":["Avital Shafran","Ilia Shumailov","Murat A. Erdogdu","Nicolas Papernot"],"pdf_url":"https://arxiv.org/pdf/2310.01959v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09812v2","updated":"2024-06-13T09:20:40Z","published":"2024-04-15T14:10:06Z","title":"Solving the Tree Containment Problem Using Graph Neural Networks","summary":"  Tree Containment is a fundamental problem in phylogenetics useful for\nverifying a proposed phylogenetic network, representing the evolutionary\nhistory of certain species. Tree Containment asks whether the given\nphylogenetic tree (for instance, constructed from a DNA fragment showing\ntree-like evolution) is contained in the given phylogenetic network. In the\ngeneral case, this is an NP-complete problem. We propose to solve it\napproximately using Graph Neural Networks. In particular, we propose to combine\nthe given network and the tree and apply a Graph Neural Network to this\nnetwork-tree graph. This way, we achieve the capability of solving the tree\ncontainment instances representing a larger number of species than the\ninstances contained in the training dataset (i.e., our algorithm has the\ninductive learning ability). Our algorithm demonstrates an accuracy of over\n$95\\%$ in solving the tree containment problem on instances with up to 100\nleaves.\n","authors":["Arkadiy Dushatskiy","Esther Julien","Leen Stougie","Leo van Iersel"],"pdf_url":"https://arxiv.org/pdf/2404.09812v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02497v3","updated":"2024-06-13T09:17:55Z","published":"2023-04-05T15:12:40Z","title":"Hyper-parameter Tuning for Adversarially Robust Models","summary":"  This work focuses on the problem of hyper-parameter tuning (HPT) for robust\n(i.e., adversarially trained) models, shedding light on the new challenges and\nopportunities arising during the HPT process for robust models. To this end, we\nconduct an extensive experimental study based on 3 popular deep models, in\nwhich we explore exhaustively 9 (discretized) HPs, 2 fidelity dimensions, and 2\nattack bounds, for a total of 19208 configurations (corresponding to 50\nthousand GPU hours). Through this study, we show that the complexity of the HPT\nproblem is further exacerbated in adversarial settings due to the need to\nindependently tune the HPs used during standard and adversarial training:\nsucceeding in doing so (i.e., adopting different HP settings in both phases)\ncan lead to a reduction of up to 80% and 43% of the error for clean and\nadversarial inputs, respectively. On the other hand, we also identify new\nopportunities to reduce the cost of HPT for robust models. Specifically, we\npropose to leverage cheap adversarial training methods to obtain inexpensive,\nyet highly correlated, estimations of the quality achievable using\nstate-of-the-art methods. We show that, by exploiting this novel idea in\nconjunction with a recent multi-fidelity optimizer (taKG), the efficiency of\nthe HPT process can be enhanced by up to 2.1x.\n","authors":["Pedro Mendes","Paolo Romano","David Garlan"],"pdf_url":"https://arxiv.org/pdf/2304.02497v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08943v1","updated":"2024-06-13T09:12:26Z","published":"2024-06-13T09:12:26Z","title":"Neural NeRF Compression","summary":"  Neural Radiance Fields (NeRFs) have emerged as powerful tools for capturing\ndetailed 3D scenes through continuous volumetric representations. Recent NeRFs\nutilize feature grids to improve rendering quality and speed; however, these\nrepresentations introduce significant storage overhead. This paper presents a\nnovel method for efficiently compressing a grid-based NeRF model, addressing\nthe storage overhead concern. Our approach is based on the non-linear transform\ncoding paradigm, employing neural compression for compressing the model's\nfeature grids. Due to the lack of training data involving many i.i.d scenes, we\ndesign an encoder-free, end-to-end optimized approach for individual scenes,\nusing lightweight decoders. To leverage the spatial inhomogeneity of the latent\nfeature grids, we introduce an importance-weighted rate-distortion objective\nand a sparse entropy model employing a masking mechanism. Our experimental\nresults validate that our proposed method surpasses existing works in terms of\ngrid-based NeRF compression efficacy and reconstruction quality.\n","authors":["Tuan Pham","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2406.08943v1.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2406.08938v1","updated":"2024-06-13T09:07:22Z","published":"2024-06-13T09:07:22Z","title":"Mirror and Preconditioned Gradient Descent in Wasserstein Space","summary":"  As the problem of minimizing functionals on the Wasserstein space encompasses\nmany applications in machine learning, different optimization algorithms on\n$\\mathbb{R}^d$ have received their counterpart analog on the Wasserstein space.\nWe focus here on lifting two explicit algorithms: mirror descent and\npreconditioned gradient descent. These algorithms have been introduced to\nbetter capture the geometry of the function to minimize and are provably\nconvergent under appropriate (namely relative) smoothness and convexity\nconditions. Adapting these notions to the Wasserstein space, we prove\nguarantees of convergence of some Wasserstein-gradient-based discrete-time\nschemes for new pairings of objective functionals and regularizers. The\ndifficulty here is to carefully select along which curves the functionals\nshould be smooth and convex. We illustrate the advantages of adapting the\ngeometry induced by the regularizer on ill-conditioned optimization tasks, and\nshowcase the improvement of choosing different discrepancies and geometries in\na computational biology task of aligning single-cells.\n","authors":["Clément Bonet","Théo Uscidda","Adam David","Pierre-Cyril Aubin-Frankowski","Anna Korba"],"pdf_url":"https://arxiv.org/pdf/2406.08938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08933v1","updated":"2024-06-13T09:03:53Z","published":"2024-06-13T09:03:53Z","title":"LaCoOT: Layer Collapse through Optimal Transport","summary":"  Although deep neural networks are well-known for their remarkable performance\nin tackling complex tasks, their hunger for computational resources remains a\nsignificant hurdle, posing energy-consumption issues and restricting their\ndeployment on resource-constrained devices, which stalls their widespread\nadoption. In this paper, we present an optimal transport method to reduce the\ndepth of over-parametrized deep neural networks, alleviating their\ncomputational burden. More specifically, we propose a new regularization\nstrategy based on the Max-Sliced Wasserstein distance to minimize the distance\nbetween the intermediate feature distributions in the neural network. We show\nthat minimizing this distance enables the complete removal of intermediate\nlayers in the network, with almost no performance loss and without requiring\nany finetuning. We assess the effectiveness of our method on traditional image\nclassification setups. We commit to releasing the source code upon acceptance\nof the article.\n","authors":["Victor Quétu","Nour Hezbri","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2406.08933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09068v2","updated":"2024-06-13T09:00:29Z","published":"2023-03-16T04:02:17Z","title":"Vortex Feature Positioning: Bridging Tabular IIoT Data and Image-Based\n  Deep Learning","summary":"  Tabular data from IIoT devices are typically analyzed using decision\ntree-based machine learning techniques, which struggle with high-dimensional\nand numeric data. To overcome these limitations, techniques converting tabular\ndata into images have been developed, leveraging the strengths of image-based\ndeep learning approaches such as Convolutional Neural Networks. These methods\ncluster similar features into distinct image areas with fixed sizes, regardless\nof the number of features, resembling actual photographs. However, this\nincreases the possibility of overfitting, as similar features, when selected\ncarefully in a tabular format, are often discarded to prevent this issue.\nAdditionally, fixed image sizes can lead to wasted pixels with fewer features,\nresulting in computational inefficiency. We introduce Vortex Feature\nPositioning (VFP) to address these issues. VFP arranges features based on their\ncorrelation, spacing similar ones in a vortex pattern from the image center,\nwith the image size determined by the attribute count. VFP outperforms\ntraditional machine learning methods and existing conversion techniques in\ntests across seven datasets with varying real-valued attributes.\n","authors":["Jong-Ik Park","Sihoon Seong","JunKyu Lee","Cheol-Ho Hong"],"pdf_url":"https://arxiv.org/pdf/2303.09068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08930v1","updated":"2024-06-13T08:58:59Z","published":"2024-06-13T08:58:59Z","title":"Efficient Multi-View Fusion and Flexible Adaptation to View Missing in\n  Cardiovascular System Signals","summary":"  The progression of deep learning and the widespread adoption of sensors have\nfacilitated automatic multi-view fusion (MVF) about the cardiovascular system\n(CVS) signals. However, prevalent MVF model architecture often amalgamates CVS\nsignals from the same temporal step but different views into a unified\nrepresentation, disregarding the asynchronous nature of cardiovascular events\nand the inherent heterogeneity across views, leading to catastrophic view\nconfusion. Efficient training strategies specifically tailored for MVF models\nto attain comprehensive representations need simultaneous consideration.\nCrucially, real-world data frequently arrives with incomplete views, an aspect\nrarely noticed by researchers. Thus, the View-Centric Transformer (VCT) and\nMultitask Masked Autoencoder (M2AE) are specifically designed to emphasize the\ncentrality of each view and harness unlabeled data to achieve superior fused\nrepresentations. Additionally, we systematically define the missing-view\nproblem for the first time and introduce prompt techniques to aid pretrained\nMVF models in flexibly adapting to various missing-view scenarios. Rigorous\nexperiments involving atrial fibrillation detection, blood pressure estimation,\nand sleep staging-typical health monitoring tasks-demonstrate the remarkable\nadvantage of our method in MVF compared to prevailing methodologies. Notably,\nthe prompt technique requires finetuning less than 3% of the entire model's\ndata, substantially fortifying the model's resilience to view missing while\ncircumventing the need for complete retraining. The results demonstrate the\neffectiveness of our approaches, highlighting their potential for practical\napplications in cardiovascular health monitoring. Codes and models are released\nat URL.\n","authors":["Qihan Hu","Daomiao Wang","Hong Wu","Jian Liu","Cuiwei Yang"],"pdf_url":"https://arxiv.org/pdf/2406.08930v1.pdf","comment":"16 pages,12 figures"},{"id":"http://arxiv.org/abs/2406.08929v1","updated":"2024-06-13T08:58:45Z","published":"2024-06-13T08:58:45Z","title":"Step-by-Step Diffusion: An Elementary Tutorial","summary":"  We present an accessible first course on diffusion models and flow matching\nfor machine learning, aimed at a technical audience with no diffusion\nexperience. We try to simplify the mathematical details as much as possible\n(sometimes heuristically), while retaining enough precision to derive correct\nalgorithms.\n","authors":["Preetum Nakkiran","Arwen Bradley","Hattie Zhou","Madhu Advani"],"pdf_url":"https://arxiv.org/pdf/2406.08929v1.pdf","comment":"35 pages, 11 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2406.09412v1","updated":"2024-06-13T17:59:53Z","published":"2024-06-13T17:59:53Z","title":"Explore the Limits of Omni-modal Pretraining at Scale","summary":"  We propose to build omni-modal intelligence, which is capable of\nunderstanding any modality and learning universal representations. In specific,\nwe propose a scalable pretraining paradigm, named Multimodal Context (MiCo),\nwhich can scale up the numbers of modalities and amount of data, together with\nthe model parameters, in the pretraining process. With MiCo, the pretrained\nmodels show significant emergent abilities in multimodal learning, which are\nevaluated on the following tasks: i) single-modality perception benchmarks of\n10 different modalities, ii) 25 cross-modality understanding tasks of\nretrieval, question-answering, captioning, and iii) 18 multimodal large\nlanguage model benchmarks. Our models establish 37 new records for\nstate-of-the-art performance. We hope that our research could contribute to the\ndevelopment of omni-modal intelligence. Code and Models are at\nhttps://github.com/invictus717/MiCo\n","authors":["Yiyuan Zhang","Handong Li","Jing Liu","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2406.09412v1.pdf","comment":"Project Website: https://invictus717.github.io/MiCo/"},{"id":"http://arxiv.org/abs/2406.09326v1","updated":"2024-06-13T17:05:23Z","published":"2024-06-13T17:05:23Z","title":"PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in\n  Piano Performance","summary":"  Recently, artificial intelligence techniques for education have been received\nincreasing attentions, while it still remains an open problem to design the\neffective music instrument instructing systems. Although key presses can be\ndirectly derived from sheet music, the transitional movements among key presses\nrequire more extensive guidance in piano performance. In this work, we\nconstruct a piano-hand motion generation benchmark to guide hand movements and\nfingerings for piano playing. To this end, we collect an annotated dataset,\nPianoMotion10M, consisting of 116 hours of piano playing videos from a\nbird's-eye view with 10 million annotated hand poses. We also introduce a\npowerful baseline model that generates hand motions from piano audios through a\nposition predictor and a position-guided gesture generator. Furthermore, a\nseries of evaluation metrics are designed to assess the performance of the\nbaseline model, including motion similarity, smoothness, positional accuracy of\nleft and right hands, and overall fidelity of movement distribution. Despite\nthat piano key presses with respect to music scores or audios are already\naccessible, PianoMotion10M aims to provide guidance on piano fingering for\ninstruction purposes. The dataset and source code can be accessed at\nhttps://agnjason.github.io/PianoMotion-page.\n","authors":["Qijun Gan","Song Wang","Shengtao Wu","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.09326v1.pdf","comment":"Codes and Dataset: https://agnjason.github.io/PianoMotion-page"},{"id":"http://arxiv.org/abs/2406.09156v1","updated":"2024-06-13T14:18:56Z","published":"2024-06-13T14:18:56Z","title":"Towards Multilingual Audio-Visual Question Answering","summary":"  In this paper, we work towards extending Audio-Visual Question Answering\n(AVQA) to multilingual settings. Existing AVQA research has predominantly\nrevolved around English and replicating it for addressing AVQA in other\nlanguages requires a substantial allocation of resources. As a scalable\nsolution, we leverage machine translation and present two multilingual AVQA\ndatasets for eight languages created from existing benchmark AVQA datasets.\nThis prevents extra human annotation efforts of collecting questions and\nanswers manually. To this end, we propose, MERA framework, by leveraging\nstate-of-the-art (SOTA) video, audio, and textual foundation models for AVQA in\nmultiple languages. We introduce a suite of models namely MERA-L, MERA-C,\nMERA-T with varied model architectures to benchmark the proposed datasets. We\nbelieve our work will open new research directions and act as a reference\nbenchmark for future works in multilingual AVQA.\n","authors":["Orchid Chetia Phukan","Priyabrata Mallick","Swarup Ranjan Behera","Aalekhya Satya Narayani","Arun Balaji Buduru","Rajesh Sharma"],"pdf_url":"https://arxiv.org/pdf/2406.09156v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2402.09871v4","updated":"2024-06-13T13:36:28Z","published":"2024-02-15T10:55:01Z","title":"MuChin: A Chinese Colloquial Description Benchmark for Evaluating\n  Language Models in the Field of Music","summary":"  The rapidly evolving multimodal Large Language Models (LLMs) urgently require\nnew benchmarks to uniformly evaluate their performance on understanding and\ntextually describing music. However, due to semantic gaps between Music\nInformation Retrieval (MIR) algorithms and human understanding, discrepancies\nbetween professionals and the public, and low precision of annotations,\nexisting music description datasets cannot serve as benchmarks. To this end, we\npresent MuChin, the first open-source music description benchmark in Chinese\ncolloquial language, designed to evaluate the performance of multimodal LLMs in\nunderstanding and describing music. We established the Caichong Music\nAnnotation Platform (CaiMAP) that employs an innovative multi-person,\nmulti-stage assurance method, and recruited both amateurs and professionals to\nensure the precision of annotations and alignment with popular semantics.\nUtilizing this method, we built a dataset with multi-dimensional,\nhigh-precision music annotations, the Caichong Music Dataset (CaiMD), and\ncarefully selected 1,000 high-quality entries to serve as the test set for\nMuChin. Based on MuChin, we analyzed the discrepancies between professionals\nand amateurs in terms of music description, and empirically demonstrated the\neffectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed\nMuChin to evaluate existing music understanding models on their ability to\nprovide colloquial descriptions of music. All data related to the benchmark,\nalong with the scoring code and detailed appendices, have been open-sourced\n(https://github.com/CarlWangChina/MuChin/).\n","authors":["Zihao Wang","Shuyu Li","Tao Zhang","Qi Wang","Pengfei Yu","Jinyang Luo","Yan Liu","Ming Xi","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.09871v4.pdf","comment":"Accepted by International Joint Conference on Artificial Intelligence\n  2024 (IJCAI 2024)"},{"id":"http://arxiv.org/abs/2406.00017v2","updated":"2024-06-13T13:26:56Z","published":"2024-05-23T01:16:45Z","title":"PTA: Enhancing Multimodal Sentiment Analysis through Pipelined\n  Prediction and Translation-based Alignment","summary":"  Multimodal aspect-based sentiment analysis (MABSA) aims to understand\nopinions in a granular manner, advancing human-computer interaction and other\nfields. Traditionally, MABSA methods use a joint prediction approach to\nidentify aspects and sentiments simultaneously. However, we argue that joint\nmodels are not always superior. Our analysis shows that joint models struggle\nto align relevant text tokens with image patches, leading to misalignment and\nineffective image utilization.\n  In contrast, a pipeline framework first identifies aspects through MATE\n(Multimodal Aspect Term Extraction) and then aligns these aspects with image\npatches for sentiment classification (MASC: Multimodal Aspect-Oriented\nSentiment Classification). This method is better suited for multimodal\nscenarios where effective image use is crucial. We present three key\nobservations: (a) MATE and MASC have different feature requirements, with MATE\nfocusing on token-level features and MASC on sequence-level features; (b) the\naspect identified by MATE is crucial for effective image utilization; and (c)\nimages play a trivial role in previous MABSA methods due to high noise.\n  Based on these observations, we propose a pipeline framework that first\npredicts the aspect and then uses translation-based alignment (TBA) to enhance\nmultimodal semantic consistency for better image utilization. Our method\nachieves state-of-the-art (SOTA) performance on widely used MABSA datasets\nTwitter-15 and Twitter-17. This demonstrates the effectiveness of the pipeline\napproach and its potential to provide valuable insights for future MABSA\nresearch.\n  For reproducibility, the code and checkpoint will be released.\n","authors":["Shezheng Song","Shasha Li","Shan Zhao","Chengyu Wang","Xiaopeng Li","Jie Yu","Qian Wan","Jun Ma","Tianwei Yan","Wentao Ma","Xiaoguang Mao"],"pdf_url":"https://arxiv.org/pdf/2406.00017v2.pdf","comment":"Code will be released upon publication"},{"id":"http://arxiv.org/abs/2309.07115v2","updated":"2024-06-13T13:08:24Z","published":"2023-09-13T17:45:41Z","title":"Getting More for Less: Using Weak Labels and AV-Mixup for Robust\n  Audio-Visual Speaker Verification","summary":"  Distance Metric Learning (DML) has typically dominated the audio-visual\nspeaker verification problem space, owing to strong performance in new and\nunseen classes. In our work, we explored multitask learning techniques to\nfurther enhance DML, and show that an auxiliary task with even weak labels can\nincrease the quality of the learned speaker representation without increasing\nmodel complexity during inference. We also extend the Generalized End-to-End\nLoss (GE2E) to multimodal inputs and demonstrate that it can achieve\ncompetitive performance in an audio-visual space. Finally, we introduce\nAV-Mixup, a multimodal augmentation technique during training time that has\nshown to reduce speaker overfit. Our network achieves state of the art\nperformance for speaker verification, reporting 0.244%, 0.252%, 0.441% Equal\nError Rate (EER) on the VoxCeleb1-O/E/H test sets, which is to our knowledge,\nthe best published results on VoxCeleb1-E and VoxCeleb1-H.\n","authors":["Anith Selvakumar","Homa Fashandi"],"pdf_url":"https://arxiv.org/pdf/2309.07115v2.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.08907v1","updated":"2024-06-13T08:06:57Z","published":"2024-06-13T08:06:57Z","title":"Dual Attribute-Spatial Relation Alignment for 3D Visual Grounding","summary":"  3D visual grounding is an emerging research area dedicated to making\nconnections between the 3D physical world and natural language, which is\ncrucial for achieving embodied intelligence. In this paper, we propose DASANet,\na Dual Attribute-Spatial relation Alignment Network that separately models and\naligns object attributes and spatial relation features between language and 3D\nvision modalities. We decompose both the language and 3D point cloud input into\ntwo separate parts and design a dual-branch attention module to separately\nmodel the decomposed inputs while preserving global context in\nattribute-spatial feature fusion by cross attentions. Our DASANet achieves the\nhighest grounding accuracy 65.1% on the Nr3D dataset, 1.3% higher than the best\ncompetitor. Besides, the visualization of the two branches proves that our\nmethod is efficient and highly interpretable.\n","authors":["Yue Xu","Kaizhi Yang","Jiebo Luo","Xuejin Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08759v1","updated":"2024-06-13T02:41:11Z","published":"2024-06-13T02:41:11Z","title":"Gaussian-Forest: Hierarchical-Hybrid 3D Gaussian Splatting for\n  Compressed Scene Modeling","summary":"  The field of novel-view synthesis has recently witnessed the emergence of 3D\nGaussian Splatting, which represents scenes in a point-based manner and renders\nthrough rasterization. This methodology, in contrast to Radiance Fields that\nrely on ray tracing, demonstrates superior rendering quality and speed.\nHowever, the explicit and unstructured nature of 3D Gaussians poses a\nsignificant storage challenge, impeding its broader application. To address\nthis challenge, we introduce the Gaussian-Forest modeling framework, which\nhierarchically represents a scene as a forest of hybrid 3D Gaussians. Each\nhybrid Gaussian retains its unique explicit attributes while sharing implicit\nones with its sibling Gaussians, thus optimizing parameterization with\nsignificantly fewer variables. Moreover, adaptive growth and pruning strategies\nare designed, ensuring detailed representation in complex regions and a notable\nreduction in the number of required Gaussians. Extensive experiments\ndemonstrate that Gaussian-Forest not only maintains comparable speed and\nquality but also achieves a compression rate surpassing 10 times, marking a\nsignificant advancement in efficient scene modeling. Codes are available at\nhttps://github.com/Xian-Bei/GaussianForest.\n","authors":["Fengyi Zhang","Tianjun Zhang","Lin Zhang","Helen Huang","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2406.08759v1.pdf","comment":null}]},"2024-06-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.10228v1","updated":"2024-06-14T17:59:40Z","published":"2024-06-14T17:59:40Z","title":"VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language\n  Large Models","summary":"  The swift progress of Multi-modal Large Models (MLLMs) has showcased their\nimpressive ability to tackle tasks blending vision and language. Yet, most\ncurrent models and benchmarks cater to scenarios with a narrow scope of visual\nand textual contexts. These models often fall short when faced with complex\ncomprehension tasks, which involve navigating through a plethora of irrelevant\nand potentially misleading information in both text and image forms. To bridge\nthis gap, we introduce a new, more demanding task known as Interleaved\nImage-Text Comprehension (IITC). This task challenges models to discern and\ndisregard superfluous elements in both images and text to accurately answer\nquestions and to follow intricate instructions to pinpoint the relevant image.\nIn support of this task, we further craft a new VEGA dataset, tailored for the\nIITC task on scientific content, and devised a subtask, Image-Text Association\n(ITA), to refine image-text correlation skills. Our evaluation of four leading\nclosed-source models, as well as various open-source models using VEGA,\nunderscores the rigorous nature of IITC. Even the most advanced models, such as\nGemini-1.5-pro and GPT4V, only achieved modest success. By employing a\nmulti-task, multi-scale post-training strategy, we have set a robust baseline\nfor MLLMs on the IITC task, attaining an $85.8\\%$ accuracy rate in image\nassociation and a $0.508$ Rouge score. These results validate the effectiveness\nof our dataset in improving MLLMs capabilities for nuanced image-text\ncomprehension.\n","authors":["Chenyu Zhou","Mengdan Zhang","Peixian Chen","Chaoyou Fu","Yunhang Shen","Xiawu Zheng","Xing Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.10228v1.pdf","comment":"Project Page: https://zhourax.github.io/VEGA/"},{"id":"http://arxiv.org/abs/2311.02248v2","updated":"2024-06-14T17:57:13Z","published":"2023-11-03T21:47:03Z","title":"COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning","summary":"  We present a cost-effective method to integrate speech into a large language\nmodel (LLM), resulting in a Contextual Speech Model with\nInstruction-following/in-context-learning Capabilities (COSMIC) multi-modal\nLLM. Using GPT-3.5, we generate Speech Comprehension Test Question-Answer (SQA)\npairs from speech transcriptions for supervised instruction tuning. With under\n30 million trainable parameters and only 450 hours of English speech data,\nCOSMIC demonstrates emerging capabilities in instruction-following and\nin-context learning. Equipped with such capabilities, COSMIC achieves a maximum\n33.18 BLEU score in 0-shot EN-to-X speech to text translation (S2TT) and a\nsignificant boost in the 1-shot setting. Additionally, there is an average\n25.8\\% relative Word Error Rate (WER) reduction for 1-shot cross-domain\nadaptation. COSMIC exhibits a significant automatic speech recognition (ASR)\naccuracy gain in contextual biasing tasks due to its instruction-following\ncapability.\n","authors":["Jing Pan","Jian Wu","Yashesh Gaur","Sunit Sivasankaran","Zhuo Chen","Shujie Liu","Jinyu Li"],"pdf_url":"https://arxiv.org/pdf/2311.02248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10221v1","updated":"2024-06-14T17:54:54Z","published":"2024-06-14T17:54:54Z","title":"Short Film Dataset (SFD): A Benchmark for Story-Level Video\n  Understanding","summary":"  Recent advances in vision-language models have significantly propelled video\nunderstanding. Existing datasets and tasks, however, have notable limitations.\nMost datasets are confined to short videos with limited events and narrow\nnarratives. For example, datasets with instructional and egocentric videos\noften document the activities of one person in a single scene. Although some\nmovie datasets offer richer content, they are often limited to short-term\ntasks, lack publicly available videos and frequently encounter data leakage\ngiven the use of movie forums and other resources in LLM training. To address\nthe above limitations, we propose the Short Film Dataset (SFD) with 1,078\npublicly available amateur movies, a wide variety of genres and minimal data\nleakage issues. SFD offers long-term story-oriented video tasks in the form of\nmultiple-choice and open-ended question answering. Our extensive experiments\nemphasize the need for long-term reasoning to solve SFD tasks. Notably, we find\nstrong signals in movie transcripts leading to the on-par performance of people\nand LLMs. We also show significantly lower performance of current models\ncompared to people when using vision data alone.\n","authors":["Ridouane Ghermi","Xi Wang","Vicky Kalogeiton","Ivan Laptev"],"pdf_url":"https://arxiv.org/pdf/2406.10221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10216v1","updated":"2024-06-14T17:49:59Z","published":"2024-06-14T17:49:59Z","title":"Regularizing Hidden States Enables Learning Generalizable Reward Model\n  for LLMs","summary":"  Reward models trained on human preference data have been proven to be\neffective for aligning Large Language Models (LLMs) with human intent within\nthe reinforcement learning from human feedback (RLHF) framework. However, the\ngeneralization capabilities of current reward models to unseen prompts and\nresponses are limited. This limitation can lead to an unexpected phenomenon\nknown as reward over-optimization, where excessive optimization of rewards\nresults in a decline in actual performance. While previous research has\nadvocated for constraining policy optimization, our study proposes a novel\napproach to enhance the reward model's generalization ability against\ndistribution shifts by regularizing the hidden states. Specifically, we retain\nthe base model's language model head and incorporate a suite of text-generation\nlosses to preserve the hidden states' text generation capabilities, while\nconcurrently learning a reward head behind the same hidden states. Our\nexperimental results demonstrate that the introduced regularization technique\nmarkedly improves the accuracy of learned reward models across a variety of\nout-of-distribution (OOD) tasks and effectively alleviate the over-optimization\nissue in RLHF, offering a more reliable and robust preference learning\nparadigm.\n","authors":["Rui Yang","Ruomeng Ding","Yong Lin","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.10216v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2406.10215v1","updated":"2024-06-14T17:49:41Z","published":"2024-06-14T17:49:41Z","title":"DevBench: A multimodal developmental benchmark for language learning","summary":"  How (dis)similar are the learning trajectories of vision-language models and\nchildren? Recent modeling work has attempted to understand the gap between\nmodels' and humans' data efficiency by constructing models trained on less\ndata, especially multimodal naturalistic data. However, such models are often\nevaluated on adult-level benchmarks, with limited breadth in language abilities\ntested, and without direct comparison to behavioral data. We introduce\nDevBench, a multimodal benchmark comprising seven language evaluation tasks\nspanning the domains of lexical, syntactic, and semantic ability, with\nbehavioral data from both children and adults. We evaluate a set of\nvision-language models on these tasks, comparing models and humans not only on\naccuracy but on their response patterns. Across tasks, models exhibit variation\nin their closeness to human response patterns, and models that perform better\non a task also more closely resemble human behavioral responses. We also\nexamine the developmental trajectory of OpenCLIP over training, finding that\ngreater training results in closer approximations to adult response patterns.\nDevBench thus provides a benchmark for comparing models to human language\ndevelopment. These comparisons highlight ways in which model and human language\nlearning processes diverge, providing insight into entry points for improving\nlanguage models.\n","authors":["Alvin Wei Ming Tan","Sunny Yu","Bria Long","Wanjing Anya Ma","Tonya Murray","Rebecca D. Silverman","Jason D. Yeatman","Michael C. Frank"],"pdf_url":"https://arxiv.org/pdf/2406.10215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01299v2","updated":"2024-06-14T17:46:02Z","published":"2024-04-01T17:59:53Z","title":"CausalChaos! Dataset for Comprehensive Causal Action Question Answering\n  Over Longer Causal Chains Grounded in Dynamic Visual Scenes","summary":"  Causal video question answering (QA) has garnered increasing interest, yet\nexisting datasets often lack depth in causal reasoning. To address this gap, we\ncapitalize on the unique properties of cartoons and construct CausalChaos!, a\nnovel, challenging causal Why-QA dataset built upon the iconic \"Tom and Jerry\"\ncartoon series. Cartoons use the principles of animation that allow animators\nto create expressive, unambiguous causal relationships between events to form a\ncoherent storyline. Utilizing these properties, along with thought-provoking\nquestions and multi-level answers (answer and detailed causal explanation), our\nquestions involve causal chains that interconnect multiple dynamic interactions\nbetween characters and visual scenes. These factors demand models to solve more\nchallenging, yet well-defined causal relationships. We also introduce hard\nincorrect answer mining, including a causally confusing version that is even\nmore challenging. While models perform well, there is much room for\nimprovement, especially, on open-ended answers. We identify more\nadvanced/explicit causal relationship modeling & joint modeling of vision and\nlanguage as the immediate areas for future efforts to focus upon. Along with\nthe other complementary datasets, our new challenging dataset will pave the way\nfor these developments in the field.\n","authors":["Paritosh Parmar","Eric Peh","Ruirui Chen","Ting En Lam","Yuhan Chen","Elston Tan","Basura Fernando"],"pdf_url":"https://arxiv.org/pdf/2404.01299v2.pdf","comment":"Project Page: https://github.com/LUNAProject22/CausalChaos"},{"id":"http://arxiv.org/abs/2406.10209v1","updated":"2024-06-14T17:44:22Z","published":"2024-06-14T17:44:22Z","title":"Be like a Goldfish, Don't Memorize! Mitigating Memorization in\n  Generative LLMs","summary":"  Large language models can memorize and repeat their training data, causing\nprivacy and copyright risks. To mitigate memorization, we introduce a subtle\nmodification to the next-token training objective that we call the goldfish\nloss. During training, a randomly sampled subset of tokens are excluded from\nthe loss computation. These dropped tokens are not memorized by the model,\nwhich prevents verbatim reproduction of a complete chain of tokens from the\ntraining set. We run extensive experiments training billion-scale Llama-2\nmodels, both pre-trained and trained from scratch, and demonstrate significant\nreductions in extractable memorization with little to no impact on downstream\nbenchmarks.\n","authors":["Abhimanyu Hans","Yuxin Wen","Neel Jain","John Kirchenbauer","Hamid Kazemi","Prajwal Singhania","Siddharth Singh","Gowthami Somepalli","Jonas Geiping","Abhinav Bhatele","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2406.10209v1.pdf","comment":"9.5 pages, 8 figures, and 1 table in the main body. Code available at\n  https://github.com/ahans30/goldfish-loss"},{"id":"http://arxiv.org/abs/2406.10203v1","updated":"2024-06-14T17:38:21Z","published":"2024-06-14T17:38:21Z","title":"A Fundamental Trade-off in Aligned Language Models and its Relation to\n  Sampling Adaptors","summary":"  The relationship between the quality of a string and its probability\n$p(\\boldsymbol{y})$ under a language model has been influential in the\ndevelopment of techniques to build good text generation systems. For example,\nseveral decoding algorithms have been motivated to manipulate\n$p(\\boldsymbol{y})$ to produce higher-quality text. In this work, we examine\nthe probability--quality relationship in language models explicitly aligned to\nhuman preferences, e.g., through Reinforcement Learning through Human Feedback\n(RLHF). We find that, given a general language model and its aligned version,\nfor corpora sampled from an aligned language model, there exists a trade-off\nbetween the average reward and average log-likelihood of the strings under the\ngeneral language model. We provide a formal treatment of this issue and\ndemonstrate how a choice of sampling adaptor allows for a selection of how much\nlikelihood we exchange for the reward.\n","authors":["Naaman Tan","Josef Valvoda","Anej Svete","Tianyu Liu","Yanxia Qin","Kan Min-Yen","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.10203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10190v1","updated":"2024-06-14T17:23:57Z","published":"2024-06-14T17:23:57Z","title":"CHIRON: Rich Character Representations in Long-Form Narratives","summary":"  Characters are integral to long-form narratives, but are poorly understood by\nexisting story analysis and generation systems. While prior work has simplified\ncharacters via graph-based methods and brief character descriptions, we aim to\nbetter tackle the problem of representing complex characters by taking\ninspiration from advice given to professional writers. We propose CHIRON, a new\n`character sheet' based representation that organizes and filters textual\ninformation about characters. We construct CHIRON sheets in two steps: a\nGeneration Module that prompts an LLM for character information via\nquestion-answering and a Validation Module that uses automated reasoning and a\ndomain-specific entailment model to eliminate false facts about a character. We\nvalidate CHIRON via the downstream task of masked-character prediction, where\nour experiments show CHIRON is better and more flexible than comparable\nsummary-based baselines. We also show that metrics derived from CHIRON can be\nused to automatically infer character-centricity in stories, and that these\nmetrics align with human judgments.\n","authors":["Alexander Gurung","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2406.10190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10177v1","updated":"2024-06-14T16:56:40Z","published":"2024-06-14T16:56:40Z","title":"Inclusive ASR for Disfluent Speech: Cascaded Large-Scale Self-Supervised\n  Learning with Targeted Fine-Tuning and Data Augmentation","summary":"  Automatic speech recognition (ASR) systems often falter while processing\nstuttering-related disfluencies -- such as involuntary blocks and word\nrepetitions -- yielding inaccurate transcripts. A critical barrier to progress\nis the scarcity of large, annotated disfluent speech datasets. Therefore, we\npresent an inclusive ASR design approach, leveraging large-scale\nself-supervised learning on standard speech followed by targeted fine-tuning\nand data augmentation on a smaller, curated dataset of disfluent speech. Our\ndata augmentation technique enriches training datasets with various\ndisfluencies, enhancing ASR processing of these speech patterns. Results show\nthat fine-tuning wav2vec 2.0 with even a relatively small, labeled dataset,\nalongside data augmentation, can significantly reduce word error rates for\ndisfluent speech. Our approach not only advances ASR inclusivity for people who\nstutter, but also paves the way for ASRs that can accommodate wider speech\nvariations.\n","authors":["Dena Mujtaba","Nihar R. Mahapatra","Megan Arney","J. Scott Yaruss","Caryn Herring","Jia Bin"],"pdf_url":"https://arxiv.org/pdf/2406.10177v1.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.10174v1","updated":"2024-06-14T16:54:48Z","published":"2024-06-14T16:54:48Z","title":"Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for\n  Beat-Aligned Poetry Generation","summary":"  The intersection between poetry and music provides an interesting case for\ncomputational creativity, yet remains relatively unexplored. This paper\nexplores the integration of poetry and music through the lens of beat patterns,\ninvestigating whether a byte-based language model can generate words that fit\nspecific beat patterns within the context of poetry. Drawing on earlier\nstudies, we developed a method to train a byte-based transformer model, ByT5,\nto align poems with beat patterns. The results demonstrate a high level of beat\nalignment while maintaining semantic coherence. Future work will aim to improve\nthe model's ability to create complete beat-aligned poems.\n","authors":["Mohamad Elzohbi","Richard Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.10174v1.pdf","comment":"5 pages, 3 figures, accepted for the 15th International Conference on\n  Computational Creativity, ICCC'24"},{"id":"http://arxiv.org/abs/2406.10173v1","updated":"2024-06-14T16:51:21Z","published":"2024-06-14T16:51:21Z","title":"IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension\n  Abilities of Language Models in E-commerce","summary":"  Enhancing Language Models' (LMs) ability to understand purchase intentions in\nE-commerce scenarios is crucial for their effective assistance in various\ndownstream tasks. However, previous approaches that distill intentions from LMs\noften fail to generate meaningful and human-centric intentions applicable in\nreal-world E-commerce contexts. This raises concerns about the true\ncomprehension and utilization of purchase intentions by LMs. In this paper, we\npresent IntentionQA, a double-task multiple-choice question answering benchmark\nto evaluate LMs' comprehension of purchase intentions in E-commerce.\nSpecifically, LMs are tasked to infer intentions based on purchased products\nand utilize them to predict additional purchases. IntentionQA consists of 4,360\ncarefully curated problems across three difficulty levels, constructed using an\nautomated pipeline to ensure scalability on large E-commerce platforms. Human\nevaluations demonstrate the high quality and low false-negative rate of our\nbenchmark. Extensive experiments across 19 language models show that they still\nstruggle with certain scenarios, such as understanding products and intentions\naccurately, jointly reasoning with products and intentions, and more, in which\nthey fall far behind human performances. Our code and data are publicly\navailable at https://github.com/HKUST-KnowComp/IntentionQA.\n","authors":["Wenxuan Ding","Weiqi Wang","Sze Heng Douglas Kwok","Minghao Liu","Tianqing Fang","Jiaxin Bai","Junxian He","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2406.10173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10172v1","updated":"2024-06-14T16:50:29Z","published":"2024-06-14T16:50:29Z","title":"Datasets for Multilingual Answer Sentence Selection","summary":"  Answer Sentence Selection (AS2) is a critical task for designing effective\nretrieval-based Question Answering (QA) systems. Most advancements in AS2 focus\non English due to the scarcity of annotated datasets for other languages. This\nlack of resources prevents the training of effective AS2 models in different\nlanguages, creating a performance gap between QA systems in English and other\nlocales. In this paper, we introduce new high-quality datasets for AS2 in five\nEuropean languages (French, German, Italian, Portuguese, and Spanish), obtained\nthrough supervised Automatic Machine Translation (AMT) of existing English AS2\ndatasets such as ASNQ, WikiQA, and TREC-QA using a Large Language Model (LLM).\nWe evaluated our approach and the quality of the translated datasets through\nmultiple experiments with different Transformer architectures. The results\nindicate that our datasets are pivotal in producing robust and powerful\nmultilingual AS2 models, significantly contributing to closing the performance\ngap between English and other languages.\n","authors":["Matteo Gabburo","Stefano Campese","Federico Agostini","Alessandro Moschitti"],"pdf_url":"https://arxiv.org/pdf/2406.10172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.21070v2","updated":"2024-06-14T16:42:47Z","published":"2024-05-31T17:57:24Z","title":"Generalization Beyond Data Imbalance: A Controlled Study on CLIP for\n  Transferable Insights","summary":"  Severe data imbalance naturally exists among web-scale vision-language\ndatasets. Despite this, we find CLIP pre-trained thereupon exhibits notable\nrobustness to the data imbalance compared to supervised learning, and\ndemonstrates significant effectiveness in learning generalizable\nrepresentations. With an aim to investigate the reasons behind this finding, we\nconduct controlled experiments to study various underlying factors, and reveal\nthat CLIP's pretext task forms a dynamic classification problem wherein only a\nsubset of classes is present in training. This isolates the bias from dominant\nclasses and implicitly balances the learning signal. Furthermore, the\nrobustness and discriminability of CLIP improve with more descriptive language\nsupervision, larger data scale, and broader open-world concepts, which are\ninaccessible to supervised learning. Our study not only uncovers the mechanisms\nbehind CLIP's generalizability beyond data imbalance but also provides\ntransferable insights for the research community. The findings are validated in\nboth supervised and self-supervised learning, enabling models trained on\nimbalanced data to achieve CLIP-level performance on diverse recognition tasks.\nCode and data are available at: https://github.com/CVMI-Lab/clip-beyond-tail.\n","authors":["Xin Wen","Bingchen Zhao","Yilun Chen","Jiangmiao Pang","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2405.21070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06908v2","updated":"2024-06-14T16:27:24Z","published":"2023-09-13T12:10:54Z","title":"Towards the TopMost: A Topic Modeling System Toolkit","summary":"  Topic models have a rich history with various applications and have recently\nbeen reinvigorated by neural topic modeling. However, these numerous topic\nmodels adopt totally distinct datasets, implementations, and evaluations. This\nimpedes quick utilization and fair comparisons, and thereby hinders their\nresearch progress and applications. To tackle this challenge, we in this paper\npropose a Topic Modeling System Toolkit (TopMost). Compared to existing\ntoolkits, TopMost stands out by supporting more extensive features. It covers a\nbroader spectrum of topic modeling scenarios with their complete lifecycles,\nincluding datasets, preprocessing, models, training, and evaluations. Thanks to\nits highly cohesive and decoupled modular design, TopMost enables rapid\nutilization, fair comparisons, and flexible extensions of diverse cutting-edge\ntopic models. Our code, tutorials, and documentation are available at\nhttps://github.com/bobxwu/topmost.\n","authors":["Xiaobao Wu","Fengjun Pan","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2309.06908v2.pdf","comment":"Accepted to ACL 2024 System Demonstrations Track"},{"id":"http://arxiv.org/abs/2406.10162v1","updated":"2024-06-14T16:26:20Z","published":"2024-06-14T16:26:20Z","title":"Sycophancy to Subterfuge: Investigating Reward-Tampering in Large\n  Language Models","summary":"  In reinforcement learning, specification gaming occurs when AI systems learn\nundesired behaviors that are highly rewarded due to misspecified training\ngoals. Specification gaming can range from simple behaviors like sycophancy to\nsophisticated and pernicious behaviors like reward-tampering, where a model\ndirectly modifies its own reward mechanism. However, these more pernicious\nbehaviors may be too complex to be discovered via exploration. In this paper,\nwe study whether Large Language Model (LLM) assistants which find easily\ndiscovered forms of specification gaming will generalize to perform rarer and\nmore blatant forms, up to and including reward-tampering. We construct a\ncurriculum of increasingly sophisticated gameable environments and find that\ntraining on early-curriculum environments leads to more specification gaming on\nremaining environments. Strikingly, a small but non-negligible proportion of\nthe time, LLM assistants trained on the full curriculum generalize zero-shot to\ndirectly rewriting their own reward function. Retraining an LLM not to game\nearly-curriculum environments mitigates, but does not eliminate,\nreward-tampering in later environments. Moreover, adding harmlessness training\nto our gameable environments does not prevent reward-tampering. These results\ndemonstrate that LLMs can generalize from common forms of specification gaming\nto more pernicious reward tampering and that such behavior may be nontrivial to\nremove.\n","authors":["Carson Denison","Monte MacDiarmid","Fazl Barez","David Duvenaud","Shauna Kravec","Samuel Marks","Nicholas Schiefer","Ryan Soklaski","Alex Tamkin","Jared Kaplan","Buck Shlegeris","Samuel R. Bowman","Ethan Perez","Evan Hubinger"],"pdf_url":"https://arxiv.org/pdf/2406.10162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10149v1","updated":"2024-06-14T16:00:29Z","published":"2024-06-14T16:00:29Z","title":"BABILong: Testing the Limits of LLMs with Long Context\n  Reasoning-in-a-Haystack","summary":"  In recent years, the input context sizes of large language models (LLMs) have\nincreased dramatically. However, existing evaluation methods have not kept\npace, failing to comprehensively assess the efficiency of models in handling\nlong contexts. To bridge this gap, we introduce the BABILong benchmark,\ndesigned to test language models' ability to reason across facts distributed in\nextremely long documents. BABILong includes a diverse set of 20 reasoning\ntasks, including fact chaining, simple induction, deduction, counting, and\nhandling lists/sets. These tasks are challenging on their own, and even more\ndemanding when the required facts are scattered across long natural text. Our\nevaluations show that popular LLMs effectively utilize only 10-20\\% of the\ncontext and their performance declines sharply with increased reasoning\ncomplexity. Among alternatives to in-context reasoning, Retrieval-Augmented\nGeneration methods achieve a modest 60\\% accuracy on single-fact question\nanswering, independent of context length. Among context extension methods, the\nhighest performance is demonstrated by recurrent memory transformers, enabling\nthe processing of lengths up to 11 million tokens. The BABILong benchmark is\nextendable to any length to support the evaluation of new upcoming models with\nincreased capabilities, and we provide splits up to 1 million token lengths.\n","authors":["Yuri Kuratov","Aydar Bulatov","Petr Anokhin","Ivan Rodkin","Dmitry Sorokin","Artyom Sorokin","Mikhail Burtsev"],"pdf_url":"https://arxiv.org/pdf/2406.10149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10133v1","updated":"2024-06-14T15:42:42Z","published":"2024-06-14T15:42:42Z","title":"Evaluation of Large Language Models: STEM education and Gender\n  Stereotypes","summary":"  Large Language Models (LLMs) have an increasing impact on our lives with use\ncases such as chatbots, study support, coding support, ideation, writing\nassistance, and more. Previous studies have revealed linguistic biases in\npronouns used to describe professions or adjectives used to describe men vs\nwomen. These issues have to some degree been addressed in updated LLM versions,\nat least to pass existing tests. However, biases may still be present in the\nmodels, and repeated use of gender stereotypical language may reinforce the\nunderlying assumptions and are therefore important to examine further. This\npaper investigates gender biases in LLMs in relation to educational choices\nthrough an open-ended, true to user-case experimental design and a quantitative\nanalysis. We investigate the biases in the context of four different cultures,\nlanguages, and educational systems (English/US/UK, Danish/DK, Catalan/ES, and\nHindi/IN) for ages ranging from 10 to 16 years, corresponding to important\neducational transition points in the different countries. We find that there\nare significant and large differences in the ratio of STEM to non-STEM\nsuggested education paths provided by chatGPT when using typical girl vs boy\nnames to prompt lists of suggested things to become. There are generally fewer\nSTEM suggestions in the Danish, Spanish, and Indian context compared to the\nEnglish. We also find subtle differences in the suggested professions, which we\ncategorise and report.\n","authors":["Smilla Due","Sneha Das","Marianne Andersen","Berta Plandolit López","Sniff Andersen Nexø","Line Clemmensen"],"pdf_url":"https://arxiv.org/pdf/2406.10133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10130v1","updated":"2024-06-14T15:41:06Z","published":"2024-06-14T15:41:06Z","title":"The Devil is in the Neurons: Interpreting and Mitigating Social Biases\n  in Pre-trained Language Models","summary":"  Pre-trained Language models (PLMs) have been acknowledged to contain harmful\ninformation, such as social biases, which may cause negative social impacts or\neven bring catastrophic results in application. Previous works on this problem\nmainly focused on using black-box methods such as probing to detect and\nquantify social biases in PLMs by observing model outputs. As a result,\nprevious debiasing methods mainly finetune or even pre-train language models on\nnewly constructed anti-stereotypical datasets, which are high-cost. In this\nwork, we try to unveil the mystery of social bias inside language models by\nintroducing the concept of {\\sc Social Bias Neurons}. Specifically, we propose\n{\\sc Integrated Gap Gradients (IG$^2$)} to accurately pinpoint units (i.e.,\nneurons) in a language model that can be attributed to undesirable behavior,\nsuch as social bias. By formalizing undesirable behavior as a distributional\nproperty of language, we employ sentiment-bearing prompts to elicit classes of\nsensitive words (demographics) correlated with such sentiments. Our IG$^2$ thus\nattributes the uneven distribution for different demographics to specific\nSocial Bias Neurons, which track the trail of unwanted behavior inside PLM\nunits to achieve interoperability. Moreover, derived from our interpretable\ntechnique, {\\sc Bias Neuron Suppression (BNS)} is further proposed to mitigate\nsocial biases. By studying BERT, RoBERTa, and their attributable differences\nfrom debiased FairBERTa, IG$^2$ allows us to locate and suppress identified\nneurons, and further mitigate undesired behaviors. As measured by prior metrics\nfrom StereoSet, our model achieves a higher degree of fairness while\nmaintaining language modeling ability with low cost.\n","authors":["Yan Liu","Yu Liu","Xiaokang Chen","Pin-Yu Chen","Daoguang Zan","Min-Yen Kan","Tsung-Yi Ho"],"pdf_url":"https://arxiv.org/pdf/2406.10130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10118v1","updated":"2024-06-14T15:23:39Z","published":"2024-06-14T15:23:39Z","title":"SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for\n  Southeast Asian Languages","summary":"  Southeast Asia (SEA) is a region rich in linguistic diversity and cultural\nvariety, with over 1,300 indigenous languages and a population of 671 million\npeople. However, prevailing AI models suffer from a significant lack of\nrepresentation of texts, images, and audio datasets from SEA, compromising the\nquality of AI models for SEA languages. Evaluating models for SEA languages is\nchallenging due to the scarcity of high-quality datasets, compounded by the\ndominance of English training data, raising concerns about potential cultural\nmisrepresentation. To address these challenges, we introduce SEACrowd, a\ncollaborative initiative that consolidates a comprehensive resource hub that\nfills the resource gap by providing standardized corpora in nearly 1,000 SEA\nlanguages across three modalities. Through our SEACrowd benchmarks, we assess\nthe quality of AI models on 36 indigenous languages across 13 tasks, offering\nvaluable insights into the current AI landscape in SEA. Furthermore, we propose\nstrategies to facilitate greater AI advancements, maximizing potential utility\nand resource equity for the future of AI in SEA.\n","authors":["Holy Lovenia","Rahmad Mahendra","Salsabil Maulana Akbar","Lester James V. Miranda","Jennifer Santoso","Elyanah Aco","Akhdan Fadhilah","Jonibek Mansurov","Joseph Marvin Imperial","Onno P. Kampman","Joel Ruben Antony Moniz","Muhammad Ravi Shulthan Habibi","Frederikus Hudi","Railey Montalan","Ryan Ignatius","Joanito Agili Lopo","William Nixon","Börje F. Karlsson","James Jaya","Ryandito Diandaru","Yuze Gao","Patrick Amadeus","Bin Wang","Jan Christian Blaise Cruz","Chenxi Whitehouse","Ivan Halim Parmonangan","Maria Khelli","Wenyu Zhang","Lucky Susanto","Reynard Adha Ryanda","Sonny Lazuardi Hermawan","Dan John Velasco","Muhammad Dehan Al Kautsar","Willy Fitra Hendria","Yasmin Moslem","Noah Flynn","Muhammad Farid Adilazuarda","Haochen Li","Johanes Lee","R. Damanhuri","Shuo Sun","Muhammad Reza Qorib","Amirbek Djanibekov","Wei Qi Leong","Quyet V. Do","Niklas Muennighoff","Tanrada Pansuwan","Ilham Firdausi Putra","Yan Xu","Ngee Chia Tai","Ayu Purwarianti","Sebastian Ruder","William Tjhi","Peerat Limkonchotiwat","Alham Fikri Aji","Sedrick Keh","Genta Indra Winata","Ruochen Zhang","Fajri Koto","Zheng-Xin Yong","Samuel Cahyawijaya"],"pdf_url":"https://arxiv.org/pdf/2406.10118v1.pdf","comment":"https://github.com/SEACrowd"},{"id":"http://arxiv.org/abs/2406.10099v1","updated":"2024-06-14T14:56:04Z","published":"2024-06-14T14:56:04Z","title":"Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction\n  Tuning","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks but still face challenges such as hallucinations. One potential\nreason for hallucinations is the lack of relevant knowledge or context. Thus, a\npromising solution to mitigate this issue involves instructing LLMs to respond\nwith \"I do not know\" when a question falls outside their knowledge domain or\nthe provided context. However, in this work, we observed that LLMs struggle to\nadmit their lack of knowledge, primarily due to existing instruction datasets\ndesigned to encourage specific answers. To improve large language models'\ncapability to recognize the boundaries of their knowledge, we propose a novel\napproach called uncertainty-sensitive tuning. This method involves two-stage\ntraining designed for uncertainty recognition and prompt-sensitive activation.\nIn the first stage, we guide the LLM to reject unknown questions. In the second\nstage, we recover the decreased performance in QA tasks by incorporating\ndesigned causal instructions. By leveraging this method, we aim to enhance the\nmodel's ability to identify areas of uncertainty. The experimental results\ndemonstrate that our proposed uncertainty-sensitive tuning method significantly\nimproves the performance of the Llama2-chat-7B model. Specifically, it achieves\na substantial 34.7% improvement in handling questions involving knowledge gaps\ncompared to the original model. Moreover, our approach outperforms GPT-4,\nexhibiting a 9.4% increase in overall performance. We open-source the model and\ncode on GitHub.\n","authors":["Jiaqi Li","Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2406.10099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09334v2","updated":"2024-06-14T14:52:05Z","published":"2024-06-13T17:15:33Z","title":"ProxyLM: Predicting Language Model Performance on Multilingual Tasks via\n  Proxy Models","summary":"  Performance prediction is a method to estimate the performance of Language\nModels (LMs) on various Natural Language Processing (NLP) tasks, mitigating\ncomputational costs associated with model capacity and data for fine-tuning.\nOur paper introduces ProxyLM, a scalable framework for predicting LM\nperformance using proxy models in multilingual tasks. These proxy models act as\nsurrogates, approximating the performance of the LM of interest. By leveraging\nproxy models, ProxyLM significantly reduces computational overhead on task\nevaluations, achieving up to a 37.08x speedup compared to traditional methods,\neven with our smallest proxy models. Additionally, our methodology showcases\nadaptability to previously unseen languages in pre-trained LMs, outperforming\nthe state-of-the-art performance by 1.89x as measured by root-mean-square error\n(RMSE). This framework streamlines model selection, enabling efficient\ndeployment and iterative LM enhancements without extensive computational\nresources.\n","authors":["David Anugraha","Genta Indra Winata","Chenyue Li","Patrick Amadeus Irawan","En-Shiun Annie Lee"],"pdf_url":"https://arxiv.org/pdf/2406.09334v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.10091v1","updated":"2024-06-14T14:47:19Z","published":"2024-06-14T14:47:19Z","title":"Exploring the Correlation between Human and Machine Evaluation of\n  Simultaneous Speech Translation","summary":"  Assessing the performance of interpreting services is a complex task, given\nthe nuanced nature of spoken language translation, the strategies that\ninterpreters apply, and the diverse expectations of users. The complexity of\nthis task become even more pronounced when automated evaluation methods are\napplied. This is particularly true because interpreted texts exhibit less\nlinearity between the source and target languages due to the strategies\nemployed by the interpreter.\n  This study aims to assess the reliability of automatic metrics in evaluating\nsimultaneous interpretations by analyzing their correlation with human\nevaluations. We focus on a particular feature of interpretation quality, namely\ntranslation accuracy or faithfulness. As a benchmark we use human assessments\nperformed by language experts, and evaluate how well sentence embeddings and\nLarge Language Models correlate with them. We quantify semantic similarity\nbetween the source and translated texts without relying on a reference\ntranslation. The results suggest GPT models, particularly GPT-3.5 with direct\nprompting, demonstrate the strongest correlation with human judgment in terms\nof semantic similarity between source and target texts, even when evaluating\nshort textual segments. Additionally, the study reveals that the size of the\ncontext window has a notable impact on this correlation.\n","authors":["Xiaoman Wang","Claudio Fantinuoli"],"pdf_url":"https://arxiv.org/pdf/2406.10091v1.pdf","comment":"Paper accepted at the European Association for Machine Translation\n  conference 2024"},{"id":"http://arxiv.org/abs/2406.10086v1","updated":"2024-06-14T14:41:44Z","published":"2024-06-14T14:41:44Z","title":"Discovering influential text using convolutional neural networks","summary":"  Experimental methods for estimating the impacts of text on human evaluation\nhave been widely used in the social sciences. However, researchers in\nexperimental settings are usually limited to testing a small number of\npre-specified text treatments. While efforts to mine unstructured texts for\nfeatures that causally affect outcomes have been ongoing in recent years, these\nmodels have primarily focused on the topics or specific words of text, which\nmay not always be the mechanism of the effect. We connect these efforts with\nNLP interpretability techniques and present a method for flexibly discovering\nclusters of similar text phrases that are predictive of human reactions to\ntexts using convolutional neural networks. When used in an experimental\nsetting, this method can identify text treatments and their effects under\ncertain assumptions. We apply the method to two datasets. The first enables\ndirect validation of the model's ability to detect phrases known to cause the\noutcome. The second demonstrates its ability to flexibly discover text\ntreatments with varying textual structures. In both cases, the model learns a\ngreater variety of text treatments compared to benchmark methods, and these\ntext features quantitatively meet or exceed the ability of benchmark methods to\npredict the outcome.\n","authors":["Megan Ayers","Luke Sanford","Margaret Roberts","Eddie Yang"],"pdf_url":"https://arxiv.org/pdf/2406.10086v1.pdf","comment":"To be published in ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.10085v1","updated":"2024-06-14T14:40:10Z","published":"2024-06-14T14:40:10Z","title":"Enhancing Question Answering on Charts Through Effective Pre-training\n  Tasks","summary":"  To completely understand a document, the use of textual information is not\nenough. Understanding visual cues, such as layouts and charts, is also\nrequired. While the current state-of-the-art approaches for document\nunderstanding (both OCR-based and OCR-free) work well, a thorough analysis of\ntheir capabilities and limitations has not yet been performed. Therefore, in\nthis work, we addresses the limitation of current VisualQA models when applied\nto charts and plots. To investigate shortcomings of the state-of-the-art\nmodels, we conduct a comprehensive behavioral analysis, using ChartQA as a case\nstudy. Our findings indicate that existing models particularly underperform in\nanswering questions related to the chart's structural and visual context, as\nwell as numerical information. To address these issues, we propose three simple\npre-training tasks that enforce the existing model in terms of both\nstructural-visual knowledge, as well as its understanding of numerical\nquestions. We evaluate our pre-trained model (called MatCha-v2) on three chart\ndatasets - both extractive and abstractive question datasets - and observe that\nit achieves an average improvement of 1.7% over the baseline model.\n","authors":["Ashim Gupta","Vivek Gupta","Shuo Zhang","Yujie He","Ning Zhang","Shalin Shah"],"pdf_url":"https://arxiv.org/pdf/2406.10085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10083v1","updated":"2024-06-14T14:37:52Z","published":"2024-06-14T14:37:52Z","title":"On the Evaluation of Speech Foundation Models for Spoken Language\n  Understanding","summary":"  The Spoken Language Understanding Evaluation (SLUE) suite of benchmark tasks\nwas recently introduced to address the need for open resources and benchmarking\nof complex spoken language understanding (SLU) tasks, including both\nclassification and sequence generation tasks, on natural speech. The benchmark\nhas demonstrated preliminary success in using pre-trained speech foundation\nmodels (SFM) for these SLU tasks. However, the community still lacks a\nfine-grained understanding of the comparative utility of different SFMs.\nInspired by this, we ask: which SFMs offer the most benefits for these complex\nSLU tasks, and what is the most effective approach for incorporating these\nSFMs? To answer this, we perform an extensive evaluation of multiple supervised\nand self-supervised SFMs using several evaluation protocols: (i) frozen SFMs\nwith a lightweight prediction head, (ii) frozen SFMs with a complex prediction\nhead, and (iii) fine-tuned SFMs with a lightweight prediction head. Although\nthe supervised SFMs are pre-trained on much more speech recognition data (with\nlabels), they do not always outperform self-supervised SFMs; the latter tend to\nperform at least as well as, and sometimes better than, supervised SFMs,\nespecially on the sequence generation tasks in SLUE. While there is no\nuniversally optimal way of incorporating SFMs, the complex prediction head\ngives the best performance for most tasks, although it increases the inference\ntime. We also introduce an open-source toolkit and performance leaderboard,\nSLUE-PERB, for these tasks and modeling strategies.\n","authors":["Siddhant Arora","Ankita Pasad","Chung-Ming Chien","Jionghao Han","Roshan Sharma","Jee-weon Jung","Hira Dhamyal","William Chen","Suwon Shon","Hung-yi Lee","Karen Livescu","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2406.10083v1.pdf","comment":"Accepted at ACL Findings 2024"},{"id":"http://arxiv.org/abs/2406.10073v1","updated":"2024-06-14T14:28:06Z","published":"2024-06-14T14:28:06Z","title":"Detecting the terminality of speech-turn boundary for spoken\n  interactions in French TV and Radio content","summary":"  Transition Relevance Places are defined as the end of an utterance where the\ninterlocutor may take the floor without interrupting the current speaker\n--i.e., a place where the turn is terminal. Analyzing turn terminality is\nuseful to study the dynamic of turn-taking in spontaneous conversations. This\npaper presents an automatic classification of spoken utterances as Terminal or\nNon-Terminal in multi-speaker settings. We compared audio, text, and fusions of\nboth approaches on a French corpus of TV and Radio extracts annotated with\nturn-terminality information at each speaker change. Our models are based on\npre-trained self-supervised representations. We report results for different\nfusion strategies and varying context sizes. This study also questions the\nproblem of performance variability by analyzing the differences in results for\nmultiple training runs with random initialization. The measured accuracy would\nallow the use of these models for large-scale analysis of turn-taking.\n","authors":["Rémi Uro","Marie Tahon","David Doukhan","Antoine Laurent","Albert Rilliard"],"pdf_url":"https://arxiv.org/pdf/2406.10073v1.pdf","comment":"keywords : Spoken interaction, Media, TV, Radio, Transition-Relevance\n  Places, Turn Taking, Interruption. Accepted to InterSpeech 2024, Kos Island,\n  Greece"},{"id":"http://arxiv.org/abs/2406.10052v1","updated":"2024-06-14T14:07:26Z","published":"2024-06-14T14:07:26Z","title":"Simul-Whisper: Attention-Guided Streaming Whisper with Truncation\n  Detection","summary":"  As a robust and large-scale multilingual speech recognition model, Whisper\nhas demonstrated impressive results in many low-resource and\nout-of-distribution scenarios. However, its encoder-decoder structure hinders\nits application to streaming speech recognition. In this paper, we introduce\nSimul-Whisper, which uses the time alignment embedded in Whisper's\ncross-attention to guide auto-regressive decoding and achieve chunk-based\nstreaming ASR without any fine-tuning of the pre-trained model. Furthermore, we\nobserve the negative effect of the truncated words at the chunk boundaries on\nthe decoding results and propose an integrate-and-fire-based truncation\ndetection model to address this issue. Experiments on multiple languages and\nWhisper architectures show that Simul-Whisper achieves an average absolute word\nerror rate degradation of only 1.46% at a chunk size of 1 second, which\nsignificantly outperforms the current state-of-the-art baseline.\n","authors":["Haoyu Wang","Guoqiang Hu","Guodong Lin","Wei-Qiang Zhang","Jian Li"],"pdf_url":"https://arxiv.org/pdf/2406.10052v1.pdf","comment":"Accepted by INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2403.00252v2","updated":"2024-06-14T13:51:01Z","published":"2024-03-01T03:30:38Z","title":"EUROPA: A Legal Multilingual Keyphrase Generation Dataset","summary":"  Keyphrase generation has primarily been explored within the context of\nacademic research articles, with a particular focus on scientific domains and\nthe English language. In this work, we present EUROPA, a dataset for\nmultilingual keyphrase generation in the legal domain. It is derived from legal\njudgments from the Court of Justice of the European Union (EU), and contains\ninstances in all 24 EU official languages. We run multilingual models on our\ncorpus and analyze the results, showing room for improvement on a\ndomain-specific multilingual corpus such as the one we present.\n","authors":["Olivier Salaün","Frédéric Piedboeuf","Guillaume Le Berre","David Alfonso Hermelo","Philippe Langlais"],"pdf_url":"https://arxiv.org/pdf/2403.00252v2.pdf","comment":"19 pages, 2 figures, accepted at ACL 2024"},{"id":"http://arxiv.org/abs/2406.10040v1","updated":"2024-06-14T13:49:07Z","published":"2024-06-14T13:49:07Z","title":"FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in\n  Biomedical Domain","summary":"  This paper describes the inference system of FZI-WIM at the SemEval-2024 Task\n2: Safe Biomedical Natural Language Inference for Clinical Trials. Our system\nutilizes the chain of thought (CoT) paradigm to tackle this complex reasoning\nproblem and further improves the CoT performance with self-consistency. Instead\nof greedy decoding, we sample multiple reasoning chains with the same prompt\nand make the final verification with majority voting. The self-consistent CoT\nsystem achieves a baseline F1 score of 0.80 (1st), faithfulness score of 0.90\n(3rd), and consistency score of 0.73 (12th). We release the code and data\npublicly https://github.com/jens5588/FZI-WIM-NLI4CT.\n","authors":["Jin Liu","Steffen Thoma"],"pdf_url":"https://arxiv.org/pdf/2406.10040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14492v2","updated":"2024-06-14T13:38:14Z","published":"2024-02-22T12:35:50Z","title":"Towards Robust Instruction Tuning on Multimodal Large Language Models","summary":"  Fine-tuning large language models (LLMs) on multi-task instruction-following\ndata has been proven to be a powerful learning paradigm for improving their\nzero-shot capabilities on new tasks. Recent works about high-quality\ninstruction-following data generation and selection require amounts of human\nlabor to conceive model-understandable instructions for the given tasks and\ncarefully filter the LLM-generated data. In this work, we introduce an\nautomatic instruction augmentation method named INSTRAUG in multimodal tasks.\nIt starts from a handful of basic and straightforward meta instructions but can\nexpand an instruction-following dataset by 30 times. Results on two popular\nmultimodal instructionfollowing benchmarks MULTIINSTRUCT and InstructBLIP show\nthat INSTRAUG can significantly improve the alignment of multimodal large\nlanguage models (MLLMs) across 12 multimodal tasks, which is even equivalent to\nthe benefits of scaling up training data multiple times.\n","authors":["Wei Han","Hui Chen","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2402.14492v2.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.10023v1","updated":"2024-06-14T13:32:43Z","published":"2024-06-14T13:32:43Z","title":"Deep Bayesian Active Learning for Preference Modeling in Large Language\n  Models","summary":"  Leveraging human preferences for steering the behavior of Large Language\nModels (LLMs) has demonstrated notable success in recent years. Nonetheless,\ndata selection and labeling are still a bottleneck for these systems,\nparticularly at large scale. Hence, selecting the most informative points for\nacquiring human feedback may considerably reduce the cost of preference\nlabeling and unleash the further development of LLMs. Bayesian Active Learning\nprovides a principled framework for addressing this challenge and has\ndemonstrated remarkable success in diverse settings. However, previous attempts\nto employ it for Preference Modeling did not meet such expectations. In this\nwork, we identify that naive epistemic uncertainty estimation leads to the\nacquisition of redundant samples. We address this by proposing the Bayesian\nActive Learner for Preference Modeling (BAL-PM), a novel stochastic acquisition\npolicy that not only targets points of high epistemic uncertainty according to\nthe preference model but also seeks to maximize the entropy of the acquired\nprompt distribution in the feature space spanned by the employed LLM. Notably,\nour experiments demonstrate that BAL-PM requires 33% to 68% fewer preference\nlabels in two popular human preference datasets and exceeds previous stochastic\nBayesian acquisition policies.\n","authors":["Luckeciano C. Melo","Panagiotis Tigas","Alessandro Abate","Yarin Gal"],"pdf_url":"https://arxiv.org/pdf/2406.10023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10019v1","updated":"2024-06-14T13:29:36Z","published":"2024-06-14T13:29:36Z","title":"Group and Shuffle: Efficient Structured Orthogonal Parametrization","summary":"  The increasing size of neural networks has led to a growing demand for\nmethods of efficient fine-tuning. Recently, an orthogonal fine-tuning paradigm\nwas introduced that uses orthogonal matrices for adapting the weights of a\npretrained model. In this paper, we introduce a new class of structured\nmatrices, which unifies and generalizes structured classes from previous works.\nWe examine properties of this class and build a structured orthogonal\nparametrization upon it. We then use this parametrization to modify the\northogonal fine-tuning framework, improving parameter and computational\nefficiency. We empirically validate our method on different domains, including\nadapting of text-to-image diffusion models and downstream task fine-tuning in\nlanguage modeling. Additionally, we adapt our construction for orthogonal\nconvolutions and conduct experiments with 1-Lipschitz neural networks.\n","authors":["Mikhail Gorbunov","Nikolay Yudin","Vera Soboleva","Aibek Alanov","Alexey Naumov","Maxim Rakhuba"],"pdf_url":"https://arxiv.org/pdf/2406.10019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10986v3","updated":"2024-06-14T13:26:47Z","published":"2024-02-16T05:05:12Z","title":"FinTral: A Family of GPT-4 Level Multimodal Financial Large Language\n  Models","summary":"  We introduce FinTral, a suite of state-of-the-art multimodal large language\nmodels (LLMs) built upon the Mistral-7b model and tailored for financial\nanalysis. FinTral integrates textual, numerical, tabular, and image data. We\nenhance FinTral with domain-specific pretraining, instruction fine-tuning, and\nRLAIF training by exploiting a large collection of textual and visual datasets\nwe curate for this work. We also introduce an extensive benchmark featuring\nnine tasks and 25 datasets for evaluation, including hallucinations in the\nfinancial domain. Our FinTral model trained with direct preference optimization\nemploying advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R,\ndemonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5\nin all tasks and surpasses GPT-4 in five out of nine tasks, marking a\nsignificant advancement in AI-driven financial technology. We also demonstrate\nthat FinTral has the potential to excel in real-time analysis and\ndecision-making in diverse financial contexts. The GitHub repository for\nFinTral is available at \\url{https://github.com/UBC-NLP/fintral}.\n","authors":["Gagan Bhatia","El Moatez Billah Nagoudi","Hasan Cavusoglu","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2402.10986v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09994v1","updated":"2024-06-14T13:07:46Z","published":"2024-06-14T13:07:46Z","title":"Precision Empowers, Excess Distracts: Visual Question Answering With\n  Dynamically Infused Knowledge In Language Models","summary":"  In the realm of multimodal tasks, Visual Question Answering (VQA) plays a\ncrucial role by addressing natural language questions grounded in visual\ncontent. Knowledge-Based Visual Question Answering (KBVQA) advances this\nconcept by adding external knowledge along with images to respond to questions.\nWe introduce an approach for KBVQA, augmenting the existing vision-language\ntransformer encoder-decoder (OFA) model. Our main contribution involves\nenhancing questions by incorporating relevant external knowledge extracted from\nknowledge graphs, using a dynamic triple extraction method. We supply a\nflexible number of triples from the knowledge graph as context, tailored to\nmeet the requirements for answering the question. Our model, enriched with\nknowledge, demonstrates an average improvement of 4.75\\% in Exact Match Score\nover the state-of-the-art on three different KBVQA datasets. Through\nexperiments and analysis, we demonstrate that furnishing variable triples for\neach question improves the reasoning capabilities of the language model in\ncontrast to supplying a fixed number of triples. This is illustrated even for\nrecent large language models. Additionally, we highlight the model's\ngeneralization capability by showcasing its SOTA-beating performance on a small\ndataset, achieved through straightforward fine-tuning.\n","authors":["Manas Jhalani","Annervaz K M","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2406.09994v1.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2405.06211v2","updated":"2024-06-14T13:07:27Z","published":"2024-05-10T02:48:45Z","title":"A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language\n  Models","summary":"  As one of the most advanced techniques in AI, Retrieval-Augmented Generation\n(RAG) can offer reliable and up-to-date external knowledge, providing huge\nconvenience for numerous tasks. Particularly in the era of AI-Generated Content\n(AIGC), the powerful capacity of retrieval in providing additional knowledge\nenables RAG to assist existing generative AI in producing high-quality outputs.\nRecently, Large Language Models (LLMs) have demonstrated revolutionary\nabilities in language understanding and generation, while still facing inherent\nlimitations, such as hallucinations and out-of-date internal knowledge. Given\nthe powerful abilities of RAG in providing the latest and helpful auxiliary\ninformation, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged\nto harness external and authoritative knowledge bases, rather than solely\nrelying on the model's internal knowledge, to augment the generation quality of\nLLMs. In this survey, we comprehensively review existing research studies in\nRA-LLMs, covering three primary technical perspectives: architectures, training\nstrategies, and applications. As the preliminary knowledge, we briefly\nintroduce the foundations and recent advances of LLMs. Then, to illustrate the\npractical significance of RAG for LLMs, we systematically review mainstream\nrelevant work by their architectures, training strategies, and application\nareas, detailing specifically the challenges of each and the corresponding\ncapabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss\ncurrent limitations and several promising directions for future research.\nUpdated information about this survey can be found at\nhttps://advanced-recommender-systems.github.io/RAG-Meets-LLMs/\n","authors":["Wenqi Fan","Yujuan Ding","Liangbo Ning","Shijie Wang","Hengyun Li","Dawei Yin","Tat-Seng Chua","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2405.06211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09988v1","updated":"2024-06-14T12:52:42Z","published":"2024-06-14T12:52:42Z","title":"Details Make a Difference: Object State-Sensitive Neurorobotic Task\n  Planning","summary":"  The state of an object reflects its current status or condition and is\nimportant for a robot's task planning and manipulation. However, detecting an\nobject's state and generating a state-sensitive plan for robots is challenging.\nRecently, pre-trained Large Language Models (LLMs) and Vision-Language Models\n(VLMs) have shown impressive capabilities in generating plans. However, to the\nbest of our knowledge, there is hardly any investigation on whether LLMs or\nVLMs can also generate object state-sensitive plans. To study this, we\nintroduce an Object State-Sensitive Agent (OSSA), a task-planning agent\nempowered by pre-trained neural networks. We propose two methods for OSSA: (i)\na modular model consisting of a pre-trained vision processing module (dense\ncaptioning model, DCM) and a natural language processing model (LLM), and (ii)\na monolithic model consisting only of a VLM. To quantitatively evaluate the\nperformances of the two methods, we use tabletop scenarios where the task is to\nclear the table. We contribute a multimodal benchmark dataset that takes object\nstates into consideration. Our results show that both methods can be used for\nobject state-sensitive tasks, but the monolithic approach outperforms the\nmodular approach. The code for OSSA is available at\n\\url{https://github.com/Xiao-wen-Sun/OSSA}\n","authors":["Xiaowen Sun","Xufeng Zhao","Jae Hee Lee","Wenhao Lu","Matthias Kerzel","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2406.09988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09979v1","updated":"2024-06-14T12:41:07Z","published":"2024-06-14T12:41:07Z","title":"HIRO: Hierarchical Information Retrieval Optimization","summary":"  Large Language Models (LLMs) excel in natural language tasks but face\nlimitations due to static training datasets, resulting in outdated or\ncontextually shallow responses. Retrieval-Augmented Generation (RAG) addresses\nthis by integrating real-time external knowledge, enhancing model accuracy and\ncredibility, especially for knowledge-intensive tasks. However, RAG-enhanced\nLLMs struggle with long contexts, causing them to \"choke\" on information\noverload, compromising response quality. Recent RAG applications use\nhierarchical data structures for storing documents, organized at various levels\nof summarization and information density. In this context, we introduce HIRO\n(Hierarchical Information Retrieval Optimization), a novel querying approach\nfor RAG applications using hierarchical structures for storing documents. HIRO\nemploys DFS-based recursive similarity score calculation and branch pruning to\nminimize the context returned to the LLM without informational loss. HIRO\noutperforms existing querying mechanisms on the NarrativeQA dataset by an\nabsolute performance gain of 10.85%.\n","authors":["Krish Goel","Mahek Chandak"],"pdf_url":"https://arxiv.org/pdf/2406.09979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09977v1","updated":"2024-06-14T12:39:39Z","published":"2024-06-14T12:39:39Z","title":"Disentangling Dialect from Social Bias via Multitask Learning to Improve\n  Fairness","summary":"  Dialects introduce syntactic and lexical variations in language that occur in\nregional or social groups. Most NLP methods are not sensitive to such\nvariations. This may lead to unfair behavior of the methods, conveying negative\nbias towards dialect speakers. While previous work has studied dialect-related\nfairness for aspects like hate speech, other aspects of biased language, such\nas lewdness, remain fully unexplored. To fill this gap, we investigate\nperformance disparities between dialects in the detection of five aspects of\nbiased language and how to mitigate them. To alleviate bias, we present a\nmultitask learning approach that models dialect language as an auxiliary task\nto incorporate syntactic and lexical variations. In our experiments with\nAfrican-American English dialect, we provide empirical evidence that\ncomplementing common learning approaches with dialect modeling improves their\nfairness. Furthermore, the results suggest that multitask learning achieves\nstate-of-the-art performance and helps to detect properties of biased language\nmore reliably.\n","authors":["Maximilian Spliethöver","Sai Nikhil Menon","Henning Wachsmuth"],"pdf_url":"https://arxiv.org/pdf/2406.09977v1.pdf","comment":"Accepted to Findings of the Association for Computational\n  Linguistics: ACL 2024"},{"id":"http://arxiv.org/abs/2406.09972v1","updated":"2024-06-14T12:31:44Z","published":"2024-06-14T12:31:44Z","title":"A Better LLM Evaluator for Text Generation: The Impact of Prompt Output\n  Sequencing and Optimization","summary":"  This research investigates prompt designs of evaluating generated texts using\nlarge language models (LLMs). While LLMs are increasingly used for scoring\nvarious inputs, creating effective prompts for open-ended text evaluation\nremains challenging due to model sensitivity and subjectivity in evaluation of\ntext generation. Our study experimented with different prompt structures,\naltering the sequence of output instructions and including explanatory reasons.\nWe found that the order of presenting reasons and scores significantly\ninfluences LLMs' scoring, with a different level of rule understanding in the\nprompt. An additional optimization may enhance scoring alignment if sufficient\ndata is available. This insight is crucial for improving the accuracy and\nconsistency of LLM-based evaluations.\n","authors":["KuanChao Chu","Yi-Pei Chen","Hideki Nakayama"],"pdf_url":"https://arxiv.org/pdf/2406.09972v1.pdf","comment":"Presented in JSAI 2024. The first two authors contributed equally.\n  arXiv admin note: substantial text overlap with arXiv:2406.02863"},{"id":"http://arxiv.org/abs/2406.09967v1","updated":"2024-06-14T12:16:08Z","published":"2024-06-14T12:16:08Z","title":"Bag of Lies: Robustness in Continuous Pre-training BERT","summary":"  This study aims to acquire more insights into the continuous pre-training\nphase of BERT regarding entity knowledge, using the COVID-19 pandemic as a case\nstudy. Since the pandemic emerged after the last update of BERT's pre-training\ndata, the model has little to no entity knowledge about COVID-19. Using\ncontinuous pre-training, we control what entity knowledge is available to the\nmodel. We compare the baseline BERT model with the further pre-trained variants\non the fact-checking benchmark Check-COVID. To test the robustness of\ncontinuous pre-training, we experiment with several adversarial methods to\nmanipulate the input data, such as training on misinformation and shuffling the\nword order until the input becomes nonsensical. Surprisingly, our findings\nreveal that these methods do not degrade, and sometimes even improve, the\nmodel's downstream performance. This suggests that continuous pre-training of\nBERT is robust against misinformation. Furthermore, we are releasing a new\ndataset, consisting of original texts from academic publications in the\nLitCovid repository and their AI-generated false counterparts.\n","authors":["Ine Gevers","Walter Daelemans"],"pdf_url":"https://arxiv.org/pdf/2406.09967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09961v1","updated":"2024-06-14T12:10:51Z","published":"2024-06-14T12:10:51Z","title":"ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via\n  Chart-to-Code Generation","summary":"  We introduce a new benchmark, ChartMimic, aimed at assessing the\nvisually-grounded code generation capabilities of large multimodal models\n(LMMs). ChartMimic utilizes information-intensive visual charts and textual\ninstructions as inputs, requiring LMMs to generate the corresponding code for\nchart rendering. ChartMimic includes 1,000 human-curated (figure, instruction,\ncode) triplets, which represent the authentic chart use cases found in\nscientific papers across various domains(e.g., Physics, Computer Science,\nEconomics, etc). These charts span 18 regular types and 4 advanced types,\ndiversifying into 191 subcategories. Furthermore, we propose multi-level\nevaluation metrics to provide an automatic and thorough assessment of the\noutput code and the rendered charts. Unlike existing code generation\nbenchmarks, ChartMimic places emphasis on evaluating LMMs' capacity to\nharmonize a blend of cognitive capabilities, encompassing visual understanding,\ncode generation, and cross-modal reasoning. The evaluation of 3 proprietary\nmodels and 11 open-weight models highlights the substantial challenges posed by\nChartMimic. Even the advanced GPT-4V, Claude-3-opus only achieve an average\nscore of 73.2 and 53.7, respectively, indicating significant room for\nimprovement. We anticipate that ChartMimic will inspire the development of\nLMMs, advancing the pursuit of artificial general intelligence.\n","authors":["Chufan Shi","Cheng Yang","Yaxin Liu","Bo Shui","Junjie Wang","Mohan Jing","Linran Xu","Xinyu Zhu","Siheng Li","Yuxiang Zhang","Gongye Liu","Xiaomei Nie","Deng Cai","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2406.09961v1.pdf","comment":"Data and code are available at\n  https://github.com/ChartMimic/ChartMimic"},{"id":"http://arxiv.org/abs/2406.09952v1","updated":"2024-06-14T11:58:49Z","published":"2024-06-14T11:58:49Z","title":"BiVLC: Extending Vision-Language Compositionality Evaluation with\n  Text-to-Image Retrieval","summary":"  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe\nare formulated as image-to-text retrieval problems, where, given an image, the\nmodels need to select between the correct textual description and a synthetic\nhard negative text. In this work we present the Bidirectional Vision-Language\nCompositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic\nhard negative image generated from the synthetic text, resulting in two\nimage-to-text retrieval examples (one for each image) and, more importantly,\ntwo text-to-image retrieval examples (one for each text). Human annotators\nfilter out ill-formed examples ensuring the validity of the benchmark. The\nexperiments on BiVLC uncover a weakness of current multimodal models, as they\nperform poorly in the text-to-image direction. In fact, when considering both\nretrieval directions, the conclusions obtained in previous works change\nsignificantly. In addition to the benchmark, we show that a contrastive model\ntrained using synthetic images and texts improves the state of the art in\nSugarCrepe and in BiVLC for both retrieval directions. The gap to human\nperformance in BiVLC confirms that Vision-Language Compositionality is still a\nchallenging problem. BiVLC and code are available at\nhttps://imirandam.github.io/BiVLC_project_page.\n","authors":["Imanol Miranda","Ander Salaberria","Eneko Agirre","Gorka Azkune"],"pdf_url":"https://arxiv.org/pdf/2406.09952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09950v1","updated":"2024-06-14T11:53:14Z","published":"2024-06-14T11:53:14Z","title":"An efficient text augmentation approach for contextualized Mandarin\n  speech recognition","summary":"  Although contextualized automatic speech recognition (ASR) systems are\ncommonly used to improve the recognition of uncommon words, their effectiveness\nis hindered by the inherent limitations of speech-text data availability. To\naddress this challenge, our study proposes to leverage extensive text-only\ndatasets and contextualize pre-trained ASR models using a straightforward\ntext-augmentation (TA) technique, all while keeping computational costs\nminimal. In particular, to contextualize a pre-trained CIF-based ASR, we\nconstruct a codebook using limited speech-text data. By utilizing a simple\ncodebook lookup process, we convert available text-only data into latent text\nembeddings. These embeddings then enhance the inputs for the contextualized\nASR. Our experiments on diverse Mandarin test sets demonstrate that our TA\napproach significantly boosts recognition performance. The top-performing\nsystem shows relative CER improvements of up to 30% on rare words and 15%\nacross all words in general.\n","authors":["Naijun Zheng","Xucheng Wan","Kai Liu","Ziqing Du","Zhou Huan"],"pdf_url":"https://arxiv.org/pdf/2406.09950v1.pdf","comment":"accepted to interspeech2024"},{"id":"http://arxiv.org/abs/2406.09948v1","updated":"2024-06-14T11:48:54Z","published":"2024-06-14T11:48:54Z","title":"BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures\n  and Languages","summary":"  Large language models (LLMs) often lack culture-specific knowledge of daily\nlife, especially across diverse regions and non-English languages. Existing\nbenchmarks for evaluating LLMs' cultural sensitivities are limited to a single\nlanguage or collected from online sources such as Wikipedia, which do not\nreflect the mundane everyday lifestyles of diverse regions. That is,\ninformation about the food people eat for their birthday celebrations, spices\nthey typically use, musical instruments youngsters play, or the sports they\npractice in school is common cultural knowledge but uncommon in easily\ncollected online sources, especially for underrepresented cultures. To address\nthis issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate\nLLMs' everyday knowledge across diverse cultures and languages. BLEnD comprises\n52.6k question-answer pairs from 16 countries/regions, in 13 different\nlanguages, including low-resource ones such as Amharic, Assamese, Azerbaijani,\nHausa, and Sundanese. We construct the benchmark to include two formats of\nquestions: short-answer and multiple-choice. We show that LLMs perform better\nfor cultures that are highly represented online, with a maximum 57.34%\ndifference in GPT-4, the best-performing model, in the short-answer format. For\ncultures represented by mid-to-high-resource languages, LLMs perform better in\ntheir local languages, but for cultures represented by low-resource languages,\nLLMs perform better in English than the local languages. We make our dataset\npublicly available at: https://github.com/nlee0212/BLEnD.\n","authors":["Junho Myung","Nayeon Lee","Yi Zhou","Jiho Jin","Rifki Afina Putri","Dimosthenis Antypas","Hsuvas Borkakoty","Eunsu Kim","Carla Perez-Almendros","Abinew Ali Ayele","Víctor Gutiérrez-Basulto","Yazmín Ibáñez-García","Hwaran Lee","Shamsuddeen Hassan Muhammad","Kiwoong Park","Anar Sabuhi Rzayev","Nina White","Seid Muhie Yimam","Mohammad Taher Pilehvar","Nedjma Ousidhoum","Jose Camacho-Collados","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2406.09948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.04675v4","updated":"2024-06-14T11:40:52Z","published":"2023-04-10T15:51:30Z","title":"Multilingual Machine Translation with Large Language Models: Empirical\n  Results and Analysis","summary":"  Large language models (LLMs) have demonstrated remarkable potential in\nhandling multilingual machine translation (MMT). In this paper, we\nsystematically investigate the advantages and challenges of LLMs for MMT by\nanswering two questions: 1) How well do LLMs perform in translating massive\nlanguages? 2) Which factors affect LLMs' performance in translation? We\nthoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our\nempirical results show that translation capabilities of LLMs are continually\ninvolving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of\ntranslation directions but still faces a large gap towards the commercial\ntranslation system like Google Translate, especially on low-resource languages.\nThrough further analysis, we discover that LLMs exhibit new working patterns\nwhen used for MMT. First, LLM can acquire translation ability in a\nresource-efficient way and generate moderate translation even on zero-resource\nlanguages. Second, instruction semantics can surprisingly be ignored when given\nin-context exemplars. Third, cross-lingual exemplars can provide better task\nguidance for low-resource translation than exemplars in the same language\npairs. Code will be released at: https://github.com/NJUNLP/MMT-LLM.\n","authors":["Wenhao Zhu","Hongyi Liu","Qingxiu Dong","Jingjing Xu","Shujian Huang","Lingpeng Kong","Jiajun Chen","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2304.04675v4.pdf","comment":"Accepted to Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2406.09938v1","updated":"2024-06-14T11:34:36Z","published":"2024-06-14T11:34:36Z","title":"Experiments in News Bias Detection with Pre-Trained Neural Transformers","summary":"  The World Wide Web provides unrivalled access to information globally,\nincluding factual news reporting and commentary. However, state actors and\ncommercial players increasingly spread biased (distorted) or fake (non-factual)\ninformation to promote their agendas. We compare several large, pre-trained\nlanguage models on the task of sentence-level news bias detection and sub-type\nclassification, providing quantitative and qualitative results.\n","authors":["Tim Menzner","Jochen L. Leidner"],"pdf_url":"https://arxiv.org/pdf/2406.09938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15529v2","updated":"2024-06-14T11:19:26Z","published":"2024-03-22T17:31:43Z","title":"LimGen: Probing the LLMs for Generating Suggestive Limitations of\n  Research Papers","summary":"  Examining limitations is a crucial step in the scholarly research reviewing\nprocess, revealing aspects where a study might lack decisiveness or require\nenhancement. This aids readers in considering broader implications for further\nresearch. In this article, we present a novel and challenging task of\nSuggestive Limitation Generation (SLG) for research papers. We compile a\ndataset called \\textbf{\\textit{LimGen}}, encompassing 4068 research papers and\ntheir associated limitations from the ACL anthology. We investigate several\napproaches to harness large language models (LLMs) for producing suggestive\nlimitations, by thoroughly examining the related challenges, practical\ninsights, and potential opportunities. Our LimGen dataset and code can be\naccessed at \\url{https://github.com/arbmf/LimGen}.\n","authors":["Abdur Rahman Bin Md Faizullah","Ashok Urlana","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2403.15529v2.pdf","comment":"Accepted at ECML-PKDD 2024"},{"id":"http://arxiv.org/abs/2406.09923v1","updated":"2024-06-14T11:10:17Z","published":"2024-06-14T11:10:17Z","title":"CliBench: Multifaceted Evaluation of Large Language Models in Clinical\n  Decisions on Diagnoses, Procedures, Lab Tests Orders and Prescriptions","summary":"  The integration of Artificial Intelligence (AI), especially Large Language\nModels (LLMs), into the clinical diagnosis process offers significant potential\nto improve the efficiency and accessibility of medical care. While LLMs have\nshown some promise in the medical domain, their application in clinical\ndiagnosis remains underexplored, especially in real-world clinical practice,\nwhere highly sophisticated, patient-specific decisions need to be made. Current\nevaluations of LLMs in this field are often narrow in scope, focusing on\nspecific diseases or specialties and employing simplified diagnostic tasks. To\nbridge this gap, we introduce CliBench, a novel benchmark developed from the\nMIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs'\ncapabilities in clinical diagnosis. This benchmark not only covers diagnoses\nfrom a diverse range of medical cases across various specialties but also\nincorporates tasks of clinical significance: treatment procedure\nidentification, lab test ordering and medication prescriptions. Supported by\nstructured output ontologies, CliBench enables a precise and multi-granular\nevaluation, offering an in-depth understanding of LLM's capability on diverse\nclinical tasks of desired granularity. We conduct a zero-shot evaluation of\nleading LLMs to assess their proficiency in clinical decision-making. Our\npreliminary results shed light on the potential and limitations of current LLMs\nin clinical settings, providing valuable insights for future advancements in\nLLM-powered healthcare.\n","authors":["Mingyu Derek Ma","Chenchen Ye","Yu Yan","Xiaoxuan Wang","Peipei Ping","Timothy S Chang","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09923v1.pdf","comment":"Project page: https://clibench.github.io"},{"id":"http://arxiv.org/abs/2406.09920v1","updated":"2024-06-14T11:02:21Z","published":"2024-06-14T11:02:21Z","title":"Knowledge Editing in Language Models via Adapted Direct Preference\n  Optimization","summary":"  Large Language Models (LLMs) can become outdated over time as they may lack\nupdated world knowledge, leading to factual knowledge errors and gaps.\nKnowledge Editing (KE) aims to overcome this challenge using weight updates\nthat do not require expensive retraining. We propose treating KE as an LLM\nalignment problem. Toward this goal, we introduce Knowledge Direct Preference\nOptimization (KDPO), a variation of the Direct Preference Optimization (DPO)\nthat is more effective for knowledge modifications. Our method is based on an\nonline approach that continually updates the knowledge stored in the model. We\nuse the current knowledge as a negative sample and the new knowledge we want to\nintroduce as a positive sample in a process called DPO. We also use\nteacher-forcing for negative sample generation and optimize using the positive\nsample, which helps maintain localized changes. We tested our KE method on\nvarious datasets and models, comparing it to several cutting-edge methods, with\n100 and 500 sequential edits. Additionally, we conducted an ablation study\ncomparing our method to the standard DPO approach. Our experimental results\nshow that our modified DPO method allows for more refined KE, achieving similar\nor better performance compared to previous methods.\n","authors":["Amit Rozner","Barak Battash","Lior Wolf","Ofir Lindenbaum"],"pdf_url":"https://arxiv.org/pdf/2406.09920v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.02982v4","updated":"2024-06-14T10:17:40Z","published":"2024-01-01T15:26:23Z","title":"FinDABench: Benchmarking Financial Data Analysis Ability of Large\n  Language Models","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of tasks. However, their proficiency and reliability in the\nspecialized domain of financial data analysis, particularly focusing on\ndata-driven thinking, remain uncertain. To bridge this gap, we introduce\n\\texttt{FinDABench}, a comprehensive benchmark designed to evaluate the\nfinancial data analysis capabilities of LLMs within this context.\n\\texttt{FinDABench} assesses LLMs across three dimensions: 1)\n\\textbf{Foundational Ability}, evaluating the models' ability to perform\nfinancial numerical calculation and corporate sentiment risk assessment; 2)\n\\textbf{Reasoning Ability}, determining the models' ability to quickly\ncomprehend textual information and analyze abnormal financial reports; and 3)\n\\textbf{Technical Skill}, examining the models' use of technical knowledge to\naddress real-world data analysis challenges involving analysis generation and\ncharts visualization from multiple perspectives. We will release\n\\texttt{FinDABench}, and the evaluation scripts at\n\\url{https://github.com/cubenlp/BIBench}. \\texttt{FinDABench} aims to provide a\nmeasure for in-depth analysis of LLM abilities and foster the advancement of\nLLMs in the field of financial data analysis.\n","authors":["Shu Liu","Shangqing Zhao","Chenghao Jia","Xinlin Zhuang","Zhaoguang Long","Jie Zhou","Aimin Zhou","Man Lan","Qingquan Wu","Chong Yang"],"pdf_url":"https://arxiv.org/pdf/2401.02982v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09900v1","updated":"2024-06-14T10:15:49Z","published":"2024-06-14T10:15:49Z","title":"GEB-1.3B: Open Lightweight Large Language Model","summary":"  Recently developed large language models (LLMs) such as ChatGPT, Claude, and\nLlama have demonstrated impressive abilities, and even surpass human-level\nperformance in several tasks. Despite their success, the resource-intensive\ndemands of these models, requiring significant computational power for both\ntraining and inference, limit their deployment to high-performance servers.\nAdditionally, the extensive calculation requirements of the models often lead\nto increased latency in response times. With the increasing need for LLMs to\noperate efficiently on CPUs, research about lightweight models that are\noptimized for CPU inference has emerged. In this work, we introduce GEB-1.3B, a\nlightweight LLM trained on 550 billion tokens in both Chinese and English\nlanguages. We employ novel training techniques, including ROPE,\nGroup-Query-Attention, and FlashAttention-2, to accelerate training while\nmaintaining model performance. Additionally, we fine-tune the model using 10\nmillion samples of instruction data to enhance alignment. GEB-1.3B exhibits\noutstanding performance on general benchmarks such as MMLU, C-Eval, and CMMLU,\noutperforming comparative models such as MindLLM-1.3B and TinyLLaMA-1.1B.\nNotably, the FP32 version of GEB-1.3B achieves commendable inference times on\nCPUs, with ongoing efforts to further enhance speed through advanced\nquantization techniques. The release of GEB-1.3B as an open-source model marks\na significant contribution to the development of lightweight LLMs, promising to\nfoster further research and innovation in the field.\n","authors":["Jie Wu","Yufeng Zhu","Lei Shen","Xuqing Lu"],"pdf_url":"https://arxiv.org/pdf/2406.09900v1.pdf","comment":"GEB-1.3B technical report"},{"id":"http://arxiv.org/abs/2403.05530v3","updated":"2024-06-14T10:14:10Z","published":"2024-03-08T18:54:20Z","title":"Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context","summary":"  In this report, we introduce the Gemini 1.5 family of models, representing\nthe next generation of highly compute-efficient multimodal models capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio. The\nfamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds\nthe February version on the great majority of capabilities and benchmarks; (2)\nGemini 1.5 Flash, a more lightweight variant designed for efficiency with\nminimal regression in quality. Gemini 1.5 models achieve near-perfect recall on\nlong-context retrieval tasks across modalities, improve the state-of-the-art in\nlong-document QA, long-video QA and long-context ASR, and match or surpass\nGemini 1.0 Ultra's state-of-the-art performance across a broad set of\nbenchmarks. Studying the limits of Gemini 1.5's long-context ability, we find\ncontinued improvement in next-token prediction and near-perfect retrieval\n(>99%) up to at least 10M tokens, a generational leap over existing models such\nas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world\nuse cases, such as Gemini 1.5 collaborating with professionals on completing\ntheir tasks achieving 26 to 75% time savings across 10 different job\ncategories, as well as surprising new capabilities of large language models at\nthe frontier; when given a grammar manual for Kalamang, a language with fewer\nthan 200 speakers worldwide, the model learns to translate English to Kalamang\nat a similar level to a person who learned from the same content.\n","authors":[" Gemini Team","Petko Georgiev","Ving Ian Lei","Ryan Burnell","Libin Bai","Anmol Gulati","Garrett Tanzer","Damien Vincent","Zhufeng Pan","Shibo Wang","Soroosh Mariooryad","Yifan Ding","Xinyang Geng","Fred Alcober","Roy Frostig","Mark Omernick","Lexi Walker","Cosmin Paduraru","Christina Sorokin","Andrea Tacchetti","Colin Gaffney","Samira Daruki","Olcan Sercinoglu","Zach Gleicher","Juliette Love","Paul Voigtlaender","Rohan Jain","Gabriela Surita","Kareem Mohamed","Rory Blevins","Junwhan Ahn","Tao Zhu","Kornraphop Kawintiranon","Orhan Firat","Yiming Gu","Yujing Zhang","Matthew Rahtz","Manaal Faruqui","Natalie Clay","Justin Gilmer","JD Co-Reyes","Ivo Penchev","Rui Zhu","Nobuyuki Morioka","Kevin Hui","Krishna Haridasan","Victor Campos","Mahdis Mahdieh","Mandy Guo","Samer Hassan","Kevin Kilgour","Arpi Vezer","Heng-Tze Cheng","Raoul de Liedekerke","Siddharth Goyal","Paul Barham","DJ Strouse","Seb Noury","Jonas Adler","Mukund Sundararajan","Sharad Vikram","Dmitry Lepikhin","Michela Paganini","Xavier Garcia","Fan Yang","Dasha Valter","Maja Trebacz","Kiran Vodrahalli","Chulayuth Asawaroengchai","Roman Ring","Norbert Kalb","Livio Baldini Soares","Siddhartha Brahma","David Steiner","Tianhe Yu","Fabian Mentzer","Antoine He","Lucas Gonzalez","Bibo Xu","Raphael Lopez Kaufman","Laurent El Shafey","Junhyuk Oh","Tom Hennigan","George van den Driessche","Seth Odoom","Mario Lucic","Becca Roelofs","Sid Lall","Amit Marathe","Betty Chan","Santiago Ontanon","Luheng He","Denis Teplyashin","Jonathan Lai","Phil Crone","Bogdan Damoc","Lewis Ho","Sebastian Riedel","Karel Lenc","Chih-Kuan Yeh","Aakanksha Chowdhery","Yang Xu","Mehran Kazemi","Ehsan Amid","Anastasia Petrushkina","Kevin Swersky","Ali Khodaei","Gowoon Chen","Chris Larkin","Mario Pinto","Geng Yan","Adria Puigdomenech Badia","Piyush Patil","Steven Hansen","Dave Orr","Sebastien M. R. Arnold","Jordan Grimstad","Andrew Dai","Sholto Douglas","Rishika Sinha","Vikas Yadav","Xi Chen","Elena Gribovskaya","Jacob Austin","Jeffrey Zhao","Kaushal Patel","Paul Komarek","Sophia Austin","Sebastian Borgeaud","Linda Friso","Abhimanyu Goyal","Ben Caine","Kris Cao","Da-Woon Chung","Matthew Lamm","Gabe Barth-Maron","Thais Kagohara","Kate Olszewska","Mia Chen","Kaushik Shivakumar","Rishabh Agarwal","Harshal Godhia","Ravi Rajwar","Javier Snaider","Xerxes Dotiwalla","Yuan Liu","Aditya Barua","Victor Ungureanu","Yuan Zhang","Bat-Orgil Batsaikhan","Mateo Wirth","James Qin","Ivo Danihelka","Tulsee Doshi","Martin Chadwick","Jilin Chen","Sanil Jain","Quoc Le","Arjun Kar","Madhu Gurumurthy","Cheng Li","Ruoxin Sang","Fangyu Liu","Lampros Lamprou","Rich Munoz","Nathan Lintz","Harsh Mehta","Heidi Howard","Malcolm Reynolds","Lora Aroyo","Quan Wang","Lorenzo Blanco","Albin Cassirer","Jordan Griffith","Dipanjan Das","Stephan Lee","Jakub Sygnowski","Zach Fisher","James Besley","Richard Powell","Zafarali Ahmed","Dominik Paulus","David Reitter","Zalan Borsos","Rishabh Joshi","Aedan Pope","Steven Hand","Vittorio Selo","Vihan Jain","Nikhil Sethi","Megha Goel","Takaki Makino","Rhys May","Zhen Yang","Johan Schalkwyk","Christina Butterfield","Anja Hauth","Alex Goldin","Will Hawkins","Evan Senter","Sergey Brin","Oliver Woodman","Marvin Ritter","Eric Noland","Minh Giang","Vijay Bolina","Lisa Lee","Tim Blyth","Ian Mackinnon","Machel Reid","Obaid Sarvana","David Silver","Alexander Chen","Lily Wang","Loren Maggiore","Oscar Chang","Nithya Attaluri","Gregory Thornton","Chung-Cheng Chiu","Oskar Bunyan","Nir Levine","Timothy Chung","Evgenii Eltyshev","Xiance Si","Timothy Lillicrap","Demetra Brady","Vaibhav Aggarwal","Boxi Wu","Yuanzhong Xu","Ross McIlroy","Kartikeya Badola","Paramjit Sandhu","Erica Moreira","Wojciech Stokowiec","Ross Hemsley","Dong Li","Alex Tudor","Pranav Shyam","Elahe Rahimtoroghi","Salem Haykal","Pablo Sprechmann","Xiang Zhou","Diana Mincu","Yujia Li","Ravi Addanki","Kalpesh Krishna","Xiao Wu","Alexandre Frechette","Matan Eyal","Allan Dafoe","Dave Lacey","Jay Whang","Thi Avrahami","Ye Zhang","Emanuel Taropa","Hanzhao Lin","Daniel Toyama","Eliza Rutherford","Motoki Sano","HyunJeong Choe","Alex Tomala","Chalence Safranek-Shrader","Nora Kassner","Mantas Pajarskas","Matt Harvey","Sean Sechrist","Meire Fortunato","Christina Lyu","Gamaleldin Elsayed","Chenkai Kuang","James Lottes","Eric Chu","Chao Jia","Chih-Wei Chen","Peter Humphreys","Kate Baumli","Connie Tao","Rajkumar Samuel","Cicero Nogueira dos Santos","Anders Andreassen","Nemanja Rakićević","Dominik Grewe","Aviral Kumar","Stephanie Winkler","Jonathan Caton","Andrew Brock","Sid Dalmia","Hannah Sheahan","Iain Barr","Yingjie Miao","Paul Natsev","Jacob Devlin","Feryal Behbahani","Flavien Prost","Yanhua Sun","Artiom Myaskovsky","Thanumalayan Sankaranarayana Pillai","Dan Hurt","Angeliki Lazaridou","Xi Xiong","Ce Zheng","Fabio Pardo","Xiaowei Li","Dan Horgan","Joe Stanton","Moran Ambar","Fei Xia","Alejandro Lince","Mingqiu Wang","Basil Mustafa","Albert Webson","Hyo Lee","Rohan Anil","Martin Wicke","Timothy Dozat","Abhishek Sinha","Enrique Piqueras","Elahe Dabir","Shyam Upadhyay","Anudhyan Boral","Lisa Anne Hendricks","Corey Fry","Josip Djolonga","Yi Su","Jake Walker","Jane Labanowski","Ronny Huang","Vedant Misra","Jeremy Chen","RJ Skerry-Ryan","Avi Singh","Shruti Rijhwani","Dian Yu","Alex Castro-Ros","Beer Changpinyo","Romina Datta","Sumit Bagri","Arnar Mar Hrafnkelsson","Marcello Maggioni","Daniel Zheng","Yury Sulsky","Shaobo Hou","Tom Le Paine","Antoine Yang","Jason Riesa","Dominika Rogozinska","Dror Marcus","Dalia El Badawy","Qiao Zhang","Luyu Wang","Helen Miller","Jeremy Greer","Lars Lowe Sjos","Azade Nova","Heiga Zen","Rahma Chaabouni","Mihaela Rosca","Jiepu Jiang","Charlie Chen","Ruibo Liu","Tara Sainath","Maxim Krikun","Alex Polozov","Jean-Baptiste Lespiau","Josh Newlan","Zeyncep Cankara","Soo Kwak","Yunhan Xu","Phil Chen","Andy Coenen","Clemens Meyer","Katerina Tsihlas","Ada Ma","Juraj Gottweis","Jinwei Xing","Chenjie Gu","Jin Miao","Christian Frank","Zeynep Cankara","Sanjay Ganapathy","Ishita Dasgupta","Steph Hughes-Fitt","Heng Chen","David Reid","Keran Rong","Hongmin Fan","Joost van Amersfoort","Vincent Zhuang","Aaron Cohen","Shixiang Shane Gu","Anhad Mohananey","Anastasija Ilic","Taylor Tobin","John Wieting","Anna Bortsova","Phoebe Thacker","Emma Wang","Emily Caveness","Justin Chiu","Eren Sezener","Alex Kaskasoli","Steven Baker","Katie Millican","Mohamed Elhawaty","Kostas Aisopos","Carl Lebsack","Nathan Byrd","Hanjun Dai","Wenhao Jia","Matthew Wiethoff","Elnaz Davoodi","Albert Weston","Lakshman Yagati","Arun Ahuja","Isabel Gao","Golan Pundak","Susan Zhang","Michael Azzam","Khe Chai Sim","Sergi Caelles","James Keeling","Abhanshu Sharma","Andy Swing","YaGuang Li","Chenxi Liu","Carrie Grimes Bostock","Yamini Bansal","Zachary Nado","Ankesh Anand","Josh Lipschultz","Abhijit Karmarkar","Lev Proleev","Abe Ittycheriah","Soheil Hassas Yeganeh","George Polovets","Aleksandra Faust","Jiao Sun","Alban Rrustemi","Pen Li","Rakesh Shivanna","Jeremiah Liu","Chris Welty","Federico Lebron","Anirudh Baddepudi","Sebastian Krause","Emilio Parisotto","Radu Soricut","Zheng Xu","Dawn Bloxwich","Melvin Johnson","Behnam Neyshabur","Justin Mao-Jones","Renshen Wang","Vinay Ramasesh","Zaheer Abbas","Arthur Guez","Constant Segal","Duc Dung Nguyen","James Svensson","Le Hou","Sarah York","Kieran Milan","Sophie Bridgers","Wiktor Gworek","Marco Tagliasacchi","James Lee-Thorp","Michael Chang","Alexey Guseynov","Ale Jakse Hartman","Michael Kwong","Ruizhe Zhao","Sheleem Kashem","Elizabeth Cole","Antoine Miech","Richard Tanburn","Mary Phuong","Filip Pavetic","Sebastien Cevey","Ramona Comanescu","Richard Ives","Sherry Yang","Cosmo Du","Bo Li","Zizhao Zhang","Mariko Iinuma","Clara Huiyi Hu","Aurko Roy","Shaan Bijwadia","Zhenkai Zhu","Danilo Martins","Rachel Saputro","Anita Gergely","Steven Zheng","Dawei Jia","Ioannis Antonoglou","Adam Sadovsky","Shane Gu","Yingying Bi","Alek Andreev","Sina Samangooei","Mina Khan","Tomas Kocisky","Angelos Filos","Chintu Kumar","Colton Bishop","Adams Yu","Sarah Hodkinson","Sid Mittal","Premal Shah","Alexandre Moufarek","Yong Cheng","Adam Bloniarz","Jaehoon Lee","Pedram Pejman","Paul Michel","Stephen Spencer","Vladimir Feinberg","Xuehan Xiong","Nikolay Savinov","Charlotte Smith","Siamak Shakeri","Dustin Tran","Mary Chesus","Bernd Bohnet","George Tucker","Tamara von Glehn","Carrie Muir","Yiran Mao","Hideto Kazawa","Ambrose Slone","Kedar Soparkar","Disha Shrivastava","James Cobon-Kerr","Michael Sharman","Jay Pavagadhi","Carlos Araya","Karolis Misiunas","Nimesh Ghelani","Michael Laskin","David Barker","Qiujia Li","Anton Briukhov","Neil Houlsby","Mia Glaese","Balaji Lakshminarayanan","Nathan Schucher","Yunhao Tang","Eli Collins","Hyeontaek Lim","Fangxiaoyu Feng","Adria Recasens","Guangda Lai","Alberto Magni","Nicola De Cao","Aditya Siddhant","Zoe Ashwood","Jordi Orbay","Mostafa Dehghani","Jenny Brennan","Yifan He","Kelvin Xu","Yang Gao","Carl Saroufim","James Molloy","Xinyi Wu","Seb Arnold","Solomon Chang","Julian Schrittwieser","Elena Buchatskaya","Soroush Radpour","Martin Polacek","Skye Giordano","Ankur Bapna","Simon Tokumine","Vincent Hellendoorn","Thibault Sottiaux","Sarah Cogan","Aliaksei Severyn","Mohammad Saleh","Shantanu Thakoor","Laurent Shefey","Siyuan Qiao","Meenu Gaba","Shuo-yiin Chang","Craig Swanson","Biao Zhang","Benjamin Lee","Paul Kishan Rubenstein","Gan Song","Tom Kwiatkowski","Anna Koop","Ajay Kannan","David Kao","Parker Schuh","Axel Stjerngren","Golnaz Ghiasi","Gena Gibson","Luke Vilnis","Ye Yuan","Felipe Tiengo Ferreira","Aishwarya Kamath","Ted Klimenko","Ken Franko","Kefan Xiao","Indro Bhattacharya","Miteyan Patel","Rui Wang","Alex Morris","Robin Strudel","Vivek Sharma","Peter Choy","Sayed Hadi Hashemi","Jessica Landon","Mara Finkelstein","Priya Jhakra","Justin Frye","Megan Barnes","Matthew Mauger","Dennis Daun","Khuslen Baatarsukh","Matthew Tung","Wael Farhan","Henryk Michalewski","Fabio Viola","Felix de Chaumont Quitry","Charline Le Lan","Tom Hudson","Qingze Wang","Felix Fischer","Ivy Zheng","Elspeth White","Anca Dragan","Jean-baptiste Alayrac","Eric Ni","Alexander Pritzel","Adam Iwanicki","Michael Isard","Anna Bulanova","Lukas Zilka","Ethan Dyer","Devendra Sachan","Srivatsan Srinivasan","Hannah Muckenhirn","Honglong Cai","Amol Mandhane","Mukarram Tariq","Jack W. Rae","Gary Wang","Kareem Ayoub","Nicholas FitzGerald","Yao Zhao","Woohyun Han","Chris Alberti","Dan Garrette","Kashyap Krishnakumar","Mai Gimenez","Anselm Levskaya","Daniel Sohn","Josip Matak","Inaki Iturrate","Michael B. Chang","Jackie Xiang","Yuan Cao","Nishant Ranka","Geoff Brown","Adrian Hutter","Vahab Mirrokni","Nanxin Chen","Kaisheng Yao","Zoltan Egyed","Francois Galilee","Tyler Liechty","Praveen Kallakuri","Evan Palmer","Sanjay Ghemawat","Jasmine Liu","David Tao","Chloe Thornton","Tim Green","Mimi Jasarevic","Sharon Lin","Victor Cotruta","Yi-Xuan Tan","Noah Fiedel","Hongkun Yu","Ed Chi","Alexander Neitz","Jens Heitkaemper","Anu Sinha","Denny Zhou","Yi Sun","Charbel Kaed","Brice Hulse","Swaroop Mishra","Maria Georgaki","Sneha Kudugunta","Clement Farabet","Izhak Shafran","Daniel Vlasic","Anton Tsitsulin","Rajagopal Ananthanarayanan","Alen Carin","Guolong Su","Pei Sun","Shashank V","Gabriel Carvajal","Josef Broder","Iulia Comsa","Alena Repina","William Wong","Warren Weilun Chen","Peter Hawkins","Egor Filonov","Lucia Loher","Christoph Hirnschall","Weiyi Wang","Jingchen Ye","Andrea Burns","Hardie Cate","Diana Gage Wright","Federico Piccinini","Lei Zhang","Chu-Cheng Lin","Ionel Gog","Yana Kulizhskaya","Ashwin Sreevatsa","Shuang Song","Luis C. Cobo","Anand Iyer","Chetan Tekur","Guillermo Garrido","Zhuyun Xiao","Rupert Kemp","Huaixiu Steven Zheng","Hui Li","Ananth Agarwal","Christel Ngani","Kati Goshvadi","Rebeca Santamaria-Fernandez","Wojciech Fica","Xinyun Chen","Chris Gorgolewski","Sean Sun","Roopal Garg","Xinyu Ye","S. M. Ali Eslami","Nan Hua","Jon Simon","Pratik Joshi","Yelin Kim","Ian Tenney","Sahitya Potluri","Lam Nguyen Thiet","Quan Yuan","Florian Luisier","Alexandra Chronopoulou","Salvatore Scellato","Praveen Srinivasan","Minmin Chen","Vinod Koverkathu","Valentin Dalibard","Yaming Xu","Brennan Saeta","Keith Anderson","Thibault Sellam","Nick Fernando","Fantine Huot","Junehyuk Jung","Mani Varadarajan","Michael Quinn","Amit Raul","Maigo Le","Ruslan Habalov","Jon Clark","Komal Jalan","Kalesha Bullard","Achintya Singhal","Thang Luong","Boyu Wang","Sujeevan Rajayogam","Julian Eisenschlos","Johnson Jia","Daniel Finchelstein","Alex Yakubovich","Daniel Balle","Michael Fink","Sameer Agarwal","Jing Li","Dj Dvijotham","Shalini Pal","Kai Kang","Jaclyn Konzelmann","Jennifer Beattie","Olivier Dousse","Diane Wu","Remi Crocker","Chen Elkind","Siddhartha Reddy Jonnalagadda","Jong Lee","Dan Holtmann-Rice","Krystal Kallarackal","Rosanne Liu","Denis Vnukov","Neera Vats","Luca Invernizzi","Mohsen Jafari","Huanjie Zhou","Lilly Taylor","Jennifer Prendki","Marcus Wu","Tom Eccles","Tianqi Liu","Kavya Kopparapu","Francoise Beaufays","Christof Angermueller","Andreea Marzoca","Shourya Sarcar","Hilal Dib","Jeff Stanway","Frank Perbet","Nejc Trdin","Rachel Sterneck","Andrey Khorlin","Dinghua Li","Xihui Wu","Sonam Goenka","David Madras","Sasha Goldshtein","Willi Gierke","Tong Zhou","Yaxin Liu","Yannie Liang","Anais White","Yunjie Li","Shreya Singh","Sanaz Bahargam","Mark Epstein","Sujoy Basu","Li Lao","Adnan Ozturel","Carl Crous","Alex Zhai","Han Lu","Zora Tung","Neeraj Gaur","Alanna Walton","Lucas Dixon","Ming Zhang","Amir Globerson","Grant Uy","Andrew Bolt","Olivia Wiles","Milad Nasr","Ilia Shumailov","Marco Selvi","Francesco Piccinno","Ricardo Aguilar","Sara McCarthy","Misha Khalman","Mrinal Shukla","Vlado Galic","John Carpenter","Kevin Villela","Haibin Zhang","Harry Richardson","James Martens","Matko Bosnjak","Shreyas Rammohan Belle","Jeff Seibert","Mahmoud Alnahlawi","Brian McWilliams","Sankalp Singh","Annie Louis","Wen Ding","Dan Popovici","Lenin Simicich","Laura Knight","Pulkit Mehta","Nishesh Gupta","Chongyang Shi","Saaber Fatehi","Jovana Mitrovic","Alex Grills","Joseph Pagadora","Dessie Petrova","Danielle Eisenbud","Zhishuai Zhang","Damion Yates","Bhavishya Mittal","Nilesh Tripuraneni","Yannis Assael","Thomas Brovelli","Prateek Jain","Mihajlo Velimirovic","Canfer Akbulut","Jiaqi Mu","Wolfgang Macherey","Ravin Kumar","Jun Xu","Haroon Qureshi","Gheorghe Comanici","Jeremy Wiesner","Zhitao Gong","Anton Ruddock","Matthias Bauer","Nick Felt","Anirudh GP","Anurag Arnab","Dustin Zelle","Jonas Rothfuss","Bill Rosgen","Ashish Shenoy","Bryan Seybold","Xinjian Li","Jayaram Mudigonda","Goker Erdogan","Jiawei Xia","Jiri Simsa","Andrea Michi","Yi Yao","Christopher Yew","Steven Kan","Isaac Caswell","Carey Radebaugh","Andre Elisseeff","Pedro Valenzuela","Kay McKinney","Kim Paterson","Albert Cui","Eri Latorre-Chimoto","Solomon Kim","William Zeng","Ken Durden","Priya Ponnapalli","Tiberiu Sosea","Christopher A. Choquette-Choo","James Manyika","Brona Robenek","Harsha Vashisht","Sebastien Pereira","Hoi Lam","Marko Velic","Denese Owusu-Afriyie","Katherine Lee","Tolga Bolukbasi","Alicia Parrish","Shawn Lu","Jane Park","Balaji Venkatraman","Alice Talbert","Lambert Rosique","Yuchung Cheng","Andrei Sozanschi","Adam Paszke","Praveen Kumar","Jessica Austin","Lu Li","Khalid Salama","Wooyeol Kim","Nandita Dukkipati","Anthony Baryshnikov","Christos Kaplanis","XiangHai Sheng","Yuri Chervonyi","Caglar Unlu","Diego de Las Casas","Harry Askham","Kathryn Tunyasuvunakool","Felix Gimeno","Siim Poder","Chester Kwak","Matt Miecnikowski","Vahab Mirrokni","Alek Dimitriev","Aaron Parisi","Dangyi Liu","Tomy Tsai","Toby Shevlane","Christina Kouridi","Drew Garmon","Adrian Goedeckemeyer","Adam R. Brown","Anitha Vijayakumar","Ali Elqursh","Sadegh Jazayeri","Jin Huang","Sara Mc Carthy","Jay Hoover","Lucy Kim","Sandeep Kumar","Wei Chen","Courtney Biles","Garrett Bingham","Evan Rosen","Lisa Wang","Qijun Tan","David Engel","Francesco Pongetti","Dario de Cesare","Dongseong Hwang","Lily Yu","Jennifer Pullman","Srini Narayanan","Kyle Levin","Siddharth Gopal","Megan Li","Asaf Aharoni","Trieu Trinh","Jessica Lo","Norman Casagrande","Roopali Vij","Loic Matthey","Bramandia Ramadhana","Austin Matthews","CJ Carey","Matthew Johnson","Kremena Goranova","Rohin Shah","Shereen Ashraf","Kingshuk Dasgupta","Rasmus Larsen","Yicheng Wang","Manish Reddy Vuyyuru","Chong Jiang","Joana Ijazi","Kazuki Osawa","Celine Smith","Ramya Sree Boppana","Taylan Bilal","Yuma Koizumi","Ying Xu","Yasemin Altun","Nir Shabat","Ben Bariach","Alex Korchemniy","Kiam Choo","Olaf Ronneberger","Chimezie Iwuanyanwu","Shubin Zhao","David Soergel","Cho-Jui Hsieh","Irene Cai","Shariq Iqbal","Martin Sundermeyer","Zhe Chen","Elie Bursztein","Chaitanya Malaviya","Fadi Biadsy","Prakash Shroff","Inderjit Dhillon","Tejasi Latkar","Chris Dyer","Hannah Forbes","Massimo Nicosia","Vitaly Nikolaev","Somer Greene","Marin Georgiev","Pidong Wang","Nina Martin","Hanie Sedghi","John Zhang","Praseem Banzal","Doug Fritz","Vikram Rao","Xuezhi Wang","Jiageng Zhang","Viorica Patraucean","Dayou Du","Igor Mordatch","Ivan Jurin","Lewis Liu","Ayush Dubey","Abhi Mohan","Janek Nowakowski","Vlad-Doru Ion","Nan Wei","Reiko Tojo","Maria Abi Raad","Drew A. Hudson","Vaishakh Keshava","Shubham Agrawal","Kevin Ramirez","Zhichun Wu","Hoang Nguyen","Ji Liu","Madhavi Sewak","Bryce Petrini","DongHyun Choi","Ivan Philips","Ziyue Wang","Ioana Bica","Ankush Garg","Jarek Wilkiewicz","Priyanka Agrawal","Xiaowei Li","Danhao Guo","Emily Xue","Naseer Shaik","Andrew Leach","Sadh MNM Khan","Julia Wiesinger","Sammy Jerome","Abhishek Chakladar","Alek Wenjiao Wang","Tina Ornduff","Folake Abu","Alireza Ghaffarkhah","Marcus Wainwright","Mario Cortes","Frederick Liu","Joshua Maynez","Slav Petrov","Yonghui Wu","Demis Hassabis","Koray Kavukcuoglu","Jeffrey Dean","Oriol Vinyals"],"pdf_url":"https://arxiv.org/pdf/2403.05530v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09897v1","updated":"2024-06-14T10:13:37Z","published":"2024-06-14T10:13:37Z","title":"3D-RPE: Enhancing Long-Context Modeling Through 3D Rotary Position\n  Encoding","summary":"  Inspired by the Bloch Sphere representation, we propose a novel rotary\nposition encoding on a three-dimensional sphere, named 3D Rotary Position\nEncoding (3D-RPE). 3D-RPE is an advanced version of the widely used 2D Rotary\nPosition Encoding (RoPE), with two major advantages for modeling long contexts:\ncontrollable long-term decay and improved position resolution. For controllable\nlong-term decay, 3D-RPE allows for the regulation of long-term decay within the\nchunk size, ensuring the modeling of relative positional information between\ntokens at a distant relative position. For enhanced position resolution, 3D-RPE\ncan mitigate the degradation of position resolution caused by position\ninterpolation on RoPE. We have conducted experiments on long-context Natural\nLanguage Understanding (NLU) and long-sequence Language Modeling (LM) tasks.\nFrom the experimental results, 3D-RPE achieved performance improvements over\nRoPE, especially in long-context NLU tasks.\n","authors":["Xindian Ma","Wenyuan Liu","Peng Zhang","Nan Xu"],"pdf_url":"https://arxiv.org/pdf/2406.09897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09881v1","updated":"2024-06-14T09:52:27Z","published":"2024-06-14T09:52:27Z","title":"A Unified Data Augmentation Framework for Low-Resource Multi-Domain\n  Dialogue Generation","summary":"  Current state-of-the-art dialogue systems heavily rely on extensive training\ndatasets. However, challenges arise in domains where domain-specific training\ndatasets are insufficient or entirely absent. To tackle this challenge, we\npropose a novel data \\textbf{A}ugmentation framework for\n\\textbf{M}ulti-\\textbf{D}omain \\textbf{D}ialogue \\textbf{G}eneration, referred\nto as \\textbf{AMD$^2$G}. The AMD$^2$G framework consists of a data augmentation\nprocess and a two-stage training approach: domain-agnostic training and domain\nadaptation training. We posit that domain corpora are a blend of\ndomain-agnostic and domain-specific features, with certain representation\npatterns shared among diverse domains. Domain-agnostic training aims to enable\nmodels to learn these common expressive patterns. To construct domain-agnostic\ndialogue corpora, we employ a \\textit{\\textbf{de-domaining}} data processing\ntechnique used to remove domain-specific features. By mitigating the effects of\ndomain-specific features, the model trained on the de-domained corpora can\neffectively learn common expression patterns in different domains.\nSubsequently, we adapt the learned domain-agnostic features to the target\ndomain through domain adaptation training. We conduct experiments on Chinese\ndialogue datasets from five different domains and show that AMD$^2$G achieves\nsuperior performance compared to both direct training on the target domain\ncorpus and collective training on all five domain corpora. Our work underscores\nAMD$^2$G as a viable alternative solution for low-resource multi-domain\ndialogue generation. Code and data associated with our work are available on\nGitHub repository$^{\\text 1}$.\n","authors":["Yongkang Liu","Ercong Nie","Zheng Hua","Zifeng Ding","Daling Wang","Yifei Zhang","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2406.09881v1.pdf","comment":"17pages,ECML-PKDD"},{"id":"http://arxiv.org/abs/2402.18409v3","updated":"2024-06-14T09:35:57Z","published":"2024-02-28T15:28:36Z","title":"A Cognitive Evaluation Benchmark of Image Reasoning and Description for\n  Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs), despite their recent success, are\nhardly comprehensively tested for their cognitive abilities. Inspired by the\nprevalent use of the \"Cookie Theft\" task in human cognition test, we propose a\nnovel evaluation benchmark to evaluate high-level cognitive ability of LVLMs\nusing images with rich semantics. It defines eight reasoning capabilities and\nconsists of an image description task and a visual question answering task. Our\nevaluation on well-known LVLMs shows that there is still a large gap in\ncognitive ability between LVLMs and humans.\n","authors":["Xiujie Song","Mengyue Wu","Kenny Q. Zhu","Chunhao Zhang","Yanyi Chen"],"pdf_url":"https://arxiv.org/pdf/2402.18409v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08183v2","updated":"2024-06-14T09:34:35Z","published":"2024-06-12T13:14:19Z","title":"Underneath the Numbers: Quantitative and Qualitative Gender Fairness in\n  LLMs for Depression Prediction","summary":"  Recent studies show bias in many machine learning models for depression\ndetection, but bias in LLMs for this task remains unexplored. This work\npresents the first attempt to investigate the degree of gender bias present in\nexisting LLMs (ChatGPT, LLaMA 2, and Bard) using both quantitative and\nqualitative approaches. From our quantitative evaluation, we found that ChatGPT\nperforms the best across various performance metrics and LLaMA 2 outperforms\nother LLMs in terms of group fairness metrics. As qualitative fairness\nevaluation remains an open research question we propose several strategies\n(e.g., word count, thematic analysis) to investigate whether and how a\nqualitative evaluation can provide valuable insights for bias analysis beyond\nwhat is possible with quantitative evaluation. We found that ChatGPT\nconsistently provides a more comprehensive, well-reasoned explanation for its\nprediction compared to LLaMA 2. We have also identified several themes adopted\nby LLMs to qualitatively evaluate gender fairness. We hope our results can be\nused as a stepping stone towards future attempts at improving qualitative\nevaluation of fairness for LLMs especially for high-stakes tasks such as\ndepression detection.\n","authors":["Micol Spitale","Jiaee Cheong","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2406.08183v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17103v2","updated":"2024-06-14T09:26:41Z","published":"2024-05-27T12:21:48Z","title":"Empowering Character-level Text Infilling by Eliminating Sub-Tokens","summary":"  In infilling tasks, sub-tokens, representing instances where a complete token\nis segmented into two parts, often emerge at the boundaries of prefixes,\nmiddles, and suffixes. Traditional methods focused on training models at the\ntoken level, leading to sub-optimal performance in character-level infilling\ntasks during the inference stage. Alternately, some approaches considered\ncharacter-level infilling, but they relied on predicting sub-tokens in\ninference, yet this strategy diminished ability in character-level infilling\ntasks due to the large perplexity of the model on sub-tokens. In this paper, we\nintroduce FIM-SE, which stands for Fill-In-the-Middle with both Starting and\nEnding character constraints. The proposed method addresses character-level\ninfilling tasks by utilizing a line-level format to avoid predicting any\nsub-token in inference. In addition, we incorporate two special tokens to\nsignify the rest of the incomplete lines, thereby enhancing generation\nguidance. Extensive experiments demonstrate that our proposed approach\nsurpasses previous methods, offering a significant advantage. Code is available\nat https://github.com/SenseLLM/FIM-SE.\n","authors":["Houxing Ren","Mingjie Zhan","Zhongyuan Wu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2405.17103v2.pdf","comment":"Accepted to ACL 2024 (main conference)"},{"id":"http://arxiv.org/abs/2406.09864v1","updated":"2024-06-14T09:22:07Z","published":"2024-06-14T09:22:07Z","title":"LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data","summary":"  Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We introduce LUMA, a unique benchmark\ndataset, featuring audio, image, and textual data from 50 classes, for learning\nfrom uncertain and multimodal data. It extends the well-known CIFAR 10/100\ndataset with audio samples extracted from three audio corpora, and text data\ngenerated using the Gemma-7B Large Language Model (LLM). The LUMA dataset\nenables the controlled injection of varying types and degrees of uncertainty to\nachieve and tailor specific experiments and benchmarking initiatives. LUMA is\nalso available as a Python package including the functions for generating\nmultiple variants of the dataset with controlling the diversity of the data,\nthe amount of noise for each modality, and adding out-of-distribution samples.\nA baseline pre-trained model is also provided alongside three uncertainty\nquantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable\nConflictive Multi-View Learning. This comprehensive dataset and its tools are\nintended to promote and support the development and benchmarking of trustworthy\nand robust multimodal deep learning approaches.\n","authors":["Grigor Bezirganyan","Sana Sellami","Laure Berti-Équille","Sébastien Fournier"],"pdf_url":"https://arxiv.org/pdf/2406.09864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09855v1","updated":"2024-06-14T09:10:24Z","published":"2024-06-14T09:10:24Z","title":"On the Encoding of Gender in Transformer-based ASR Representations","summary":"  While existing literature relies on performance differences to uncover gender\nbiases in ASR models, a deeper analysis is essential to understand how gender\nis encoded and utilized during transcript generation. This work investigates\nthe encoding and utilization of gender in the latent representations of two\ntransformer-based ASR models, Wav2Vec2 and HuBERT. Using linear erasure, we\ndemonstrate the feasibility of removing gender information from each layer of\nan ASR model and show that such an intervention has minimal impacts on the ASR\nperformance. Additionally, our analysis reveals a concentration of gender\ninformation within the first and last frames in the final layers, explaining\nthe ease of erasing gender in these layers. Our findings suggest the prospect\nof creating gender-neutral embeddings that can be integrated into ASR\nframeworks without compromising their efficacy.\n","authors":["Aravind Krishnan","Badr M. Abdullah","Dietrich Klakow"],"pdf_url":"https://arxiv.org/pdf/2406.09855v1.pdf","comment":"Accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2402.07446v3","updated":"2024-06-14T08:50:53Z","published":"2024-02-12T07:03:14Z","title":"Quality Does Matter: A Detailed Look at the Quality and Utility of\n  Web-Mined Parallel Corpora","summary":"  We conducted a detailed analysis on the quality of web-mined corpora for two\nlow-resource languages (making three language pairs, English-Sinhala,\nEnglish-Tamil and Sinhala-Tamil). We ranked each corpus according to a\nsimilarity measure and carried out an intrinsic and extrinsic evaluation on\ndifferent portions of this ranked corpus. We show that there are significant\nquality differences between different portions of web-mined corpora and that\nthe quality varies across languages and datasets. We also show that, for some\nweb-mined datasets, Neural Machine Translation (NMT) models trained with their\nhighest-ranked 25k portion can be on par with human-curated datasets.\n","authors":["Surangika Ranathunga","Nisansa de Silva","Menan Velayuthan","Aloka Fernando","Charitha Rathnayake"],"pdf_url":"https://arxiv.org/pdf/2402.07446v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09839v1","updated":"2024-06-14T08:47:15Z","published":"2024-06-14T08:47:15Z","title":"Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for\n  Improving User Experience at First Meeting","summary":"  Rapport is known as a conversational aspect focusing on relationship\nbuilding, which influences outcomes in collaborative tasks. This study aims to\nestablish human-agent rapport through small talk by using a rapport-building\nstrategy. We implemented this strategy for the virtual agents based on dialogue\nstrategies by prompting a large language model (LLM). In particular, we\nutilized two dialogue strategies-predefined sequence and free-form-to guide the\ndialogue generation framework. We conducted analyses based on human\nevaluations, examining correlations between total turn, utterance characters,\nrapport score, and user experience variables: naturalness, satisfaction,\ninterest, engagement, and usability. We investigated correlations between\nrapport score and naturalness, satisfaction, engagement, and conversation flow.\nOur experimental results also indicated that using free-form to prompt the\nrapport-building strategy performed the best in subjective scores.\n","authors":["Muhammad Yeza Baihaqi","Angel García Contreras","Seiya Kawano","Koichiro Yoshino"],"pdf_url":"https://arxiv.org/pdf/2406.09839v1.pdf","comment":"will be presented at INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.09831v1","updated":"2024-06-14T08:40:58Z","published":"2024-06-14T08:40:58Z","title":"Federated Learning driven Large Language Models for Swarm Intelligence:\n  A Survey","summary":"  Federated learning (FL) offers a compelling framework for training large\nlanguage models (LLMs) while addressing data privacy and decentralization\nchallenges. This paper surveys recent advancements in the federated learning of\nlarge language models, with a particular focus on machine unlearning, a crucial\naspect for complying with privacy regulations like the Right to be Forgotten.\nMachine unlearning in the context of federated LLMs involves systematically and\nsecurely removing individual data contributions from the learned model without\nretraining from scratch. We explore various strategies that enable effective\nunlearning, such as perturbation techniques, model decomposition, and\nincremental learning, highlighting their implications for maintaining model\nperformance and data privacy. Furthermore, we examine case studies and\nexperimental results from recent literature to assess the effectiveness and\nefficiency of these approaches in real-world scenarios. Our survey reveals a\ngrowing interest in developing more robust and scalable federated unlearning\nmethods, suggesting a vital area for future research in the intersection of AI\nethics and distributed machine learning technologies.\n","authors":["Youyang Qu"],"pdf_url":"https://arxiv.org/pdf/2406.09831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05343v2","updated":"2024-06-14T08:35:06Z","published":"2024-06-08T04:07:09Z","title":"M3GIA: A Cognition Inspired Multilingual and Multimodal General\n  Intelligence Ability Benchmark","summary":"  As recent multi-modality large language models (MLLMs) have shown formidable\nproficiency on various complex tasks, there has been increasing attention on\ndebating whether these models could eventually mirror human intelligence.\nHowever, existing benchmarks mainly focus on evaluating solely on task\nperformance, such as the accuracy of identifying the attribute of an object.\nCombining well-developed cognitive science to understand the intelligence of\nMLLMs beyond superficial achievements remains largely unexplored. To this end,\nwe introduce the first cognitive-driven multi-lingual and multi-modal benchmark\nto evaluate the general intelligence ability of MLLMs, dubbed M3GIA.\nSpecifically, we identify five key cognitive factors based on the\nwell-recognized Cattell-Horn-Carrol (CHC) model of intelligence and propose a\nnovel evaluation metric. In addition, since most MLLMs are trained to perform\nin different languages, a natural question arises: is language a key factor\ninfluencing the cognitive ability of MLLMs? As such, we go beyond English to\nencompass other languages based on their popularity, including Chinese, French,\nSpanish, Portuguese and Korean, to construct our M3GIA. We make sure all the\ndata relevant to the cultural backgrounds are collected from their native\ncontext to avoid English-centric bias. We collected a significant corpus of\ndata from human participants, revealing that the most advanced MLLM reaches the\nlower boundary of human intelligence in English. Yet, there remains a\npronounced disparity in the other five languages assessed. We also reveals an\ninteresting winner takes all phenomenon that are aligned with the discovery in\ncognitive studies. Our benchmark will be open-sourced, with the aspiration of\nfacilitating the enhancement of cognitive capabilities in MLLMs.\n","authors":["Wei Song","Yadong Li","Jianhua Xu","Guowei Wu","Lingfeng Ming","Kexin Yi","Weihua Luo","Houyi Li","Yi Du","Fangda Guo","Kaicheng Yu"],"pdf_url":"https://arxiv.org/pdf/2406.05343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09827v1","updated":"2024-06-14T08:32:45Z","published":"2024-06-14T08:32:45Z","title":"HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical\n  Attention Pruning","summary":"  In modern large language models (LLMs), increasing sequence lengths is a\ncrucial challenge for enhancing their comprehension and coherence in handling\ncomplex tasks such as multi-modal question answering. However, handling long\ncontext sequences with LLMs is prohibitively costly due to the conventional\nattention mechanism's quadratic time and space complexity, and the context\nwindow size is limited by the GPU memory. Although recent works have proposed\nlinear and sparse attention mechanisms to address this issue, their real-world\napplicability is often limited by the need to re-train pre-trained models. In\nresponse, we propose a novel approach, Hierarchically Pruned Attention (HiP),\nwhich simultaneously reduces the training and inference time complexity from\n$O(T^2)$ to $O(T \\log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To\nthis end, we devise a dynamic sparse attention mechanism that generates an\nattention mask through a novel tree-search-like algorithm for a given query on\nthe fly. HiP is training-free as it only utilizes the pre-trained attention\nscores to spot the positions of the top-$k$ most significant elements for each\nquery. Moreover, it ensures that no token is overlooked, unlike the sliding\nwindow-based sub-quadratic attention methods, such as StreamingLLM. Extensive\nexperiments on diverse real-world benchmarks demonstrate that HiP significantly\nreduces prompt (i.e., prefill) and decoding latency and memory usage while\nmaintaining high generation performance with little or no degradation. As HiP\nallows pretrained LLMs to scale to millions of tokens on commodity GPUs with no\nadditional engineering due to its easy plug-and-play deployment, we believe\nthat our work will have a large practical impact, opening up the possibility to\nmany long-context LLM applications previously infeasible.\n","authors":["Heejun Lee","Geon Park","Youngwan Lee","Jina Kim","Wonyoung Jeong","Myeongjae Jeon","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2406.09827v1.pdf","comment":"26 pages, 15 figures"},{"id":"http://arxiv.org/abs/2405.15329v2","updated":"2024-06-14T08:32:19Z","published":"2024-05-24T08:12:30Z","title":"Decompose and Aggregate: A Step-by-Step Interpretable Evaluation\n  Framework","summary":"  The acceleration of Large Language Models (LLMs) research has opened up new\npossibilities for evaluating generated texts. They serve as scalable and\neconomical evaluators, but the question of how reliable these evaluators are\nhas emerged as a crucial research question. Prior research efforts in the\nmeta-evaluation of LLMs as judges limit the prompting of an LLM to a single use\nto obtain a final evaluation decision. They then compute the agreement between\nLLMs' outputs and human labels. This lacks interpretability in understanding\nthe evaluation capability of LLMs. In light of this challenge, we propose\nDecompose and Aggregate, which breaks down the evaluation process into\ndifferent stages based on pedagogical practices. Our experiments illustrate\nthat it not only provides a more interpretable window for how well LLMs\nevaluate, but also leads to improvements up to 39.6% for different LLMs on a\nvariety of meta-evaluation benchmarks.\n","authors":["Minzhi Li","Zhengyuan Liu","Shumin Deng","Shafiq Joty","Nancy F. Chen","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2405.15329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20215v3","updated":"2024-06-14T08:23:33Z","published":"2024-05-30T16:17:40Z","title":"TS-Align: A Teacher-Student Collaborative Framework for Scalable\n  Iterative Finetuning of Large Language Models","summary":"  Mainstream approaches to aligning large language models (LLMs) heavily rely\non human preference data, particularly when models require periodic updates.\nThe standard process for iterative alignment of LLMs involves collecting new\nhuman feedback for each update. However, the data collection process is costly\nand challenging to scale. To address this issue, we introduce the \"TS-Align\"\nframework, which fine-tunes a policy model using pairwise feedback data\nautomatically mined from its outputs. This automatic mining process is\nefficiently accomplished through the collaboration between a large-scale\nteacher model and a small-scale student model. The policy fine-tuning process\ncan be iteratively repeated using on-policy generations within our proposed\nteacher-student collaborative framework. Through extensive experiments, we\ndemonstrate that our final aligned policy outperforms the base policy model\nwith an average win rate of 69.7% across seven conversational or\ninstruction-following datasets. Furthermore, we show that the ranking\ncapability of the teacher is effectively distilled into the student through our\npipeline, resulting in a small-scale yet effective reward model for policy\nmodel alignment.\n","authors":["Chen Zhang","Chengguang Tang","Dading Chong","Ke Shi","Guohua Tang","Feng Jiang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2405.20215v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09815v1","updated":"2024-06-14T08:13:34Z","published":"2024-06-14T08:13:34Z","title":"Retrieval Augmented Fact Verification by Synthesizing Contrastive\n  Arguments","summary":"  The rapid propagation of misinformation poses substantial risks to public\ninterest. To combat misinformation, large language models (LLMs) are adapted to\nautomatically verify claim credibility. Nevertheless, existing methods heavily\nrely on the embedded knowledge within LLMs and / or black-box APIs for evidence\ncollection, leading to subpar performance with smaller LLMs or upon unreliable\ncontext. In this paper, we propose retrieval augmented fact verification\nthrough the synthesis of contrasting arguments (RAFTS). Upon input claims,\nRAFTS starts with evidence retrieval, where we design a retrieval pipeline to\ncollect and re-rank relevant documents from verifiable sources. Then, RAFTS\nforms contrastive arguments (i.e., supporting or refuting) conditioned on the\nretrieved evidence. In addition, RAFTS leverages an embedding model to identify\ninformative demonstrations, followed by in-context prompting to generate the\nprediction and explanation. Our method effectively retrieves relevant documents\nas evidence and evaluates arguments from varying perspectives, incorporating\nnuanced information for fine-grained decision-making. Combined with informative\nin-context examples as prior, RAFTS achieves significant improvements to\nsupervised and LLM baselines without complex prompts. We demonstrate the\neffectiveness of our method through extensive experiments, where RAFTS can\noutperform GPT-based methods with a significantly smaller 7B LLM.\n","authors":["Zhenrui Yue","Huimin Zeng","Lanyu Shang","Yifan Liu","Yang Zhang","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09815v1.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2301.11564v2","updated":"2024-06-14T07:58:35Z","published":"2023-01-27T07:00:54Z","title":"Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance\n  Grounding","summary":"  Robotic grasping is a fundamental ability for a robot to interact with the\nenvironment. Current methods focus on how to obtain a stable and reliable\ngrasping pose in object level, while little work has been studied on part\n(shape)-wise grasping which is related to fine-grained grasping and robotic\naffordance. Parts can be seen as atomic elements to compose an object, which\ncontains rich semantic knowledge and a strong correlation with affordance.\nHowever, lacking a large part-wise 3D robotic dataset limits the development of\npart representation learning and downstream applications. In this paper, we\npropose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to\npromote 3D part-level affordance and grasping ability learning. From the\nperspective of robotic cognition, we design a two-stage fine-grained robotic\ngrasping framework (named LangPartGPD), including a novel 3D part language\ngrounding model and a part-aware grasp pose detection model, in which explicit\nlanguage input from human or large language models (LLMs) could guide a robot\nto generate part-level 6-DoF grasping pose with textual explanation. Our method\ncombines the advantages of human-robot collaboration and LLMs' planning ability\nusing explicit language as a symbolic intermediate. To evaluate the\neffectiveness of our proposed method, we perform 3D part grounding and\nfine-grained grasp detection experiments on both simulation and physical robot\nsettings, following language instructions across different degrees of textual\ncomplexity. Results show our method achieves competitive performance in 3D\ngeometry fine-grained grounding, object affordance inference, and 3D part-aware\ngrasping tasks. Our dataset and code are available on our project website\nhttps://sites.google.com/view/lang-shape\n","authors":["Yaoxian Song","Penglei Sun","Piaopiao Jin","Yi Ren","Yu Zheng","Zhixu Li","Xiaowen Chu","Yue Zhang","Tiefeng Li","Jason Gu"],"pdf_url":"https://arxiv.org/pdf/2301.11564v2.pdf","comment":"14 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.02528v4","updated":"2024-06-14T07:48:33Z","published":"2024-06-04T17:50:34Z","title":"Scalable MatMul-free Language Modeling","summary":"  Matrix multiplication (MatMul) typically dominates the overall computational\ncost of large language models (LLMs). This cost only grows as LLMs scale to\nlarger embedding dimensions and context lengths. In this work, we show that\nMatMul operations can be completely eliminated from LLMs while maintaining\nstrong performance at billion-parameter scales. Our experiments show that our\nproposed MatMul-free models achieve performance on-par with state-of-the-art\nTransformers that require far more memory during inference at a scale up to at\nleast 2.7B parameters. We investigate the scaling laws and find that the\nperformance gap between our MatMul-free models and full precision Transformers\nnarrows as the model size increases. We also provide a GPU-efficient\nimplementation of this model which reduces memory usage by up to 61% over an\nunoptimized baseline during training. By utilizing an optimized kernel during\ninference, our model's memory consumption can be reduced by more than 10x\ncompared to unoptimized models. To properly quantify the efficiency of our\narchitecture, we build a custom hardware solution on an FPGA which exploits\nlightweight operations beyond what GPUs are capable of. We processed\nbillion-parameter scale models at 13W beyond human readable throughput, moving\nLLMs closer to brain-like efficiency. This work not only shows how far LLMs can\nbe stripped back while still performing effectively, but also points at the\ntypes of operations future accelerators should be optimized for in processing\nthe next generation of lightweight LLMs. Our code implementation is available\nat https://github.com/ridgerchu/matmulfreellm.\n","authors":["Rui-Jie Zhu","Yu Zhang","Ethan Sifferman","Tyler Sheaves","Yiqiao Wang","Dustin Richmond","Peng Zhou","Jason K. Eshraghian"],"pdf_url":"https://arxiv.org/pdf/2406.02528v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13866v2","updated":"2024-06-14T07:47:47Z","published":"2023-12-21T14:03:30Z","title":"Understanding Inter-Session Intentions via Complex Logical Reasoning","summary":"  Understanding user intentions is essential for improving product\nrecommendations, navigation suggestions, and query reformulations. However,\nuser intentions can be intricate, involving multiple sessions and attribute\nrequirements connected by logical operators such as And, Or, and Not. For\ninstance, a user may search for Nike or Adidas running shoes across various\nsessions, with a preference for purple. In another example, a user may have\npurchased a mattress in a previous session and is now looking for a matching\nbed frame without intending to buy another mattress. Existing research on\nsession understanding has not adequately addressed making product or attribute\nrecommendations for such complex intentions. In this paper, we present the task\nof logical session complex query answering (LS-CQA), where sessions are treated\nas hyperedges of items, and we frame the problem of complex intention\nunderstanding as an LS-CQA task on an aggregated hypergraph of sessions, items,\nand attributes. This is a unique complex query answering task with sessions as\nordered hyperedges. We also introduce a new model, the Logical Session Graph\nTransformer (LSGT), which captures interactions among items across different\nsessions and their logical connections using a transformer structure. We\nanalyze the expressiveness of LSGT and prove the permutation invariance of the\ninputs for the logical operators. By evaluating LSGT on three datasets, we\ndemonstrate that it achieves state-of-the-art results.\n","authors":["Jiaxin Bai","Chen Luo","Zheng Li","Qingyu Yin","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2312.13866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09790v1","updated":"2024-06-14T07:40:07Z","published":"2024-06-14T07:40:07Z","title":"Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic\n  Textual Similarity","summary":"  Semantic Textual Similarity (STS) constitutes a critical research direction\nin computational linguistics and serves as a key indicator of the encoding\ncapabilities of embedding models. Driven by advances in pre-trained language\nmodels and contrastive learning techniques, leading sentence representation\nmethods can already achieved average Spearman's correlation scores of\napproximately 86 across seven STS benchmarks in SentEval. However, further\nimprovements have become increasingly marginal, with no existing method\nattaining an average score higher than 87 on these tasks. This paper conducts\nan in-depth analysis of this phenomenon and concludes that the upper limit for\nSpearman's correlation scores using contrastive learning is 87.5. To transcend\nthis ceiling, we propose an innovative approach termed Pcc-tuning, which\nemploys Pearson's correlation coefficient as a loss function to refine model\nperformance beyond contrastive learning. Experimental results demonstrate that\nPcc-tuning markedly surpasses previous state-of-the-art strategies, raising the\nSpearman's correlation score to above 90.\n","authors":["Bowen Zhang","Chunping Li"],"pdf_url":"https://arxiv.org/pdf/2406.09790v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2312.10987v3","updated":"2024-06-14T07:30:33Z","published":"2023-12-18T07:22:39Z","title":"Cross-Subject Data Splitting for Brain-to-Text Decoding","summary":"  Recent major milestones have successfully decoded non-invasive brain signals\n(e.g. functional Magnetic Resonance Imaging (fMRI) and electroencephalogram\n(EEG)) into natural language. Despite the progress in model design, how to\nsplit the datasets for training, validating, and testing still remains a matter\nof debate. Most of the prior researches applied subject-specific data\nsplitting, where the decoding model is trained and evaluated per subject. Such\nsplitting method poses challenges to the utilization efficiency of dataset as\nwell as the generalization of models. In this study, we propose a cross-subject\ndata splitting criterion for brain-to-text decoding on various types of\ncognitive dataset (fMRI, EEG), aiming to maximize dataset utilization and\nimprove model generalization. We undertake a comprehensive analysis on existing\ncross-subject data splitting strategies and prove that all these methods suffer\nfrom data leakage, namely the leakage of test data to training set, which\nsignificantly leads to overfitting and overestimation of decoding models. The\nproposed cross-subject splitting method successfully addresses the data leakage\nproblem and we re-evaluate some SOTA brain-to-text decoding models as baselines\nfor further research.\n","authors":["Congchi Yin","Qian Yu","Zhiwei Fang","Jie He","Changping Peng","Zhangang Lin","Jingping Shao","Piji Li"],"pdf_url":"https://arxiv.org/pdf/2312.10987v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09779v1","updated":"2024-06-14T07:28:02Z","published":"2024-06-14T07:28:02Z","title":"OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst","summary":"  Memes, which rapidly disseminate personal opinions and positions across the\ninternet, also pose significant challenges in propagating social bias and\nprejudice. This study presents a novel approach to detecting harmful memes,\nparticularly within the multicultural and multilingual context of Singapore.\nOur methodology integrates image captioning, Optical Character Recognition\n(OCR), and Large Language Model (LLM) analysis to comprehensively understand\nand classify harmful memes. Utilizing the BLIP model for image captioning,\nPP-OCR and TrOCR for text recognition across multiple languages, and the Qwen\nLLM for nuanced language understanding, our system is capable of identifying\nharmful content in memes created in English, Chinese, Malay, and Tamil. To\nenhance the system's performance, we fine-tuned our approach by leveraging\nadditional data labeled using GPT-4V, aiming to distill the understanding\ncapability of GPT-4V for harmful memes to our system. Our framework achieves\ntop-1 at the public leaderboard of the Online Safety Prize Challenge hosted by\nAI Singapore, with the AUROC as 0.7749 and accuracy as 0.7087, significantly\nahead of the other teams. Notably, our approach outperforms previous\nbenchmarks, with FLAVA achieving an AUROC of 0.5695 and VisualBERT an AUROC of\n0.5561.\n","authors":["Jingtao Cao","Zheng Zhang","Hongru Wang","Bin Liang","Hao Wang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2406.09779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10570v5","updated":"2024-06-14T07:26:19Z","published":"2023-10-16T16:45:12Z","title":"On Context Utilization in Summarization with Large Language Models","summary":"  Large language models (LLMs) excel in abstractive summarization tasks,\ndelivering fluent and pertinent summaries. Recent advancements have extended\ntheir capabilities to handle long-input contexts, exceeding 100k tokens.\nHowever, in question answering, language models exhibit uneven utilization of\ntheir input context. They tend to favor the initial and final segments,\nresulting in a U-shaped performance pattern concerning where the answer is\nlocated within the input. This bias raises concerns, particularly in\nsummarization where crucial content may be dispersed throughout the source\ndocument(s). Besides, in summarization, mapping facts from the source to the\nsummary is not trivial as salient content is usually re-phrased. In this paper,\nwe conduct the first comprehensive study on context utilization and position\nbias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5\nevaluation metrics. We introduce a new evaluation benchmark called MiddleSum on\nthe which we benchmark two alternative inference methods to alleviate position\nbias: hierarchical summarization and incremental summarization. Our code and\ndata can be found here: https://github.com/ntunlp/MiddleSum.\n","authors":["Mathieu Ravaut","Aixin Sun","Nancy F. Chen","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2310.10570v5.pdf","comment":"ACL 2024. 9 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.07714v3","updated":"2024-06-14T07:19:56Z","published":"2024-03-12T14:57:40Z","title":"StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models","summary":"  Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.\n","authors":["Zhicheng Guo","Sijie Cheng","Hao Wang","Shihao Liang","Yujia Qin","Peng Li","Zhiyuan Liu","Maosong Sun","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07714v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03280v3","updated":"2024-06-14T07:19:51Z","published":"2024-06-05T13:54:28Z","title":"FusionBench: A Comprehensive Benchmark of Deep Model Fusion","summary":"  Deep model fusion is an emerging technique that unifies the predictions or\nparameters of several deep neural networks into a single model in a\ncost-effective and data-efficient manner. This enables the unified model to\ntake advantage of the original models' strengths, potentially exceeding their\nperformance. Although a variety of deep model fusion techniques have been\nintroduced, their evaluations tend to be inconsistent and often inadequate to\nvalidate their effectiveness and robustness against distribution shifts. To\naddress this issue, we introduce FusionBench, which is the first comprehensive\nbenchmark dedicated to deep model fusion. FusionBench covers a wide range of\ntasks, including open-vocabulary image classification, text classification, and\ntext-to-text generation. Each category includes up to eight tasks with\ncorresponding task-specific models, featuring both full fine-tuning and LoRA\nfine-tuning, as well as models of different sizes, to ensure fair and balanced\ncomparisons of various multi-task model fusion techniques across different\ntasks, model scales, and fine-tuning strategies. We implement and evaluate a\nbroad spectrum of deep model fusion techniques. These techniques range from\nmodel ensemble methods, which combine the predictions to improve the overall\nperformance, to model merging, which integrates different models into a single\none, and model mixing methods, which upscale or recombine the components of the\noriginal models. FusionBench now contains 26 distinct tasks, 74 fine-tuned\nmodels, and 16 fusion techniques, and we are committed to consistently\nexpanding the benchmark with more tasks, models, and fusion techniques. In\naddition, we offer a well-documented set of resources and guidelines to aid\nresearchers in understanding and replicating the benchmark results. Homepage\nhttps://github.com/tanganke/fusion_bench\n","authors":["Anke Tang","Li Shen","Yong Luo","Han Hu","Bo Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2406.03280v3.pdf","comment":"Project homepage: https://github.com/tanganke/fusion_bench"},{"id":"http://arxiv.org/abs/2406.09765v1","updated":"2024-06-14T07:06:24Z","published":"2024-06-14T07:06:24Z","title":"Application of Natural Language Processing in Financial Risk Detection","summary":"  This paper explores the application of Natural Language Processing (NLP) in\nfinancial risk detection. By constructing an NLP-based financial risk detection\nmodel, this study aims to identify and predict potential risks in financial\ndocuments and communications. First, the fundamental concepts of NLP and its\ntheoretical foundation, including text mining methods, NLP model design\nprinciples, and machine learning algorithms, are introduced. Second, the\nprocess of text data preprocessing and feature extraction is described.\nFinally, the effectiveness and predictive performance of the model are\nvalidated through empirical research. The results show that the NLP-based\nfinancial risk detection model performs excellently in risk identification and\nprediction, providing effective risk management tools for financial\ninstitutions. This study offers valuable references for the field of financial\nrisk management, utilizing advanced NLP techniques to improve the accuracy and\nefficiency of financial risk detection.\n","authors":["Liyang Wang","Yu Cheng","Ao Xiang","Jingyu Zhang","Haowei Yang"],"pdf_url":"https://arxiv.org/pdf/2406.09765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09760v1","updated":"2024-06-14T06:57:18Z","published":"2024-06-14T06:57:18Z","title":"Bootstrapping Language Models with DPO Implicit Rewards","summary":"  Human alignment in large language models (LLMs) is an active area of\nresearch. A recent groundbreaking work, direct preference optimization (DPO),\nhas greatly simplified the process from past work in reinforcement learning\nfrom human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO,\nafter training, provides an implicit reward model. In this work, we make a\nnovel observation that this implicit reward model can by itself be used in a\nbootstrapping fashion to further align the LLM. Our approach is to use the\nrewards from a current LLM model to construct a preference dataset, which is\nthen used in subsequent DPO rounds. We incorporate refinements that debias the\nlength of the responses and improve the quality of the preference dataset to\nfurther improve our approach. Our approach, named self-alignment with DPO\nImpliCit rEwards (DICE), shows great improvements in alignment and achieves\nsuperior performance than Gemini Pro on AlpacaEval 2, reaching 27.55%\nlength-controlled win rate against GPT-4 Turbo, but with only 8B parameters and\nno external feedback. Our code is available at https://github.com/sail-sg/dice.\n","authors":["Changyu Chen","Zichen Liu","Chao Du","Tianyu Pang","Qian Liu","Arunesh Sinha","Pradeep Varakantham","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2406.09760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14005v2","updated":"2024-06-14T06:40:31Z","published":"2023-07-26T07:36:25Z","title":"Unsupervised extraction of local and global keywords from a single text","summary":"  We propose an unsupervised, corpus-independent method to extract keywords\nfrom a single text. It is based on the spatial distribution of words and the\nresponse of this distribution to a random permutation of words. As compared to\nexisting methods (such as e.g. YAKE) our method has three advantages. First, it\nis significantly more effective at extracting keywords from long texts. Second,\nit allows inference of two types of keywords: local and global. Third, it\nuncovers basic themes in texts. Additionally, our method is\nlanguage-independent and applies to short texts. The results are obtained via\nhuman annotators with previous knowledge of texts from our database of\nclassical literary works (the agreement between annotators is from moderate to\nsubstantial). Our results are supported via human-independent arguments based\non the average length of extracted content words and on the average number of\nnouns in extracted words. We discuss relations of keywords with higher-order\ntextual features and reveal a connection between keywords and chapter\ndivisions.\n","authors":["Lida Aleksanyan","Armen E. Allahverdyan"],"pdf_url":"https://arxiv.org/pdf/2307.14005v2.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2402.17019v3","updated":"2024-06-14T06:22:51Z","published":"2024-02-26T20:56:06Z","title":"Leveraging Large Language Models for Learning Complex Legal Concepts\n  through Storytelling","summary":"  Making legal knowledge accessible to non-experts is crucial for enhancing\ngeneral legal literacy and encouraging civic participation in democracy.\nHowever, legal documents are often challenging to understand for people without\nlegal backgrounds. In this paper, we present a novel application of large\nlanguage models (LLMs) in legal education to help non-experts learn intricate\nlegal concepts through storytelling, an effective pedagogical tool in conveying\ncomplex and abstract concepts. We also introduce a new dataset LegalStories,\nwhich consists of 294 complex legal doctrines, each accompanied by a story and\na set of multiple-choice questions generated by LLMs. To construct the dataset,\nwe experiment with various LLMs to generate legal stories explaining these\nconcepts. Furthermore, we use an expert-in-the-loop approach to iteratively\ndesign multiple-choice questions. Then, we evaluate the effectiveness of\nstorytelling with LLMs through randomized controlled trials (RCTs) with legal\nnovices on 10 samples from the dataset. We find that LLM-generated stories\nenhance comprehension of legal concepts and interest in law among non-native\nspeakers compared to only definitions. Moreover, stories consistently help\nparticipants relate legal concepts to their lives. Finally, we find that\nlearning with stories shows a higher retention rate for non-native speakers in\nthe follow-up assessment. Our work has strong implications for using LLMs in\npromoting teaching and learning in the legal field and beyond.\n","authors":["Hang Jiang","Xiajie Zhang","Robert Mahari","Daniel Kessler","Eric Ma","Tal August","Irene Li","Alex 'Sandy' Pentland","Yoon Kim","Jad Kabbara","Deb Roy"],"pdf_url":"https://arxiv.org/pdf/2402.17019v3.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2405.00675v4","updated":"2024-06-14T05:57:01Z","published":"2024-05-01T17:59:20Z","title":"Self-Play Preference Optimization for Language Model Alignment","summary":"  Traditional reinforcement learning from human feedback (RLHF) approaches\nrelying on parametric models like the Bradley-Terry model fall short in\ncapturing the intransitivity and irrationality in human preferences. Recent\nadvancements suggest that directly working with preference probabilities can\nyield a more accurate reflection of human preferences, enabling more flexible\nand accurate language model alignment. In this paper, we propose a\nself-play-based method for language model alignment, which treats the problem\nas a constant-sum two-player game aimed at identifying the Nash equilibrium\npolicy. Our approach, dubbed Self-Play Preference Optimization (SPPO),\napproximates the Nash equilibrium through iterative policy updates and enjoys a\ntheoretical convergence guarantee. Our method can effectively increase the\nlog-likelihood of the chosen response and decrease that of the rejected\nresponse, which cannot be trivially achieved by symmetric pairwise loss such as\nDirect Preference Optimization (DPO) and Identity Preference Optimization\n(IPO). In our experiments, using only 60k prompts (without responses) from the\nUltraFeedback dataset and without any prompt augmentation, by leveraging a\npre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain\na model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the\nstate-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on\nAlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and\nthe Open LLM Leaderboard. Starting from a stronger base model\nLlama-3-8B-Instruct, we are able to achieve a length-controlled win rate of\n38.77%. Notably, the strong performance of SPPO is achieved without additional\nexternal supervision (e.g., responses, preferences, etc.) from GPT-4 or other\nstronger language models. Codes are available at\nhttps://github.com/uclaml/SPPO.\n","authors":["Yue Wu","Zhiqing Sun","Huizhuo Yuan","Kaixuan Ji","Yiming Yang","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2405.00675v4.pdf","comment":"27 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.09719v1","updated":"2024-06-14T05:11:32Z","published":"2024-06-14T05:11:32Z","title":"Self-Knowledge Distillation for Learning Ambiguity","summary":"  Recent language models have shown remarkable performance on natural language\nunderstanding (NLU) tasks. However, they are often sub-optimal when faced with\nambiguous samples that can be interpreted in multiple ways, over-confidently\npredicting a single label without consideration for its correctness. To address\nthis issue, we propose a novel self-knowledge distillation method that enables\nmodels to learn label distributions more accurately by leveraging knowledge\ndistilled from their lower layers. This approach also includes a learning phase\nthat re-calibrates the unnecessarily strengthened confidence for training\nsamples judged as extremely ambiguous based on the distilled distribution\nknowledge. We validate our method on diverse NLU benchmark datasets and the\nexperimental results demonstrate its effectiveness in producing better label\ndistributions. Particularly, through the process of re-calibrating the\nconfidence for highly ambiguous samples, the issue of over-confidence when\npredictions for unseen samples do not match with their ground-truth labels has\nbeen significantly alleviated. This has been shown to contribute to generating\nbetter distributions than the existing state-of-the-art method. Moreover, our\nmethod is more efficient in training the models compared to the existing\nmethod, as it does not involve additional training processes to refine label\ndistributions.\n","authors":["Hancheol Park","Soyeong Jeong","Sukmin Cho","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2406.09719v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.10587v2","updated":"2024-06-14T05:07:32Z","published":"2024-05-17T07:22:02Z","title":"RDRec: Rationale Distillation for LLM-based Recommendation","summary":"  Large language model (LLM)-based recommender models that bridge users and\nitems through textual prompts for effective semantic reasoning have gained\nconsiderable attention. However, few methods consider the underlying rationales\nbehind interactions, such as user preferences and item attributes, limiting the\nreasoning capability of LLMs for recommendations. This paper proposes a\nrationale distillation recommender (RDRec), a compact model designed to learn\nrationales generated by a larger language model (LM). By leveraging rationales\nfrom reviews related to users and items, RDRec remarkably specifies their\nprofiles for recommendations. Experiments show that RDRec achieves\nstate-of-the-art (SOTA) performance in both top-N and sequential\nrecommendations. Our source code is released at\nhttps://github.com/WangXFng/RDRec.\n","authors":["Xinfeng Wang","Jin Cui","Yoshimi Suzuki","Fumiyo Fukumoto"],"pdf_url":"https://arxiv.org/pdf/2405.10587v2.pdf","comment":"10 pages. Accepted to ACL 2024 Main as a short paper"},{"id":"http://arxiv.org/abs/2406.09717v1","updated":"2024-06-14T04:55:30Z","published":"2024-06-14T04:55:30Z","title":"UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for\n  Low-Resource Languages","summary":"  In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with\nOptimized Embeddings and Vocabulary), a comprehensive approach developed to\nimprove the effectiveness of Cross-Lingual Transfer Learning, particularly in\nlanguages with limited resources. Our approach tackles two essential elements\nof a language model: the initialization of embeddings and the optimal\nvocabulary size. Specifically, we propose a novel embedding initialization\nmethod that leverages both lexical and semantic alignment for a language. In\naddition, we present a method for systematically searching for the optimal\nvocabulary size, ensuring a balance between model complexity and linguistic\ncoverage. Our experiments across multilingual datasets show that our approach\ngreatly improves the F1-Score in several languages. UniBridge is a robust and\nadaptable solution for cross-lingual systems in various languages, highlighting\nthe significance of initializing embeddings and choosing the right vocabulary\nsize in cross-lingual environments.\n","authors":["Trinh Pham","Khoi M. Le","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2406.09717v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2403.06064v3","updated":"2024-06-14T04:15:20Z","published":"2024-03-10T02:16:13Z","title":"L^2GC:Lorentzian Linear Graph Convolutional Networks for Node\n  Classification","summary":"  Linear Graph Convolutional Networks (GCNs) are used to classify the node in\nthe graph data. However, we note that most existing linear GCN models perform\nneural network operations in Euclidean space, which do not explicitly capture\nthe tree-like hierarchical structure exhibited in real-world datasets that\nmodeled as graphs. In this paper, we attempt to introduce hyperbolic space into\nlinear GCN and propose a novel framework for Lorentzian linear GCN.\nSpecifically, we map the learned features of graph nodes into hyperbolic space,\nand then perform a Lorentzian linear feature transformation to capture the\nunderlying tree-like structure of data. Experimental results on standard\ncitation networks datasets with semi-supervised learning show that our approach\nyields new state-of-the-art results of accuracy 74.7$\\%$ on Citeseer and\n81.3$\\%$ on PubMed datasets. Furthermore, we observe that our approach can be\ntrained up to two orders of magnitude faster than other nonlinear GCN models on\nPubMed dataset. Our code is publicly available at\nhttps://github.com/llqy123/LLGC-master.\n","authors":["Qiuyu Liang","Weihua Wang","Feilong Bao","Guanglai Gao"],"pdf_url":"https://arxiv.org/pdf/2403.06064v3.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2406.09702v1","updated":"2024-06-14T04:03:24Z","published":"2024-06-14T04:03:24Z","title":"Detecting Response Generation Not Requiring Factual Judgment","summary":"  With the remarkable development of large language models (LLMs), ensuring the\nfactuality of output has become a challenge. However, having all the contents\nof the response with given knowledge or facts is not necessarily a good thing\nin dialogues. This study aimed to achieve both attractiveness and factuality in\na dialogue response for which a task was set to predict sentences that do not\nrequire factual correctness judgment such as agreeing, or personal\nopinions/feelings. We created a dataset, dialogue dataset annotated with\nfact-check-needed label (DDFC), for this task via crowdsourcing, and\nclassification tasks were performed on several models using this dataset. The\nmodel with the highest classification accuracy could yield about 88% accurate\nclassification results.\n","authors":["Ryohei Kamei","Daiki Shiono","Reina Akama","Jun Suzuki"],"pdf_url":"https://arxiv.org/pdf/2406.09702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13803v2","updated":"2024-06-14T03:54:31Z","published":"2024-05-22T16:30:24Z","title":"Sunnie: An Anthropomorphic LLM-Based Conversational Agent for Mental\n  Well-Being Activity Recommendation","summary":"  A longstanding challenge in mental well-being support is the reluctance of\npeople to adopt psychologically beneficial activities, often due to lack of\nmotivation, low perceived trustworthiness, and limited personalization of\nrecommendations. Chatbots have shown promise in promoting positive mental\nhealth practices, yet their rigid interaction flows and less human-like\nconversational experiences present significant limitations. In this work, we\nexplore whether the anthropomorphic design (both LLM's persona design and\nconversational experience design) can enhance users' perception of the system\nand their willingness to adopt mental well-being activity recommendations. To\nthis end, we introduce Sunnie, an anthropomorphic LLM-based conversational\nagent designed to offer personalized well-being support through multi-turn\nconversation and recommend practical actions grounded in positive psychology\nand social psychology. An empirical user study comparing the user experience\nwith Sunnie and with a traditional survey-based activity recommendation system\nsuggests that the anthropomorphic characteristics of Sunnie significantly\nenhance users' perception of the system and the overall usability;\nnevertheless, users' willingness to adopt activity recommendations did not\nchange significantly.\n","authors":["Siyi Wu","Feixue Han","Bingsheng Yao","Tianyi Xie","Xuan Zhao","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2405.13803v2.pdf","comment":"In Submission"},{"id":"http://arxiv.org/abs/2210.03963v3","updated":"2024-06-14T03:42:17Z","published":"2022-10-08T08:07:47Z","title":"SDA: Simple Discrete Augmentation for Contrastive Sentence\n  Representation Learning","summary":"  Contrastive learning has recently achieved compelling performance in\nunsupervised sentence representation. As an essential element, data\naugmentation protocols, however, have not been well explored. The pioneering\nwork SimCSE resorting to a simple dropout mechanism (viewed as continuous\naugmentation) surprisingly dominates discrete augmentations such as cropping,\nword deletion, and synonym replacement as reported. To understand the\nunderlying rationales, we revisit existing approaches and attempt to\nhypothesize the desiderata of reasonable data augmentation methods: balance of\nsemantic consistency and expression diversity. We then develop three simple yet\neffective discrete sentence augmentation schemes: punctuation insertion, modal\nverbs, and double negation. They act as minimal noises at lexical level to\nproduce diverse forms of sentences. Furthermore, standard negation is\ncapitalized on to generate negative samples for alleviating feature suppression\ninvolved in contrastive learning. We experimented extensively with semantic\ntextual similarity on diverse datasets. The results support the superiority of\nthe proposed methods consistently. Our key code is available at\nhttps://github.com/Zhudongsheng75/SDA\n","authors":["Dongsheng Zhu","Zhenyu Mao","Jinghui Lu","Rui Zhao","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2210.03963v3.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.05074v2","updated":"2024-06-14T03:28:49Z","published":"2023-11-09T00:12:21Z","title":"GRASP: A Disagreement Analysis Framework to Assess Group Associations in\n  Perspectives","summary":"  Human annotation plays a core role in machine learning -- annotations for\nsupervised models, safety guardrails for generative models, and human feedback\nfor reinforcement learning, to cite a few avenues. However, the fact that many\nof these human annotations are inherently subjective is often overlooked.\nRecent work has demonstrated that ignoring rater subjectivity (typically\nresulting in rater disagreement) is problematic within specific tasks and for\nspecific subgroups. Generalizable methods to harness rater disagreement and\nthus understand the socio-cultural leanings of subjective tasks remain elusive.\nIn this paper, we propose GRASP, a comprehensive disagreement analysis\nframework to measure group association in perspectives among different rater\nsub-groups, and demonstrate its utility in assessing the extent of systematic\ndisagreements in two datasets: (1) safety annotations of human-chatbot\nconversations, and (2) offensiveness annotations of social media posts, both\nannotated by diverse rater pools across different socio-demographic axes. Our\nframework (based on disagreement metrics) reveals specific rater groups that\nhave significantly different perspectives than others on certain tasks, and\nhelps identify demographic axes that are crucial to consider in specific task\ncontexts.\n","authors":["Vinodkumar Prabhakaran","Christopher Homan","Lora Aroyo","Aida Mostafazadeh Davani","Alicia Parrish","Alex Taylor","Mark Díaz","Ding Wang","Gregory Serapio-García"],"pdf_url":"https://arxiv.org/pdf/2311.05074v2.pdf","comment":"Presented as a long paper at NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2406.09688v1","updated":"2024-06-14T03:18:28Z","published":"2024-06-14T03:18:28Z","title":"FreeCtrl: Constructing Control Centers with Feedforward Layers for\n  Learning-Free Controllable Text Generation","summary":"  Controllable text generation (CTG) seeks to craft texts adhering to specific\nattributes, traditionally employing learning-based techniques such as training,\nfine-tuning, or prefix-tuning with attribute-specific datasets. These\napproaches, while effective, demand extensive computational and data resources.\nIn contrast, some proposed learning-free alternatives circumvent learning but\noften yield inferior results, exemplifying the fundamental machine learning\ntrade-off between computational expense and model efficacy. To overcome these\nlimitations, we propose FreeCtrl, a learning-free approach that dynamically\nadjusts the weights of selected feedforward neural network (FFN) vectors to\nsteer the outputs of large language models (LLMs). FreeCtrl hinges on the\nprinciple that the weights of different FFN vectors influence the likelihood of\ndifferent tokens appearing in the output. By identifying and adaptively\nadjusting the weights of attribute-related FFN vectors, FreeCtrl can control\nthe output likelihood of attribute keywords in the generated content. Extensive\nexperiments on single- and multi-attribute control reveal that the\nlearning-free FreeCtrl outperforms other learning-free and learning-based\nmethods, successfully resolving the dilemma between learning costs and model\nperformance.\n","authors":["Zijian Feng","Hanzhang Zhou","Zixiao Zhu","Kezhi Mao"],"pdf_url":"https://arxiv.org/pdf/2406.09688v1.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2403.12416v3","updated":"2024-06-14T03:18:18Z","published":"2024-03-19T03:59:14Z","title":"Eye-gaze Guided Multi-modal Alignment for Medical Representation\n  Learning","summary":"  In the medical multi-modal frameworks, the alignment of cross-modality\nfeatures presents a significant challenge. However, existing works have learned\nfeatures that are implicitly aligned from the data, without considering the\nexplicit relationships in the medical context. This data-reliance may lead to\nlow generalization of the learned alignment relationships. In this work, we\npropose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness\neye-gaze data for better alignment of medical visual and textual features. We\nexplore the natural auxiliary role of radiologists' eye-gaze data in aligning\nmedical images and text, and introduce a novel approach by using eye-gaze data,\ncollected synchronously by radiologists during diagnostic evaluations. We\nconduct downstream tasks of image classification and image-text retrieval on\nfour medical datasets, where EGMA achieved state-of-the-art performance and\nstronger generalization across different datasets. Additionally, we explore the\nimpact of varying amounts of eye-gaze data on model performance, highlighting\nthe feasibility and utility of integrating this auxiliary data into multi-modal\nalignment framework.\n","authors":["Chong Ma","Hanqi Jiang","Wenting Chen","Yiwei Li","Zihao Wu","Xiaowei Yu","Zhengliang Liu","Lei Guo","Dajiang Zhu","Tuo Zhang","Dinggang Shen","Tianming Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.12416v3.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.09676v1","updated":"2024-06-14T02:58:19Z","published":"2024-06-14T02:58:19Z","title":"Optimizing Byte-level Representation for End-to-end ASR","summary":"  We propose a novel approach to optimizing a byte-level representation for\nend-to-end automatic speech recognition (ASR). Byte-level representation is\noften used by large scale multilingual ASR systems when the character set of\nthe supported languages is large. The compactness and universality of\nbyte-level representation allow the ASR models to use smaller output\nvocabularies and therefore, provide more flexibility. UTF-8 is a commonly used\nbyte-level representation for multilingual ASR, but it is not designed to\noptimize machine learning tasks directly. By using auto-encoder and vector\nquantization, we show that we can optimize a byte-level representation for ASR\nand achieve better accuracy. Our proposed framework can incorporate information\nfrom different modalities, and provides an error correction mechanism. In an\nEnglish/Mandarin dictation task, we show that a bilingual ASR model built with\nthis approach can outperform UTF-8 representation by 5% relative in error rate.\n","authors":["Roger Hsiao","Liuhui Deng","Erik McDermott","Ruchir Travadi","Xiaodan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2406.09676v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2402.13625v2","updated":"2024-06-14T02:55:46Z","published":"2024-02-21T08:54:47Z","title":"MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning","summary":"  Since commonsense information has been recorded significantly less frequently\nthan its existence, language models pre-trained by text generation have\ndifficulty to learn sufficient commonsense knowledge. Several studies have\nleveraged text retrieval to augment the models' commonsense ability. Unlike\ntext, images capture commonsense information inherently but little effort has\nbeen paid to effectively utilize them. In this work, we propose a novel\nMulti-mOdal REtrieval (MORE) augmentation framework, to leverage both text and\nimages to enhance the commonsense ability of language models. Extensive\nexperiments on the Common-Gen task have demonstrated the efficacy of MORE based\non the pre-trained models of both single and multiple modalities.\n","authors":["Wanqing Cui","Keping Bi","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2402.13625v2.pdf","comment":"Published as a conference paper at ACL Findings 2024"},{"id":"http://arxiv.org/abs/2406.09671v1","updated":"2024-06-14T02:42:30Z","published":"2024-06-14T02:42:30Z","title":"Evaluating ChatGPT-4 Vision on Brazil's National Undergraduate Computer\n  Science Exam","summary":"  The recent integration of visual capabilities into Large Language Models\n(LLMs) has the potential to play a pivotal role in science and technology\neducation, where visual elements such as diagrams, charts, and tables are\ncommonly used to improve the learning experience. This study investigates the\nperformance of ChatGPT-4 Vision, OpenAI's most advanced visual model at the\ntime the study was conducted, on the Bachelor in Computer Science section of\nBrazil's 2021 National Undergraduate Exam (ENADE). By presenting the model with\nthe exam's open and multiple-choice questions in their original image format\nand allowing for reassessment in response to differing answer keys, we were\nable to evaluate the model's reasoning and self-reflecting capabilities in a\nlarge-scale academic assessment involving textual and visual content. ChatGPT-4\nVision significantly outperformed the average exam participant, positioning\nitself within the top 10 best score percentile. While it excelled in questions\nthat incorporated visual elements, it also encountered challenges with question\ninterpretation, logical reasoning, and visual acuity. The involvement of an\nindependent expert panel to review cases of disagreement between the model and\nthe answer key revealed some poorly constructed questions containing vague or\nambiguous statements, calling attention to the critical need for improved\nquestion design in future exams. Our findings suggest that while ChatGPT-4\nVision shows promise in multimodal academic evaluations, human oversight\nremains crucial for verifying the model's accuracy and ensuring the fairness of\nhigh-stakes educational exams. The paper's research materials are publicly\navailable at https://github.com/nabormendonca/gpt-4v-enade-cs-2021.\n","authors":["Nabor C. Mendonça"],"pdf_url":"https://arxiv.org/pdf/2406.09671v1.pdf","comment":"Accepted for publication"},{"id":"http://arxiv.org/abs/2406.03814v2","updated":"2024-06-14T02:36:39Z","published":"2024-06-06T07:39:17Z","title":"Improving Zero-Shot Chinese-English Code-Switching ASR with kNN-CTC and\n  Gated Monolingual Datastores","summary":"  The kNN-CTC model has proven to be effective for monolingual automatic speech\nrecognition (ASR). However, its direct application to multilingual scenarios\nlike code-switching, presents challenges. Although there is potential for\nperformance improvement, a kNN-CTC model utilizing a single bilingual datastore\ncan inadvertently introduce undesirable noise from the alternative language. To\naddress this, we propose a novel kNN-CTC-based code-switching ASR (CS-ASR)\nframework that employs dual monolingual datastores and a gated datastore\nselection mechanism to reduce noise interference. Our method selects the\nappropriate datastore for decoding each frame, ensuring the injection of\nlanguage-specific information into the ASR process. We apply this framework to\ncutting-edge CTC-based models, developing an advanced CS-ASR system. Extensive\nexperiments demonstrate the remarkable effectiveness of our gated datastore\nmechanism in enhancing the performance of zero-shot Chinese-English CS-ASR.\n","authors":["Jiaming Zhou","Shiwan Zhao","Hui Wang","Tian-Hao Zhang","Haoqin Sun","Xuechen Wang","Yong Qin"],"pdf_url":"https://arxiv.org/pdf/2406.03814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06459v3","updated":"2024-06-14T02:27:00Z","published":"2024-05-10T13:10:55Z","title":"Are EEG-to-Text Models Working?","summary":"  This work critically analyzes existing models for open-vocabulary EEG-to-Text\ntranslation. We identify a crucial limitation: previous studies often employed\nimplicit teacher-forcing during evaluation, artificially inflating performance\nmetrics. Additionally, they lacked a critical benchmark - comparing model\nperformance on pure noise inputs. We propose a methodology to differentiate\nbetween models that truly learn from EEG signals and those that simply memorize\ntraining data. Our analysis reveals that model performance on noise data can be\ncomparable to that on EEG data. These findings highlight the need for stricter\nevaluation practices in EEG-to-Text research, emphasizing transparent reporting\nand rigorous benchmarking with noise inputs. This approach will lead to more\nreliable assessments of model capabilities and pave the way for robust\nEEG-to-Text communication systems.\n","authors":["Hyejeong Jo","Yiqian Yang","Juhyeok Han","Yiqun Duan","Hui Xiong","Won Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2405.06459v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09662v1","updated":"2024-06-14T02:21:53Z","published":"2024-06-14T02:21:53Z","title":"Learning Language Structures through Grounding","summary":"  Language is highly structured, with syntactic and semantic structures, to\nsome extent, agreed upon by speakers of the same language. With implicit or\nexplicit awareness of such structures, humans can learn and use language\nefficiently and generalize to sentences that contain unseen words. Motivated by\nhuman language learning, in this dissertation, we consider a family of machine\nlearning tasks that aim to learn language structures through grounding. We seek\ndistant supervision from other data sources (i.e., grounds), including but not\nlimited to other modalities (e.g., vision), execution results of programs, and\nother languages.\n  We demonstrate the potential of this task formulation and advocate for its\nadoption through three schemes. In Part I, we consider learning syntactic\nparses through visual grounding. We propose the task of visually grounded\ngrammar induction, present the first models to induce syntactic structures from\nvisually grounded text and speech, and find that the visual grounding signals\ncan help improve the parsing quality over language-only models. As a side\ncontribution, we propose a novel evaluation metric that enables the evaluation\nof speech parsing without text or automatic speech recognition systems\ninvolved. In Part II, we propose two execution-aware methods to map sentences\ninto corresponding semantic structures (i.e., programs), significantly\nimproving compositional generalization and few-shot program synthesis. In Part\nIII, we propose methods that learn language structures from annotations in\nother languages. Specifically, we propose a method that sets a new state of the\nart on cross-lingual word alignment. We then leverage the learned word\nalignments to improve the performance of zero-shot cross-lingual dependency\nparsing, by proposing a novel substructure-based projection method that\npreserves structural knowledge learned from the source language.\n","authors":["Freda Shi"],"pdf_url":"https://arxiv.org/pdf/2406.09662v1.pdf","comment":"Ph.D. Thesis"},{"id":"http://arxiv.org/abs/2406.09295v2","updated":"2024-06-14T02:14:49Z","published":"2024-06-13T16:30:14Z","title":"AlignMMBench: Evaluating Chinese Multimodal Alignment in Large\n  Vision-Language Models","summary":"  Evaluating the alignment capabilities of large Vision-Language Models (VLMs)\nis essential for determining their effectiveness as helpful assistants.\nHowever, existing benchmarks primarily focus on basic abilities using nonverbal\nmethods, such as yes-no and multiple-choice questions. In this paper, we\naddress this gap by introducing AlignMMBench, a comprehensive alignment\nbenchmark specifically designed for emerging Chinese VLMs. This benchmark is\nmeticulously curated from real-world scenarios and Chinese Internet sources,\nencompassing thirteen specific tasks across three categories, and includes both\nsingle-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite\nstrategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer\npairs. To facilitate the evaluation pipeline, we propose CritiqueVLM, a\nrule-calibrated evaluator that exceeds GPT-4's evaluation ability. Finally, we\nreport the performance of representative VLMs on AlignMMBench, offering\ninsights into the capabilities and limitations of different VLM architectures.\nAll evaluation codes and data are available on https://alignmmbench.github.io.\n","authors":["Yuhang Wu","Wenmeng Yu","Yean Cheng","Yan Wang","Xiaohan Zhang","Jiazheng Xu","Ming Ding","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2406.09295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01638v3","updated":"2024-06-14T01:39:29Z","published":"2024-06-03T00:27:29Z","title":"TimeCMA: Towards LLM-Empowered Time Series Forecasting via\n  Cross-Modality Alignment","summary":"  The widespread adoption of scalable mobile sensing has led to large amounts\nof time series data for real-world applications. A fundamental application is\nmultivariate time series forecasting (MTSF), which aims to predict future time\nseries values based on historical observations. Existing MTSF methods suffer\nfrom limited parameterization and small-scale training data. Recently, Large\nlanguage models (LLMs) have been introduced in time series, which achieve\npromising forecasting performance but incur heavy computational costs. To solve\nthese challenges, we propose TimeCMA, an LLM-empowered framework for time\nseries forecasting with cross-modality alignment. We design a dual-modality\nencoding module with two branches, where the time series encoding branch\nextracts relatively low-quality yet pure embeddings of time series through an\ninverted Transformer. In addition, the LLM-empowered encoding branch wraps the\nsame time series as prompts to obtain high-quality yet entangled prompt\nembeddings via a Pre-trained LLM. Then, we design a cross-modality alignment\nmodule to retrieve high-quality and pure time series embeddings from the prompt\nembeddings. Moreover, we develop a time series forecasting module to decode the\naligned embeddings while capturing dependencies among multiple variables for\nforecasting. Notably, we tailor the prompt to encode sufficient temporal\ninformation into a last token and design the last token embedding storage to\nreduce computational costs. Extensive experiments on real data offer insight\ninto the accuracy and efficiency of the proposed framework.\n","authors":["Chenxi Liu","Qianxiong Xu","Hao Miao","Sun Yang","Lingzheng Zhang","Cheng Long","Ziyue Li","Rui Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.01638v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16973v3","updated":"2024-06-14T00:29:46Z","published":"2024-03-25T17:38:32Z","title":"VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild","summary":"  We introduce VoiceCraft, a token infilling neural codec language model, that\nachieves state-of-the-art performance on both speech editing and zero-shot\ntext-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft\nemploys a Transformer decoder architecture and introduces a token rearrangement\nprocedure that combines causal masking and delayed stacking to enable\ngeneration within an existing sequence. On speech editing tasks, VoiceCraft\nproduces edited speech that is nearly indistinguishable from unedited\nrecordings in terms of naturalness, as evaluated by humans; for zero-shot TTS,\nour model outperforms prior SotA models including VALLE and the popular\ncommercial model XTTS-v2. Crucially, the models are evaluated on challenging\nand realistic datasets, that consist of diverse accents, speaking styles,\nrecording conditions, and background noise and music, and our model performs\nconsistently well compared to other models and real recordings. In particular,\nfor speech editing evaluation, we introduce a high quality, challenging, and\nrealistic dataset named RealEdit. We encourage readers to listen to the demos\nat https://jasonppy.github.io/VoiceCraft_web.\n","authors":["Puyuan Peng","Po-Yao Huang","Shang-Wen Li","Abdelrahman Mohamed","David Harwath"],"pdf_url":"https://arxiv.org/pdf/2403.16973v3.pdf","comment":"ACL 2024. Data, code, and model weights are available at\n  https://github.com/jasonppy/VoiceCraft"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.10228v1","updated":"2024-06-14T17:59:40Z","published":"2024-06-14T17:59:40Z","title":"VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language\n  Large Models","summary":"  The swift progress of Multi-modal Large Models (MLLMs) has showcased their\nimpressive ability to tackle tasks blending vision and language. Yet, most\ncurrent models and benchmarks cater to scenarios with a narrow scope of visual\nand textual contexts. These models often fall short when faced with complex\ncomprehension tasks, which involve navigating through a plethora of irrelevant\nand potentially misleading information in both text and image forms. To bridge\nthis gap, we introduce a new, more demanding task known as Interleaved\nImage-Text Comprehension (IITC). This task challenges models to discern and\ndisregard superfluous elements in both images and text to accurately answer\nquestions and to follow intricate instructions to pinpoint the relevant image.\nIn support of this task, we further craft a new VEGA dataset, tailored for the\nIITC task on scientific content, and devised a subtask, Image-Text Association\n(ITA), to refine image-text correlation skills. Our evaluation of four leading\nclosed-source models, as well as various open-source models using VEGA,\nunderscores the rigorous nature of IITC. Even the most advanced models, such as\nGemini-1.5-pro and GPT4V, only achieved modest success. By employing a\nmulti-task, multi-scale post-training strategy, we have set a robust baseline\nfor MLLMs on the IITC task, attaining an $85.8\\%$ accuracy rate in image\nassociation and a $0.508$ Rouge score. These results validate the effectiveness\nof our dataset in improving MLLMs capabilities for nuanced image-text\ncomprehension.\n","authors":["Chenyu Zhou","Mengdan Zhang","Peixian Chen","Chaoyou Fu","Yunhang Shen","Xiawu Zheng","Xing Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.10228v1.pdf","comment":"Project Page: https://zhourax.github.io/VEGA/"},{"id":"http://arxiv.org/abs/2405.08813v2","updated":"2024-06-14T17:59:34Z","published":"2024-05-14T17:59:02Z","title":"CinePile: A Long Video Question Answering Dataset and Benchmark","summary":"  Current datasets for long-form video understanding often fall short of\nproviding genuine long-form comprehension challenges, as many tasks derived\nfrom these datasets can be successfully tackled by analyzing just one or a few\nrandom frames from a video. To address this issue, we present a novel dataset\nand benchmark, CinePile, specifically designed for authentic long-form video\nunderstanding. This paper details our innovative approach for creating a\nquestion-answer dataset, utilizing advanced LLMs with human-in-the-loop and\nbuilding upon human-generated raw data. Our comprehensive dataset comprises\n305,000 multiple-choice questions (MCQs), covering various visual and\nmultimodal aspects, including temporal comprehension, understanding\nhuman-object interactions, and reasoning about events or actions within a\nscene. Additionally, we evaluate recent video-centric LLMs, both open-source\nand proprietary, on the test split of our dataset. The findings reveal that\neven state-of-the-art video-centric LLMs significantly lag behind human\nperformance in these tasks, highlighting the complexity and challenge inherent\nin video understanding. The dataset is available at\nhttps://hf.co/datasets/tomg-group-umd/cinepile\n","authors":["Ruchit Rawal","Khalid Saifullah","Ronen Basri","David Jacobs","Gowthami Somepalli","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2405.08813v2.pdf","comment":"Project page with all the artifacts -\n  https://ruchitrawal.github.io/cinepile/. Updated version with results on\n  Gemini Flash model and additional related work"},{"id":"http://arxiv.org/abs/2406.10227v1","updated":"2024-06-14T17:59:08Z","published":"2024-06-14T17:59:08Z","title":"VideoGUI: A Benchmark for GUI Automation from Instructional Videos","summary":"  Graphical User Interface (GUI) automation holds significant promise for\nenhancing human productivity by assisting with computer tasks. Existing task\nformulations primarily focus on simple tasks that can be specified by a single,\nlanguage-only instruction, such as \"Insert a new slide.\" In this work, we\nintroduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI\nassistants on visual-centric GUI tasks. Sourced from high-quality web\ninstructional videos, our benchmark focuses on tasks involving professional and\nnovel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex\nactivities (e.g., video editing). VideoGUI evaluates GUI assistants through a\nhierarchical process, allowing for identification of the specific levels at\nwhich they may fail: (i) high-level planning: reconstruct procedural subtasks\nfrom visual conditions without language descriptions; (ii) middle-level\nplanning: generate sequences of precise action narrations based on visual state\n(i.e., screenshot) and goals; (iii) atomic action execution: perform specific\nactions such as accurately clicking designated elements. For each level, we\ndesign evaluation metrics across individual dimensions to provide clear\nsignals, such as individual performance in clicking, dragging, typing, and\nscrolling for atomic action execution. Our evaluation on VideoGUI reveals that\neven the SoTA large multimodal model GPT4o performs poorly on visual-centric\nGUI tasks, especially for high-level planning.\n","authors":["Kevin Qinghong Lin","Linjie Li","Difei Gao","Qinchen WU","Mingyi Yan","Zhengyuan Yang","Lijuan Wang","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2406.10227v1.pdf","comment":"24 pages, 16 tables, 17 figures"},{"id":"http://arxiv.org/abs/2406.10225v1","updated":"2024-06-14T17:58:28Z","published":"2024-06-14T17:58:28Z","title":"SatDiffMoE: A Mixture of Estimation Method for Satellite Image\n  Super-resolution with Latent Diffusion Models","summary":"  During the acquisition of satellite images, there is generally a trade-off\nbetween spatial resolution and temporal resolution (acquisition frequency) due\nto the onboard sensors of satellite imaging systems. High-resolution satellite\nimages are very important for land crop monitoring, urban planning, wildfire\nmanagement and a variety of applications. It is a significant yet challenging\ntask to achieve high spatial-temporal resolution in satellite imaging. With the\nadvent of diffusion models, we can now learn strong generative priors to\ngenerate realistic satellite images with high resolution, which can be utilized\nto promote the super-resolution task as well. In this work, we propose a novel\ndiffusion-based fusion algorithm called \\textbf{SatDiffMoE} that can take an\narbitrary number of sequential low-resolution satellite images at the same\nlocation as inputs, and fuse them into one high-resolution reconstructed image\nwith more fine details, by leveraging and fusing the complementary information\nfrom different time points. Our algorithm is highly flexible and allows\ntraining and inference on arbitrary number of low-resolution images.\nExperimental results show that our proposed SatDiffMoE method not only achieves\nsuperior performance for the satellite image super-resolution tasks on a\nvariety of datasets, but also gets an improved computational efficiency with\nreduced model parameters, compared with previous methods.\n","authors":["Zhaoxu Luo","Bowen Song","Liyue Shen"],"pdf_url":"https://arxiv.org/pdf/2406.10225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10224v1","updated":"2024-06-14T17:57:35Z","published":"2024-06-14T17:57:35Z","title":"EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric\n  Foundation Models","summary":"  The advent of wearable computers enables a new source of context for AI that\nis embedded in egocentric sensor data. This new egocentric data comes equipped\nwith fine-grained 3D location information and thus presents the opportunity for\na novel class of spatial foundation models that are rooted in 3D space. To\nmeasure progress on what we term Egocentric Foundation Models (EFMs) we\nestablish EFM3D, a benchmark with two core 3D egocentric perception tasks.\nEFM3D is the first benchmark for 3D object detection and surface regression on\nhigh quality annotated egocentric data of Project Aria. We propose Egocentric\nVoxel Lifting (EVL), a baseline for 3D EFMs. EVL leverages all available\negocentric modalities and inherits foundational capabilities from 2D foundation\nmodels. This model, trained on a large simulated dataset, outperforms existing\nmethods on the EFM3D benchmark.\n","authors":["Julian Straub","Daniel DeTone","Tianwei Shen","Nan Yang","Chris Sweeney","Richard Newcombe"],"pdf_url":"https://arxiv.org/pdf/2406.10224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10221v1","updated":"2024-06-14T17:54:54Z","published":"2024-06-14T17:54:54Z","title":"Short Film Dataset (SFD): A Benchmark for Story-Level Video\n  Understanding","summary":"  Recent advances in vision-language models have significantly propelled video\nunderstanding. Existing datasets and tasks, however, have notable limitations.\nMost datasets are confined to short videos with limited events and narrow\nnarratives. For example, datasets with instructional and egocentric videos\noften document the activities of one person in a single scene. Although some\nmovie datasets offer richer content, they are often limited to short-term\ntasks, lack publicly available videos and frequently encounter data leakage\ngiven the use of movie forums and other resources in LLM training. To address\nthe above limitations, we propose the Short Film Dataset (SFD) with 1,078\npublicly available amateur movies, a wide variety of genres and minimal data\nleakage issues. SFD offers long-term story-oriented video tasks in the form of\nmultiple-choice and open-ended question answering. Our extensive experiments\nemphasize the need for long-term reasoning to solve SFD tasks. Notably, we find\nstrong signals in movie transcripts leading to the on-par performance of people\nand LLMs. We also show significantly lower performance of current models\ncompared to people when using vision data alone.\n","authors":["Ridouane Ghermi","Xi Wang","Vicky Kalogeiton","Ivan Laptev"],"pdf_url":"https://arxiv.org/pdf/2406.10221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10219v1","updated":"2024-06-14T17:53:55Z","published":"2024-06-14T17:53:55Z","title":"PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting","summary":"  Recent advancements in novel view synthesis have enabled real-time rendering\nspeeds and high reconstruction accuracy. 3D Gaussian Splatting (3D-GS), a\nfoundational point-based parametric 3D scene representation, models scenes as\nlarge sets of 3D Gaussians. Complex scenes can comprise of millions of\nGaussians, amounting to large storage and memory requirements that limit the\nviability of 3D-GS on devices with limited resources. Current techniques for\ncompressing these pretrained models by pruning Gaussians rely on combining\nheuristics to determine which ones to remove. In this paper, we propose a\nprincipled spatial sensitivity pruning score that outperforms these approaches.\nIt is computed as a second-order approximation of the reconstruction error on\nthe training views with respect to the spatial parameters of each Gaussian.\nAdditionally, we propose a multi-round prune-refine pipeline that can be\napplied to any pretrained 3D-GS model without changing the training pipeline.\nAfter pruning 88.44% of the Gaussians, we observe that our PUP 3D-GS pipeline\nincreases the average rendering speed of 3D-GS by 2.65$\\times$ while retaining\nmore salient foreground information and achieving higher image quality metrics\nthan previous pruning techniques on scenes from the Mip-NeRF 360, Tanks &\nTemples, and Deep Blending datasets.\n","authors":["Alex Hanson","Allen Tu","Vasu Singla","Mayuka Jayawardhana","Matthias Zwicker","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2406.10219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10212v1","updated":"2024-06-14T17:48:45Z","published":"2024-06-14T17:48:45Z","title":"NeST: Neural Stress Tensor Tomography by leveraging 3D Photoelasticity","summary":"  Photoelasticity enables full-field stress analysis in transparent objects\nthrough stress-induced birefringence. Existing techniques are limited to 2D\nslices and require destructively slicing the object. Recovering the internal 3D\nstress distribution of the entire object is challenging as it involves solving\na tensor tomography problem and handling phase wrapping ambiguities. We\nintroduce NeST, an analysis-by-synthesis approach for reconstructing 3D stress\ntensor fields as neural implicit representations from polarization\nmeasurements. Our key insight is to jointly handle phase unwrapping and tensor\ntomography using a differentiable forward model based on Jones calculus. Our\nnon-linear model faithfully matches real captures, unlike prior linear\napproximations. We develop an experimental multi-axis polariscope setup to\ncapture 3D photoelasticity and experimentally demonstrate that NeST\nreconstructs the internal stress distribution for objects with varying shape\nand force conditions. Additionally, we showcase novel applications in stress\nanalysis, such as visualizing photoelastic fringes by virtually slicing the\nobject and viewing photoelastic fringes from unseen viewpoints. NeST paves the\nway for scalable non-destructive 3D photoelastic analysis.\n","authors":["Akshat Dave","Tianyi Zhang","Aaron Young","Ramesh Raskar","Wolfgang Heidrich","Ashok Veeraraghavan"],"pdf_url":"https://arxiv.org/pdf/2406.10212v1.pdf","comment":"Project webpage: https://akshatdave.github.io/nest"},{"id":"http://arxiv.org/abs/2406.10211v1","updated":"2024-06-14T17:47:50Z","published":"2024-06-14T17:47:50Z","title":"DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion\n  Score Blending for 3D Computed Tomography Reconstruction","summary":"  Diffusion models face significant challenges when employed for large-scale\nmedical image reconstruction in real practice such as 3D Computed Tomography\n(CT). Due to the demanding memory, time, and data requirements, it is difficult\nto train a diffusion model directly on the entire volume of high-dimensional\ndata to obtain an efficient 3D diffusion prior. Existing works utilizing\ndiffusion priors on single 2D image slice with hand-crafted cross-slice\nregularization would sacrifice the z-axis consistency, which results in severe\nartifacts along the z-axis. In this work, we propose a novel framework that\nenables learning the 3D image prior through position-aware 3D-patch diffusion\nscore blending for reconstructing large-scale 3D medical images. To the best of\nour knowledge, we are the first to utilize a 3D-patch diffusion prior for 3D\nmedical image reconstruction. Extensive experiments on sparse view and limited\nangle CT reconstruction show that our DiffusionBlend method significantly\noutperforms previous methods and achieves state-of-the-art performance on\nreal-world CT reconstruction problems with high-dimensional 3D image (i.e.,\n$256 \\times 256 \\times 500$). Our algorithm also comes with better or\ncomparable computational efficiency than previous state-of-the-art methods.\n","authors":["Bowen Song","Jason Hu","Zhaoxu Luo","Jeffrey A. Fessler","Liyue Shen"],"pdf_url":"https://arxiv.org/pdf/2406.10211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10210v1","updated":"2024-06-14T17:46:08Z","published":"2024-06-14T17:46:08Z","title":"Make It Count: Text-to-Image Generation with an Accurate Number of\n  Objects","summary":"  Despite the unprecedented success of text-to-image diffusion models,\ncontrolling the number of depicted objects using text is surprisingly hard.\nThis is important for various applications from technical documents, to\nchildren's books to illustrating cooking recipes. Generating object-correct\ncounts is fundamentally challenging because the generative model needs to keep\na sense of separate identity for every instance of the object, even if several\nobjects look identical or overlap, and then carry out a global computation\nimplicitly during generation. It is still unknown if such representations\nexist. To address count-correct generation, we first identify features within\nthe diffusion model that can carry the object identity information. We then use\nthem to separate and count instances of objects during the denoising process\nand detect over-generation and under-generation. We fix the latter by training\na model that predicts both the shape and location of a missing object, based on\nthe layout of existing ones, and show how it can be used to guide denoising\nwith correct object count. Our approach, CountGen, does not depend on external\nsource to determine object layout, but rather uses the prior from the diffusion\nmodel itself, creating prompt-dependent and seed-dependent layouts. Evaluated\non two benchmark datasets, we find that CountGen strongly outperforms the\ncount-accuracy of existing baselines.\n","authors":["Lital Binyamin","Yoad Tewel","Hilit Segev","Eran Hirsch","Royi Rassin","Gal Chechik"],"pdf_url":"https://arxiv.org/pdf/2406.10210v1.pdf","comment":"Project page is at https://make-it-count-paper.github.io/"},{"id":"http://arxiv.org/abs/2404.01299v2","updated":"2024-06-14T17:46:02Z","published":"2024-04-01T17:59:53Z","title":"CausalChaos! Dataset for Comprehensive Causal Action Question Answering\n  Over Longer Causal Chains Grounded in Dynamic Visual Scenes","summary":"  Causal video question answering (QA) has garnered increasing interest, yet\nexisting datasets often lack depth in causal reasoning. To address this gap, we\ncapitalize on the unique properties of cartoons and construct CausalChaos!, a\nnovel, challenging causal Why-QA dataset built upon the iconic \"Tom and Jerry\"\ncartoon series. Cartoons use the principles of animation that allow animators\nto create expressive, unambiguous causal relationships between events to form a\ncoherent storyline. Utilizing these properties, along with thought-provoking\nquestions and multi-level answers (answer and detailed causal explanation), our\nquestions involve causal chains that interconnect multiple dynamic interactions\nbetween characters and visual scenes. These factors demand models to solve more\nchallenging, yet well-defined causal relationships. We also introduce hard\nincorrect answer mining, including a causally confusing version that is even\nmore challenging. While models perform well, there is much room for\nimprovement, especially, on open-ended answers. We identify more\nadvanced/explicit causal relationship modeling & joint modeling of vision and\nlanguage as the immediate areas for future efforts to focus upon. Along with\nthe other complementary datasets, our new challenging dataset will pave the way\nfor these developments in the field.\n","authors":["Paritosh Parmar","Eric Peh","Ruirui Chen","Ting En Lam","Yuhan Chen","Elston Tan","Basura Fernando"],"pdf_url":"https://arxiv.org/pdf/2404.01299v2.pdf","comment":"Project Page: https://github.com/LUNAProject22/CausalChaos"},{"id":"http://arxiv.org/abs/2406.10208v1","updated":"2024-06-14T17:44:09Z","published":"2024-06-14T17:44:09Z","title":"Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual\n  Visual Text Rendering","summary":"  Recently, Glyph-ByT5 has achieved highly accurate visual text rendering\nperformance in graphic design images. However, it still focuses solely on\nEnglish and performs relatively poorly in terms of visual appeal. In this work,\nwe address these two fundamental limitations by presenting Glyph-ByT5-v2 and\nGlyph-SDXL-v2, which not only support accurate visual text rendering for 10\ndifferent languages but also achieve much better aesthetic quality. To achieve\nthis, we make the following contributions: (i) creating a high-quality\nmultilingual glyph-text and graphic design dataset consisting of more than 1\nmillion glyph-text pairs and 10 million graphic design image-text pairs\ncovering nine other languages, (ii) building a multilingual visual paragraph\nbenchmark consisting of 1,000 prompts, with 100 for each language, to assess\nmultilingual visual spelling accuracy, and (iii) leveraging the latest\nstep-aware preference learning approach to enhance the visual aesthetic\nquality. With the combination of these techniques, we deliver a powerful\ncustomized multilingual text encoder, Glyph-ByT5-v2, and a strong aesthetic\ngraphic generation model, Glyph-SDXL-v2, that can support accurate spelling in\n10 different languages. We perceive our work as a significant advancement,\nconsidering that the latest DALL-E3 and Ideogram 1.0 still struggle with the\nmultilingual visual text rendering task.\n","authors":["Zeyu Liu","Weicong Liang","Yiming Zhao","Bohan Chen","Ji Li","Yuhui Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.10208v1.pdf","comment":"Project page: https://glyph-byt5-v2.github.io/"},{"id":"http://arxiv.org/abs/2406.10200v1","updated":"2024-06-14T17:33:11Z","published":"2024-06-14T17:33:11Z","title":"SSTFB: Leveraging self-supervised pretext learning and temporal\n  self-attention with feature branching for real-time video polyp segmentation","summary":"  Polyps are early cancer indicators, so assessing occurrences of polyps and\ntheir removal is critical. They are observed through a colonoscopy screening\nprocedure that generates a stream of video frames. Segmenting polyps in their\nnatural video screening procedure has several challenges, such as the\nco-existence of imaging artefacts, motion blur, and floating debris. Most\nexisting polyp segmentation algorithms are developed on curated still image\ndatasets that do not represent real-world colonoscopy. Their performance often\ndegrades on video data. We propose a video polyp segmentation method that\nperforms self-supervised learning as an auxiliary task and a spatial-temporal\nself-attention mechanism for improved representation learning. Our end-to-end\nconfiguration and joint optimisation of losses enable the network to learn more\ndiscriminative contextual features in videos. Our experimental results\ndemonstrate an improvement with respect to several state-of-the-art (SOTA)\nmethods. Our ablation study also confirms that the choice of the proposed joint\nend-to-end training improves network accuracy by over 3% and nearly 10% on both\nthe Dice similarity coefficient and intersection-over-union compared to the\nrecently proposed method PNS+ and Polyp-PVT, respectively. Results on\npreviously unseen video data indicate that the proposed method generalises.\n","authors":["Ziang Xu","Jens Rittscher","Sharib Ali"],"pdf_url":"https://arxiv.org/pdf/2406.10200v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2406.10197v1","updated":"2024-06-14T17:31:29Z","published":"2024-06-14T17:31:29Z","title":"Crafting Parts for Expressive Object Composition","summary":"  Text-to-image generation from large generative models like Stable Diffusion,\nDALLE-2, etc., have become a common base for various tasks due to their\nsuperior quality and extensive knowledge bases. As image composition and\ngeneration are creative processes the artists need control over various parts\nof the images being generated. We find that just adding details about parts in\nthe base text prompt either leads to an entirely different image (e.g.,\nmissing/incorrect identity) or the extra part details simply being ignored. To\nmitigate these issues, we introduce PartCraft, which enables image generation\nbased on fine-grained part-level details specified for objects in the base text\nprompt. This allows more control for artists and enables novel object\ncompositions by combining distinctive object parts. PartCraft first localizes\nobject parts by denoising the object region from a specific diffusion process.\nThis enables each part token to be localized to the right object region. After\nobtaining part masks, we run a localized diffusion process in each of the part\nregions based on fine-grained part descriptions and combine them to produce the\nfinal image. All the stages of PartCraft are based on repurposing a pre-trained\ndiffusion model, which enables it to generalize across various domains without\ntraining. We demonstrate the effectiveness of part-level control provided by\nPartCraft qualitatively through visual examples and quantitatively in\ncomparison to the contemporary baselines.\n","authors":["Harsh Rangwani","Aishwarya Agarwal","Kuldeep Kulkarni","R. Venkatesh Babu","Srikrishna Karanam"],"pdf_url":"https://arxiv.org/pdf/2406.10197v1.pdf","comment":"Project Page Will Be Here: https://rangwani-harsh.github.io/PartCraft"},{"id":"http://arxiv.org/abs/2311.04157v3","updated":"2024-06-14T17:28:14Z","published":"2023-11-07T17:32:55Z","title":"A Simple Interpretable Transformer for Fine-Grained Image Classification\n  and Analysis","summary":"  We present a novel usage of Transformers to make image classification\ninterpretable. Unlike mainstream classifiers that wait until the last fully\nconnected layer to incorporate class information to make predictions, we\ninvestigate a proactive approach, asking each class to search for itself in an\nimage. We realize this idea via a Transformer encoder-decoder inspired by\nDEtection TRansformer (DETR). We learn \"class-specific\" queries (one for each\nclass) as input to the decoder, enabling each class to localize its patterns in\nan image via cross-attention. We name our approach INterpretable TRansformer\n(INTR), which is fairly easy to implement and exhibits several compelling\nproperties. We show that INTR intrinsically encourages each class to attend\ndistinctively; the cross-attention weights thus provide a faithful\ninterpretation of the prediction. Interestingly, via \"multi-head\"\ncross-attention, INTR could identify different \"attributes\" of a class, making\nit particularly suitable for fine-grained classification and analysis, which we\ndemonstrate on eight datasets. Our code and pre-trained models are publicly\naccessible at the Imageomics Institute GitHub site:\nhttps://github.com/Imageomics/INTR.\n","authors":["Dipanjyoti Paul","Arpita Chowdhury","Xinqi Xiong","Feng-Ju Chang","David Carlyn","Samuel Stevens","Kaiya L. Provost","Anuj Karpatne","Bryan Carstens","Daniel Rubenstein","Charles Stewart","Tanya Berger-Wolf","Yu Su","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2311.04157v3.pdf","comment":"Accepted to International Conference on Learning Representations 2024\n  (ICLR 2024)"},{"id":"http://arxiv.org/abs/2406.09394v2","updated":"2024-06-14T17:19:47Z","published":"2024-06-13T17:59:10Z","title":"WonderWorld: Interactive 3D Scene Generation from a Single Image","summary":"  We present WonderWorld, a novel framework for interactive 3D scene\nextrapolation that enables users to explore and shape virtual environments\nbased on a single input image and user-specified text. While significant\nimprovements have been made to the visual quality of scene generation, existing\nmethods are run offline, taking tens of minutes to hours to generate a scene.\nBy leveraging Fast Gaussian Surfels and a guided diffusion-based depth\nestimation method, WonderWorld generates geometrically consistent extrapolation\nwhile significantly reducing computational time. Our framework generates\nconnected and diverse 3D scenes in less than 10 seconds on a single A6000 GPU,\nenabling real-time user interaction and exploration. We demonstrate the\npotential of WonderWorld for applications in virtual reality, gaming, and\ncreative design, where users can quickly generate and navigate immersive,\npotentially infinite virtual worlds from a single image. Our approach\nrepresents a significant advancement in interactive 3D scene generation,\nopening up new possibilities for user-driven content creation and exploration\nin virtual environments. We will release full code and software for\nreproducibility. Project website: https://WonderWorld-2024.github.io/\n","authors":["Hong-Xing Yu","Haoyi Duan","Charles Herrmann","William T. Freeman","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2406.09394v2.pdf","comment":"Project website: https://WonderWorld-2024.github.io/"},{"id":"http://arxiv.org/abs/2406.10185v1","updated":"2024-06-14T17:14:22Z","published":"2024-06-14T17:14:22Z","title":"Detecting and Evaluating Medical Hallucinations in Large Vision Language\n  Models","summary":"  Large Vision Language Models (LVLMs) are increasingly integral to healthcare\napplications, including medical visual question answering and imaging report\ngeneration. While these models inherit the robust capabilities of foundational\nLarge Language Models (LLMs), they also inherit susceptibility to\nhallucinations-a significant concern in high-stakes medical contexts where the\nmargin for error is minimal. However, currently, there are no dedicated methods\nor benchmarks for hallucination detection and evaluation in the medical field.\nTo bridge this gap, we introduce Med-HallMark, the first benchmark specifically\ndesigned for hallucination detection and evaluation within the medical\nmultimodal domain. This benchmark provides multi-tasking hallucination support,\nmultifaceted hallucination data, and hierarchical hallucination categorization.\nFurthermore, we propose the MediHall Score, a new medical evaluative metric\ndesigned to assess LVLMs' hallucinations through a hierarchical scoring system\nthat considers the severity and type of hallucination, thereby enabling a\ngranular assessment of potential clinical impacts. We also present\nMediHallDetector, a novel Medical LVLM engineered for precise hallucination\ndetection, which employs multitask training for hallucination detection.\nThrough extensive experimental evaluations, we establish baselines for popular\nLVLMs using our benchmark. The findings indicate that MediHall Score provides a\nmore nuanced understanding of hallucination impacts compared to traditional\nmetrics and demonstrate the enhanced performance of MediHallDetector. We hope\nthis work can significantly improve the reliability of LVLMs in medical\napplications. All resources of this work will be released soon.\n","authors":["Jiawei Chen","Dingkang Yang","Tong Wu","Yue Jiang","Xiaolu Hou","Mingcheng Li","Shunli Wang","Dongling Xiao","Ke Li","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.10185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10180v1","updated":"2024-06-14T16:59:04Z","published":"2024-06-14T16:59:04Z","title":"MeshPose: Unifying DensePose and 3D Body Mesh reconstruction","summary":"  DensePose provides a pixel-accurate association of images with 3D mesh\ncoordinates, but does not provide a 3D mesh, while Human Mesh Reconstruction\n(HMR) systems have high 2D reprojection error, as measured by DensePose\nlocalization metrics. In this work we introduce MeshPose to jointly tackle\nDensePose and HMR. For this we first introduce new losses that allow us to use\nweak DensePose supervision to accurately localize in 2D a subset of the mesh\nvertices ('VertexPose'). We then lift these vertices to 3D, yielding a low-poly\nbody mesh ('MeshPose'). Our system is trained in an end-to-end manner and is\nthe first HMR method to attain competitive DensePose accuracy, while also being\nlightweight and amenable to efficient inference, making it suitable for\nreal-time AR applications.\n","authors":["Eric-Tuan Lê","Antonis Kakolyris","Petros Koutras","Himmy Tam","Efstratios Skordos","George Papandreou","Rıza Alp Güler","Iasonas Kokkinos"],"pdf_url":"https://arxiv.org/pdf/2406.10180v1.pdf","comment":"IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"},{"id":"http://arxiv.org/abs/2406.10175v1","updated":"2024-06-14T16:54:53Z","published":"2024-06-14T16:54:53Z","title":"Enhancing Incomplete Multi-modal Brain Tumor Segmentation with\n  Intra-modal Asymmetry and Inter-modal Dependency","summary":"  Deep learning-based brain tumor segmentation (BTS) models for multi-modal MRI\nimages have seen significant advancements in recent years. However, a common\nproblem in practice is the unavailability of some modalities due to varying\nscanning protocols and patient conditions, making segmentation from incomplete\nMRI modalities a challenging issue. Previous methods have attempted to address\nthis by fusing accessible multi-modal features, leveraging attention\nmechanisms, and synthesizing missing modalities using generative models.\nHowever, these methods ignore the intrinsic problems of medical image\nsegmentation, such as the limited availability of training samples,\nparticularly for cases with tumors. Furthermore, these methods require training\nand deploying a specific model for each subset of missing modalities. To\naddress these issues, we propose a novel approach that enhances the BTS model\nfrom two perspectives. Firstly, we introduce a pre-training stage that\ngenerates a diverse pre-training dataset covering a wide range of different\ncombinations of tumor shapes and brain anatomy. Secondly, we propose a\npost-training stage that enables the model to reconstruct missing modalities in\nthe prediction results when only partial modalities are available. To achieve\nthe pre-training stage, we conceptually decouple the MRI image into two parts:\n`anatomy' and `tumor'. We pre-train the BTS model using synthesized data\ngenerated from the anatomy and tumor parts across different training samples.\n... Extensive experiments demonstrate that our proposed method significantly\nimproves the performance over the baseline and achieves new state-of-the-art\nresults on three brain tumor segmentation datasets: BRATS2020, BRATS2018, and\nBRATS2015.\n","authors":["Weide Liu","Jingwen Hou","Xiaoyang Zhong","Huijing Zhan","Jun Cheng","Yuming Fang","Guanghui Yue"],"pdf_url":"https://arxiv.org/pdf/2406.10175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.21070v2","updated":"2024-06-14T16:42:47Z","published":"2024-05-31T17:57:24Z","title":"Generalization Beyond Data Imbalance: A Controlled Study on CLIP for\n  Transferable Insights","summary":"  Severe data imbalance naturally exists among web-scale vision-language\ndatasets. Despite this, we find CLIP pre-trained thereupon exhibits notable\nrobustness to the data imbalance compared to supervised learning, and\ndemonstrates significant effectiveness in learning generalizable\nrepresentations. With an aim to investigate the reasons behind this finding, we\nconduct controlled experiments to study various underlying factors, and reveal\nthat CLIP's pretext task forms a dynamic classification problem wherein only a\nsubset of classes is present in training. This isolates the bias from dominant\nclasses and implicitly balances the learning signal. Furthermore, the\nrobustness and discriminability of CLIP improve with more descriptive language\nsupervision, larger data scale, and broader open-world concepts, which are\ninaccessible to supervised learning. Our study not only uncovers the mechanisms\nbehind CLIP's generalizability beyond data imbalance but also provides\ntransferable insights for the research community. The findings are validated in\nboth supervised and self-supervised learning, enabling models trained on\nimbalanced data to achieve CLIP-level performance on diverse recognition tasks.\nCode and data are available at: https://github.com/CVMI-Lab/clip-beyond-tail.\n","authors":["Xin Wen","Bingchen Zhao","Yilun Chen","Jiangmiao Pang","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2405.21070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10167v1","updated":"2024-06-14T16:38:00Z","published":"2024-06-14T16:38:00Z","title":"4DRecons: 4D Neural Implicit Deformable Objects Reconstruction from a\n  single RGB-D Camera with Geometrical and Topological Regularizations","summary":"  This paper presents a novel approach 4DRecons that takes a single camera\nRGB-D sequence of a dynamic subject as input and outputs a complete textured\ndeforming 3D model over time. 4DRecons encodes the output as a 4D neural\nimplicit surface and presents an optimization procedure that combines a data\nterm and two regularization terms. The data term fits the 4D implicit surface\nto the input partial observations. We address fundamental challenges in fitting\na complete implicit surface to partial observations. The first regularization\nterm enforces that the deformation among adjacent frames is as rigid as\npossible (ARAP). To this end, we introduce a novel approach to compute\ncorrespondences between adjacent textured implicit surfaces, which are used to\ndefine the ARAP regularization term. The second regularization term enforces\nthat the topology of the underlying object remains fixed over time. This\nregularization is critical for avoiding self-intersections that are typical in\nimplicit-based reconstructions. We have evaluated the performance of 4DRecons\non a variety of datasets. Experimental results show that 4DRecons can handle\nlarge deformations and complex inter-part interactions and outperform\nstate-of-the-art approaches considerably.\n","authors":["Xiaoyan Cong","Haitao Yang","Liyan Chen","Kaifeng Zhang","Li Yi","Chandrajit Bajaj","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2406.10167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10165v1","updated":"2024-06-14T16:35:47Z","published":"2024-06-14T16:35:47Z","title":"CarLLaVA: Vision language models for camera-only closed-loop driving","summary":"  In this technical report, we present CarLLaVA, a Vision Language Model (VLM)\nfor autonomous driving, developed for the CARLA Autonomous Driving Challenge\n2.0. CarLLaVA uses the vision encoder of the LLaVA VLM and the LLaMA\narchitecture as backbone, achieving state-of-the-art closed-loop driving\nperformance with only camera input and without the need for complex or\nexpensive labels. Additionally, we show preliminary results on predicting\nlanguage commentary alongside the driving output. CarLLaVA uses a\nsemi-disentangled output representation of both path predictions and waypoints,\ngetting the advantages of the path for better lateral control and the waypoints\nfor better longitudinal control. We propose an efficient training recipe to\ntrain on large driving datasets without wasting compute on easy, trivial data.\nCarLLaVA ranks 1st place in the sensor track of the CARLA Autonomous Driving\nChallenge 2.0 outperforming the previous state of the art by 458% and the best\nconcurrent submission by 32.6%.\n","authors":["Katrin Renz","Long Chen","Ana-Maria Marcu","Jan Hünermann","Benoit Hanotte","Alice Karnsund","Jamie Shotton","Elahe Arani","Oleg Sinavski"],"pdf_url":"https://arxiv.org/pdf/2406.10165v1.pdf","comment":"Outstanding Champion & Innovation Award @ CARLA Autonomous Driving\n  Challenge 2024; Project video: https://youtu.be/E1nsEgcHRuc"},{"id":"http://arxiv.org/abs/2406.10163v1","updated":"2024-06-14T16:30:25Z","published":"2024-06-14T16:30:25Z","title":"MeshAnything: Artist-Created Mesh Generation with Autoregressive\n  Transformers","summary":"  Recently, 3D assets created via reconstruction and generation have matched\nthe quality of manually crafted assets, highlighting their potential for\nreplacement. However, this potential is largely unrealized because these assets\nalways need to be converted to meshes for 3D industry applications, and the\nmeshes produced by current mesh extraction methods are significantly inferior\nto Artist-Created Meshes (AMs), i.e., meshes created by human artists.\nSpecifically, current mesh extraction methods rely on dense faces and ignore\ngeometric features, leading to inefficiencies, complicated post-processing, and\nlower representation quality. To address these issues, we introduce\nMeshAnything, a model that treats mesh extraction as a generation problem,\nproducing AMs aligned with specified shapes. By converting 3D assets in any 3D\nrepresentation into AMs, MeshAnything can be integrated with various 3D asset\nproduction methods, thereby enhancing their application across the 3D industry.\nThe architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned\ndecoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE,\nthen train the shape-conditioned decoder-only transformer on this vocabulary\nfor shape-conditioned autoregressive mesh generation. Our extensive experiments\nshow that our method generates AMs with hundreds of times fewer faces,\nsignificantly improving storage, rendering, and simulation efficiencies, while\nachieving precision comparable to previous methods.\n","authors":["Yiwen Chen","Tong He","Di Huang","Weicai Ye","Sijin Chen","Jiaxiang Tang","Xin Chen","Zhongang Cai","Lei Yang","Gang Yu","Guosheng Lin","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.10163v1.pdf","comment":"Project Page: https://buaacyw.github.io/mesh-anything/ Code:\n  https://github.com/buaacyw/MeshAnything"},{"id":"http://arxiv.org/abs/2406.01954v2","updated":"2024-06-14T15:53:07Z","published":"2024-06-04T04:22:47Z","title":"Plug-and-Play Diffusion Distillation","summary":"  Diffusion models have shown tremendous results in image generation. However,\ndue to the iterative nature of the diffusion process and its reliance on\nclassifier-free guidance, inference times are slow. In this paper, we propose a\nnew distillation approach for guided diffusion models in which an external\nlightweight guide model is trained while the original text-to-image model\nremains frozen. We show that our method reduces the inference computation of\nclassifier-free guided latent-space diffusion models by almost half, and only\nrequires 1\\% trainable parameters of the base model. Furthermore, once trained,\nour guide model can be applied to various fine-tuned, domain-specific versions\nof the base diffusion model without the need for additional training: this\n\"plug-and-play\" functionality drastically improves inference computation while\nmaintaining the visual fidelity of generated images. Empirically, we show that\nour approach is able to produce visually appealing results and achieve a\ncomparable FID score to the teacher with as few as 8 to 16 steps.\n","authors":["Yi-Ting Hsiao","Siavash Khodadadeh","Kevin Duarte","Wei-An Lin","Hui Qu","Mingi Kwon","Ratheesh Kalarot"],"pdf_url":"https://arxiv.org/pdf/2406.01954v2.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  2024 project page:\n  https://5410tiffany.github.io/plug-and-play-diffusion-distillation.github.io/"},{"id":"http://arxiv.org/abs/2211.05226v2","updated":"2024-06-14T15:52:29Z","published":"2022-11-08T09:54:34Z","title":"A kinetic approach to consensus-based segmentation of biomedical images","summary":"  In this work, we apply a kinetic version of a bounded confidence consensus\nmodel to biomedical segmentation problems. In the presented approach,\ntime-dependent information on the microscopic state of each particle/pixel\nincludes its space position and a feature representing a static characteristic\nof the system, i.e. the gray level of each pixel. From the introduced\nmicroscopic model we derive a kinetic formulation of the model. The large time\nbehavior of the system is then computed with the aid of a surrogate\nFokker-Planck approach that can be obtained in the quasi-invariant scaling. We\nexploit the computational efficiency of direct simulation Monte Carlo methods\nfor the obtained Boltzmann-type description of the problem for parameter\nidentification tasks. Based on a suitable loss function measuring the distance\nbetween the ground truth segmentation mask and the evaluated mask, we minimize\nthe introduced segmentation metric for a relevant set of 2D gray-scale images.\nApplications to biomedical segmentation concentrate on different imaging\nresearch contexts.\n","authors":["Raffaella Fiamma Cabini","Anna Pichiecchio","Alessandro Lascialfari","Silvia Figini","Mattia Zanella"],"pdf_url":"https://arxiv.org/pdf/2211.05226v2.pdf","comment":"29 pages, 13 figures"},{"id":"http://arxiv.org/abs/2311.06141v3","updated":"2024-06-14T15:52:13Z","published":"2023-11-10T15:58:53Z","title":"Federated Learning Across Decentralized and Unshared Archives for Remote\n  Sensing Image Classification","summary":"  Federated learning (FL) enables the collaboration of multiple deep learning\nmodels to learn from decentralized data archives (i.e., clients) without\naccessing data on clients. Although FL offers ample opportunities in knowledge\ndiscovery from distributed image archives, it is seldom considered in remote\nsensing (RS). In this paper, as a first time in RS, we present a comparative\nstudy of state-of-the-art FL algorithms for RS image classification problems.\nTo this end, we initially provide a systematic review of the FL algorithms\npresented in the computer vision and machine learning communities. Then, we\nselect several state-of-the-art FL algorithms based on their effectiveness with\nrespect to training data heterogeneity across clients (known as non-IID data).\nAfter presenting an extensive overview of the selected algorithms, a\ntheoretical comparison of the algorithms is conducted based on their: 1) local\ntraining complexity; 2) aggregation complexity; 3) learning efficiency; 4)\ncommunication cost; and 5) scalability in terms of number of clients. After the\ntheoretical comparison, experimental analyses are presented to compare them\nunder different decentralization scenarios. For the experimental analyses, we\nfocus our attention on multi-label image classification problems in RS. Based\non our comprehensive analyses, we finally derive a guideline for selecting\nsuitable FL algorithms in RS. The code of this work is publicly available at\nhttps://git.tu-berlin.de/rsim/FL-RS.\n","authors":["Barış Büyüktaş","Gencer Sumbul","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2311.06141v3.pdf","comment":"Accepted at the IEEE Geoscience and Remote Sensing Magazine"},{"id":"http://arxiv.org/abs/2406.10139v1","updated":"2024-06-14T15:48:43Z","published":"2024-06-14T15:48:43Z","title":"YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their\n  application in the agricultural domain","summary":"  This survey investigates the transformative potential of various YOLO\nvariants, from YOLOv1 to the state-of-the-art YOLOv10, in the context of\nagricultural advancements. The primary objective is to elucidate how these\ncutting-edge object detection models can re-energise and optimize diverse\naspects of agriculture, ranging from crop monitoring to livestock management.\nIt aims to achieve key objectives, including the identification of contemporary\nchallenges in agriculture, a detailed assessment of YOLO's incremental\nadvancements, and an exploration of its specific applications in agriculture.\nThis is one of the first surveys to include the latest YOLOv10, offering a\nfresh perspective on its implications for precision farming and sustainable\nagricultural practices in the era of Artificial Intelligence and automation.\nFurther, the survey undertakes a critical analysis of YOLO's performance,\nsynthesizes existing research, and projects future trends. By scrutinizing the\nunique capabilities packed in YOLO variants and their real-world applications,\nthis survey provides valuable insights into the evolving relationship between\nYOLO variants and agriculture. The findings contribute towards a nuanced\nunderstanding of the potential for precision farming and sustainable\nagricultural practices, marking a significant step forward in the integration\nof advanced object detection technologies within the agricultural sector.\n","authors":["Mujadded Al Rabbani Alif","Muhammad Hussain"],"pdf_url":"https://arxiv.org/pdf/2406.10139v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2305.11616v4","updated":"2024-06-14T15:46:55Z","published":"2023-05-19T11:47:51Z","title":"Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD\n  Detection, Calibration, and Accuracy","summary":"  Deep ensembles are capable of achieving state-of-the-art results in\nclassification and out-of-distribution (OOD) detection. However, their\neffectiveness is limited due to the homogeneity of learned patterns within\nensembles. To overcome this issue, our study introduces Saliency Diversified\nDeep Ensemble (SDDE), a novel approach that promotes diversity among ensemble\nmembers by leveraging saliency maps. Through incorporating saliency map\ndiversification, our method outperforms conventional ensemble techniques and\nimproves calibration in multiple classification and OOD detection tasks. In\nparticular, the proposed method achieves state-of-the-art OOD detection\nquality, calibration, and accuracy on multiple benchmarks, including\nCIFAR10/100 and large-scale ImageNet datasets.\n","authors":["Stanislav Dereka","Ivan Karpukhin","Maksim Zhdanov","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2305.11616v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10128v1","updated":"2024-06-14T15:38:21Z","published":"2024-06-14T15:38:21Z","title":"SmartRSD: An Intelligent Multimodal Approach to Real-Time Road Surface\n  Detection for Safe Driving","summary":"  Precise and prompt identification of road surface conditions enables vehicles\nto adjust their actions, like changing speed or using specific traction control\ntechniques, to lower the chance of accidents and potential danger to drivers\nand pedestrians. However, most of the existing methods for detecting road\nsurfaces solely rely on visual data, which may be insufficient in certain\nsituations, such as when the roads are covered by debris, in low light\nconditions, or in the presence of fog. Therefore, we introduce a multimodal\napproach for the automated detection of road surface conditions by integrating\naudio and images. The robustness of the proposed method is tested on a diverse\ndataset collected under various environmental conditions and road surface\ntypes. Through extensive evaluation, we demonstrate the effectiveness and\nreliability of our multimodal approach in accurately identifying road surface\nconditions in real-time scenarios. Our findings highlight the potential of\nintegrating auditory and visual cues for enhancing road safety and minimizing\naccident risks\n","authors":["Adnan Md Tayeb","Mst Ayesha Khatun","Mohtasin Golam","Md Facklasur Rahaman","Ali Aouto","Oroceo Paul Angelo","Minseon Lee","Dong-Seong Kim","Jae-Min Lee","Jung-Hyeon Kim"],"pdf_url":"https://arxiv.org/pdf/2406.10128v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2310.13876v2","updated":"2024-06-14T15:36:41Z","published":"2023-10-21T00:56:11Z","title":"Multimodal Transformer Using Cross-Channel attention for Object\n  Detection in Remote Sensing Images","summary":"  Object detection in Remote Sensing Images (RSI) is a critical task for\nnumerous applications in Earth Observation (EO). Differing from object\ndetection in natural images, object detection in remote sensing images faces\nchallenges of scarcity of annotated data and the presence of small objects\nrepresented by only a few pixels. Multi-modal fusion has been determined to\nenhance the accuracy by fusing data from multiple modalities such as RGB,\ninfrared (IR), lidar, and synthetic aperture radar (SAR). To this end, the\nfusion of representations at the mid or late stage, produced by parallel\nsubnetworks, is dominant, with the disadvantages of increasing computational\ncomplexity in the order of the number of modalities and the creation of\nadditional engineering obstacles. Using the cross-attention mechanism, we\npropose a novel multi-modal fusion strategy for mapping relationships between\ndifferent channels at the early stage, enabling the construction of a coherent\ninput by aligning the different modalities. By addressing fusion in the early\nstage, as opposed to mid or late-stage methods, our method achieves competitive\nand even superior performance compared to existing techniques. Additionally, we\nenhance the SWIN transformer by integrating convolution layers into the\nfeed-forward of non-shifting blocks. This augmentation strengthens the model's\ncapacity to merge separated windows through local attention, thereby improving\nsmall object detection. Extensive experiments prove the effectiveness of the\nproposed multimodal fusion module and the architecture, demonstrating their\napplicability to object detection in multimodal aerial imagery.\n","authors":["Bissmella Bahaduri","Zuheng Ming","Fangchen Feng","Anissa Mokraou"],"pdf_url":"https://arxiv.org/pdf/2310.13876v2.pdf","comment":"Accepted by ICIP2024"},{"id":"http://arxiv.org/abs/2406.10126v1","updated":"2024-06-14T15:33:00Z","published":"2024-06-14T15:33:00Z","title":"Training-free Camera Control for Video Generation","summary":"  We propose a training-free and robust solution to offer camera movement\ncontrol for off-the-shelf video diffusion models. Unlike previous work, our\nmethod does not require any supervised finetuning on camera-annotated datasets\nor self-supervised training via data augmentation. Instead, it can be plugged\nand played with most pretrained video diffusion models and generate camera\ncontrollable videos with a single image or text prompt as input. The\ninspiration of our work comes from the layout prior that intermediate latents\nhold towards generated results, thus rearranging noisy pixels in them will make\noutput content reallocated as well. As camera move could also be seen as a kind\nof pixel rearrangement caused by perspective change, videos could be\nreorganized following specific camera motion if their noisy latents change\naccordingly. Established on this, we propose our method CamTrol, which enables\nrobust camera control for video diffusion models. It is achieved by a two-stage\nprocess. First, we model image layout rearrangement through explicit camera\nmovement in 3D point cloud space. Second, we generate videos with camera motion\nusing layout prior of noisy latents formed by a series of rearranged images.\nExtensive experiments have demonstrated the robustness our method holds in\ncontrolling camera motion of generated videos. Furthermore, we show that our\nmethod can produce impressive results in generating 3D rotation videos with\ndynamic content. Project page at https://lifedecoder.github.io/CamTrol/.\n","authors":["Chen Hou","Guoqiang Wei","Yan Zeng","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.10126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10125v1","updated":"2024-06-14T15:31:45Z","published":"2024-06-14T15:31:45Z","title":"MapVision: CVPR 2024 Autonomous Grand Challenge Mapless Driving Tech\n  Report","summary":"  Autonomous driving without high-definition (HD) maps demands a higher level\nof active scene understanding. In this competition, the organizers provided the\nmulti-perspective camera images and standard-definition (SD) maps to explore\nthe boundaries of scene reasoning capabilities. We found that most existing\nalgorithms construct Bird's Eye View (BEV) features from these\nmulti-perspective images and use multi-task heads to delineate road\ncenterlines, boundary lines, pedestrian crossings, and other areas. However,\nthese algorithms perform poorly at the far end of roads and struggle when the\nprimary subject in the image is occluded. Therefore, in this competition, we\nnot only used multi-perspective images as input but also incorporated SD maps\nto address this issue. We employed map encoder pre-training to enhance the\nnetwork's geometric encoding capabilities and utilized YOLOX to improve traffic\nelement detection precision. Additionally, for area detection, we innovatively\nintroduced LDTR and auxiliary tasks to achieve higher precision. As a result,\nour final OLUS score is 0.58.\n","authors":["Zhongyu Yang","Mai Liu","Jinluo Xie","Yueming Zhang","Chen Shen","Wei Shao","Jichao Jiao","Tengfei Xing","Runbo Hu","Pengfei Xu"],"pdf_url":"https://arxiv.org/pdf/2406.10125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10119v1","updated":"2024-06-14T15:24:49Z","published":"2024-06-14T15:24:49Z","title":"Modified Risk Formulation for Improving the Prediction of Knee\n  Osteoarthritis Progression","summary":"  Current methods for predicting osteoarthritis (OA) outcomes do not\nincorporate disease specific prior knowledge to improve the outcome prediction\nmodels. We developed a novel approach that effectively uses consecutive imaging\nstudies to improve OA outcome predictions by incorporating an OA severity\nconstraint. This constraint ensures that the risk of OA for a knee should\neither increase or remain the same over time. DL models were trained to predict\nTKR within multiple time periods (1 year, 2 years, and 4 years) using knee\nradiographs and MRI scans. Models with and without the risk constraint were\nevaluated using the area under the receiver operator curve (AUROC) and the area\nunder the precision recall curve (AUPRC) analysis. The novel RiskFORM2 method,\nleveraging a dual model risk constraint architecture, demonstrated superior\nperformance, yielding an AUROC of 0.87 and AUPRC of 0.47 for 1 year TKR\nprediction on the OAI radiograph test set, a marked improvement over the 0.79\nAUROC and 0.34 AUPRC of the baseline approach. The performance advantage\nextended to longer followup periods, with RiskFORM2 maintaining a high AUROC of\n0.86 and AUPRC of 0.75 in predicting TKR within 4 years. Additionally, when\ngeneralizing to the external MOST radiograph test set, RiskFORM2 generalized\nbetter with an AUROC of 0.77 and AUPRC of 0.25 for 1 year predictions, which\nwas higher than the 0.71 AUROC and 0.19 AUPRC of the baseline approach. In the\nMRI test sets, similar patterns emerged, with RiskFORM2 outperforming the\nbaseline approach consistently. However, RiskFORM1 exhibited the highest AUROC\nof 0.86 and AUPRC of 0.72 for 4 year predictions on the OAI set.\n","authors":["Haresh Rengaraj Rajamohan","Richard Kijowski","Kyunghyun Cho","Cem M. Deniz"],"pdf_url":"https://arxiv.org/pdf/2406.10119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10115v1","updated":"2024-06-14T15:21:57Z","published":"2024-06-14T15:21:57Z","title":"Shelf-Supervised Multi-Modal Pre-Training for 3D Object Detection","summary":"  State-of-the-art 3D object detectors are often trained on massive labeled\ndatasets. However, annotating 3D bounding boxes remains prohibitively expensive\nand time-consuming, particularly for LiDAR. Instead, recent works demonstrate\nthat self-supervised pre-training with unlabeled data can improve detection\naccuracy with limited labels. Contemporary methods adapt best-practices for\nself-supervised learning from the image domain to point clouds (such as\ncontrastive learning). However, publicly available 3D datasets are considerably\nsmaller and less diverse than those used for image-based self-supervised\nlearning, limiting their effectiveness. We do note, however, that such data is\nnaturally collected in a multimodal fashion, often paired with images. Rather\nthan pre-training with only self-supervised objectives, we argue that it is\nbetter to bootstrap point cloud representations using image-based foundation\nmodels trained on internet-scale image data. Specifically, we propose a\nshelf-supervised approach (e.g. supervised with off-the-shelf image foundation\nmodels) for generating zero-shot 3D bounding boxes from paired RGB and LiDAR\ndata. Pre-training 3D detectors with such pseudo-labels yields significantly\nbetter semi-supervised detection accuracy than prior self-supervised pretext\ntasks. Importantly, we show that image-based shelf-supervision is helpful for\ntraining LiDAR-only and multi-modal (RGB + LiDAR) detectors. We demonstrate the\neffectiveness of our approach on nuScenes and WOD, significantly improving over\nprior work in limited data settings.\n","authors":["Mehar Khurana","Neehar Peri","Deva Ramanan","James Hays"],"pdf_url":"https://arxiv.org/pdf/2406.10115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10114v1","updated":"2024-06-14T15:20:46Z","published":"2024-06-14T15:20:46Z","title":"Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part\n  Representations","summary":"  Part-aware panoptic segmentation (PPS) requires (a) that each foreground\nobject and background region in an image is segmented and classified, and (b)\nthat all parts within foreground objects are segmented, classified and linked\nto their parent object. Existing methods approach PPS by separately conducting\nobject-level and part-level segmentation. However, their part-level predictions\nare not linked to individual parent objects. Therefore, their learning\nobjective is not aligned with the PPS task objective, which harms the PPS\nperformance. To solve this, and make more accurate PPS predictions, we propose\nTask-Aligned Part-aware Panoptic Segmentation (TAPPS). This method uses a set\nof shared queries to jointly predict (a) object-level segments, and (b) the\npart-level segments within those same objects. As a result, TAPPS learns to\npredict part-level segments that are linked to individual parent objects,\naligning the learning objective with the task objective, and allowing TAPPS to\nleverage joint object-part representations. With experiments, we show that\nTAPPS considerably outperforms methods that predict objects and parts\nseparately, and achieves new state-of-the-art PPS results.\n","authors":["Daan de Geus","Gijs Dubbelman"],"pdf_url":"https://arxiv.org/pdf/2406.10114v1.pdf","comment":"CVPR 2024. Project page and code: https://tue-mps.github.io/tapps/"},{"id":"http://arxiv.org/abs/2406.10111v1","updated":"2024-06-14T15:19:21Z","published":"2024-06-14T15:19:21Z","title":"GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors","summary":"  Achieving high-resolution novel view synthesis (HRNVS) from low-resolution\ninput views is a challenging task due to the lack of high-resolution data.\nPrevious methods optimize high-resolution Neural Radiance Field (NeRF) from\nlow-resolution input views but suffer from slow rendering speed. In this work,\nwe base our method on 3D Gaussian Splatting (3DGS) due to its capability of\nproducing high-quality images at a faster rendering speed. To alleviate the\nshortage of data for higher-resolution synthesis, we propose to leverage\noff-the-shelf 2D diffusion priors by distilling the 2D knowledge into 3D with\nScore Distillation Sampling (SDS). Nevertheless, applying SDS directly to\nGaussian-based 3D super-resolution leads to undesirable and redundant 3D\nGaussian primitives, due to the randomness brought by generative priors. To\nmitigate this issue, we introduce two simple yet effective techniques to reduce\nstochastic disturbances introduced by SDS. Specifically, we 1) shrink the range\nof diffusion timestep in SDS with an annealing strategy; 2) randomly discard\nredundant Gaussian primitives during densification. Extensive experiments have\ndemonstrated that our proposed GaussainSR can attain high-quality results for\nHRNVS with only low-resolution inputs on both synthetic and real-world\ndatasets. Project page: https://chchnii.github.io/GaussianSR/\n","authors":["Xiqian Yu","Hanxin Zhu","Tianyu He","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.10111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10107v1","updated":"2024-06-14T15:08:04Z","published":"2024-06-14T15:08:04Z","title":"Annotation Cost-Efficient Active Learning for Deep Metric Learning\n  Driven Remote Sensing Image Retrieval","summary":"  Deep metric learning (DML) has shown to be very effective for content-based\nimage retrieval (CBIR) in remote sensing (RS). Most of DML methods for CBIR\nrely on many annotated images to accurately learn model parameters of deep\nneural networks. However, gathering many image annotations is time consuming\nand costly. To address this, we propose an annotation cost-efficient active\nlearning (ANNEAL) method specifically designed for DML driven CBIR in RS.\nANNEAL aims to create a small but informative training set made up of similar\nand dissimilar image pairs to be utilized for learning a deep metric space. The\ninformativeness of the image pairs is assessed combining uncertainty and\ndiversity criteria. To assess the uncertainty of image pairs, we introduce two\nalgorithms: 1) metric-guided uncertainty estimation (MGUE); and 2) binary\nclassifier guided uncertainty estimation (BCGUE). MGUE automatically estimates\na threshold value that acts as a \"boundary\" between similar and dissimilar\nimage pairs based on the distances in the metric space. The closer the\nsimilarity between image pairs to the estimated threshold value the higher\ntheir uncertainty. BCGUE estimates the uncertainty of the image pairs based on\nthe confidence of the classifier in assigning the correct similarity label. The\ndiversity criterion is assessed through a clustering-based strategy. ANNEAL\nselects the most informative image pairs by combining either MGUE or BCGUE with\nclustering-based strategy. The selected image pairs are sent to expert\nannotators to be labeled as similar or dissimilar. This way of annotating\nimages significantly reduces the annotation cost compared to the cost of\nannotating images with LULC labels. Experimental results carried out on two RS\nbenchmark datasets demonstrate the effectiveness of our method. The code of the\nproposed method will be publicly available upon the acceptance of the paper.\n","authors":["Genc Hoxha","Gencer Sumbul","Julia Henkel","Lars Möllenbrok","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2406.10107v1.pdf","comment":"Submitted to IEEE Transactions on Geoscience and Remote Sensing"},{"id":"http://arxiv.org/abs/2406.10100v1","updated":"2024-06-14T14:57:07Z","published":"2024-06-14T14:57:07Z","title":"SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for\n  Remote Sensing Vision-Language Understanding","summary":"  Remote Sensing Large Multi-Modal Models (RSLMMs) are developing rapidly and\nshowcase significant capabilities in remote sensing imagery (RSI)\ncomprehension. However, due to the limitations of existing datasets, RSLMMs\nhave shortcomings in understanding the rich semantic relations among objects in\ncomplex remote sensing scenes. To unlock RSLMMs' complex comprehension ability,\nwe propose a large-scale instruction tuning dataset FIT-RS, containing\n1,800,851 instruction samples. FIT-RS covers common interpretation tasks and\ninnovatively introduces several complex comprehension tasks of escalating\ndifficulty, ranging from relation reasoning to image-level scene graph\ngeneration. Based on FIT-RS, we build the FIT-RSFG benchmark. Furthermore, we\nestablish a new benchmark to evaluate the fine-grained relation comprehension\ncapabilities of LMMs, named FIT-RSRC. Based on combined instruction data, we\npropose SkySenseGPT, which achieves outstanding performance on both public\ndatasets and FIT-RSFG, surpassing existing RSLMMs. We hope the FIT-RS dataset\ncan enhance the relation comprehension capability of RSLMMs and provide a\nlarge-scale fine-grained data source for the remote sensing community. The\ndataset will be available at https://github.com/Luo-Z13/SkySenseGPT\n","authors":["Junwei Luo","Zhen Pang","Yongjun Zhang","Tingzhu Wang","Linlin Wang","Bo Dang","Jiangwei Lao","Jian Wang","Jingdong Chen","Yihua Tan","Yansheng Li"],"pdf_url":"https://arxiv.org/pdf/2406.10100v1.pdf","comment":"30 pages, 5 figures, 19 tables, dataset and code see\n  https://github.com/Luo-Z13/SkySenseGPT"},{"id":"http://arxiv.org/abs/2401.01543v2","updated":"2024-06-14T14:55:26Z","published":"2024-01-03T05:26:57Z","title":"Retraining-free Model Quantization via One-Shot Weight-Coupling Learning","summary":"  Quantization is of significance for compressing the over-parameterized deep\nneural models and deploying them on resource-limited devices. Fixed-precision\nquantization suffers from performance drop due to the limited numerical\nrepresentation ability. Conversely, mixed-precision quantization (MPQ) is\nadvocated to compress the model effectively by allocating heterogeneous\nbit-width for layers. MPQ is typically organized into a searching-retraining\ntwo-stage process. In this paper, we devise a one-shot training-searching\nparadigm for mixed-precision model compression. Specifically, in the first\nstage, all potential bit-width configurations are coupled and thus optimized\nsimultaneously within a set of shared weights. However, our observations reveal\na previously unseen and severe bit-width interference phenomenon among highly\ncoupled weights during optimization, leading to considerable performance\ndegradation under a high compression ratio. To tackle this problem, we first\ndesign a bit-width scheduler to dynamically freeze the most turbulent bit-width\nof layers during training, to ensure the rest bit-widths converged properly.\nThen, taking inspiration from information theory, we present an information\ndistortion mitigation technique to align the behavior of the bad-performing\nbit-widths to the well-performing ones. In the second stage, an inference-only\ngreedy search scheme is devised to evaluate the goodness of configurations\nwithout introducing any additional training costs. Extensive experiments on\nthree representative models and three datasets demonstrate the effectiveness of\nthe proposed method. Code can be available on\n\\href{https://www.github.com/1hunters/retraining-free-quantization}{https://github.com/1hunters/retraining-free-quantization}.\n","authors":["Chen Tang","Yuan Meng","Jiacheng Jiang","Shuzhao Xie","Rongwei Lu","Xinzhu Ma","Zhi Wang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.01543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09406v2","updated":"2024-06-14T14:43:26Z","published":"2024-06-13T17:59:42Z","title":"4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities","summary":"  Current multimodal and multitask foundation models like 4M or UnifiedIO show\npromising results, but in practice their out-of-the-box abilities to accept\ndiverse inputs and perform diverse tasks are limited by the (usually rather\nsmall) number of modalities and tasks they are trained on. In this paper, we\nexpand upon the capabilities of them by training a single model on tens of\nhighly diverse modalities and by performing co-training on large-scale\nmultimodal datasets and text corpora. This includes training on several\nsemantic and geometric modalities, feature maps from recent state of the art\nmodels like DINOv2 and ImageBind, pseudo labels of specialist models like SAM\nand 4DHumans, and a range of new modalities that allow for novel ways to\ninteract with the model and steer the generation, for example image metadata or\ncolor palettes. A crucial step in this process is performing discrete\ntokenization on various modalities, whether they are image-like, neural network\nfeature maps, vectors, structured data like instance segmentation or human\nposes, or data that can be represented as text. Through this, we expand on the\nout-of-the-box capabilities of multimodal models and specifically show the\npossibility of training one model to solve at least 3x more tasks/modalities\nthan existing ones and doing so without a loss in performance. This enables\nmore fine-grained and controllable multimodal generation capabilities and\nallows us to study the distillation of models trained on diverse data and\nobjectives into a unified model. We successfully scale the training to a three\nbillion parameter model using tens of modalities and different datasets. The\nresulting models and training code are open sourced at 4m.epfl.ch.\n","authors":["Roman Bachmann","Oğuzhan Fatih Kar","David Mizrahi","Ali Garjani","Mingfei Gao","David Griffiths","Jiaming Hu","Afshin Dehghan","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2406.09406v2.pdf","comment":"Project page at 4m.epfl.ch"},{"id":"http://arxiv.org/abs/2406.10082v1","updated":"2024-06-14T14:36:54Z","published":"2024-06-14T14:36:54Z","title":"Whisper-Flamingo: Integrating Visual Features into Whisper for\n  Audio-Visual Speech Recognition and Translation","summary":"  Audio-Visual Speech Recognition (AVSR) uses lip-based video to improve\nperformance in noise. Since videos are harder to obtain than audio, the video\ntraining data of AVSR models is usually limited to a few thousand hours. In\ncontrast, speech models such as Whisper are trained with hundreds of thousands\nof hours of data, and thus learn a better speech-to-text decoder. The huge\ntraining data difference motivates us to adapt Whisper to handle video inputs.\nInspired by Flamingo which injects visual features into language models, we\npropose Whisper-Flamingo which integrates visual features into the Whisper\nspeech recognition and translation model with gated cross attention. Our\naudio-visual Whisper-Flamingo outperforms audio-only Whisper on English speech\nrecognition and En-X translation for 6 languages in noisy conditions. Moreover,\nWhisper-Flamingo is a versatile model and conducts all of these tasks using one\nset of parameters, while prior methods are trained separately on each language.\n","authors":["Andrew Rouditchenko","Yuan Gong","Samuel Thomas","Leonid Karlinsky","Hilde Kuehne","Rogerio Feris","James Glass"],"pdf_url":"https://arxiv.org/pdf/2406.10082v1.pdf","comment":"Interspeech 2024. Code https://github.com/roudimit/whisper-flamingo"},{"id":"http://arxiv.org/abs/2406.10079v1","updated":"2024-06-14T14:35:58Z","published":"2024-06-14T14:35:58Z","title":"Localizing Events in Videos with Multimodal Queries","summary":"  Video understanding is a pivotal task in the digital era, yet the dynamic and\nmultievent nature of videos makes them labor-intensive and computationally\ndemanding to process. Thus, localizing a specific event given a semantic query\nhas gained importance in both user-oriented applications like video search and\nacademic research into video foundation models. A significant limitation in\ncurrent research is that semantic queries are typically in natural language\nthat depicts the semantics of the target event. This setting overlooks the\npotential for multimodal semantic queries composed of images and texts. To\naddress this gap, we introduce a new benchmark, ICQ, for localizing events in\nvideos with multimodal queries, along with a new evaluation dataset\nICQ-Highlight. Our new benchmark aims to evaluate how well models can localize\nan event given a multimodal semantic query that consists of a reference image,\nwhich depicts the event, and a refinement text to adjust the images' semantics.\nTo systematically benchmark model performance, we include 4 styles of reference\nimages and 5 types of refinement texts, allowing us to explore model\nperformance across different domains. We propose 3 adaptation methods that\ntailor existing models to our new setting and evaluate 10 SOTA models, ranging\nfrom specialized to large-scale foundation models. We believe this benchmark is\nan initial step toward investigating multimodal queries in video event\nlocalization.\n","authors":["Gengyuan Zhang","Mang Ling Ada Fok","Yan Xia","Yansong Tang","Daniel Cremers","Philip Torr","Volker Tresp","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2406.10079v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.10078v1","updated":"2024-06-14T14:35:44Z","published":"2024-06-14T14:35:44Z","title":"D-NPC: Dynamic Neural Point Clouds for Non-Rigid View Synthesis from\n  Monocular Video","summary":"  Dynamic reconstruction and spatiotemporal novel-view synthesis of non-rigidly\ndeforming scenes recently gained increased attention. While existing work\nachieves impressive quality and performance on multi-view or teleporting camera\nsetups, most methods fail to efficiently and faithfully recover motion and\nappearance from casual monocular captures. This paper contributes to the field\nby introducing a new method for dynamic novel view synthesis from monocular\nvideo, such as casual smartphone captures.\n  Our approach represents the scene as a $\\textit{dynamic neural point cloud}$,\nan implicit time-conditioned point distribution that encodes local geometry and\nappearance in separate hash-encoded neural feature grids for static and dynamic\nregions. By sampling a discrete point cloud from our model, we can efficiently\nrender high-quality novel views using a fast differentiable rasterizer and\nneural rendering network. Similar to recent work, we leverage advances in\nneural scene analysis by incorporating data-driven priors like monocular depth\nestimation and object segmentation to resolve motion and depth ambiguities\noriginating from the monocular captures. In addition to guiding the\noptimization process, we show that these priors can be exploited to explicitly\ninitialize our scene representation to drastically improve optimization speed\nand final image quality. As evidenced by our experimental evaluation, our\ndynamic point cloud model not only enables fast optimization and real-time\nframe rates for interactive applications, but also achieves competitive image\nquality on monocular benchmark sequences.\n  Our project page is available at\nhttps://moritzkappel.github.io/projects/dnpc.\n","authors":["Moritz Kappel","Florian Hahlbohm","Timon Scholz","Susana Castillo","Christian Theobalt","Martin Eisemann","Vladislav Golyanik","Marcus Magnor"],"pdf_url":"https://arxiv.org/pdf/2406.10078v1.pdf","comment":"16 pages, 5 figures, 10 tables. Project page:\n  https://moritzkappel.github.io/projects/dnpc"},{"id":"http://arxiv.org/abs/2405.19009v2","updated":"2024-06-14T14:29:41Z","published":"2024-05-29T11:48:17Z","title":"Enhancing Vision-Language Model with Unmasked Token Alignment","summary":"  Contrastive pre-training on image-text pairs, exemplified by CLIP, becomes a\nstandard technique for learning multi-modal visual-language representations.\nAlthough CLIP has demonstrated remarkable performance, training it from scratch\non noisy web-scale datasets is computationally demanding. On the other hand,\nmask-then-predict pre-training approaches, like Masked Image Modeling (MIM),\noffer efficient self-supervised learning for single-modal representations. This\npaper introduces Unmasked Token Alignment (UTA), a method that leverages\nexisting CLIP models to further enhance its vision-language representations.\nUTA trains a Vision Transformer (ViT) by aligning unmasked visual tokens to the\ncorresponding image tokens from a frozen CLIP vision encoder, which\nautomatically aligns the ViT model with the CLIP text encoder. The pre-trained\nViT can be directly applied for zero-shot evaluation even without training on\nimage-text pairs. Compared to MIM approaches, UTA does not suffer from\ntraining-finetuning inconsistency and is much more training-efficient by\navoiding using the extra [MASK] tokens. Extensive experimental results\ndemonstrate that UTA can enhance CLIP models and outperform existing MIM\nmethods on various uni- and multi-modal benchmarks. Code and models are\navailable at https://github.com/jihaonew/UTA.\n","authors":["Jihao Liu","Jinliang Zheng","Boxiao Liu","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2405.19009v2.pdf","comment":"Accepted by TMLR; Code and models are available at\n  https://github.com/jihaonew/UTA"},{"id":"http://arxiv.org/abs/2404.15081v2","updated":"2024-06-14T14:26:38Z","published":"2024-04-23T14:31:15Z","title":"Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\n  Perturbations That Efficiently Fool Customized Diffusion Models","summary":"  Diffusion models (DMs) embark a new era of generative modeling and offer more\nopportunities for efficient generating high-quality and realistic data samples.\nHowever, their widespread use has also brought forth new challenges in model\nsecurity, which motivates the creation of more effective adversarial attackers\non DMs to understand its vulnerability. We propose CAAT, a simple but generic\nand efficient approach that does not require costly training to effectively\nfool latent diffusion models (LDMs). The approach is based on the observation\nthat cross-attention layers exhibits higher sensitivity to gradient change,\nallowing for leveraging subtle perturbations on published images to\nsignificantly corrupt the generated images. We show that a subtle perturbation\non an image can significantly impact the cross-attention layers, thus changing\nthe mapping between text and image during the fine-tuning of customized\ndiffusion models. Extensive experiments demonstrate that CAAT is compatible\nwith diverse diffusion models and outperforms baseline attack methods in a more\neffective (more noise) and efficient (twice as fast as Anti-DreamBooth and\nMist) manner.\n","authors":["Jingyao Xu","Yuetong Lu","Yandong Li","Siyang Lu","Dongdong Wang","Xiang Wei"],"pdf_url":"https://arxiv.org/pdf/2404.15081v2.pdf","comment":"Published at CVPR 2024, code:https://github.com/CO2-cityao/CAAT"},{"id":"http://arxiv.org/abs/2312.10986v3","updated":"2024-06-14T14:26:31Z","published":"2023-12-18T07:14:25Z","title":"Long-Tailed 3D Detection via 2D Late Fusion","summary":"  Long-Tailed 3D Object Detection (LT3D) addresses the problem of accurately\ndetecting objects from both common and rare classes. Contemporary multi-modal\ndetectors achieve low AP on rare-classes (e.g., CMT only achieves 9.4 AP on\nstroller), presumably because training detectors end-to-end with significant\nclass imbalance is challenging. To address this limitation, we delve into a\nsimple late-fusion framework that ensembles independently trained uni-modal\nLiDAR and RGB detectors. Importantly, such a late-fusion framework allows us to\nleverage large-scale uni-modal datasets (with more examples for rare classes)\nto train better uni-modal RGB detectors, unlike prevailing multimodal detectors\nthat require paired multi-modal training data. Notably, our approach\nsignificantly improves rare-class detection by 7.2% over prior work. Further,\nwe examine three critical components of our simple late-fusion approach from\nfirst principles and investigate whether to train 2D or 3D RGB detectors,\nwhether to match RGB and LiDAR detections in 3D or the projected 2D image plane\nfor fusion, and how to fuse matched detections. Extensive experiments reveal\nthat 2D RGB detectors achieve better recognition accuracy for rare classes than\n3D RGB detectors and matching on the 2D image plane mitigates depth estimation\nerrors. Our late-fusion approach achieves 51.4 mAP on the established nuScenes\nLT3D benchmark, improving over prior work by 5.9 mAP!\n","authors":["Yechi Ma","Neehar Peri","Shuoquan Wei","Wei Hua","Deva Ramanan","Yanan Li","Shu Kong"],"pdf_url":"https://arxiv.org/pdf/2312.10986v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10068v1","updated":"2024-06-14T14:24:05Z","published":"2024-06-14T14:24:05Z","title":"DurLAR: A High-fidelity 128-channel LiDAR Dataset with Panoramic Ambient\n  and Reflectivity Imagery for Multi-modal Autonomous Driving Applications","summary":"  We present DurLAR, a high-fidelity 128-channel 3D LiDAR dataset with\npanoramic ambient (near infrared) and reflectivity imagery, as well as a sample\nbenchmark task using depth estimation for autonomous driving applications. Our\ndriving platform is equipped with a high resolution 128 channel LiDAR, a 2MPix\nstereo camera, a lux meter and a GNSS/INS system. Ambient and reflectivity\nimages are made available along with the LiDAR point clouds to facilitate\nmulti-modal use of concurrent ambient and reflectivity scene information.\nLeveraging DurLAR, with a resolution exceeding that of prior benchmarks, we\nconsider the task of monocular depth estimation and use this increased\navailability of higher resolution, yet sparse ground truth scene depth\ninformation to propose a novel joint supervised/self-supervised loss\nformulation. We compare performance over both our new DurLAR dataset, the\nestablished KITTI benchmark and the Cityscapes dataset. Our evaluation shows\nour joint use supervised and self-supervised loss terms, enabled via the\nsuperior ground truth resolution and availability within DurLAR improves the\nquantitative and qualitative performance of leading contemporary monocular\ndepth estimation approaches (RMSE=3.639, Sq Rel=0.936).\n","authors":["Li Li","Khalid N. Ismail","Hubert P. H. Shum","Toby P. Breckon"],"pdf_url":"https://arxiv.org/pdf/2406.10068v1.pdf","comment":"Accepted by 3DV 2021; 13 pages, 14 figures; Dataset at\n  https://github.com/l1997i/durlar"},{"id":"http://arxiv.org/abs/2406.10057v1","updated":"2024-06-14T14:15:35Z","published":"2024-06-14T14:15:35Z","title":"First Multi-Dimensional Evaluation of Flowchart Comprehension for\n  Multimodal Large Language Models","summary":"  With the development of multimodal large language models (MLLMs) technology,\nits general capabilities are increasingly powerful. To evaluate the various\nabilities of MLLMs, numerous evaluation systems have emerged. But now there is\nstill a lack of a comprehensive method to evaluate MLLMs in the tasks related\nto flowcharts, which are very important in daily life and work. We propose the\nfirst comprehensive method, FlowCE, to assess MLLMs across various dimensions\nfor tasks related to flowcharts. It encompasses evaluating MLLMs' abilities in\nReasoning, Localization Recognition, Information Extraction, Logical\nVerification, and Summarization on flowcharts. However, we find that even the\nGPT4o model achieves only a score of 56.63. Among open-source models,\nPhi-3-Vision obtained the highest score of 49.97. We hope that FlowCE can\ncontribute to future research on multimodal large language models (MLLMs) for\ntasks based on flowcharts. We are open-sourcing this project:\n\\url{https://github.com/360AILAB-NLP/FlowCE}\n","authors":["Enming Zhang","Ruobing Yao","Huanyong Liu","Junhui Yu","Jiale Wang"],"pdf_url":"https://arxiv.org/pdf/2406.10057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03060v2","updated":"2024-06-14T14:11:43Z","published":"2024-01-05T20:32:40Z","title":"Super-resolution multi-contrast unbiased eye atlases with deep\n  probabilistic refinement","summary":"  Purpose: Eye morphology varies significantly across the population,\nespecially for the orbit and optic nerve. These variations limit the\nfeasibility and robustness of generalizing population-wise features of eye\norgans to an unbiased spatial reference.\n  Approach: To tackle these limitations, we propose a process for creating\nhigh-resolution unbiased eye atlases. First, to restore spatial details from\nscans with a low through-plane resolution compared to a high in-plane\nresolution, we apply a deep learning-based super-resolution algorithm. Then, we\ngenerate an initial unbiased reference with an iterative metric-based\nregistration using a small portion of subject scans. We register the remaining\nscans to this template and refine the template using an unsupervised deep\nprobabilistic approach that generates a more expansive deformation field to\nenhance the organ boundary alignment. We demonstrate this framework using\nmagnetic resonance images across four different tissue contrasts, generating\nfour atlases in separate spatial alignments.\n  Results: For each tissue contrast, we find a significant improvement using\nthe Wilcoxon signed-rank test in the average Dice score across four labeled\nregions compared to a standard registration framework consisting of rigid,\naffine, and deformable transformations. These results highlight the effective\nalignment of eye organs and boundaries using our proposed process.\n  Conclusions: By combining super-resolution preprocessing and deep\nprobabilistic models, we address the challenge of generating an eye atlas to\nserve as a standardized reference across a largely variable population.\n","authors":["Ho Hin Lee","Adam M. Saunders","Michael E. Kim","Samuel W. Remedios","Lucas W. Remedios","Yucheng Tang","Qi Yang","Xin Yu","Shunxing Bao","Chloe Cho","Louise A. Mawn","Tonia S. Rex","Kevin L. Schey","Blake E. Dewey","Jeffrey M. Spraggins","Jerry L. Prince","Yuankai Huo","Bennett A. Landman"],"pdf_url":"https://arxiv.org/pdf/2401.03060v2.pdf","comment":"Revised for submission to SPIE Journal of Medical Imaging. 26 pages,\n  6 figures"},{"id":"http://arxiv.org/abs/2401.00929v2","updated":"2024-06-14T14:10:19Z","published":"2024-01-01T18:20:43Z","title":"GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable\n  Simulation, Demonstration, and Imitation","summary":"  This paper presents GenH2R, a framework for learning generalizable\nvision-based human-to-robot (H2R) handover skills. The goal is to equip robots\nwith the ability to reliably receive objects with unseen geometry handed over\nby humans in various complex trajectories. We acquire such generalizability by\nlearning H2R handover at scale with a comprehensive solution including\nprocedural simulation assets creation, automated demonstration generation, and\neffective imitation learning. We leverage large-scale 3D model repositories,\ndexterous grasp generation methods, and curve-based 3D animation to create an\nH2R handover simulation environment named \\simabbns, surpassing the number of\nscenes in existing simulators by three orders of magnitude. We further\nintroduce a distillation-friendly demonstration generation method that\nautomatically generates a million high-quality demonstrations suitable for\nlearning. Finally, we present a 4D imitation learning method augmented by a\nfuture forecasting objective to distill demonstrations into a visuo-motor\nhandover policy. Experimental evaluations in both simulators and the real world\ndemonstrate significant improvements (at least +10\\% success rate) over\nbaselines in all cases. The project page is https://GenH2R.github.io/.\n","authors":["Zifan Wang","Junyu Chen","Ziqing Chen","Pengwei Xie","Rui Chen","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2401.00929v2.pdf","comment":"The project page is https://GenH2R.github.io/, accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2312.14494v3","updated":"2024-06-14T14:09:29Z","published":"2023-12-22T07:42:00Z","title":"Revisiting Few-Shot Object Detection with Vision-Language Models","summary":"  The era of vision-language models (VLMs) trained on large web-scale datasets\nchallenges conventional formulations of \"open-world\" perception. In this work,\nwe revisit the task of few-shot object detection (FSOD) in the context of\nrecent foundational VLMs. First, we point out that zero-shot VLMs such as\nGroundingDINO significantly outperform state-of-the-art few-shot detectors (48\nvs. 33 AP) on COCO. Despite their strong zero-shot performance, such\nfoundational models may still be sub-optimal. For example, trucks on the web\nmay be defined differently from trucks for a target application such as\nautonomous vehicle perception. We argue that the task of few-shot recognition\ncan be reformulated as aligning foundation models to target concepts using a\nfew examples. Interestingly, such examples can be multi-modal, using both text\nand visual cues, mimicking instructions that are often given to human\nannotators when defining a target concept of interest. Concretely, we propose\nFoundational FSOD, a new benchmark protocol that evaluates detectors\npre-trained on any external datasets and fine-tuned on multi-modal (text and\nvisual) K-shot examples per target class. We repurpose nuImages for\nFoundational FSOD, benchmark several popular open-source VLMs, and provide an\nempirical analysis of state-of-the-art methods. Lastly, we discuss our recent\nCVPR 2024 Foundational FSOD competition and share insights from the community.\nNotably, the winning team significantly outperforms our baseline by 23.9 mAP!\n","authors":["Anish Madan","Neehar Peri","Shu Kong","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2312.14494v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.02283v2","updated":"2024-06-14T14:09:18Z","published":"2022-04-05T15:31:06Z","title":"Lost in Latent Space: Disentangled Models and the Challenge of\n  Combinatorial Generalisation","summary":"  Recent research has shown that generative models with highly disentangled\nrepresentations fail to generalise to unseen combination of generative factor\nvalues. These findings contradict earlier research which showed improved\nperformance in out-of-training distribution settings when compared to entangled\nrepresentations. Additionally, it is not clear if the reported failures are due\nto (a) encoders failing to map novel combinations to the proper regions of the\nlatent space or (b) novel combinations being mapped correctly but the\ndecoder/downstream process is unable to render the correct output for the\nunseen combinations. We investigate these alternatives by testing several\nmodels on a range of datasets and training settings. We find that (i) when\nmodels fail, their encoders also fail to map unseen combinations to correct\nregions of the latent space and (ii) when models succeed, it is either because\nthe test conditions do not exclude enough examples, or because excluded\ngenerative factors determine independent parts of the output image. Based on\nthese results, we argue that to generalise properly, models not only need to\ncapture factors of variation, but also understand how to invert the generative\nprocess that was used to generate the data.\n","authors":["Milton L. Montero","Jeffrey S. Bowers","Rui Ponte Costa","Casimir J. H. Ludwig","Gaurav Malhotra"],"pdf_url":"https://arxiv.org/pdf/2204.02283v2.pdf","comment":"10 pages and 7 figures in main text (not including references). 27\n  pages and 31 figures in appendix. Updated to match the camera-ready version"},{"id":"http://arxiv.org/abs/2406.10050v1","updated":"2024-06-14T14:00:02Z","published":"2024-06-14T14:00:02Z","title":"Comparison of fine-tuning strategies for transfer learning in medical\n  image classification","summary":"  In the context of medical imaging and machine learning, one of the most\npressing challenges is the effective adaptation of pre-trained models to\nspecialized medical contexts. Despite the availability of advanced pre-trained\nmodels, their direct application to the highly specialized and diverse field of\nmedical imaging often falls short due to the unique characteristics of medical\ndata. This study provides a comprehensive analysis on the performance of\nvarious fine-tuning methods applied to pre-trained models across a spectrum of\nmedical imaging domains, including X-ray, MRI, Histology, Dermoscopy, and\nEndoscopic surgery. We evaluated eight fine-tuning strategies, including\nstandard techniques such as fine-tuning all layers or fine-tuning only the\nclassifier layers, alongside methods such as gradually unfreezing layers,\nregularization based fine-tuning and adaptive learning rates. We selected three\nwell-established CNN architectures (ResNet-50, DenseNet-121, and VGG-19) to\ncover a range of learning and feature extraction scenarios. Although our\nresults indicate that the efficacy of these fine-tuning methods significantly\nvaries depending on both the architecture and the medical imaging type,\nstrategies such as combining Linear Probing with Full Fine-tuning resulted in\nnotable improvements in over 50% of the evaluated cases, demonstrating general\neffectiveness across medical domains. Moreover, Auto-RGN, which dynamically\nadjusts learning rates, led to performance enhancements of up to 11% for\nspecific modalities. Additionally, the DenseNet architecture showed more\npronounced benefits from alternative fine-tuning approaches compared to\ntraditional full fine-tuning. This work not only provides valuable insights for\noptimizing pre-trained models in medical image analysis but also suggests the\npotential for future research into more advanced architectures and fine-tuning\nmethods.\n","authors":["Ana Davila","Jacinto Colan","Yasuhisa Hasegawa"],"pdf_url":"https://arxiv.org/pdf/2406.10050v1.pdf","comment":"Accepted at Image and Vision Computing"},{"id":"http://arxiv.org/abs/2406.10045v1","updated":"2024-06-14T13:52:58Z","published":"2024-06-14T13:52:58Z","title":"Unobtrusive Monitoring of Physical Weakness: A Simulated Approach","summary":"  Aging and chronic conditions affect older adults' daily lives, making early\ndetection of developing health issues crucial. Weakness, common in many\nconditions, alters physical movements and daily activities subtly. However,\ndetecting such changes can be challenging due to their subtle and gradual\nnature. To address this, we employ a non-intrusive camera sensor to monitor\nindividuals' daily sitting and relaxing activities for signs of weakness. We\nsimulate weakness in healthy subjects by having them perform physical exercise\nand observing the behavioral changes in their daily activities before and after\nworkouts. The proposed system captures fine-grained features related to body\nmotion, inactivity, and environmental context in real-time while prioritizing\nprivacy. A Bayesian Network is used to model the relationships between\nfeatures, activities, and health conditions. We aim to identify specific\nfeatures and activities that indicate such changes and determine the most\nsuitable time scale for observing the change. Results show 0.97 accuracy in\ndistinguishing simulated weakness at the daily level. Fine-grained behavioral\nfeatures, including non-dominant upper body motion speed and scale, and\ninactivity distribution, along with a 300-second window, are found most\neffective. However, individual-specific models are recommended as no universal\nset of optimal features and activities was identified across all participants.\n","authors":["Chen Long-fei","Muhammad Ahmed Raza","Craig Innes","Subramanian Ramamoorthy","Robert B. Fisher"],"pdf_url":"https://arxiv.org/pdf/2406.10045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10025v1","updated":"2024-06-14T13:36:30Z","published":"2024-06-14T13:36:30Z","title":"ProtoS-ViT: Visual foundation models for sparse self-explainable\n  classifications","summary":"  Prototypical networks aim to build intrinsically explainable models based on\nthe linear summation of concepts. However, important challenges remain in the\ntransparency, compactness, and meaningfulness of the explanations provided by\nthese models. This work demonstrates how frozen pre-trained ViT backbones can\nbe effectively turned into prototypical models for both general and\ndomain-specific tasks, in our case biomedical image classifiers. By leveraging\nstrong spatial features combined with a novel prototypical head, ProtoS-ViT\nsurpasses existing prototypical models showing strong performance in terms of\naccuracy, compactness, and explainability. Model explainability is evaluated\nthrough an extensive set of quantitative and qualitative metrics which serve as\na general benchmark for the development of prototypical models. Code is\navailable at https://github.com/hturbe/protosvit.\n","authors":["Hugues Turbé","Mina Bjelogrlic","Gianmarco Mengaldo","Christian Lovis"],"pdf_url":"https://arxiv.org/pdf/2406.10025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10019v1","updated":"2024-06-14T13:29:36Z","published":"2024-06-14T13:29:36Z","title":"Group and Shuffle: Efficient Structured Orthogonal Parametrization","summary":"  The increasing size of neural networks has led to a growing demand for\nmethods of efficient fine-tuning. Recently, an orthogonal fine-tuning paradigm\nwas introduced that uses orthogonal matrices for adapting the weights of a\npretrained model. In this paper, we introduce a new class of structured\nmatrices, which unifies and generalizes structured classes from previous works.\nWe examine properties of this class and build a structured orthogonal\nparametrization upon it. We then use this parametrization to modify the\northogonal fine-tuning framework, improving parameter and computational\nefficiency. We empirically validate our method on different domains, including\nadapting of text-to-image diffusion models and downstream task fine-tuning in\nlanguage modeling. Additionally, we adapt our construction for orthogonal\nconvolutions and conduct experiments with 1-Lipschitz neural networks.\n","authors":["Mikhail Gorbunov","Nikolay Yudin","Vera Soboleva","Aibek Alanov","Alexey Naumov","Maxim Rakhuba"],"pdf_url":"https://arxiv.org/pdf/2406.10019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10017v1","updated":"2024-06-14T13:27:56Z","published":"2024-06-14T13:27:56Z","title":"Tilt and Average : Geometric Adjustment of the Last Layer for\n  Recalibration","summary":"  After the revelation that neural networks tend to produce overconfident\npredictions, the problem of calibration, which aims to align confidence with\naccuracy to enhance the reliability of predictions, has gained significant\nimportance. Several solutions based on calibration maps have been proposed to\naddress the problem of recalibrating a trained classifier using additional\ndatasets. In this paper, we offer an algorithm that transforms the weights of\nthe last layer of the classifier, distinct from the calibration-map-based\napproach. We concentrate on the geometry of the final linear layer,\nspecifically its angular aspect, and adjust the weights of the corresponding\nlayer. We name the method Tilt and Average(\\textsc{Tna}), and validate the\ncalibration effect empirically and theoretically. Through this, we demonstrate\nthat our approach, in addition to the existing calibration-map-based\ntechniques, can yield improved calibration performance. Code available :\nhttps://github.com/GYYYYYUUUUU/TNA_Angular_Scaling.\n","authors":["Gyusang Cho","Chan-Hyun Youn"],"pdf_url":"https://arxiv.org/pdf/2406.10017v1.pdf","comment":"20 pages, 11 figures, to appear in International Conference on\n  Machine Learning (ICML2024)"},{"id":"http://arxiv.org/abs/2406.10007v1","updated":"2024-06-14T13:20:05Z","published":"2024-06-14T13:20:05Z","title":"Real-time, accurate, and open source upper-limb musculoskeletal analysis\n  using a single RGBD camera","summary":"  Biomechanical biofeedback may enhance rehabilitation and provide clinicians\nwith more objective task evaluation. These feedbacks often rely on expensive\nmotion capture systems, which restricts their widespread use, leading to the\ndevelopment of computer vision-based methods. These methods are subject to\nlarge joint angle errors, considering the upper limb, and exclude the scapula\nand clavicle motion in the analysis. Our open-source approach offers a\nuser-friendly solution for high-fidelity upper-limb kinematics using a single\nlow-cost RGBD camera and includes semi-automatic skin marker labeling.\nReal-time biomechanical analysis, ranging from kinematics to muscle force\nestimation, was conducted on eight participants performing a hand-cycling\nmotion to demonstrate the applicability of our approach on the upper limb.\nMarkers were recorded by the RGBD camera and an optoelectronic camera system,\nconsidered as a reference. Muscle activity and external load were recorded\nusing eight EMG and instrumented hand pedals, respectively. Bland-Altman\nanalysis revealed significant agreements in the 3D markers' positions between\nthe two motion capture methods, with errors averaging 3.3$\\pm$3.9 mm. For the\nbiomechanical analysis, the level of agreement was sensitive to whether the\nsame marker set was used. For example, joint angle differences averaging\n2.3$\\pm$2.8{\\deg} when using the same marker set, compared to 4.5$\\pm$2.9{\\deg}\notherwise. Biofeedback from the RGBD camera was provided at 63 Hz. Our study\nintroduces a novel method for using an RGBD camera as a low-cost motion capture\nsolution, emphasizing its potential for accurate kinematic reconstruction and\ncomprehensive upper-limb biomechanical studies.\n","authors":["Amedeo Ceglia","Kael Facon","Mickaël Begon","Lama Seoud"],"pdf_url":"https://arxiv.org/pdf/2406.10007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10000v1","updated":"2024-06-14T13:16:18Z","published":"2024-06-14T13:16:18Z","title":"OrientDream: Streamlining Text-to-3D Generation with Explicit\n  Orientation Control","summary":"  In the evolving landscape of text-to-3D technology, Dreamfusion has showcased\nits proficiency by utilizing Score Distillation Sampling (SDS) to optimize\nimplicit representations such as NeRF. This process is achieved through the\ndistillation of pretrained large-scale text-to-image diffusion models. However,\nDreamfusion encounters fidelity and efficiency constraints: it faces the\nmulti-head Janus issue and exhibits a relatively slow optimization process. To\ncircumvent these challenges, we introduce OrientDream, a camera orientation\nconditioned framework designed for efficient and multi-view consistent 3D\ngeneration from textual prompts. Our strategy emphasizes the implementation of\nan explicit camera orientation conditioned feature in the pre-training of a 2D\ntext-to-image diffusion module. This feature effectively utilizes data from\nMVImgNet, an extensive external multi-view dataset, to refine and bolster its\nfunctionality. Subsequently, we utilize the pre-conditioned 2D images as a\nbasis for optimizing a randomly initialized implicit representation (NeRF).\nThis process is significantly expedited by a decoupled back-propagation\ntechnique, allowing for multiple updates of implicit parameters per\noptimization cycle. Our experiments reveal that our method not only produces\nhigh-quality NeRF models with consistent multi-view properties but also\nachieves an optimization speed significantly greater than existing methods, as\nquantified by comparative metrics.\n","authors":["Yuzhong Huang","Zhong Li","Zhang Chen","Zhiyuan Ren","Guosheng Lin","Fred Morstatter","Yi Xu"],"pdf_url":"https://arxiv.org/pdf/2406.10000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04254v3","updated":"2024-06-14T12:58:05Z","published":"2024-06-06T17:00:10Z","title":"GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions","summary":"  We introduce a new generative approach for synthesizing 3D geometry and\nimages from single-view collections. Most existing approaches predict\nvolumetric density to render multi-view consistent images. By employing\nvolumetric rendering using neural radiance fields, they inherit a key\nlimitation: the generated geometry is noisy and unconstrained, limiting the\nquality and utility of the output meshes. To address this issue, we propose\nGeoGen, a new SDF-based 3D generative model trained in an end-to-end manner.\nInitially, we reinterpret the volumetric density as a Signed Distance Function\n(SDF). This allows us to introduce useful priors to generate valid meshes.\nHowever, those priors prevent the generative model from learning details,\nlimiting the applicability of the method to real-world scenarios. To alleviate\nthat problem, we make the transformation learnable and constrain the rendered\ndepth map to be consistent with the zero-level set of the SDF. Through the lens\nof adversarial training, we encourage the network to produce higher fidelity\ndetails on the output meshes. For evaluation, we introduce a synthetic dataset\nof human avatars captured from 360-degree camera angles, to overcome the\nchallenges presented by real-world datasets, which often lack 3D consistency\nand do not cover all camera angles. Our experiments on multiple datasets show\nthat GeoGen produces visually and quantitatively better geometry than the\nprevious generative models based on neural radiance fields.\n","authors":["Salvatore Esposito","Qingshan Xu","Kacper Kania","Charlie Hewitt","Octave Mariotti","Lohit Petikam","Julien Valentin","Arno Onken","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2406.04254v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09981v1","updated":"2024-06-14T12:44:04Z","published":"2024-06-14T12:44:04Z","title":"Challenges in explaining deep learning models for data with biological\n  variation","summary":"  Much machine learning research progress is based on developing models and\nevaluating them on a benchmark dataset (e.g., ImageNet for images). However,\napplying such benchmark-successful methods to real-world data often does not\nwork as expected. This is particularly the case for biological data where we\nexpect variability at multiple time and spatial scales. In this work, we are\nusing grain data and the goal is to detect diseases and damages. Pink fusarium,\nskinned grains, and other diseases and damages are key factors in setting the\nprice of grains or excluding dangerous grains from food production. Apart from\nchallenges stemming from differences of the data from the standard toy\ndatasets, we also present challenges that need to be overcome when explaining\ndeep learning models. For example, explainability methods have many\nhyperparameters that can give different results, and the ones published in the\npapers do not work on dissimilar images. Other challenges are more general:\nproblems with visualization of the explanations and their comparison since the\nmagnitudes of their values differ from method to method. An open fundamental\nquestion also is: How to evaluate explanations? It is a non-trivial task\nbecause the \"ground truth\" is usually missing or ill-defined. Also, human\nannotators may create what they think is an explanation of the task at hand,\nyet the machine learning model might solve it in a different and perhaps\ncounter-intuitive way. We discuss several of these challenges and evaluate\nvarious post-hoc explainability methods on grain data. We focus on robustness,\nquality of explanations, and similarity to particular \"ground truth\"\nannotations made by experts. The goal is to find the methods that overall\nperform well and could be used in this challenging task. We hope the proposed\npipeline will be used as a framework for evaluating explainability methods in\nspecific use cases.\n","authors":["Lenka Tětková","Erik Schou Dreier","Robin Malm","Lars Kai Hansen"],"pdf_url":"https://arxiv.org/pdf/2406.09981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09980v1","updated":"2024-06-14T12:43:16Z","published":"2024-06-14T12:43:16Z","title":"Deep Learning Models to Automate the Scoring of Hand Radiographs for\n  Rheumatoid Arthritis","summary":"  The van der Heijde modification of the Sharp (SvdH) score is a widely used\nradiographic scoring method to quantify damage in Rheumatoid Arthritis (RA) in\nclinical trials. However, its complexity with a necessity to score each\nindividual joint, and the expertise required limit its application in clinical\npractice, especially in disease progression measurement. In this work, we\naddressed this limitation by developing a bespoke, automated pipeline that is\ncapable of predicting the SvdH score and RA severity from hand radiographs\nwithout the need to localise the joints first. Using hand radiographs from RA\nand suspected RA patients, we first investigated the performance of the\nstate-of-the-art architectures in predicting the total SvdH score for hands and\nwrists and its corresponding severity class. Secondly, we leveraged publicly\navailable data sets to perform transfer learning with different finetuning\nschemes and ensemble learning, which resulted in substantial improvement in\nmodel performance being on par with an experienced human reader. The best model\nfor RA scoring achieved a Pearson's correlation coefficient (PCC) of 0.925 and\nroot mean squared error (RMSE) of 18.02, while the best model for RA severity\nclassification achieved an accuracy of 0.358 and PCC of 0.859. Our score\nprediction model attained almost comparable accuracy with experienced\nradiologists (PCC = 0.97, RMSE = 18.75). Finally, using Grad-CAM, we showed\nthat our models could focus on the anatomical structures in hands and wrists\nwhich clinicians deemed as relevant to RA progression in the majority of cases.\n","authors":["Zhiyan Bo","Laura C. Coates","Bartlomiej W. Papiez"],"pdf_url":"https://arxiv.org/pdf/2406.09980v1.pdf","comment":"16 pages, 5 figures, accepted by MIUA 2024"},{"id":"http://arxiv.org/abs/2406.09973v1","updated":"2024-06-14T12:31:48Z","published":"2024-06-14T12:31:48Z","title":"InstructRL4Pix: Training Diffusion for Image Editing by Reinforcement\n  Learning","summary":"  Instruction-based image editing has made a great process in using natural\nhuman language to manipulate the visual content of images. However, existing\nmodels are limited by the quality of the dataset and cannot accurately localize\nediting regions in images with complex object relationships. In this paper, we\npropose Reinforcement Learning Guided Image Editing Method(InstructRL4Pix) to\ntrain a diffusion model to generate images that are guided by the attention\nmaps of the target object. Our method maximizes the output of the reward model\nby calculating the distance between attention maps as a reward function and\nfine-tuning the diffusion model using proximal policy optimization (PPO). We\nevaluate our model in object insertion, removal, replacement, and\ntransformation. Experimental results show that InstructRL4Pix breaks through\nthe limitations of traditional datasets and uses unsupervised learning to\noptimize editing goals and achieve accurate image editing based on natural\nhuman commands.\n","authors":["Tiancheng Li","Jinxiu Liu","Huajun Chen","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2406.09973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06048v2","updated":"2024-06-14T12:29:19Z","published":"2024-06-10T06:29:00Z","title":"Robust Latent Representation Tuning for Image-text Classification","summary":"  Large models have demonstrated exceptional generalization capabilities in\ncomputer vision and natural language processing. Recent efforts have focused on\nenhancing these models with multimodal processing abilities. However,\naddressing the challenges posed by scenarios where one modality is absent\nremains a significant hurdle. In response to this issue, we propose a robust\nlatent representation tuning method for large models. Specifically, our\napproach introduces a modality latent translation module to maximize the\ncorrelation between modalities, resulting in a robust representation. Following\nthis, a newly designed fusion module is employed to facilitate information\ninteraction between the modalities. Within this framework, common semantics are\nrefined during training, and robust performance is achieved even in the absence\nof one modality. Importantly, our method maintains the frozen state of the\nimage and text foundation models to preserve their capabilities acquired\nthrough large-scale pretraining. We conduct experiments on several public\ndatasets, and the results underscore the effectiveness of our proposed method.\n","authors":["Hao Sun","Yu Song"],"pdf_url":"https://arxiv.org/pdf/2406.06048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06406v2","updated":"2024-06-14T12:15:15Z","published":"2024-04-09T15:54:03Z","title":"Emergent Dynamics in Neural Cellular Automata","summary":"  Neural Cellular Automata (NCA) models are trainable variations of traditional\nCellular Automata (CA). Emergent motion in the patterns created by NCA has been\nsuccessfully applied to synthesize dynamic textures. However, the conditions\nrequired for an NCA to display dynamic patterns remain unexplored. Here, we\ninvestigate the relationship between the NCA architecture and the emergent\ndynamics of the trained models. Specifically, we vary the number of channels in\nthe cell state and the number of hidden neurons in the MultiLayer Perceptron\n(MLP), and draw a relationship between the combination of these two variables\nand the motion strength between successive frames. Our analysis reveals that\nthe disparity and proportionality between these two variables have a strong\ncorrelation with the emergent dynamics in the NCA output. We thus propose a\ndesign principle for creating dynamic NCA.\n","authors":["Yitao Xu","Ehsan Pajouheshgar","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2404.06406v2.pdf","comment":"2 pages"},{"id":"http://arxiv.org/abs/2406.09961v1","updated":"2024-06-14T12:10:51Z","published":"2024-06-14T12:10:51Z","title":"ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via\n  Chart-to-Code Generation","summary":"  We introduce a new benchmark, ChartMimic, aimed at assessing the\nvisually-grounded code generation capabilities of large multimodal models\n(LMMs). ChartMimic utilizes information-intensive visual charts and textual\ninstructions as inputs, requiring LMMs to generate the corresponding code for\nchart rendering. ChartMimic includes 1,000 human-curated (figure, instruction,\ncode) triplets, which represent the authentic chart use cases found in\nscientific papers across various domains(e.g., Physics, Computer Science,\nEconomics, etc). These charts span 18 regular types and 4 advanced types,\ndiversifying into 191 subcategories. Furthermore, we propose multi-level\nevaluation metrics to provide an automatic and thorough assessment of the\noutput code and the rendered charts. Unlike existing code generation\nbenchmarks, ChartMimic places emphasis on evaluating LMMs' capacity to\nharmonize a blend of cognitive capabilities, encompassing visual understanding,\ncode generation, and cross-modal reasoning. The evaluation of 3 proprietary\nmodels and 11 open-weight models highlights the substantial challenges posed by\nChartMimic. Even the advanced GPT-4V, Claude-3-opus only achieve an average\nscore of 73.2 and 53.7, respectively, indicating significant room for\nimprovement. We anticipate that ChartMimic will inspire the development of\nLMMs, advancing the pursuit of artificial general intelligence.\n","authors":["Chufan Shi","Cheng Yang","Yaxin Liu","Bo Shui","Junjie Wang","Mohan Jing","Linran Xu","Xinyu Zhu","Siheng Li","Yuxiang Zhang","Gongye Liu","Xiaomei Nie","Deng Cai","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2406.09961v1.pdf","comment":"Data and code are available at\n  https://github.com/ChartMimic/ChartMimic"},{"id":"http://arxiv.org/abs/2406.09952v1","updated":"2024-06-14T11:58:49Z","published":"2024-06-14T11:58:49Z","title":"BiVLC: Extending Vision-Language Compositionality Evaluation with\n  Text-to-Image Retrieval","summary":"  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe\nare formulated as image-to-text retrieval problems, where, given an image, the\nmodels need to select between the correct textual description and a synthetic\nhard negative text. In this work we present the Bidirectional Vision-Language\nCompositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic\nhard negative image generated from the synthetic text, resulting in two\nimage-to-text retrieval examples (one for each image) and, more importantly,\ntwo text-to-image retrieval examples (one for each text). Human annotators\nfilter out ill-formed examples ensuring the validity of the benchmark. The\nexperiments on BiVLC uncover a weakness of current multimodal models, as they\nperform poorly in the text-to-image direction. In fact, when considering both\nretrieval directions, the conclusions obtained in previous works change\nsignificantly. In addition to the benchmark, we show that a contrastive model\ntrained using synthetic images and texts improves the state of the art in\nSugarCrepe and in BiVLC for both retrieval directions. The gap to human\nperformance in BiVLC confirms that Vision-Language Compositionality is still a\nchallenging problem. BiVLC and code are available at\nhttps://imirandam.github.io/BiVLC_project_page.\n","authors":["Imanol Miranda","Ander Salaberria","Eneko Agirre","Gorka Azkune"],"pdf_url":"https://arxiv.org/pdf/2406.09952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10434v2","updated":"2024-06-14T11:57:09Z","published":"2024-03-15T16:14:34Z","title":"Using an LLM to Turn Sign Spottings into Spoken Language Sentences","summary":"  Sign Language Translation (SLT) is a challenging task that aims to generate\nspoken language sentences from sign language videos. In this paper, we\nintroduce a hybrid SLT approach, Spotter+GPT, that utilizes a sign spotter and\na powerful Large Language Model (LLM) to improve SLT performance. Spotter+GPT\nbreaks down the SLT task into two stages. The videos are first processed by the\nSpotter, which is trained on a linguistic sign language dataset, to identify\nindividual signs. These spotted signs are then passed to an LLM, which\ntransforms them into coherent and contextually appropriate spoken language\nsentences. The source code of the Spotter is available at\nhttps://gitlab.surrey.ac.uk/cogvispublic/sign-spotter.\n","authors":["Ozge Mercanoglu Sincan","Necati Cihan Camgoz","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2403.10434v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06279v3","updated":"2024-06-14T11:48:51Z","published":"2024-04-09T13:02:33Z","title":"NoiseNCA: Noisy Seed Improves Spatio-Temporal Continuity of Neural\n  Cellular Automata","summary":"  Neural Cellular Automata (NCA) is a class of Cellular Automata where the\nupdate rule is parameterized by a neural network that can be trained using\ngradient descent. In this paper, we focus on NCA models used for texture\nsynthesis, where the update rule is inspired by partial differential equations\n(PDEs) describing reaction-diffusion systems. To train the NCA model, the\nspatio-temporal domain is discretized, and Euler integration is used to\nnumerically simulate the PDE. However, whether a trained NCA truly learns the\ncontinuous dynamic described by the corresponding PDE or merely overfits the\ndiscretization used in training remains an open question. We study NCA models\nat the limit where space-time discretization approaches continuity. We find\nthat existing NCA models tend to overfit the training discretization,\nespecially in the proximity of the initial condition, also called \"seed\". To\naddress this, we propose a solution that utilizes uniform noise as the initial\ncondition. We demonstrate the effectiveness of our approach in preserving the\nconsistency of NCA dynamics across a wide range of spatio-temporal\ngranularities. Our improved NCA model enables two new test-time interactions by\nallowing continuous control over the speed of pattern formation and the scale\nof the synthesized patterns. We demonstrate this new NCA feature in our\ninteractive online demo. Our work reveals that NCA models can learn continuous\ndynamics and opens new venues for NCA research from a dynamical system's\nperspective.\n","authors":["Ehsan Pajouheshgar","Yitao Xu","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2404.06279v3.pdf","comment":"9 pages, 12 figures"},{"id":"http://arxiv.org/abs/2406.09945v1","updated":"2024-06-14T11:46:48Z","published":"2024-06-14T11:46:48Z","title":"SemanticSpray++: A Multimodal Dataset for Autonomous Driving in Wet\n  Surface Conditions","summary":"  Autonomous vehicles rely on camera, LiDAR, and radar sensors to navigate the\nenvironment. Adverse weather conditions like snow, rain, and fog are known to\nbe problematic for both camera and LiDAR-based perception systems. Currently,\nit is difficult to evaluate the performance of these methods due to the lack of\npublicly available datasets containing multimodal labeled data. To address this\nlimitation, we propose the SemanticSpray++ dataset, which provides labels for\ncamera, LiDAR, and radar data of highway-like scenarios in wet surface\nconditions. In particular, we provide 2D bounding boxes for the camera image,\n3D bounding boxes for the LiDAR point cloud, and semantic labels for the radar\ntargets. By labeling all three sensor modalities, the SemanticSpray++ dataset\noffers a comprehensive test bed for analyzing the performance of different\nperception methods when vehicles travel on wet surface conditions. Together\nwith comprehensive label statistics, we also evaluate multiple baseline methods\nacross different tasks and analyze their performances. The dataset will be\navailable at https://semantic-spray-dataset.github.io .\n","authors":["Aldi Piroli","Vinzenz Dallabetta","Johannes Kopp","Marc Walessa","Daniel Meissner","Klaus Dietmayer"],"pdf_url":"https://arxiv.org/pdf/2406.09945v1.pdf","comment":"Accepted at IEEE Intelligent Vehicles Symposium (IV 2024)"},{"id":"http://arxiv.org/abs/2402.07417v2","updated":"2024-06-14T11:40:31Z","published":"2024-02-12T05:44:10Z","title":"An Empirical Study Into What Matters for Calibrating Vision-Language\n  Models","summary":"  Vision-Language Models (VLMs) have emerged as the dominant approach for\nzero-shot recognition, adept at handling diverse scenarios and significant\ndistribution changes. However, their deployment in risk-sensitive areas\nrequires a deeper understanding of their uncertainty estimation capabilities, a\nrelatively uncharted area. In this study, we explore the calibration properties\nof VLMs across different architectures, datasets, and training strategies. In\nparticular, we analyze the uncertainty estimation performance of VLMs when\ncalibrated in one domain, label set or hierarchy level, and tested in a\ndifferent one. Our findings reveal that while VLMs are not inherently\ncalibrated for uncertainty, temperature scaling significantly and consistently\nimproves calibration, even across shifts in distribution and changes in label\nset. Moreover, VLMs can be calibrated with a very small set of examples.\nThrough detailed experimentation, we highlight the potential applications and\nimportance of our insights, aiming for more reliable and effective use of VLMs\nin critical, real-world scenarios.\n","authors":["Weijie Tu","Weijian Deng","Dylan Campbell","Stephen Gould","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2402.07417v2.pdf","comment":"ICML 2024 Camera Ready"},{"id":"http://arxiv.org/abs/2309.06807v2","updated":"2024-06-14T11:39:01Z","published":"2023-09-13T08:54:22Z","title":"Bayesian uncertainty-weighted loss for improved generalisability on\n  polyp segmentation task","summary":"  While several previous studies have devised methods for segmentation of\npolyps, most of these methods are not rigorously assessed on multi-center\ndatasets. Variability due to appearance of polyps from one center to another,\ndifference in endoscopic instrument grades, and acquisition quality result in\nmethods with good performance on in-distribution test data, and poor\nperformance on out-of-distribution or underrepresented samples. Unfair models\nhave serious implications and pose a critical challenge to clinical\napplications. We adapt an implicit bias mitigation method which leverages\nBayesian predictive uncertainties during training to encourage the model to\nfocus on underrepresented sample regions. We demonstrate the potential of this\napproach to improve generalisability without sacrificing state-of-the-art\nperformance on a challenging multi-center polyp segmentation dataset (PolypGen)\nwith different centers and image modalities.\n","authors":["Rebecca S. Stone","Pedro E. Chavarrias-Solano","Andrew J. Bulpitt","David C. Hogg","Sharib Ali"],"pdf_url":"https://arxiv.org/pdf/2309.06807v2.pdf","comment":"To be presented at the Fairness of AI in Medical Imaging (FAIMI)\n  MICCAI 2023 Workshop and published in volumes of the Springer Lecture Notes\n  Computer Science (LNCS) series"},{"id":"http://arxiv.org/abs/2406.09936v1","updated":"2024-06-14T11:31:21Z","published":"2024-06-14T11:31:21Z","title":"ALGM: Adaptive Local-then-Global Token Merging for Efficient Semantic\n  Segmentation with Plain Vision Transformers","summary":"  This work presents Adaptive Local-then-Global Merging (ALGM), a token\nreduction method for semantic segmentation networks that use plain Vision\nTransformers. ALGM merges tokens in two stages: (1) In the first network layer,\nit merges similar tokens within a small local window and (2) halfway through\nthe network, it merges similar tokens across the entire image. This is\nmotivated by an analysis in which we found that, in those situations, tokens\nwith a high cosine similarity can likely be merged without a drop in\nsegmentation quality. With extensive experiments across multiple datasets and\nnetwork configurations, we show that ALGM not only significantly improves the\nthroughput by up to 100%, but can also enhance the mean IoU by up to +1.1,\nthereby achieving a better trade-off between segmentation quality and\nefficiency than existing methods. Moreover, our approach is adaptive during\ninference, meaning that the same model can be used for optimal efficiency or\naccuracy, depending on the application. Code is available at\nhttps://tue-mps.github.io/ALGM.\n","authors":["Narges Norouzi","Svetlana Orlova","Daan de Geus","Gijs Dubbelman"],"pdf_url":"https://arxiv.org/pdf/2406.09936v1.pdf","comment":"CVPR 2024. Project page and code: https://tue-mps.github.io/ALGM"},{"id":"http://arxiv.org/abs/2406.09931v1","updated":"2024-06-14T11:25:53Z","published":"2024-06-14T11:25:53Z","title":"SCKansformer: Fine-Grained Classification of Bone Marrow Cells via\n  Kansformer Backbone and Hierarchical Attention Mechanisms","summary":"  The incidence and mortality rates of malignant tumors, such as acute\nleukemia, have risen significantly. Clinically, hospitals rely on cytological\nexamination of peripheral blood and bone marrow smears to diagnose malignant\ntumors, with accurate blood cell counting being crucial. Existing automated\nmethods face challenges such as low feature expression capability, poor\ninterpretability, and redundant feature extraction when processing\nhigh-dimensional microimage data. We propose a novel fine-grained\nclassification model, SCKansformer, for bone marrow blood cells, which\naddresses these challenges and enhances classification accuracy and efficiency.\nThe model integrates the Kansformer Encoder, SCConv Encoder, and Global-Local\nAttention Encoder. The Kansformer Encoder replaces the traditional MLP layer\nwith the KAN, improving nonlinear feature representation and interpretability.\nThe SCConv Encoder, with its Spatial and Channel Reconstruction Units, enhances\nfeature representation and reduces redundancy. The Global-Local Attention\nEncoder combines Multi-head Self-Attention with a Local Part module to capture\nboth global and local features. We validated our model using the Bone Marrow\nBlood Cell Fine-Grained Classification Dataset (BMCD-FGCD), comprising over\n10,000 samples and nearly 40 classifications, developed with a partner\nhospital. Comparative experiments on our private dataset, as well as the\npublicly available PBC and ALL-IDB datasets, demonstrate that SCKansformer\noutperforms both typical and advanced microcell classification methods across\nall datasets. Our source code and private BMCD-FGCD dataset are available at\nhttps://github.com/JustlfC03/SCKansformer.\n","authors":["Yifei Chen","Zhu Zhu","Shenghao Zhu","Linwei Qiu","Binfeng Zou","Fan Jia","Yunpeng Zhu","Chenyan Zhang","Zhaojie Fang","Feiwei Qin","Jin Fan","Changmiao Wang","Yu Gao","Gang Yu"],"pdf_url":"https://arxiv.org/pdf/2406.09931v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.13261v3","updated":"2024-06-14T11:04:12Z","published":"2023-11-22T09:25:08Z","title":"Immunohistochemistry guided segmentation of benign epithelial cells, in\n  situ lesions, and invasive epithelial cells in breast cancer slides","summary":"  Digital pathology enables automatic analysis of histopathological sections\nusing artificial intelligence (AI). Automatic evaluation could improve\ndiagnostic efficiency and help find associations between morphological features\nand clinical outcome. For development of such prediction models, identifying\ninvasive epithelial cells, and separating these from benign epithelial cells\nand in situ lesions would be the first step. In this study, we aimed to develop\nan AI model for segmentation of epithelial cells in sections from breast\ncancer. We generated epithelial ground truth masks by restaining hematoxylin\nand eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists'\nannotations. HE/CK image pairs were used to train a convolutional neural\nnetwork, and data augmentation was used to make the model more robust. Tissue\nmicroarrays (TMAs) from 839 patients, and whole slide images from two patients\nwere used for training and evaluation of the models. The sections were derived\nfrom four cohorts of breast cancer patients. TMAs from 21 patients from a fifth\ncohort was used as a second test set. In quantitative evaluation, a mean Dice\nscore of 0.70, 0.79, and 0.75 for invasive epithelial cells, benign epithelial\ncells, and in situ lesions, respectively, were achieved. In qualitative scoring\n(0-5) by pathologists, results were best for all epithelium and invasive\nepithelium, with scores of 4.7 and 4.4. Scores for benign epithelium and in\nsitu lesions were 3.7 and 2.0. The proposed model segmented epithelial cells in\nHE stained breast cancer slides well, but further work is needed for accurate\ndivision between the classes. Immunohistochemistry, together with pathologists'\nannotations, enabled the creation of accurate ground truths. The model is made\nfreely available in FastPathology and the code is available at\nhttps://github.com/AICAN-Research/breast-epithelium-segmentation\n","authors":["Maren Høibø","André Pedersen","Vibeke Grotnes Dale","Sissel Marie Berget","Borgny Ytterhus","Cecilia Lindskog","Elisabeth Wik","Lars A. Akslen","Ingerid Reinertsen","Erik Smistad","Marit Valla"],"pdf_url":"https://arxiv.org/pdf/2311.13261v3.pdf","comment":"19 pages, 6 figures. Submitted to a scientific journal"},{"id":"http://arxiv.org/abs/2406.09914v1","updated":"2024-06-14T10:48:17Z","published":"2024-06-14T10:48:17Z","title":"Robust compressive tracking via online weighted multiple instance\n  learning","summary":"  Developing a robust object tracker is a challenging task due to factors such\nas occlusion, motion blur, fast motion, illumination variations, rotation,\nbackground clutter, low resolution and deformation across the frames. In the\nliterature, lots of good approaches based on sparse representation have already\nbeen presented to tackle the above problems. However, most of the algorithms do\nnot focus on the learning of sparse representation. They only consider the\nmodeling of target appearance and therefore drift away from the target with the\nimprecise training samples. By considering all the above factors in mind, we\nhave proposed a visual object tracking algorithm by integrating a\ncoarse-to-fine search strategy based on sparse representation and the weighted\nmultiple instance learning (WMIL) algorithm. Compared with the other trackers,\nour approach has more information of the original signal with less complexity\ndue to the coarse-to-fine search method, and also has weights for important\nsamples. Thus, it can easily discriminate the background features from the\nforeground. Furthermore, we have also selected the samples from the un-occluded\nsub-regions to efficiently develop the strong classifier. As a consequence, a\nstable and robust object tracker is achieved to tackle all the aforementioned\nproblems. Experimental results with quantitative as well as qualitative\nanalysis on challenging benchmark datasets show the accuracy and efficiency of\nour method.\n","authors":["Sandeep Singh Sengar"],"pdf_url":"https://arxiv.org/pdf/2406.09914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09913v1","updated":"2024-06-14T10:47:52Z","published":"2024-06-14T10:47:52Z","title":"OpenECAD: An Efficient Visual Language Model for Computer-Aided Design","summary":"  Computer-aided design (CAD) tools are utilized in the manufacturing industry\nfor modeling everything from cups to spacecraft. These programs are complex to\nuse and typically require years of training and experience to master.\nStructured and well-constrained 2D sketches and 3D constructions are crucial\ncomponents of CAD modeling. A well-executed CAD model can be seamlessly\nintegrated into the manufacturing process, thereby enhancing production\nefficiency. Deep generative models of 3D shapes and 3D object reconstruction\nmodels has garnered significant research interest. However, most of these\nmodels are represented in discrete forms. Moreover, the few models based on CAD\noperations often have substantial input restrictions. In this work, we\nfine-tuned pre-trained models to create OpenECAD (0.55B, 0.89B, and 4.2B),\nleveraging the visual, logical, coding, and general capabilities of visual\nlanguage models. OpenECAD can process images of 3D designs as input and\ngenerate highly structured 2D sketches and 3D construction commands. These\noutputs can be directly used with existing CAD tools' APIs to generate project\nfiles. To train our network, we created a new CAD dataset. This dataset is\nbased on existing public CAD datasets, with adjustments and augmentations to\nmeet the requirements of ~VLM training.\n","authors":["Zhe Yuan","Jianqi Shi"],"pdf_url":"https://arxiv.org/pdf/2406.09913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07248v3","updated":"2024-06-14T10:46:54Z","published":"2023-10-11T07:25:50Z","title":"IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via\n  Improved Box-dice and Contrastive Latent-anchors","summary":"  Box-supervised polyp segmentation attracts increasing attention for its\ncost-effective potential. Existing solutions often rely on learning-free\nmethods or pretrained models to laboriously generate pseudo masks, triggering\nDice constraint subsequently. In this paper, we found that a model guided by\nthe simplest box-filled masks can accurately predict polyp locations/sizes, but\nsuffers from shape collapsing. In response, we propose two innovative learning\nfashions, Improved Box-dice (IBox) and Contrastive Latent-Anchors (CLA), and\ncombine them to train a robust box-supervised model IBoxCLA. The core idea\nbehind IBoxCLA is to decouple the learning of location/size and shape, allowing\nfor focused constraints on each of them. Specifically, IBox transforms the\nsegmentation map into a proxy map using shape decoupling and confusion-region\nswapping sequentially. Within the proxy map, shapes are disentangled, while\nlocations/sizes are encoded as box-like responses. By constraining the proxy\nmap instead of the raw prediction, the box-filled mask can well supervise\nIBoxCLA without misleading its shape learning. Furthermore, CLA contributes to\nshape learning by generating two types of latent anchors, which are learned and\nupdated using momentum and segmented polyps to steadily represent polyp and\nbackground features. The latent anchors facilitate IBoxCLA to capture\ndiscriminative features within and outside boxes in a contrastive manner,\nyielding clearer boundaries. We benchmark IBoxCLA on five public polyp\ndatasets. The experimental results demonstrate the competitive performance of\nIBoxCLA compared to recent fully-supervised polyp segmentation methods, and its\nsuperiority over other box-supervised state-of-the-arts with a relative\nincrease of overall mDice and mIoU by at least 6.5% and 7.5%, respectively.\n","authors":["Zhiwei Wang","Qiang Hu","Hongkuan Shi","Li He","Man He","Wenxuan Dai","Ting Li","Yitong Zhang","Dun Li","Mei Liu","Qiang Li"],"pdf_url":"https://arxiv.org/pdf/2310.07248v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09908v1","updated":"2024-06-14T10:36:26Z","published":"2024-06-14T10:36:26Z","title":"What Does Softmax Probability Tell Us about Classifiers Ranking Across\n  Diverse Test Conditions?","summary":"  This work aims to develop a measure that can accurately rank the performance\nof various classifiers when they are tested on unlabeled data from\nout-of-distribution (OOD) distributions. We commence by demonstrating that\nconventional uncertainty metrics, notably the maximum Softmax prediction\nprobability, possess inherent utility in forecasting model generalization\nacross certain OOD contexts. Building on this insight, we introduce a new\nmeasure called Softmax Correlation (SoftmaxCorr). It calculates the cosine\nsimilarity between a class-class correlation matrix, constructed from Softmax\noutput vectors across an unlabeled test dataset, and a predefined reference\nmatrix that embodies ideal class correlations. A high resemblance of\npredictions to the reference matrix signals that the model delivers confident\nand uniform predictions across all categories, reflecting minimal uncertainty\nand confusion. Through rigorous evaluation across a suite of datasets,\nincluding ImageNet, CIFAR-10, and WILDS, we affirm the predictive validity of\nSoftmaxCorr in accurately forecasting model performance within both\nin-distribution (ID) and OOD settings. Furthermore, we discuss the limitations\nof our proposed measure and suggest avenues for future research.\n","authors":["Weijie Tu","Weijian Deng","Liang Zheng","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2406.09908v1.pdf","comment":"TMLR 2024 (https://openreview.net/forum?id=vtiDUgGjyx)"},{"id":"http://arxiv.org/abs/2406.09906v1","updated":"2024-06-14T10:29:00Z","published":"2024-06-14T10:29:00Z","title":"Label-Efficient Semantic Segmentation of LiDAR Point Clouds in Adverse\n  Weather Conditions","summary":"  Adverse weather conditions can severely affect the performance of LiDAR\nsensors by introducing unwanted noise in the measurements. Therefore,\ndifferentiating between noise and valid points is crucial for the reliable use\nof these sensors. Current approaches for detecting adverse weather points\nrequire large amounts of labeled data, which can be difficult and expensive to\nobtain. This paper proposes a label-efficient approach to segment LiDAR point\nclouds in adverse weather. We develop a framework that uses few-shot semantic\nsegmentation to learn to segment adverse weather points from only a few labeled\nexamples. Then, we use a semi-supervised learning approach to generate\npseudo-labels for unlabelled point clouds, significantly increasing the amount\nof training data without requiring any additional labeling. We also integrate\ngood weather data in our training pipeline, allowing for high performance in\nboth good and adverse weather conditions. Results on real and synthetic\ndatasets show that our method performs well in detecting snow, fog, and spray.\nFurthermore, we achieve competitive performance against fully supervised\nmethods while using only a fraction of labeled data.\n","authors":["Aldi Piroli","Vinzenz Dallabetta","Johannes Kopp","Marc Walessa","Daniel Meissner","Klaus Dietmayer"],"pdf_url":"https://arxiv.org/pdf/2406.09906v1.pdf","comment":"Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)"},{"id":"http://arxiv.org/abs/2406.09905v1","updated":"2024-06-14T10:23:53Z","published":"2024-06-14T10:23:53Z","title":"Nymeria: A Massive Collection of Multimodal Egocentric Daily Motion in\n  the Wild","summary":"  We introduce Nymeria - a large-scale, diverse, richly annotated human motion\ndataset collected in the wild with multiple multimodal egocentric devices. The\ndataset comes with a) full-body 3D motion ground truth; b) egocentric\nmultimodal recordings from Project Aria devices with RGB, grayscale,\neye-tracking cameras, IMUs, magnetometer, barometer, and microphones; and c) an\nadditional \"observer\" device providing a third-person viewpoint. We compute\nworld-aligned 6DoF transformations for all sensors, across devices and capture\nsessions. The dataset also provides 3D scene point clouds and calibrated gaze\nestimation. We derive a protocol to annotate hierarchical language descriptions\nof in-context human motion, from fine-grain pose narrations, to atomic actions\nand activity summarization. To the best of our knowledge, the Nymeria dataset\nis the world largest in-the-wild collection of human motion with natural and\ndiverse activities; first of its kind to provide synchronized and localized\nmulti-device multimodal egocentric data; and the world largest dataset with\nmotion-language descriptions. It contains 1200 recordings of 300 hours of daily\nactivities from 264 participants across 50 locations, travelling a total of\n399Km. The motion-language descriptions provide 310.5K sentences in 8.64M words\nfrom a vocabulary size of 6545. To demonstrate the potential of the dataset we\ndefine key research tasks for egocentric body tracking, motion synthesis, and\naction recognition and evaluate several state-of-the-art baseline algorithms.\nData and code will be open-sourced.\n","authors":["Lingni Ma","Yuting Ye","Fangzhou Hong","Vladimir Guzov","Yifeng Jiang","Rowan Postyeni","Luis Pesqueira","Alexander Gamino","Vijay Baiyya","Hyo Jin Kim","Kevin Bailey","David Soriano Fosas","C. Karen Liu","Ziwei Liu","Jakob Engel","Renzo De Nardi","Richard Newcombe"],"pdf_url":"https://arxiv.org/pdf/2406.09905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09896v1","updated":"2024-06-14T10:13:37Z","published":"2024-06-14T10:13:37Z","title":"Exploring the Benefits of Vision Foundation Models for Unsupervised\n  Domain Adaptation","summary":"  Achieving robust generalization across diverse data domains remains a\nsignificant challenge in computer vision. This challenge is important in\nsafety-critical applications, where deep-neural-network-based systems must\nperform reliably under various environmental conditions not seen during\ntraining. Our study investigates whether the generalization capabilities of\nVision Foundation Models (VFMs) and Unsupervised Domain Adaptation (UDA)\nmethods for the semantic segmentation task are complementary. Results show that\ncombining VFMs with UDA has two main benefits: (a) it allows for better UDA\nperformance while maintaining the out-of-distribution performance of VFMs, and\n(b) it makes certain time-consuming UDA components redundant, thus enabling\nsignificant inference speedups. Specifically, with equivalent model sizes, the\nresulting VFM-UDA method achieves an 8.4$\\times$ speed increase over the prior\nnon-VFM state of the art, while also improving performance by +1.2 mIoU in the\nUDA setting and by +6.1 mIoU in terms of out-of-distribution generalization.\nMoreover, when we use a VFM with 3.6$\\times$ more parameters, the VFM-UDA\napproach maintains a 3.3$\\times$ speed up, while improving the UDA performance\nby +3.1 mIoU and the out-of-distribution performance by +10.3 mIoU. These\nresults underscore the significant benefits of combining VFMs with UDA, setting\nnew standards and baselines for Unsupervised Domain Adaptation in semantic\nsegmentation.\n","authors":["Brunó B. Englert","Fabrizio J. Piva","Tommie Kerssies","Daan de Geus","Gijs Dubbelman"],"pdf_url":"https://arxiv.org/pdf/2406.09896v1.pdf","comment":"CVPR 2024 Workshop Proceedings for the Second Workshop on Foundation\n  Models"},{"id":"http://arxiv.org/abs/2402.18409v3","updated":"2024-06-14T09:35:57Z","published":"2024-02-28T15:28:36Z","title":"A Cognitive Evaluation Benchmark of Image Reasoning and Description for\n  Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs), despite their recent success, are\nhardly comprehensively tested for their cognitive abilities. Inspired by the\nprevalent use of the \"Cookie Theft\" task in human cognition test, we propose a\nnovel evaluation benchmark to evaluate high-level cognitive ability of LVLMs\nusing images with rich semantics. It defines eight reasoning capabilities and\nconsists of an image description task and a visual question answering task. Our\nevaluation on well-known LVLMs shows that there is still a large gap in\ncognitive ability between LVLMs and humans.\n","authors":["Xiujie Song","Mengyue Wu","Kenny Q. Zhu","Chunhao Zhang","Yanyi Chen"],"pdf_url":"https://arxiv.org/pdf/2402.18409v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09867v1","updated":"2024-06-14T09:27:56Z","published":"2024-06-14T09:27:56Z","title":"Rethinking the Evaluation of Out-of-Distribution Detection: A Sorites\n  Paradox","summary":"  Most existing out-of-distribution (OOD) detection benchmarks classify samples\nwith novel labels as the OOD data. However, some marginal OOD samples actually\nhave close semantic contents to the in-distribution (ID) sample, which makes\ndetermining the OOD sample a Sorites Paradox. In this paper, we construct a\nbenchmark named Incremental Shift OOD (IS-OOD) to address the issue, in which\nwe divide the test samples into subsets with different semantic and covariate\nshift degrees relative to the ID dataset. The data division is achieved through\na shift measuring method based on our proposed Language Aligned Image feature\nDecomposition (LAID). Moreover, we construct a Synthetic Incremental Shift\n(Syn-IS) dataset that contains high-quality generated images with more diverse\ncovariate contents to complement the IS-OOD benchmark. We evaluate current OOD\ndetection methods on our benchmark and find several important insights: (1) The\nperformance of most OOD detection methods significantly improves as the\nsemantic shift increases; (2) Some methods like GradNorm may have different OOD\ndetection mechanisms as they rely less on semantic shifts to make decisions;\n(3) Excessive covariate shifts in the image are also likely to be considered as\nOOD for some methods. Our code and data are released in\nhttps://github.com/qqwsad5/IS-OOD.\n","authors":["Xingming Long","Jie Zhang","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09867v1.pdf","comment":"v1"},{"id":"http://arxiv.org/abs/2406.09864v1","updated":"2024-06-14T09:22:07Z","published":"2024-06-14T09:22:07Z","title":"LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data","summary":"  Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We introduce LUMA, a unique benchmark\ndataset, featuring audio, image, and textual data from 50 classes, for learning\nfrom uncertain and multimodal data. It extends the well-known CIFAR 10/100\ndataset with audio samples extracted from three audio corpora, and text data\ngenerated using the Gemma-7B Large Language Model (LLM). The LUMA dataset\nenables the controlled injection of varying types and degrees of uncertainty to\nachieve and tailor specific experiments and benchmarking initiatives. LUMA is\nalso available as a Python package including the functions for generating\nmultiple variants of the dataset with controlling the diversity of the data,\nthe amount of noise for each modality, and adding out-of-distribution samples.\nA baseline pre-trained model is also provided alongside three uncertainty\nquantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable\nConflictive Multi-View Learning. This comprehensive dataset and its tools are\nintended to promote and support the development and benchmarking of trustworthy\nand robust multimodal deep learning approaches.\n","authors":["Grigor Bezirganyan","Sana Sellami","Laure Berti-Équille","Sébastien Fournier"],"pdf_url":"https://arxiv.org/pdf/2406.09864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09860v1","updated":"2024-06-14T09:20:44Z","published":"2024-06-14T09:20:44Z","title":"Dataset Condensation with Latent Quantile Matching","summary":"  Dataset condensation (DC) methods aim to learn a smaller synthesized dataset\nwith informative data records to accelerate the training of machine learning\nmodels. Current distribution matching (DM) based DC methods learn a synthesized\ndataset by matching the mean of the latent embeddings between the synthetic and\nthe real dataset. However two distributions with the same mean can still be\nvastly different. In this work we demonstrate the shortcomings of using Maximum\nMean Discrepancy to match latent distributions i.e. the weak matching power and\nlack of outlier regularization. To alleviate these shortcomings we propose our\nnew method: Latent Quantile Matching (LQM) which matches the quantiles of the\nlatent embeddings to minimize the goodness of fit test statistic between two\ndistributions. Empirical experiments on both image and graph-structured\ndatasets show that LQM matches or outperforms previous state of the art in\ndistribution matching based DC. Moreover we show that LQM improves the\nperformance in continual graph learning (CGL) setting where memory efficiency\nand privacy can be important. Our work sheds light on the application of DM\nbased DC for CGL.\n","authors":["Wei Wei","Tom De Schepper","Kevin Mets"],"pdf_url":"https://arxiv.org/pdf/2406.09860v1.pdf","comment":"Accepted by CVPR Workshop 2024: 1st Workshop on Dataset Distillation\n  for Computer Vision"},{"id":"http://arxiv.org/abs/2406.09858v1","updated":"2024-06-14T09:18:28Z","published":"2024-06-14T09:18:28Z","title":"Vision Language Modeling of Content, Distortion and Appearance for Image\n  Quality Assessment","summary":"  The visual quality of an image is confounded by a number of intertwined\nfactors including its semantic content, distortion characteristics and\nappearance properties such as brightness, contrast, sharpness, and\ncolourfulness. Distilling high level knowledge about all these quality bearing\nattributes is crucial for developing objective Image Quality Assessment\n(IQA).While existing solutions have modeled some of these aspects, a\ncomprehensive solution that involves all these important quality related\nattributes has not yet been developed. In this paper, we present a new blind\nIQA (BIQA) model termed Self-supervision and Vision-Language supervision Image\nQUality Evaluator (SLIQUE) that features a joint vision-language and visual\ncontrastive representation learning framework for acquiring high level\nknowledge about the images semantic contents, distortion characteristics and\nappearance properties for IQA. For training SLIQUE, we have developed a\nsystematic approach to constructing a first of its kind large image database\nannotated with all three categories of quality relevant texts. The Text\nAnnotated Distortion, Appearance and Content (TADAC) database has over 1.6\nmillion images annotated with textual descriptions of their semantic contents,\ndistortion characteristics and appearance properties. The method for\nconstructing TADAC and the database itself will be particularly useful for\nexploiting vision-language modeling for advanced IQA applications. Extensive\nexperimental results show that SLIQUE has superior performances over state of\nthe art, demonstrating the soundness of its design principle and the\neffectiveness of its implementation.\n","authors":["Fei Zhou","Zhicong Huang","Tianhao Gu","Guoping Qiu"],"pdf_url":"https://arxiv.org/pdf/2406.09858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16098v7","updated":"2024-06-14T09:14:05Z","published":"2022-11-29T11:17:34Z","title":"Three-stage binarization of color document images based on discrete\n  wavelet transform and generative adversarial networks","summary":"  The efficient segmentation of text information from the background in\ndegraded color document images is an important challenge in the preservation of\nancient manuscripts. The imperfect preservation of ancient manuscripts has led\nto various types of degradation over time, such as staining, yellowing, and ink\nseepage, badly affecting document image binarization results. This work\nproposes a three-stage method to generate binarization image results for\ndegraded colour document images using generative adversarial networks (GANs).\nStage-1 involves applying discrete wavelet transform and retaining the low-low\nsubband images for document image enhancement. In Stage-2, the original input\nimage is split into red, green, and blue (RGB) three single-channel images and\none grayscale image, and each image is trained with independent GANs to extract\ncolor foreground information. In Stage-3, the output images of Stage-2 and the\nresized input images are used to train independent GANs to generate document\nbinarization results, enabling the combination of global and local features.\nThe experimental results show that the Avg-Score of the proposed method is\n77.64, 77.95, 79.05, 76.38, 75.34, and 77.00 on the (H)-DIBCO 2011, 2013, 2014,\n2016, 2017, and 2018 datasets, which achieves the state-of-the-art level. The\nimplementation code for this work is available at\nhttps://github.com/abcpp12383/ThreeStageBinarization.\n","authors":["Rui-Yang Ju","Yu-Shian Lin","Yanlin Jin","Chih-Chia Chen","Chun-Tse Chien","Jen-Shiun Chiang"],"pdf_url":"https://arxiv.org/pdf/2211.16098v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08394v2","updated":"2024-06-14T09:00:00Z","published":"2024-06-12T16:44:50Z","title":"VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model\n  for Hundreds of Vision-Language Tasks","summary":"  We present VisionLLM v2, an end-to-end generalist multimodal large model\n(MLLM) that unifies visual perception, understanding, and generation within a\nsingle framework. Unlike traditional MLLMs limited to text output, VisionLLM v2\nsignificantly broadens its application scope. It excels not only in\nconventional visual question answering (VQA) but also in open-ended,\ncross-domain vision tasks such as object localization, pose estimation, and\nimage generation and editing. To this end, we propose a new information\ntransmission mechanism termed \"super link\", as a medium to connect MLLM with\ntask-specific decoders. It not only allows flexible transmission of task\ninformation and gradient feedback between the MLLM and multiple downstream\ndecoders but also effectively resolves training conflicts in multi-tasking\nscenarios. In addition, to support the diverse range of tasks, we carefully\ncollected and combed training data from hundreds of public vision and\nvision-language tasks. In this way, our model can be joint-trained end-to-end\non hundreds of vision language tasks and generalize to these tasks using a set\nof shared parameters through different user prompts, achieving performance\ncomparable to task-specific models. We believe VisionLLM v2 will offer a new\nperspective on the generalization of MLLMs.\n","authors":["Jiannan Wu","Muyan Zhong","Sen Xing","Zeqiang Lai","Zhaoyang Liu","Wenhai Wang","Zhe Chen","Xizhou Zhu","Lewei Lu","Tong Lu","Ping Luo","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2406.08394v2.pdf","comment":"43 pages"},{"id":"http://arxiv.org/abs/2406.09850v1","updated":"2024-06-14T08:58:28Z","published":"2024-06-14T08:58:28Z","title":"GradeADreamer: Enhanced Text-to-3D Generation Using Gaussian Splatting\n  and Multi-View Diffusion","summary":"  Text-to-3D generation has shown promising results, yet common challenges such\nas the Multi-face Janus problem and extended generation time for high-quality\nassets. In this paper, we address these issues by introducing a novel\nthree-stage training pipeline called GradeADreamer. This pipeline is capable of\nproducing high-quality assets with a total generation time of under 30 minutes\nusing only a single RTX 3090 GPU. Our proposed method employs a Multi-view\nDiffusion Model, MVDream, to generate Gaussian Splats as a prior, followed by\nrefining geometry and texture using StableDiffusion. Experimental results\ndemonstrate that our approach significantly mitigates the Multi-face Janus\nproblem and achieves the highest average user preference ranking compared to\nprevious state-of-the-art methods. The project code is available at\nhttps://github.com/trapoom555/GradeADreamer.\n","authors":["Trapoom Ukarapol","Kevin Pruvost"],"pdf_url":"https://arxiv.org/pdf/2406.09850v1.pdf","comment":"Code: https://github.com/trapoom555/GradeADreamer"},{"id":"http://arxiv.org/abs/2406.09838v1","updated":"2024-06-14T08:46:44Z","published":"2024-06-14T08:46:44Z","title":"Vision-Language Models Meet Meteorology: Developing Models for Extreme\n  Weather Events Detection with Heatmaps","summary":"  Real-time detection and prediction of extreme weather protect human lives and\ninfrastructure. Traditional methods rely on numerical threshold setting and\nmanual interpretation of weather heatmaps with Geographic Information Systems\n(GIS), which can be slow and error-prone. Our research redefines Extreme\nWeather Events Detection (EWED) by framing it as a Visual Question Answering\n(VQA) problem, thereby introducing a more precise and automated solution.\nLeveraging Vision-Language Models (VLM) to simultaneously process visual and\ntextual data, we offer an effective aid to enhance the analysis process of\nweather heatmaps. Our initial assessment of general-purpose VLMs (e.g.,\nGPT-4-Vision) on EWED revealed poor performance, characterized by low accuracy\nand frequent hallucinations due to inadequate color differentiation and\ninsufficient meteorological knowledge. To address these challenges, we\nintroduce ClimateIQA, the first meteorological VQA dataset, which includes\n8,760 wind gust heatmaps and 254,040 question-answer pairs covering four\nquestion types, both generated from the latest climate reanalysis data. We also\npropose Sparse Position and Outline Tracking (SPOT), an innovative technique\nthat leverages OpenCV and K-Means clustering to capture and depict color\ncontours in heatmaps, providing ClimateIQA with more accurate color spatial\nlocation information. Finally, we present Climate-Zoo, the first meteorological\nVLM collection, which adapts VLMs to meteorological applications using the\nClimateIQA dataset. Experiment results demonstrate that models from Climate-Zoo\nsubstantially outperform state-of-the-art general VLMs, achieving an accuracy\nincrease from 0% to over 90% in EWED verification. The datasets and models in\nthis study are publicly available for future climate science research:\nhttps://github.com/AlexJJJChen/Climate-Zoo.\n","authors":["Jian Chen","Peilin Zhou","Yining Hua","Dading Chong","Meng Cao","Yaowei Li","Zixuan Yuan","Bing Zhu","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2406.09838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09829v1","updated":"2024-06-14T08:34:20Z","published":"2024-06-14T08:34:20Z","title":"Open-Vocabulary Semantic Segmentation with Image Embedding Balancing","summary":"  Open-vocabulary semantic segmentation is a challenging task, which requires\nthe model to output semantic masks of an image beyond a close-set vocabulary.\nAlthough many efforts have been made to utilize powerful CLIP models to\naccomplish this task, they are still easily overfitting to training classes due\nto the natural gaps in semantic information between training and new classes.\nTo overcome this challenge, we propose a novel framework for openvocabulary\nsemantic segmentation called EBSeg, incorporating an Adaptively Balanced\nDecoder (AdaB Decoder) and a Semantic Structure Consistency loss (SSC Loss).\nThe AdaB Decoder is designed to generate different image embeddings for both\ntraining and new classes. Subsequently, these two types of embeddings are\nadaptively balanced to fully exploit their ability to recognize training\nclasses and generalization ability for new classes. To learn a consistent\nsemantic structure from CLIP, the SSC Loss aligns the inter-classes affinity in\nthe image feature space with that in the text feature space of CLIP, thereby\nimproving the generalization ability of our model. Furthermore, we employ a\nfrozen SAM image encoder to complement the spatial information that CLIP\nfeatures lack due to the low training image resolution and image-level\nsupervision inherent in CLIP. Extensive experiments conducted across various\nbenchmarks demonstrate that the proposed EBSeg outperforms the state-of-the-art\nmethods. Our code and trained models will be here:\nhttps://github.com/slonetime/EBSeg.\n","authors":["Xiangheng Shan","Dongyue Wu","Guilin Zhu","Yuanjie Shao","Nong Sang","Changxin Gao"],"pdf_url":"https://arxiv.org/pdf/2406.09829v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2406.09143v2","updated":"2024-06-14T08:33:11Z","published":"2024-06-13T14:11:19Z","title":"Generative AI-based Prompt Evolution Engineering Design Optimization\n  With Vision-Language Model","summary":"  Engineering design optimization requires an efficient combination of a 3D\nshape representation, an optimization algorithm, and a design performance\nevaluation method, which is often computationally expensive. We present a\nprompt evolution design optimization (PEDO) framework contextualized in a\nvehicle design scenario that leverages a vision-language model for penalizing\nimpractical car designs synthesized by a generative model. The backbone of our\nframework is an evolutionary strategy coupled with an optimization objective\nfunction that comprises a physics-based solver and a vision-language model for\npractical or functional guidance in the generated car designs. In the prompt\nevolutionary search, the optimizer iteratively generates a population of text\nprompts, which embed user specifications on the aerodynamic performance and\nvisual preferences of the 3D car designs. Then, in addition to the\ncomputational fluid dynamics simulations, the pre-trained vision-language model\nis used to penalize impractical designs and, thus, foster the evolutionary\nalgorithm to seek more viable designs. Our investigations on a car design\noptimization problem show a wide spread of potential car designs generated at\nthe early phase of the search, which indicates a good diversity of designs in\nthe initial populations, and an increase of over 20\\% in the probability of\ngenerating practical designs compared to a baseline framework without using a\nvision-language model. Visual inspection of the designs against the performance\nresults demonstrates prompt evolution as a very promising paradigm for finding\nnovel designs with good optimization performance while providing ease of use in\nspecifying design specifications and preferences via a natural language\ninterface.\n","authors":["Melvin Wong","Thiago Rios","Stefan Menzel","Yew Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2406.09143v2.pdf","comment":"Accepted and to be published in IEEE Congress on Evolutionary\n  Computation (CEC) 2024. Copyright 2024 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses"},{"id":"http://arxiv.org/abs/2406.09827v1","updated":"2024-06-14T08:32:45Z","published":"2024-06-14T08:32:45Z","title":"HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical\n  Attention Pruning","summary":"  In modern large language models (LLMs), increasing sequence lengths is a\ncrucial challenge for enhancing their comprehension and coherence in handling\ncomplex tasks such as multi-modal question answering. However, handling long\ncontext sequences with LLMs is prohibitively costly due to the conventional\nattention mechanism's quadratic time and space complexity, and the context\nwindow size is limited by the GPU memory. Although recent works have proposed\nlinear and sparse attention mechanisms to address this issue, their real-world\napplicability is often limited by the need to re-train pre-trained models. In\nresponse, we propose a novel approach, Hierarchically Pruned Attention (HiP),\nwhich simultaneously reduces the training and inference time complexity from\n$O(T^2)$ to $O(T \\log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To\nthis end, we devise a dynamic sparse attention mechanism that generates an\nattention mask through a novel tree-search-like algorithm for a given query on\nthe fly. HiP is training-free as it only utilizes the pre-trained attention\nscores to spot the positions of the top-$k$ most significant elements for each\nquery. Moreover, it ensures that no token is overlooked, unlike the sliding\nwindow-based sub-quadratic attention methods, such as StreamingLLM. Extensive\nexperiments on diverse real-world benchmarks demonstrate that HiP significantly\nreduces prompt (i.e., prefill) and decoding latency and memory usage while\nmaintaining high generation performance with little or no degradation. As HiP\nallows pretrained LLMs to scale to millions of tokens on commodity GPUs with no\nadditional engineering due to its easy plug-and-play deployment, we believe\nthat our work will have a large practical impact, opening up the possibility to\nmany long-context LLM applications previously infeasible.\n","authors":["Heejun Lee","Geon Park","Youngwan Lee","Jina Kim","Wonyoung Jeong","Myeongjae Jeon","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2406.09827v1.pdf","comment":"26 pages, 15 figures"},{"id":"http://arxiv.org/abs/2406.09822v1","updated":"2024-06-14T08:24:52Z","published":"2024-06-14T08:24:52Z","title":"An I2I Inpainting Approach for Efficient Channel Knowledge Map\n  Construction","summary":"  Channel knowledge map (CKM) has received widespread attention as an emerging\nenabling technology for environment-aware wireless communications. It involves\nthe construction of databases containing location-specific channel knowledge,\nwhich are then leveraged to facilitate channel state information (CSI)\nacquisition and transceiver design. In this context, a fundamental challenge\nlies in efficiently constructing the CKM based on a given wireless propagation\nenvironment. Most existing methods are based on stochastic modeling and\nsequence prediction, which do not fully exploit the inherent physical\ncharacteristics of the propagation environment, resulting in low accuracy and\nhigh computational complexity. To address these limitations, we propose a\nLaplacian pyramid (LP)-based CKM construction scheme to predict the channel\nknowledge at arbitrary locations in a targeted area. Specifically, we first\nview the channel knowledge as a 2-D image and transform the CKM construction\nproblem into an image-to-image (I2I) inpainting task, which predicts the\nchannel knowledge at a specific location by recovering the corresponding pixel\nvalue in the image matrix. Then, inspired by the reversible and closed-form\nstructure of the LP, we show its natural suitability for our task in designing\na fast I2I mapping network. For different frequency components of LP\ndecomposition, we design tailored networks accordingly. Besides, to encode the\nglobal structural information of the propagation environment, we introduce\nself-attention and cross-covariance attention mechanisms in different layers,\nrespectively. Finally, experimental results show that the proposed scheme\noutperforms the benchmark, achieving higher reconstruction accuracy while with\nlower computational complexity. Moreover, the proposed approach has a strong\ngeneralization ability and can be implemented in different wireless\ncommunication scenarios.\n","authors":["Zhenzhou Jin","Li You","Jue Wang","Xiang-Gen Xia","Xiqi Gao"],"pdf_url":"https://arxiv.org/pdf/2406.09822v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.08528v2","updated":"2024-06-14T08:19:28Z","published":"2024-06-12T08:51:08Z","title":"Adaptive Teaching with Shared Classifier for Knowledge Distillation","summary":"  Knowledge distillation (KD) is a technique used to transfer knowledge from an\noverparameterized teacher network to a less-parameterized student network,\nthereby minimizing the incurred performance loss. KD methods can be categorized\ninto offline and online approaches. Offline KD leverages a powerful pretrained\nteacher network, while online KD allows the teacher network to be adjusted\ndynamically to enhance the learning effectiveness of the student network.\nRecently, it has been discovered that sharing the classifier of the teacher\nnetwork can significantly boost the performance of the student network with\nonly a minimal increase in the number of network parameters. Building on these\ninsights, we propose adaptive teaching with a shared classifier (ATSC). In\nATSC, the pretrained teacher network self-adjusts to better align with the\nlearning needs of the student network based on its capabilities, and the\nstudent network benefits from the shared classifier, enhancing its performance.\nAdditionally, we extend ATSC to environments with multiple teachers. We conduct\nextensive experiments, demonstrating the effectiveness of the proposed KD\nmethod. Our approach achieves state-of-the-art results on the CIFAR-100 and\nImageNet datasets in both single-teacher and multiteacher scenarios, with only\na modest increase in the number of required model parameters. The source code\nis publicly available at https://github.com/random2314235/ATSC.\n","authors":["Jaeyeon Jang","Young-Ik Kim","Jisu Lim","Hyeonseong Lee"],"pdf_url":"https://arxiv.org/pdf/2406.08528v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11564v2","updated":"2024-06-14T07:58:35Z","published":"2023-01-27T07:00:54Z","title":"Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance\n  Grounding","summary":"  Robotic grasping is a fundamental ability for a robot to interact with the\nenvironment. Current methods focus on how to obtain a stable and reliable\ngrasping pose in object level, while little work has been studied on part\n(shape)-wise grasping which is related to fine-grained grasping and robotic\naffordance. Parts can be seen as atomic elements to compose an object, which\ncontains rich semantic knowledge and a strong correlation with affordance.\nHowever, lacking a large part-wise 3D robotic dataset limits the development of\npart representation learning and downstream applications. In this paper, we\npropose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to\npromote 3D part-level affordance and grasping ability learning. From the\nperspective of robotic cognition, we design a two-stage fine-grained robotic\ngrasping framework (named LangPartGPD), including a novel 3D part language\ngrounding model and a part-aware grasp pose detection model, in which explicit\nlanguage input from human or large language models (LLMs) could guide a robot\nto generate part-level 6-DoF grasping pose with textual explanation. Our method\ncombines the advantages of human-robot collaboration and LLMs' planning ability\nusing explicit language as a symbolic intermediate. To evaluate the\neffectiveness of our proposed method, we perform 3D part grounding and\nfine-grained grasp detection experiments on both simulation and physical robot\nsettings, following language instructions across different degrees of textual\ncomplexity. Results show our method achieves competitive performance in 3D\ngeometry fine-grained grounding, object affordance inference, and 3D part-aware\ngrasping tasks. Our dataset and code are available on our project website\nhttps://sites.google.com/view/lang-shape\n","authors":["Yaoxian Song","Penglei Sun","Piaopiao Jin","Yi Ren","Yu Zheng","Zhixu Li","Xiaowen Chu","Yue Zhang","Tiefeng Li","Jason Gu"],"pdf_url":"https://arxiv.org/pdf/2301.11564v2.pdf","comment":"14 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.09801v1","updated":"2024-06-14T07:54:25Z","published":"2024-06-14T07:54:25Z","title":"RaNeuS: Ray-adaptive Neural Surface Reconstruction","summary":"  Our objective is to leverage a differentiable radiance field \\eg NeRF to\nreconstruct detailed 3D surfaces in addition to producing the standard novel\nview renderings. There have been related methods that perform such tasks,\nusually by utilizing a signed distance field (SDF). However, the\nstate-of-the-art approaches still fail to correctly reconstruct the small-scale\ndetails, such as the leaves, ropes, and textile surfaces. Considering that\ndifferent methods formulate and optimize the projection from SDF to radiance\nfield with a globally constant Eikonal regularization, we improve with a\nray-wise weighting factor to prioritize the rendering and zero-crossing surface\nfitting on top of establishing a perfect SDF. We propose to adaptively adjust\nthe regularization on the signed distance field so that unsatisfying rendering\nrays won't enforce strong Eikonal regularization which is ineffective, and\nallow the gradients from regions with well-learned radiance to effectively\nback-propagated to the SDF. Consequently, balancing the two objectives in order\nto generate accurate and detailed surfaces. Additionally, concerning whether\nthere is a geometric bias between the zero-crossing surface in SDF and\nrendering points in the radiance field, the projection becomes adjustable as\nwell depending on different 3D locations during optimization. Our proposed\n\\textit{RaNeuS} are extensively evaluated on both synthetic and real datasets,\nachieving state-of-the-art results on both novel view synthesis and geometric\nreconstruction.\n","authors":["Yida Wang","David Joseph Tan","Nassir Navab","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2406.09801v1.pdf","comment":"3DV 2024, oral. In: Proceedings of the IEEE/CVF International\n  Conference on 3D Vision (2023)"},{"id":"http://arxiv.org/abs/2406.09798v1","updated":"2024-06-14T07:50:09Z","published":"2024-06-14T07:50:09Z","title":"Sim-to-Real Transfer via 3D Feature Fields for Vision-and-Language\n  Navigation","summary":"  Vision-and-language navigation (VLN) enables the agent to navigate to a\nremote location in 3D environments following the natural language instruction.\nIn this field, the agent is usually trained and evaluated in the navigation\nsimulators, lacking effective approaches for sim-to-real transfer. The VLN\nagents with only a monocular camera exhibit extremely limited performance,\nwhile the mainstream VLN models trained with panoramic observation, perform\nbetter but are difficult to deploy on most monocular robots. For this case, we\npropose a sim-to-real transfer approach to endow the monocular robots with\npanoramic traversability perception and panoramic semantic understanding, thus\nsmoothly transferring the high-performance panoramic VLN models to the common\nmonocular robots. In this work, the semantic traversable map is proposed to\npredict agent-centric navigable waypoints, and the novel view representations\nof these navigable waypoints are predicted through the 3D feature fields. These\nmethods broaden the limited field of view of the monocular robots and\nsignificantly improve navigation performance in the real world. Our VLN system\noutperforms previous SOTA monocular VLN methods in R2R-CE and RxR-CE benchmarks\nwithin the simulation environments and is also validated in real-world\nenvironments, providing a practical and high-performance solution for\nreal-world VLN.\n","authors":["Zihan Wang","Xiangyang Li","Jiahao Yang"," Yeqi","Shuqiang Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.09798v1.pdf","comment":"Submitted to CoRL 2024. The code is available at\n  https://github.com/MrZihan/Sim2Real-VLN-3DFF"},{"id":"http://arxiv.org/abs/2406.09794v1","updated":"2024-06-14T07:43:23Z","published":"2024-06-14T07:43:23Z","title":"SuperSVG: Superpixel-based Scalable Vector Graphics Synthesis","summary":"  SVG (Scalable Vector Graphics) is a widely used graphics format that\npossesses excellent scalability and editability. Image vectorization, which\naims to convert raster images to SVGs, is an important yet challenging problem\nin computer vision and graphics. Existing image vectorization methods either\nsuffer from low reconstruction accuracy for complex images or require long\ncomputation time. To address this issue, we propose SuperSVG, a\nsuperpixel-based vectorization model that achieves fast and high-precision\nimage vectorization. Specifically, we decompose the input image into\nsuperpixels to help the model focus on areas with similar colors and textures.\nThen, we propose a two-stage self-training framework, where a coarse-stage\nmodel is employed to reconstruct the main structure and a refinement-stage\nmodel is used for enriching the details. Moreover, we propose a novel dynamic\npath warping loss to help the refinement-stage model to inherit knowledge from\nthe coarse-stage model. Extensive qualitative and quantitative experiments\ndemonstrate the superior performance of our method in terms of reconstruction\naccuracy and inference time compared to state-of-the-art approaches. The code\nis available in \\url{https://github.com/sjtuplayer/SuperSVG}.\n","authors":["Teng Hu","Ran Yi","Baihong Qian","Jiangning Zhang","Paul L. Rosin","Yu-Kun Lai"],"pdf_url":"https://arxiv.org/pdf/2406.09794v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.09792v1","updated":"2024-06-14T07:42:27Z","published":"2024-06-14T07:42:27Z","title":"A Two-Stage Masked Autoencoder Based Network for Indoor Depth Completion","summary":"  Depth images have a wide range of applications, such as 3D reconstruction,\nautonomous driving, augmented reality, robot navigation, and scene\nunderstanding. Commodity-grade depth cameras are hard to sense depth for\nbright, glossy, transparent, and distant surfaces. Although existing depth\ncompletion methods have achieved remarkable progress, their performance is\nlimited when applied to complex indoor scenarios. To address these problems, we\npropose a two-step Transformer-based network for indoor depth completion.\nUnlike existing depth completion approaches, we adopt a self-supervision\npre-training encoder based on the masked autoencoder to learn an effective\nlatent representation for the missing depth value; then we propose a decoder\nbased on a token fusion mechanism to complete (i.e., reconstruct) the full\ndepth from the jointly RGB and incomplete depth image. Compared to the existing\nmethods, our proposed network, achieves the state-of-the-art performance on the\nMatterport3D dataset. In addition, to validate the importance of the depth\ncompletion task, we apply our methods to indoor 3D reconstruction. The code,\ndataset, and demo are available at\nhttps://github.com/kailaisun/Indoor-Depth-Completion.\n","authors":["Kailai Sun","Zhou Yang","Qianchuan Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.09792v1.pdf","comment":"Accepted by 2024 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshop"},{"id":"http://arxiv.org/abs/2406.09788v1","updated":"2024-06-14T07:37:28Z","published":"2024-06-14T07:37:28Z","title":"OpenCapBench: A Benchmark to Bridge Pose Estimation and Biomechanics","summary":"  Pose estimation has promised to impact healthcare by enabling more practical\nmethods to quantify nuances of human movement and biomechanics. However,\ndespite the inherent connection between pose estimation and biomechanics, these\ndisciplines have largely remained disparate. For example, most current pose\nestimation benchmarks use metrics such as Mean Per Joint Position Error,\nPercentage of Correct Keypoints, or mean Average Precision to assess\nperformance, without quantifying kinematic and physiological correctness - key\naspects for biomechanics. To alleviate this challenge, we develop OpenCapBench\nto offer an easy-to-use unified benchmark to assess common tasks in human pose\nestimation, evaluated under physiological constraints. OpenCapBench computes\nconsistent kinematic metrics through joints angles provided by an open-source\nmusculoskeletal modeling software (OpenSim). Through OpenCapBench, we\ndemonstrate that current pose estimation models use keypoints that are too\nsparse for accurate biomechanics analysis. To mitigate this challenge, we\nintroduce SynthPose, a new approach that enables finetuning of pre-trained 2D\nhuman pose models to predict an arbitrarily denser set of keypoints for\naccurate kinematic analysis through the use of synthetic data. Incorporating\nsuch finetuning on synthetic data of prior models leads to twofold reduced\njoint angle errors. Moreover, OpenCapBench allows users to benchmark their own\ndeveloped models on our clinically relevant cohort. Overall, OpenCapBench\nbridges the computer vision and biomechanics communities, aiming to drive\nsimultaneous advances in both areas.\n","authors":["Yoni Gozlan","Antoine Falisse","Scott Uhlrich","Anthony Gatti","Michael Black","Akshay Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2406.09788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09782v1","updated":"2024-06-14T07:31:20Z","published":"2024-06-14T07:31:20Z","title":"Unsupervised Monocular Depth Estimation Based on Hierarchical\n  Feature-Guided Diffusion","summary":"  Unsupervised monocular depth estimation has received widespread attention\nbecause of its capability to train without ground truth. In real-world\nscenarios, the images may be blurry or noisy due to the influence of weather\nconditions and inherent limitations of the camera. Therefore, it is\nparticularly important to develop a robust depth estimation model. Benefiting\nfrom the training strategies of generative networks, generative-based methods\noften exhibit enhanced robustness. In light of this, we employ a\nwell-converging diffusion model among generative networks for unsupervised\nmonocular depth estimation. Additionally, we propose a hierarchical\nfeature-guided denoising module. This model significantly enriches the model's\ncapacity for learning and interpreting depth distribution by fully leveraging\nimage features to guide the denoising process. Furthermore, we explore the\nimplicit depth within reprojection and design an implicit depth consistency\nloss. This loss function serves to enhance the performance of the model and\nensure the scale consistency of depth within a video sequence. We conduct\nexperiments on the KITTI, Make3D, and our self-collected SIMIT datasets. The\nresults indicate that our approach stands out among generative-based models,\nwhile also showcasing remarkable robustness.\n","authors":["Runze Liu","Dongchen Zhu","Guanghui Zhang","Yue Xu","Wenjun Shi","Xiaolin Zhang","Lei Wang","Jiamao Li"],"pdf_url":"https://arxiv.org/pdf/2406.09782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09781v1","updated":"2024-06-14T07:30:26Z","published":"2024-06-14T07:30:26Z","title":"GPT-4o: Visual perception performance of multimodal large language\n  models in piglet activity understanding","summary":"  Animal ethology is an crucial aspect of animal research, and animal behavior\nlabeling is the foundation for studying animal behavior. This process typically\ninvolves labeling video clips with behavioral semantic tags, a task that is\ncomplex, subjective, and multimodal. With the rapid development of multimodal\nlarge language models(LLMs), new application have emerged for animal behavior\nunderstanding tasks in livestock scenarios. This study evaluates the visual\nperception capabilities of multimodal LLMs in animal activity recognition. To\nachieve this, we created piglet test data comprising close-up video clips of\nindividual piglets and annotated full-shot video clips. These data were used to\nassess the performance of four multimodal LLMs-Video-LLaMA, MiniGPT4-Video,\nVideo-Chat2, and GPT-4 omni (GPT-4o)-in piglet activity understanding. Through\ncomprehensive evaluation across five dimensions, including counting, actor\nreferring, semantic correspondence, time perception, and robustness, we found\nthat while current multimodal LLMs require improvement in semantic\ncorrespondence and time perception, they have initially demonstrated visual\nperception capabilities for animal activity recognition. Notably, GPT-4o showed\noutstanding performance, with Video-Chat2 and GPT-4o exhibiting significantly\nbetter semantic correspondence and time perception in close-up video clips\ncompared to full-shot clips. The initial evaluation experiments in this study\nvalidate the potential of multimodal large language models in livestock scene\nvideo understanding and provide new directions and references for future\nresearch on animal behavior video understanding. Furthermore, by deeply\nexploring the influence of visual prompts on multimodal large language models,\nwe expect to enhance the accuracy and efficiency of animal behavior recognition\nin livestock scenarios through human visual processing methods.\n","authors":["Yiqi Wu","Xiaodan Hu","Ziming Fu","Siling Zhou","Jiangong Li"],"pdf_url":"https://arxiv.org/pdf/2406.09781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09779v1","updated":"2024-06-14T07:28:02Z","published":"2024-06-14T07:28:02Z","title":"OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst","summary":"  Memes, which rapidly disseminate personal opinions and positions across the\ninternet, also pose significant challenges in propagating social bias and\nprejudice. This study presents a novel approach to detecting harmful memes,\nparticularly within the multicultural and multilingual context of Singapore.\nOur methodology integrates image captioning, Optical Character Recognition\n(OCR), and Large Language Model (LLM) analysis to comprehensively understand\nand classify harmful memes. Utilizing the BLIP model for image captioning,\nPP-OCR and TrOCR for text recognition across multiple languages, and the Qwen\nLLM for nuanced language understanding, our system is capable of identifying\nharmful content in memes created in English, Chinese, Malay, and Tamil. To\nenhance the system's performance, we fine-tuned our approach by leveraging\nadditional data labeled using GPT-4V, aiming to distill the understanding\ncapability of GPT-4V for harmful memes to our system. Our framework achieves\ntop-1 at the public leaderboard of the Online Safety Prize Challenge hosted by\nAI Singapore, with the AUROC as 0.7749 and accuracy as 0.7087, significantly\nahead of the other teams. Notably, our approach outperforms previous\nbenchmarks, with FLAVA achieving an AUROC of 0.5695 and VisualBERT an AUROC of\n0.5561.\n","authors":["Jingtao Cao","Zheng Zhang","Hongru Wang","Bin Liang","Hao Wang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2406.09779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09774v1","updated":"2024-06-14T07:20:49Z","published":"2024-06-14T07:20:49Z","title":"A lightweight residual network for unsupervised deformable image\n  registration","summary":"  Accurate volumetric image registration is highly relevant for clinical\nroutines and computer-aided medical diagnosis. Recently, researchers have begun\nto use transformers in learning-based methods for medical image registration,\nand have achieved remarkable success. Due to the strong global modeling\ncapability, Transformers are considered a better option than convolutional\nneural networks (CNNs) for registration. However, they use bulky models with\nhuge parameter sets, which require high computation edge devices for deployment\nas portable devices or in hospitals. Transformers also need a large amount of\ntraining data to produce significant results, and it is often challenging to\ncollect suitable annotated data. Although existing CNN-based image registration\ncan offer rich local information, their global modeling capability is poor for\nhandling long-distance information interaction and limits registration\nperformance. In this work, we propose a CNN-based registration method with an\nenhanced receptive field, a low number of parameters, and significant results\non a limited training dataset. For this, we propose a residual U-Net with\nembedded parallel dilated-convolutional blocks to enhance the receptive field.\nThe proposed method is evaluated on inter-patient and atlas-based datasets. We\nshow that the performance of the proposed method is comparable and slightly\nbetter than transformer-based methods by using only $\\SI{1.5}{\\percent}$ of its\nnumber of parameters.\n","authors":["Ahsan Raza Siyal","Astrid Ellen Grams","Markus Haltmeier"],"pdf_url":"https://arxiv.org/pdf/2406.09774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09773v1","updated":"2024-06-14T07:18:54Z","published":"2024-06-14T07:18:54Z","title":"Research on Edge Detection of LiDAR Images Based on Artificial\n  Intelligence Technology","summary":"  With the widespread application of Light Detection and Ranging (LiDAR)\ntechnology in fields such as autonomous driving, robot navigation, and terrain\nmapping, the importance of edge detection in LiDAR images has become\nincreasingly prominent. Traditional edge detection methods often face\nchallenges in accuracy and computational complexity when processing LiDAR\nimages. To address these issues, this study proposes an edge detection method\nfor LiDAR images based on artificial intelligence technology. This paper first\nreviews the current state of research on LiDAR technology and image edge\ndetection, introducing common edge detection algorithms and their applications\nin LiDAR image processing. Subsequently, a deep learning-based edge detection\nmodel is designed and implemented, optimizing the model training process\nthrough preprocessing and enhancement of the LiDAR image dataset. Experimental\nresults indicate that the proposed method outperforms traditional methods in\nterms of detection accuracy and computational efficiency, showing significant\npractical application value. Finally, improvement strategies are proposed for\nthe current method's shortcomings, and the improvements are validated through\nexperiments.\n","authors":["Haowei Yang","Liyang Wang","Jingyu Zhang","Yu Cheng","Ao Xiang"],"pdf_url":"https://arxiv.org/pdf/2406.09773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09768v1","updated":"2024-06-14T07:13:03Z","published":"2024-06-14T07:13:03Z","title":"Bayesian Conditioned Diffusion Models for Inverse Problems","summary":"  Diffusion models have recently been shown to excel in many image\nreconstruction tasks that involve inverse problems based on a forward\nmeasurement operator. A common framework uses task-agnostic unconditional\nmodels that are later post-conditioned for reconstruction, an approach that\ntypically suffers from suboptimal task performance. While task-specific\nconditional models have also been proposed, current methods heuristically\ninject measured data as a naive input channel that elicits sampling\ninaccuracies. Here, we address the optimal conditioning of diffusion models for\nsolving challenging inverse problems that arise during image reconstruction.\nSpecifically, we propose a novel Bayesian conditioning technique for diffusion\nmodels, BCDM, based on score-functions associated with the conditional\ndistribution of desired images given measured data. We rigorously derive the\ntheory to express and train the conditional score-function. Finally, we show\nstate-of-the-art performance in image dealiasing, deblurring, super-resolution,\nand inpainting with the proposed technique.\n","authors":["Alper Güngör","Bahri Batuhan Bilecen","Tolga Çukur"],"pdf_url":"https://arxiv.org/pdf/2406.09768v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2403.07362v2","updated":"2024-06-14T07:03:36Z","published":"2024-03-12T06:50:32Z","title":"Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine\n  Unlearning","summary":"  The trustworthy machine learning (ML) community is increasingly recognizing\nthe crucial need for models capable of selectively 'unlearning' data points\nafter training. This leads to the problem of machine unlearning (MU), aiming to\neliminate the influence of chosen data points on model performance, while still\nmaintaining the model's utility post-unlearning. Despite various MU methods for\ndata influence erasure, evaluations have largely focused on random data\nforgetting, ignoring the vital inquiry into which subset should be chosen to\ntruly gauge the authenticity of unlearning performance. To tackle this issue,\nwe introduce a new evaluative angle for MU from an adversarial viewpoint. We\npropose identifying the data subset that presents the most significant\nchallenge for influence erasure, i.e., pinpointing the worst-case forget set.\nUtilizing a bi-level optimization principle, we amplify unlearning challenges\nat the upper optimization level to emulate worst-case scenarios, while\nsimultaneously engaging in standard training and unlearning at the lower level,\nachieving a balance between data influence erasure and model utility. Our\nproposal offers a worst-case evaluation of MU's resilience and effectiveness.\nThrough extensive experiments across different datasets (including CIFAR-10,\n100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image\nclassifiers and generative models), we expose critical pros and cons in\nexisting (approximate) unlearning strategies. Our results illuminate the\ncomplex challenges of MU in practice, guiding the future development of more\naccurate and robust unlearning algorithms. The code is available at\nhttps://github.com/OPTML-Group/Unlearn-WorstCase.\n","authors":["Chongyu Fan","Jiancheng Liu","Alfred Hero","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07362v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13282v2","updated":"2024-06-14T07:00:30Z","published":"2024-03-20T03:47:53Z","title":"AdaViPro: Region-based Adaptive Visual Prompt for Large-Scale Models\n  Adapting","summary":"  Recently, prompt-based methods have emerged as a new alternative\n`parameter-efficient fine-tuning' paradigm, which only fine-tunes a small\nnumber of additional parameters while keeping the original model frozen.\nHowever, despite achieving notable results, existing prompt methods mainly\nfocus on `what to add', while overlooking the equally important aspect of\n`where to add', typically relying on the manually crafted placement. To this\nend, we propose a region-based Adaptive Visual Prompt, named AdaViPro, which\nintegrates the `where to add' optimization of the prompt into the learning\nprocess. Specifically, we reconceptualize the `where to add' optimization as a\nproblem of regional decision-making. During inference, AdaViPro generates a\nregionalized mask map for the whole image, which is composed of 0 and 1, to\ndesignate whether to apply or discard the prompt in each specific area.\nTherefore, we employ Gumbel-Softmax sampling to enable AdaViPro's end-to-end\nlearning through standard back-propagation. Extensive experiments demonstrate\nthat our AdaViPro yields new efficiency and accuracy trade-offs for adapting\npre-trained models.\n","authors":["Mengyu Yang","Ye Tian","Lanshan Zhang","Xiao Liang","Xuming Ran","Wendong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13282v2.pdf","comment":"Accepted by ICIP 2024"},{"id":"http://arxiv.org/abs/2406.09762v1","updated":"2024-06-14T06:59:54Z","published":"2024-06-14T06:59:54Z","title":"Full-reference Point Cloud Quality Assessment Using Spectral Graph\n  Wavelets","summary":"  Point clouds in 3D applications frequently experience quality degradation\nduring processing, e.g., scanning and compression. Reliable point cloud quality\nassessment (PCQA) is important for developing compression algorithms with good\nbitrate-quality trade-offs and techniques for quality improvement (e.g.,\ndenoising). This paper introduces a full-reference (FR) PCQA method utilizing\nspectral graph wavelets (SGWs). First, we propose novel SGW-based PCQA metrics\nthat compare SGW coefficients of coordinate and color signals between reference\nand distorted point clouds. Second, we achieve accurate PCQA by integrating\nseveral conventional FR metrics and our SGW-based metrics using support vector\nregression. To our knowledge, this is the first study to introduce SGWs for\nPCQA. Experimental results demonstrate the proposed PCQA metric is more\naccurately correlated with subjective quality scores compared to conventional\nPCQA metrics.\n","authors":["Ryosuke Watanabe","Keisuke Nonaka","Eduardo Pavez","Tatsuya Kobayashi","Antonio Ortega"],"pdf_url":"https://arxiv.org/pdf/2406.09762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09761v1","updated":"2024-06-14T06:59:37Z","published":"2024-06-14T06:59:37Z","title":"Towards Full Integration of Artificial Intelligence in Colon Capsule\n  Endoscopy's Pathway","summary":"  Despite recent surge of interest in deploying colon capsule endoscopy (CCE)\nfor early diagnosis of colorectal diseases, there remains a large gap between\nthe current state of CCE in clinical practice, and the state of its counterpart\noptical colonoscopy (OC). Our study is aimed at closing this gap, by focusing\non the full integration of AI in CCE's pathway, where image processing steps\nlinked to the detection, localization and characterisation of important\nfindings are carried out autonomously using various AI algorithms. We developed\na recognition network, that with an impressive sensitivity of 99.9%, a\nspecificity of 99.4%, and a negative predictive value (NPV) of 99.8%, detected\ncolorectal polyps. After recognising a polyp within a sequence of images, only\nthose images containing polyps were fed into two parallel independent networks\nfor characterisation, and estimation of the size of those important findings.\nThe characterisation network reached a sensitivity of 82% and a specificity of\n80% in classifying polyps to two groups, namely neoplastic vs. non-neoplastic.\nThe size estimation network reached an accuracy of 88% in correctly segmenting\nthe polyps. By automatically incorporating this crucial information into CCE's\npathway, we moved a step closer towards the full integration of AI in CCE's\nroutine clinical practice.\n","authors":["Esmaeil S. Nadimi","Jan-Matthias Braun","Benedicte Schelde-Olesen","Emile Prudhomme","Victoria Blanes-Vidal","Gunnar Baatrup"],"pdf_url":"https://arxiv.org/pdf/2406.09761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11201v2","updated":"2024-06-14T06:48:24Z","published":"2024-02-17T05:31:10Z","title":"A Decoding Scheme with Successive Aggregation of Multi-Level Features\n  for Light-Weight Semantic Segmentation","summary":"  Multi-scale architecture, including hierarchical vision transformer, has been\ncommonly applied to high-resolution semantic segmentation to deal with\ncomputational complexity with minimum performance loss. In this paper, we\npropose a novel decoding scheme for semantic segmentation in this regard, which\ntakes multi-level features from the encoder with multi-scale architecture. The\ndecoding scheme based on a multi-level vision transformer aims to achieve not\nonly reduced computational expense but also higher segmentation accuracy, by\nintroducing successive cross-attention in aggregation of the multi-level\nfeatures. Furthermore, a way to enhance the multi-level features by the\naggregated semantics is proposed. The effort is focused on maintaining the\ncontextual consistency from the perspective of attention allocation and brings\nimproved performance with significantly lower computational cost. Set of\nexperiments on popular datasets demonstrates superiority of the proposed scheme\nto the state-of-the-art semantic segmentation models in terms of computational\ncost without loss of accuracy, and extensive ablation studies prove the\neffectiveness of ideas proposed.\n","authors":["Jiwon Yoo","Jangwon Lee","Gyeonghwan Kim"],"pdf_url":"https://arxiv.org/pdf/2402.11201v2.pdf","comment":"7 pages, 4 figures, ICIP2024 Accepted paper"},{"id":"http://arxiv.org/abs/2406.09756v1","updated":"2024-06-14T06:46:30Z","published":"2024-06-14T06:46:30Z","title":"Grounding Image Matching in 3D with MASt3R","summary":"  Image Matching is a core component of all best-performing algorithms and\npipelines in 3D vision. Yet despite matching being fundamentally a 3D problem,\nintrinsically linked to camera pose and scene geometry, it is typically treated\nas a 2D problem. This makes sense as the goal of matching is to establish\ncorrespondences between 2D pixel fields, but also seems like a potentially\nhazardous choice. In this work, we take a different stance and propose to cast\nmatching as a 3D task with DUSt3R, a recent and powerful 3D reconstruction\nframework based on Transformers. Based on pointmaps regression, this method\ndisplayed impressive robustness in matching views with extreme viewpoint\nchanges, yet with limited accuracy. We aim here to improve the matching\ncapabilities of such an approach while preserving its robustness. We thus\npropose to augment the DUSt3R network with a new head that outputs dense local\nfeatures, trained with an additional matching loss. We further address the\nissue of quadratic complexity of dense matching, which becomes prohibitively\nslow for downstream applications if not carefully treated. We introduce a fast\nreciprocal matching scheme that not only accelerates matching by orders of\nmagnitude, but also comes with theoretical guarantees and, lastly, yields\nimproved results. Extensive experiments show that our approach, coined MASt3R,\nsignificantly outperforms the state of the art on multiple matching tasks. In\nparticular, it beats the best published methods by 30% (absolute improvement)\nin VCRE AUC on the extremely challenging Map-free localization dataset.\n","authors":["Vincent Leroy","Yohann Cabon","Jérôme Revaud"],"pdf_url":"https://arxiv.org/pdf/2406.09756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09754v1","updated":"2024-06-14T06:44:01Z","published":"2024-06-14T06:44:01Z","title":"LAVIB: A Large-scale Video Interpolation Benchmark","summary":"  This paper introduces a LArge-scale Video Interpolation Benchmark (LAVIB) for\nthe low-level video task of video frame interpolation (VFI). LAVIB comprises a\nlarge collection of high-resolution videos sourced from the web through an\nautomated pipeline with minimal requirements for human verification. Metrics\nare computed for each video's motion magnitudes, luminance conditions, frame\nsharpness, and contrast. The collection of videos and the creation of\nquantitative challenges based on these metrics are under-explored by current\nlow-level video task datasets. In total, LAVIB includes 283K clips from 17K\nultra-HD videos, covering 77.6 hours. Benchmark train, val, and test sets\nmaintain similar video metric distributions. Further splits are also created\nfor out-of-distribution (OOD) challenges, with train and test splits including\nvideos of dissimilar attributes.\n","authors":["Alexandros Stergiou"],"pdf_url":"https://arxiv.org/pdf/2406.09754v1.pdf","comment":"Website: https://alexandrosstergiou.github.io/datasets/LAVIB/"},{"id":"http://arxiv.org/abs/2406.09750v1","updated":"2024-06-14T06:35:33Z","published":"2024-06-14T06:35:33Z","title":"ControlVAR: Exploring Controllable Visual Autoregressive Modeling","summary":"  Conditional visual generation has witnessed remarkable progress with the\nadvent of diffusion models (DMs), especially in tasks like control-to-image\ngeneration. However, challenges such as expensive computational cost, high\ninference latency, and difficulties of integration with large language models\n(LLMs) have necessitated exploring alternatives to DMs. This paper introduces\nControlVAR, a novel framework that explores pixel-level controls in visual\nautoregressive (VAR) modeling for flexible and efficient conditional\ngeneration. In contrast to traditional conditional models that learn the\nconditional distribution, ControlVAR jointly models the distribution of image\nand pixel-level conditions during training and imposes conditional controls\nduring testing. To enhance the joint modeling, we adopt the next-scale AR\nprediction paradigm and unify control and image representations. A\nteacher-forcing guidance strategy is proposed to further facilitate\ncontrollable generation with joint modeling. Extensive experiments demonstrate\nthe superior efficacy and flexibility of ControlVAR across various conditional\ngeneration tasks against popular conditional DMs, \\eg, ControlNet and\nT2I-Adaptor.\n","authors":["Xiang Li","Kai Qiu","Hao Chen","Jason Kuen","Zhe Lin","Rita Singh","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2406.09750v1.pdf","comment":"24 pages, 19 figures, 4 tables"},{"id":"http://arxiv.org/abs/2310.01055v3","updated":"2024-06-14T06:26:48Z","published":"2023-10-02T10:05:30Z","title":"Improved Crop and Weed Detection with Diverse Data Ensemble Learning","summary":"  Modern agriculture heavily relies on Site-Specific Farm Management practices,\nnecessitating accurate detection, localization, and quantification of crops and\nweeds in the field, which can be achieved using deep learning techniques. In\nthis regard, crop and weed-specific binary segmentation models have shown\npromise. However, uncontrolled field conditions limit their performance from\none field to the other. To improve semantic model generalization, existing\nmethods augment and synthesize agricultural data to account for uncontrolled\nfield conditions. However, given highly varied field conditions, these methods\nhave limitations. To overcome the challenges of model deterioration in such\nconditions, we propose utilizing data specific to other crops and weeds for our\nspecific target problem. To achieve this, we propose a novel ensemble\nframework. Our approach involves utilizing different crop and weed models\ntrained on diverse datasets and employing a teacher-student configuration. By\nusing homogeneous stacking of base models and a trainable meta-architecture to\ncombine their outputs, we achieve significant improvements for Canola crops and\nKochia weeds on unseen test data, surpassing the performance of single semantic\nsegmentation models. We identify the UNET meta-architecture as the most\neffective in this context. Finally, through ablation studies, we demonstrate\nand validate the effectiveness of our proposed model. We observe that including\nbase models trained on other target crops and weeds can help generalize the\nmodel to capture varied field conditions. Lastly, we propose two novel datasets\nwith varied conditions for comparisons.\n","authors":["Muhammad Hamza Asad","Saeed Anwar","Abdul Bais"],"pdf_url":"https://arxiv.org/pdf/2310.01055v3.pdf","comment":"Accepted in CVPR Workshop as an Oral"},{"id":"http://arxiv.org/abs/2406.09739v1","updated":"2024-06-14T06:00:14Z","published":"2024-06-14T06:00:14Z","title":"Decoupling Forgery Semantics for Generalizable Deepfake Detection","summary":"  In this paper, we propose a novel method for detecting DeepFakes, enhancing\nthe generalization of detection through semantic decoupling. There are now\nmultiple DeepFake forgery technologies that not only possess unique forgery\nsemantics but may also share common forgery semantics. The unique forgery\nsemantics and irrelevant content semantics may promote over-fitting and hamper\ngeneralization for DeepFake detectors. For our proposed method, after\ndecoupling, the common forgery semantics could be extracted from DeepFakes, and\nsubsequently be employed for developing the generalizability of DeepFake\ndetectors. Also, to pursue additional generalizability, we designed an adaptive\nhigh-pass module and a two-stage training strategy to improve the independence\nof decoupled semantics. Evaluation on FF++, Celeb-DF, DFD, and DFDC datasets\nshowcases our method's excellent detection and generalization performance. Code\nis available at: https://anonymous.4open.science/r/DFS-GDD-0F42.\n","authors":["Wei Ye","Xinan He","Feng Ding"],"pdf_url":"https://arxiv.org/pdf/2406.09739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09738v1","updated":"2024-06-14T05:53:00Z","published":"2024-06-14T05:53:00Z","title":"Contrastive Imitation Learning for Language-guided Multi-Task Robotic\n  Manipulation","summary":"  Developing robots capable of executing various manipulation tasks, guided by\nnatural language instructions and visual observations of intricate real-world\nenvironments, remains a significant challenge in robotics. Such robot agents\nneed to understand linguistic commands and distinguish between the requirements\nof different tasks. In this work, we present Sigma-Agent, an end-to-end\nimitation learning agent for multi-task robotic manipulation. Sigma-Agent\nincorporates contrastive Imitation Learning (contrastive IL) modules to\nstrengthen vision-language and current-future representations. An effective and\nefficient multi-view querying Transformer (MVQ-Former) for aggregating\nrepresentative semantic information is introduced. Sigma-Agent shows\nsubstantial improvement over state-of-the-art methods under diverse settings in\n18 RLBench tasks, surpassing RVT by an average of 5.2% and 5.9% in 10 and 100\ndemonstration training, respectively. Sigma-Agent also achieves 62% success\nrate with a single policy in 5 real-world manipulation tasks. The code will be\nreleased upon acceptance.\n","authors":["Teli Ma","Jiaming Zhou","Zifan Wang","Ronghe Qiu","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2406.09738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06534v2","updated":"2024-06-14T05:43:12Z","published":"2024-04-23T00:54:31Z","title":"Compressed Meta-Optical Encoder for Image Classification","summary":"  Optical and hybrid convolutional neural networks (CNNs) recently have become\nof increasing interest to achieve low-latency, low-power image classification\nand computer vision tasks. However, implementing optical nonlinearity is\nchallenging, and omitting the nonlinear layers in a standard CNN comes at a\nsignificant reduction in accuracy. In this work, we use knowledge distillation\nto compress modified AlexNet to a single linear convolutional layer and an\nelectronic backend (two fully connected layers). We obtain comparable\nperformance to a purely electronic CNN with five convolutional layers and three\nfully connected layers. We implement the convolution optically via engineering\nthe point spread function of an inverse-designed meta-optic. Using this hybrid\napproach, we estimate a reduction in multiply-accumulate operations from 17M in\na conventional electronic modified AlexNet to only 86K in the hybrid compressed\nnetwork enabled by the optical frontend. This constitutes over two orders of\nmagnitude reduction in latency and power consumption. Furthermore, we\nexperimentally demonstrate that the classification accuracy of the system\nexceeds 93% on the MNIST dataset.\n","authors":["Anna Wirth-Singh","Jinlin Xiang","Minho Choi","Johannes E. Fröch","Luocheng Huang","Shane Colburn","Eli Shlizerman","Arka Majumdar"],"pdf_url":"https://arxiv.org/pdf/2406.06534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12359v2","updated":"2024-06-14T05:39:24Z","published":"2023-11-21T05:27:16Z","title":"Shedding the Bits: Pushing the Boundaries of Quantization with\n  Minifloats on FPGAs","summary":"  Post-training quantization (PTQ) is a powerful technique for model\ncompression, reducing the numerical precision in neural networks without\nadditional training overhead. Recent works have investigated adopting 8-bit\nfloating-point formats(FP8) in the context of PTQ for model inference. However,\nfloating-point formats smaller than 8 bits and their relative comparison in\nterms of accuracy-hardware cost with integers remains unexplored on FPGAs. In\nthis work, we present minifloats, which are reduced-precision floating-point\nformats capable of further reducing the memory footprint, latency, and energy\ncost of a model while approaching full-precision model accuracy. We implement a\ncustom FPGA-based multiply-accumulate operator library and explore the vast\ndesign space, comparing minifloat and integer representations across 3 to 8\nbits for both weights and activations. We also examine the applicability of\nvarious integerbased quantization techniques to minifloats. Our experiments\nshow that minifloats offer a promising alternative for emerging workloads such\nas vision transformers.\n","authors":["Shivam Aggarwal","Hans Jakob Damsgaard","Alessandro Pappalardo","Giuseppe Franco","Thomas B. Preußer","Michaela Blott","Tulika Mitra"],"pdf_url":"https://arxiv.org/pdf/2311.12359v2.pdf","comment":"Accepted in FPL (International Conference on Field-Programmable Logic\n  and Applications) 2024 conference. Revised with updated results"},{"id":"http://arxiv.org/abs/2406.09731v1","updated":"2024-06-14T05:36:50Z","published":"2024-06-14T05:36:50Z","title":"Automated GIS-Based Framework for Detecting Crosswalk Changes from\n  Bi-Temporal High-Resolution Aerial Images","summary":"  Identification of changes in pavement markings has become crucial for\ninfrastructure monitoring, maintenance, development, traffic management, and\nsafety. Automated extraction of roadway geometry is critical in helping with\nthis, given the increasing availability of high-resolution images and\nadvancements in computer vision and object detection. Specifically, due to the\nsubstantial volume of satellite and high-resolution aerial images captured at\ndifferent time instances, change detection has become a viable solution. In\nthis study, an automated framework is developed to detect changes in crosswalks\nof Orange, Osceola, and Seminole counties in Florida, utilizing data extracted\nfrom high-resolution images obtained at various time intervals. Specifically,\nfor Orange County, crosswalk changes between 2019 and 2021 were manually\nextracted, verified, and categorized as either new or modified crosswalks. For\nSeminole County, the developed model was used to automatically extract\ncrosswalk changes between 2018 and 2021, while for Osceola County, changes\nbetween 2019 and 2020 were extracted. Findings indicate that Orange County\nwitnessed approximately 2,094 crosswalk changes, with 312 occurring on state\nroads. In Seminole and Osceola counties, on the other hand, 1,040 and 1,402\ncrosswalk changes were observed on both local and state roads, respectively.\nAmong these, 340 and 344 were identified on state roads in Seminole and\nOsceola, respectively. Spatiotemporal changes observed in crosswalks can be\nutilized to regularly update the existing crosswalk inventories, which is\nessential for agencies engaged in traffic and safety studies. Data extracted\nfrom these crosswalk changes can be combined with traffic and crash data to\nprovide valuable insights to policymakers.\n","authors":["Richard Boadu Antwi","Samuel Takyi","Alican Karaer","Eren Erman Ozguven","Michael Kimollo","Ren Moses","Maxim A. Dulebenets","Thobias Sando"],"pdf_url":"https://arxiv.org/pdf/2406.09731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09728v1","updated":"2024-06-14T05:33:01Z","published":"2024-06-14T05:33:01Z","title":"Neural Pose Representation Learning for Generating and Transferring\n  Non-Rigid Object Poses","summary":"  We propose a novel method for learning representations of poses for 3D\ndeformable objects, which specializes in 1) disentangling pose information from\nthe object's identity, 2) facilitating the learning of pose variations, and 3)\ntransferring pose information to other object identities. Based on these\nproperties, our method enables the generation of 3D deformable objects with\ndiversity in both identities and poses, using variations of a single object. It\ndoes not require explicit shape parameterization such as skeletons or joints,\npoint-level or shape-level correspondence supervision, or variations of the\ntarget object for pose transfer. To achieve pose disentanglement, compactness\nfor generative models, and transferability, we first design the pose extractor\nto represent the pose as a keypoint-based hybrid representation and the pose\napplier to learn an implicit deformation field. To better distill pose\ninformation from the object's geometry, we propose the implicit pose applier to\noutput an intrinsic mesh property, the face Jacobian. Once the extracted pose\ninformation is transferred to the target object, the pose applier is fine-tuned\nin a self-supervised manner to better describe the target object's shapes with\npose variations. The extracted poses are also used to train a cascaded\ndiffusion model to enable the generation of novel poses. Our experiments with\nthe DeformThings4D and Human datasets demonstrate state-of-the-art performance\nin pose transfer and the ability to generate diverse deformed shapes with\nvarious objects and poses.\n","authors":["Seungwoo Yoo","Juil Koo","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2406.09728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09726v1","updated":"2024-06-14T05:28:45Z","published":"2024-06-14T05:28:45Z","title":"PixRO: Pixel-Distributed Rotational Odometry with Gaussian Belief\n  Propagation","summary":"  Visual sensors are not only becoming better at capturing high-quality images\nbut also they have steadily increased their capabilities in processing data on\ntheir own on-chip. Yet the majority of VO pipelines rely on the transmission\nand processing of full images in a centralized unit (e.g. CPU or GPU), which\noften contain much redundant and low-quality information for the task. In this\npaper, we address the task of frame-to-frame rotational estimation but, instead\nof reasoning about relative motion between frames using the full images,\ndistribute the estimation at pixel-level. In this paradigm, each pixel produces\nan estimate of the global motion by only relying on local information and local\nmessage-passing with neighbouring pixels. The resulting per-pixel estimates can\nthen be communicated to downstream tasks, yielding higher-level, informative\ncues instead of the original raw pixel-readings. We evaluate the proposed\napproach on real public datasets, where we offer detailed insights about this\nnovel technique and open-source our implementation for the future benefit of\nthe community.\n","authors":["Ignacio Alzugaray","Riku Murai","Andrew Davison"],"pdf_url":"https://arxiv.org/pdf/2406.09726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09722v1","updated":"2024-06-14T05:14:54Z","published":"2024-06-14T05:14:54Z","title":"Cross-view geo-localization: a survey","summary":"  Cross-view geo-localization has garnered notable attention in the realm of\ncomputer vision, spurred by the widespread availability of copious geotagged\ndatasets and the advancements in machine learning techniques. This paper\nprovides a thorough survey of cutting-edge methodologies, techniques, and\nassociated challenges that are integral to this domain, with a focus on\nfeature-based and deep learning strategies. Feature-based methods capitalize on\nunique features to establish correspondences across disparate viewpoints,\nwhereas deep learning-based methodologies deploy convolutional neural networks\nto embed view-invariant attributes. This work also delineates the multifaceted\nchallenges encountered in cross-view geo-localization, such as variations in\nviewpoints and illumination, the occurrence of occlusions, and it elucidates\ninnovative solutions that have been formulated to tackle these issues.\nFurthermore, we delineate benchmark datasets and relevant evaluation metrics,\nand also perform a comparative analysis of state-of-the-art techniques.\nFinally, we conclude the paper with a discussion on prospective avenues for\nfuture research and the burgeoning applications of cross-view geo-localization\nin an intricately interconnected global landscape.\n","authors":["Abhilash Durgam","Sidike Paheding","Vikas Dhiman","Vijay Devabhaktuni"],"pdf_url":"https://arxiv.org/pdf/2406.09722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08177v2","updated":"2024-06-14T05:11:14Z","published":"2024-06-12T13:10:31Z","title":"One-Step Effective Diffusion Network for Real-World Image\n  Super-Resolution","summary":"  The pre-trained text-to-image diffusion models have been increasingly\nemployed to tackle the real-world image super-resolution (Real-ISR) problem due\nto their powerful generative image priors. Most of the existing methods start\nfrom random noise to reconstruct the high-quality (HQ) image under the guidance\nof the given low-quality (LQ) image. While promising results have been\nachieved, such Real- ISR methods require multiple diffusion steps to reproduce\nthe HQ image, increasing the computational cost. Meanwhile, the random noise\nintroduces uncertainty in the output, which is unfriendly to image restoration\ntasks. To address these issues, we propose a one-step effective diffusion\nnetwork, namely OSEDiff, for the Real- ISR problem. We argue that the LQ image\ncontains rich information to restore its HQ counterpart, and hence the given LQ\nimage can be directly taken as the starting point for diffusion, eliminating\nthe uncertainty introduced by random noise sampling. We finetune the\npre-trained diffusion network with trainable layers to adapt it to complex\nimage degradations. To ensure that the one-step diffusion model could yield HQ\nReal-ISR output, we apply variational score distillation in the latent space to\nconduct KL-divergence regularization. As a result, our OSEDiff model can\nefficiently and effectively generate HQ images in just one diffusion step. Our\nexperiments demonstrate that OSEDiff achieves comparable or even better\nReal-ISR results, in terms of both objective metrics and subjective\nevaluations, than previous diffusion model based Real-ISR methods that require\ndozens or hundreds of steps. The source codes will be released at\nhttps://github.com/cswry/OSEDiff.\n","authors":["Rongyuan Wu","Lingchen Sun","Zhiyuan Ma","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08177v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05175v3","updated":"2024-06-14T05:10:07Z","published":"2023-06-08T13:14:35Z","title":"Large-scale Dataset Pruning with Dynamic Uncertainty","summary":"  The state of the art of many learning tasks, e.g., image classification, is\nadvanced by collecting larger datasets and then training larger models on them.\nAs the outcome, the increasing computational cost is becoming unaffordable. In\nthis paper, we investigate how to prune the large-scale datasets, and thus\nproduce an informative subset for training sophisticated deep models with\nnegligible performance drop. We propose a simple yet effective dataset pruning\nmethod by exploring both the prediction uncertainty and training dynamics. We\nstudy dataset pruning by measuring the variation of predictions during the\nwhole training process on large-scale datasets, i.e., ImageNet-1K and\nImageNet-21K, and advanced models, i.e., Swin Transformer and ConvNeXt.\nExtensive experimental results indicate that our method outperforms the state\nof the art and achieves 25% lossless pruning ratio on both ImageNet-1K and\nImageNet-21K. The code and pruned datasets are available at\nhttps://github.com/BAAI-DCAI/Dataset-Pruning.\n","authors":["Muyang He","Shuo Yang","Tiejun Huang","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2306.05175v3.pdf","comment":"1st Workshop on Dataset Distillation for Computer Vision, CVPR2024,\n  see\n  https://openaccess.thecvf.com/content/CVPR2024W/DDCV/html/He_Large-scale_Dataset_Pruning_with_Dynamic_Uncertainty_CVPRW_2024_paper.html"},{"id":"http://arxiv.org/abs/2406.09711v1","updated":"2024-06-14T04:42:44Z","published":"2024-06-14T04:42:44Z","title":"AnimalFormer: Multimodal Vision Framework for Behavior-based Precision\n  Livestock Farming","summary":"  We introduce a multimodal vision framework for precision livestock farming,\nharnessing the power of GroundingDINO, HQSAM, and ViTPose models. This\nintegrated suite enables comprehensive behavioral analytics from video data\nwithout invasive animal tagging. GroundingDINO generates accurate bounding\nboxes around livestock, while HQSAM segments individual animals within these\nboxes. ViTPose estimates key body points, facilitating posture and movement\nanalysis. Demonstrated on a sheep dataset with grazing, running, sitting,\nstanding, and walking activities, our framework extracts invaluable insights:\nactivity and grazing patterns, interaction dynamics, and detailed postural\nevaluations. Applicable across species and video resolutions, this framework\nrevolutionizes non-invasive livestock monitoring for activity detection,\ncounting, health assessments, and posture analyses. It empowers data-driven\nfarm management, optimizing animal welfare and productivity through AI-powered\nbehavioral understanding.\n","authors":["Ahmed Qazi","Taha Razzaq","Asim Iqbal"],"pdf_url":"https://arxiv.org/pdf/2406.09711v1.pdf","comment":"In Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshops, 2024"},{"id":"http://arxiv.org/abs/2406.09710v1","updated":"2024-06-14T04:42:29Z","published":"2024-06-14T04:42:29Z","title":"Fine-Grained Urban Flow Inference with Multi-scale Representation\n  Learning","summary":"  Fine-grained urban flow inference (FUFI) is a crucial transportation service\naimed at improving traffic efficiency and safety. FUFI can infer fine-grained\nurban traffic flows based solely on observed coarse-grained data. However, most\nof existing methods focus on the influence of single-scale static geographic\ninformation on FUFI, neglecting the interactions and dynamic information\nbetween different-scale regions within the city. Different-scale geographical\nfeatures can capture redundant information from the same spatial areas. In\norder to effectively learn multi-scale information across time and space, we\npropose an effective fine-grained urban flow inference model called UrbanMSR,\nwhich uses self-supervised contrastive learning to obtain dynamic multi-scale\nrepresentations of neighborhood-level and city-level geographic information,\nand fuses multi-scale representations to improve fine-grained accuracy. The\nfusion of multi-scale representations enhances fine-grained. We validate the\nperformance through extensive experiments on three real-world datasets. The\nresutls compared with state-of-the-art methods demonstrate the superiority of\nthe proposed model.\n","authors":["Shilu Yuan","Dongfeng Li","Wei Liu","Xinxin Zhang","Meng Chen","Junjie Zhang","Yongshun Gong"],"pdf_url":"https://arxiv.org/pdf/2406.09710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09296v2","updated":"2024-06-14T04:40:09Z","published":"2024-06-13T16:30:32Z","title":"Parameter-Efficient Active Learning for Foundational models","summary":"  Foundational vision transformer models have shown impressive few shot\nperformance on many vision tasks. This research presents a novel investigation\ninto the application of parameter efficient fine-tuning methods within an\nactive learning (AL) framework, to advance the sampling selection process in\nextremely budget constrained classification tasks. The focus on image datasets,\nknown for their out-of-distribution characteristics, adds a layer of complexity\nand relevance to our study. Through a detailed evaluation, we illustrate the\nimproved AL performance on these challenging datasets, highlighting the\nstrategic advantage of merging parameter efficient fine tuning methods with\nfoundation models. This contributes to the broader discourse on optimizing AL\nstrategies, presenting a promising avenue for future exploration in leveraging\nfoundation models for efficient and effective data annotation in specialized\ndomains.\n","authors":["Athmanarayanan Lakshmi Narayanan","Ranganath Krishnan","Amrutha Machireddy","Mahesh Subedar"],"pdf_url":"https://arxiv.org/pdf/2406.09296v2.pdf","comment":"Accepted for CVPR2024 Transformers for Vision Workshop"},{"id":"http://arxiv.org/abs/2402.11846v3","updated":"2024-06-14T03:52:26Z","published":"2024-02-19T05:25:53Z","title":"UnlearnCanvas: Stylized Image Dataset for Enhanced Machine Unlearning\n  Evaluation in Diffusion Models","summary":"  The technological advancements in diffusion models (DMs) have demonstrated\nunprecedented capabilities in text-to-image generation and are widely used in\ndiverse applications. However, they have also raised significant societal\nconcerns, such as the generation of harmful content and copyright disputes.\nMachine unlearning (MU) has emerged as a promising solution, capable of\nremoving undesired generative capabilities from DMs. However, existing MU\nevaluation systems present several key challenges that can result in incomplete\nand inaccurate assessments. To address these issues, we propose UnlearnCanvas,\na comprehensive high-resolution stylized image dataset that facilitates the\nevaluation of the unlearning of artistic styles and associated objects. This\ndataset enables the establishment of a standardized, automated evaluation\nframework with 7 quantitative metrics assessing various aspects of the\nunlearning performance for DMs. Through extensive experiments, we benchmark 9\nstate-of-the-art MU methods for DMs, revealing novel insights into their\nstrengths, weaknesses, and underlying mechanisms. Additionally, we explore\nchallenging unlearning scenarios for DMs to evaluate worst-case performance\nagainst adversarial prompts, the unlearning of finer-scale concepts, and\nsequential unlearning. We hope that this study can pave the way for developing\nmore effective, accurate, and robust DM unlearning methods, ensuring safer and\nmore ethical applications of DMs in the future. The dataset, benchmark, and\ncodes are publicly available at https://unlearn-canvas.netlify.app/.\n","authors":["Yihua Zhang","Chongyu Fan","Yimeng Zhang","Yuguang Yao","Jinghan Jia","Jiancheng Liu","Gaoyuan Zhang","Gaowen Liu","Ramana Rao Kompella","Xiaoming Liu","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2402.11846v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09696v1","updated":"2024-06-14T03:44:33Z","published":"2024-06-14T03:44:33Z","title":"MoME: Mixture of Multimodal Experts for Cancer Survival Prediction","summary":"  Survival analysis, as a challenging task, requires integrating Whole Slide\nImages (WSIs) and genomic data for comprehensive decision-making. There are two\nmain challenges in this task: significant heterogeneity and complex inter- and\nintra-modal interactions between the two modalities. Previous approaches\nutilize co-attention methods, which fuse features from both modalities only\nonce after separate encoding. However, these approaches are insufficient for\nmodeling the complex task due to the heterogeneous nature between the\nmodalities. To address these issues, we propose a Biased Progressive Encoding\n(BPE) paradigm, performing encoding and fusion simultaneously. This paradigm\nuses one modality as a reference when encoding the other. It enables deep\nfusion of the modalities through multiple alternating iterations, progressively\nreducing the cross-modal disparities and facilitating complementary\ninteractions. Besides modality heterogeneity, survival analysis involves\nvarious biomarkers from WSIs, genomics, and their combinations. The critical\nbiomarkers may exist in different modalities under individual variations,\nnecessitating flexible adaptation of the models to specific scenarios.\nTherefore, we further propose a Mixture of Multimodal Experts (MoME) layer to\ndynamically selects tailored experts in each stage of the BPE paradigm. Experts\nincorporate reference information from another modality to varying degrees,\nenabling a balanced or biased focus on different modalities during the encoding\nprocess. Extensive experimental results demonstrate the superior performance of\nour method on various datasets, including TCGA-BLCA, TCGA-UCEC and TCGA-LUAD.\nCodes are available at https://github.com/BearCleverProud/MoME.\n","authors":["Conghao Xiong","Hao Chen","Hao Zheng","Dong Wei","Yefeng Zheng","Joseph J. Y. Sung","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2406.09696v1.pdf","comment":"8 + 1/2 pages, early accepted to MICCAI2024"},{"id":"http://arxiv.org/abs/2309.06255v4","updated":"2024-06-14T03:37:46Z","published":"2023-09-12T14:16:34Z","title":"Enhancing multimodal cooperation via sample-level modality valuation","summary":"  One primary topic of multimodal learning is to jointly incorporate\nheterogeneous information from different modalities. However most models often\nsuffer from unsatisfactory multimodal cooperation which cannot jointly utilize\nall modalities well. Some methods are proposed to identify and enhance the\nworse learnt modality but they are often hard to provide the fine-grained\nobservation of multimodal cooperation at sample-level with theoretical support.\nHence it is essential to reasonably observe and improve the fine-grained\ncooperation between modalities especially when facing realistic scenarios where\nthe modality discrepancy could vary across different samples. To this end we\nintroduce a sample-level modality valuation metric to evaluate the contribution\nof each modality for each sample. Via modality valuation we observe that\nmodality discrepancy indeed could be different at sample-level beyond the\nglobal contribution discrepancy at dataset-level. We further analyze this issue\nand improve cooperation between modalities at sample-level by enhancing the\ndiscriminative ability of low-contributing modalities in a targeted manner.\nOverall our methods reasonably observe the fine-grained uni-modal contribution\nand achieve considerable improvement. The source code and dataset are available\nat https://github.com/GeWu-Lab/Valuate-and-Enhance-Multimodal-Cooperation.\n","authors":["Yake Wei","Ruoxuan Feng","Zihe Wang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2309.06255v4.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2406.09693v1","updated":"2024-06-14T03:36:55Z","published":"2024-06-14T03:36:55Z","title":"Compressed Video Quality Enhancement with Temporal Group Alignment and\n  Fusion","summary":"  In this paper, we propose a temporal group alignment and fusion network to\nenhance the quality of compressed videos by using the long-short term\ncorrelations between frames. The proposed model consists of the intra-group\nfeature alignment (IntraGFA) module, the inter-group feature fusion (InterGFF)\nmodule, and the feature enhancement (FE) module. We form the group of pictures\n(GoP) by selecting frames from the video according to their temporal distances\nto the target enhanced frame. With this grouping, the composed GoP can contain\neither long- or short-term correlated information of neighboring frames. We\ndesign the IntraGFA module to align the features of frames of each GoP to\neliminate the motion existing between frames. We construct the InterGFF module\nto fuse features belonging to different GoPs and finally enhance the fused\nfeatures with the FE module to generate high-quality video frames. The\nexperimental results show that our proposed method achieves up to 0.05dB gain\nand lower complexity compared to the state-of-the-art method.\n","authors":["Qiang Zhu","Yajun Qiu","Yu Liu","Shuyuan Zhu","Bing Zeng"],"pdf_url":"https://arxiv.org/pdf/2406.09693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19688v2","updated":"2024-06-14T03:18:50Z","published":"2024-05-30T04:57:55Z","title":"DNPM: A Neural Parametric Model for the Synthesis of Facial Geometric\n  Details","summary":"  Parametric 3D models have enabled a wide variety of computer vision and\ngraphics tasks, such as modeling human faces, bodies and hands. In 3D face\nmodeling, 3DMM is the most widely used parametric model, but can't generate\nfine geometric details solely from identity and expression inputs. To tackle\nthis limitation, we propose a neural parametric model named DNPM for the facial\ngeometric details, which utilizes deep neural network to extract latent codes\nfrom facial displacement maps encoding details and wrinkles. Built upon DNPM, a\nnovel 3DMM named Detailed3DMM is proposed, which augments traditional 3DMMs by\nincluding the synthesis of facial details only from the identity and expression\ninputs. Moreover, we show that DNPM and Detailed3DMM can facilitate two\ndownstream applications: speech-driven detailed 3D facial animation and 3D face\nreconstruction from a degraded image. Extensive experiments have shown the\nusefulness of DNPM and Detailed3DMM, and the progressiveness of two proposed\napplications.\n","authors":["Haitao Cao","Baoping Cheng","Qiran Pu","Haocheng Zhang","Bin Luo","Yixiang Zhuang","Juncong Lin","Liyan Chen","Xuan Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.19688v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12416v3","updated":"2024-06-14T03:18:18Z","published":"2024-03-19T03:59:14Z","title":"Eye-gaze Guided Multi-modal Alignment for Medical Representation\n  Learning","summary":"  In the medical multi-modal frameworks, the alignment of cross-modality\nfeatures presents a significant challenge. However, existing works have learned\nfeatures that are implicitly aligned from the data, without considering the\nexplicit relationships in the medical context. This data-reliance may lead to\nlow generalization of the learned alignment relationships. In this work, we\npropose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness\neye-gaze data for better alignment of medical visual and textual features. We\nexplore the natural auxiliary role of radiologists' eye-gaze data in aligning\nmedical images and text, and introduce a novel approach by using eye-gaze data,\ncollected synchronously by radiologists during diagnostic evaluations. We\nconduct downstream tasks of image classification and image-text retrieval on\nfour medical datasets, where EGMA achieved state-of-the-art performance and\nstronger generalization across different datasets. Additionally, we explore the\nimpact of varying amounts of eye-gaze data on model performance, highlighting\nthe feasibility and utility of integrating this auxiliary data into multi-modal\nalignment framework.\n","authors":["Chong Ma","Hanqi Jiang","Wenting Chen","Yiwei Li","Zihao Wu","Xiaowei Yu","Zhengliang Liu","Lei Guo","Dajiang Zhu","Tuo Zhang","Dinggang Shen","Tianming Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.12416v3.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.15243v2","updated":"2024-06-14T03:11:43Z","published":"2024-05-24T06:10:23Z","title":"Less is More: Discovering Concise Network Explanations","summary":"  We introduce Discovering Conceptual Network Explanations (DCNE), a new\napproach for generating human-comprehensible visual explanations to enhance the\ninterpretability of deep neural image classifiers. Our method automatically\nfinds visual explanations that are critical for discriminating between classes.\nThis is achieved by simultaneously optimizing three criteria: the explanations\nshould be few, diverse, and human-interpretable. Our approach builds on the\nrecently introduced Concept Relevance Propagation (CRP) explainability method.\nWhile CRP is effective at describing individual neuronal activations, it\ngenerates too many concepts, which impacts human comprehension. Instead, DCNE\nselects the few most important explanations. We introduce a new evaluation\ndataset centered on the challenging task of classifying birds, enabling us to\ncompare the alignment of DCNE's explanations to those of human expert-defined\nones. Compared to existing eXplainable Artificial Intelligence (XAI) methods,\nDCNE has a desirable trade-off between conciseness and completeness when\nsummarizing network explanations. It produces 1/30 of CRP's explanations while\nonly resulting in a slight reduction in explanation quality. DCNE represents a\nstep forward in making neural network decisions accessible and interpretable to\nhumans, providing a valuable tool for both researchers and practitioners in XAI\nand model alignment.\n","authors":["Neehar Kondapaneni","Markus Marks","Oisin MacAodha","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2405.15243v2.pdf","comment":"9 pages, 5 figures; ICLR Re-Align Workshop 2024; Project Page:\n  https://www.vision.caltech.edu/dcne/ Github:\n  https://github.com/nkondapa/DiscoveringConciseNetworkExplanations"},{"id":"http://arxiv.org/abs/2406.09681v1","updated":"2024-06-14T03:07:23Z","published":"2024-06-14T03:07:23Z","title":"Asymmetrical Siamese Network for Point Clouds Normal Estimation","summary":"  In recent years, deep learning-based point cloud normal estimation has made\ngreat progress. However, existing methods mainly rely on the PCPNet dataset,\nleading to overfitting. In addition, the correlation between point clouds with\ndifferent noise scales remains unexplored, resulting in poor performance in\ncross-domain scenarios. In this paper, we explore the consistency of intrinsic\nfeatures learned from clean and noisy point clouds using an Asymmetric Siamese\nNetwork architecture. By applying reasonable constraints between features\nextracted from different branches, we enhance the quality of normal estimation.\nMoreover, we introduce a novel multi-view normal estimation dataset that\nincludes a larger variety of shapes with different noise levels. Evaluation of\nexisting methods on this new dataset reveals their inability to adapt to\ndifferent types of shapes, indicating a degree of overfitting. Extensive\nexperiments show that the proposed dataset poses significant challenges for\npoint cloud normal estimation and that our feature constraint mechanism\neffectively improves upon existing methods and reduces overfitting in current\narchitectures.\n","authors":["Wei Jin","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.09681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09679v1","updated":"2024-06-14T03:04:05Z","published":"2024-06-14T03:04:05Z","title":"Exploring Training on Heterogeneous Data with Mixture of Low-rank\n  Adapters","summary":"  Training a unified model to take multiple targets into account is a trend\ntowards artificial general intelligence. However, how to efficiently mitigate\nthe training conflicts among heterogeneous data collected from different\ndomains or tasks remains under-explored. In this study, we explore to leverage\nMixture of Low-rank Adapters (MoLA) to mitigate conflicts in heterogeneous data\ntraining, which requires to jointly train the multiple low-rank adapters and\ntheir shared backbone. Specifically, we introduce two variants of MoLA, namely,\nMoLA-Grad and MoLA-Router, to respectively handle the target-aware and\ntarget-agnostic scenarios during inference. The former uses task identifiers to\nassign personalized low-rank adapters to each task, disentangling task-specific\nknowledge towards their adapters, thereby mitigating heterogeneity conflicts.\nThe latter uses a novel Task-wise Decorrelation (TwD) loss to intervene the\nrouter to learn oriented weight combinations of adapters to homogeneous tasks,\nachieving similar effects. We conduct comprehensive experiments to verify the\nsuperiority of MoLA over previous state-of-the-art methods and present in-depth\nanalysis on its working mechanism. Source code is available at:\nhttps://github.com/MediaBrain-SJTU/MoLA\n","authors":["Yuhang Zhou","Zihua Zhao","Haolin Li","Siyuan Du","Jiangchao Yao","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09679v1.pdf","comment":"ICML2024"},{"id":"http://arxiv.org/abs/2311.17425v4","updated":"2024-06-14T02:28:05Z","published":"2023-11-29T07:57:30Z","title":"SpeechAct: Towards Generating Whole-body Motion from Speech","summary":"  This paper addresses the problem of generating whole-body motion from speech.\nDespite great successes, prior methods still struggle to produce reasonable and\ndiverse whole-body motions from speech. This is due to their reliance on\nsuboptimal representations and a lack of strategies for generating diverse\nresults. To address these challenges, we present a novel hybrid point\nrepresentation to achieve accurate and continuous motion generation, e.g.,\navoiding foot skating, and this representation can be transformed into an\neasy-to-use representation, i.e., SMPL-X body mesh, for many applications. To\ngenerate whole-body motion from speech, for facial motion, closely tied to the\naudio signal, we introduce an encoder-decoder architecture to achieve\ndeterministic outcomes. However, for the body and hands, which have weaker\nconnections to the audio signal, we aim to generate diverse yet reasonable\nmotions. To boost diversity in motion generation, we propose a contrastive\nmotion learning method to encourage the model to produce more distinctive\nrepresentations. Specifically, we design a robust VQ-VAE to learn a quantized\nmotion codebook using our hybrid representation. Then, we regress the motion\nrepresentation from the audio signal by a translation model employing our\ncontrastive motion learning method. Experimental results validate the superior\nperformance and the correctness of our model. The project page is available for\nresearch purposes at http://cic.tju.edu.cn/faculty/likun/projects/SpeechAct.\n","authors":["Jinsong Zhang","Minjie Zhu","Yuxiang Zhang","Yebin Liu","Kun Li"],"pdf_url":"https://arxiv.org/pdf/2311.17425v4.pdf","comment":"The paper has been archived without permission from the newly added\n  author"},{"id":"http://arxiv.org/abs/2406.09662v1","updated":"2024-06-14T02:21:53Z","published":"2024-06-14T02:21:53Z","title":"Learning Language Structures through Grounding","summary":"  Language is highly structured, with syntactic and semantic structures, to\nsome extent, agreed upon by speakers of the same language. With implicit or\nexplicit awareness of such structures, humans can learn and use language\nefficiently and generalize to sentences that contain unseen words. Motivated by\nhuman language learning, in this dissertation, we consider a family of machine\nlearning tasks that aim to learn language structures through grounding. We seek\ndistant supervision from other data sources (i.e., grounds), including but not\nlimited to other modalities (e.g., vision), execution results of programs, and\nother languages.\n  We demonstrate the potential of this task formulation and advocate for its\nadoption through three schemes. In Part I, we consider learning syntactic\nparses through visual grounding. We propose the task of visually grounded\ngrammar induction, present the first models to induce syntactic structures from\nvisually grounded text and speech, and find that the visual grounding signals\ncan help improve the parsing quality over language-only models. As a side\ncontribution, we propose a novel evaluation metric that enables the evaluation\nof speech parsing without text or automatic speech recognition systems\ninvolved. In Part II, we propose two execution-aware methods to map sentences\ninto corresponding semantic structures (i.e., programs), significantly\nimproving compositional generalization and few-shot program synthesis. In Part\nIII, we propose methods that learn language structures from annotations in\nother languages. Specifically, we propose a method that sets a new state of the\nart on cross-lingual word alignment. We then leverage the learned word\nalignments to improve the performance of zero-shot cross-lingual dependency\nparsing, by proposing a novel substructure-based projection method that\npreserves structural knowledge learned from the source language.\n","authors":["Freda Shi"],"pdf_url":"https://arxiv.org/pdf/2406.09662v1.pdf","comment":"Ph.D. Thesis"},{"id":"http://arxiv.org/abs/2312.05826v3","updated":"2024-06-14T02:17:38Z","published":"2023-12-10T08:59:43Z","title":"R2Human: Real-Time 3D Human Appearance Rendering from a Single Image","summary":"  Rendering 3D human appearance in different views is crucial for achieving\nholographic communication and immersive VR/AR. Existing methods either rely on\nmulti-camera setups or have low-quality rendered images from a single image. In\nthis paper, we propose R2Human, the first approach for real-time inference and\nrendering of photorealistic 3D human appearance from a single image. The core\nof our approach is to combine the strengths of implicit texture fields and\nexplicit neural rendering with our novel representation, namely Z-map. Based on\nthis, we present an end-to-end network that performs high-fidelity color\nreconstruction of visible areas and provides reliable color inference for\noccluded regions. To further enhance the 3D perception ability of our network,\nwe leverage the Fourier occupancy field as a prior for generating the texture\nfield and providing a sampling surface in the rendering stage. We also propose\na consistency loss and a spatio-temporal fusion strategy to ensure the\nmulti-view coherence. Experimental results show that our method outperforms the\nstate-of-the-art methods on both synthetic data and challenging real-world\nimages, in real time.\n","authors":["Yuanwang Yang","Qiao Feng","Yu-Kun Lai","Kun Li"],"pdf_url":"https://arxiv.org/pdf/2312.05826v3.pdf","comment":"The manuscript should be revised because the authors aim to achieve\n  better organization in writing and more experimental analysis"},{"id":"http://arxiv.org/abs/2406.09181v2","updated":"2024-06-14T02:17:04Z","published":"2024-06-13T14:42:59Z","title":"A Large-scale Universal Evaluation Benchmark For Face Forgery Detection","summary":"  With the rapid development of AI-generated content (AIGC) technology, the\nproduction of realistic fake facial images and videos that deceive human visual\nperception has become possible. Consequently, various face forgery detection\ntechniques have been proposed to identify such fake facial content. However,\nevaluating the effectiveness and generalizability of these detection techniques\nremains a significant challenge. To address this, we have constructed a\nlarge-scale evaluation benchmark called DeepFaceGen, aimed at quantitatively\nassessing the effectiveness of face forgery detection and facilitating the\niterative development of forgery detection technology. DeepFaceGen consists of\n776,990 real face image/video samples and 773,812 face forgery image/video\nsamples, generated using 34 mainstream face generation techniques. During the\nconstruction process, we carefully consider important factors such as content\ndiversity, fairness across ethnicities, and availability of comprehensive\nlabels, in order to ensure the versatility and convenience of DeepFaceGen.\nSubsequently, DeepFaceGen is employed in this study to evaluate and analyze the\nperformance of 13 mainstream face forgery detection techniques from various\nperspectives. Through extensive experimental analysis, we derive significant\nfindings and propose potential directions for future research. The code and\ndataset for DeepFaceGen are available at\nhttps://github.com/HengruiLou/DeepFaceGen.\n","authors":["Yijun Bei","Hengrui Lou","Jinsong Geng","Erteng Liu","Lechao Cheng","Jie Song","Mingli Song","Zunlei Feng"],"pdf_url":"https://arxiv.org/pdf/2406.09181v2.pdf","comment":"This is a paper about constructing a large-scale universal evaluation\n  benchmark for face forgery detection.The full text is 30 pages"},{"id":"http://arxiv.org/abs/2406.09295v2","updated":"2024-06-14T02:14:49Z","published":"2024-06-13T16:30:14Z","title":"AlignMMBench: Evaluating Chinese Multimodal Alignment in Large\n  Vision-Language Models","summary":"  Evaluating the alignment capabilities of large Vision-Language Models (VLMs)\nis essential for determining their effectiveness as helpful assistants.\nHowever, existing benchmarks primarily focus on basic abilities using nonverbal\nmethods, such as yes-no and multiple-choice questions. In this paper, we\naddress this gap by introducing AlignMMBench, a comprehensive alignment\nbenchmark specifically designed for emerging Chinese VLMs. This benchmark is\nmeticulously curated from real-world scenarios and Chinese Internet sources,\nencompassing thirteen specific tasks across three categories, and includes both\nsingle-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite\nstrategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer\npairs. To facilitate the evaluation pipeline, we propose CritiqueVLM, a\nrule-calibrated evaluator that exceeds GPT-4's evaluation ability. Finally, we\nreport the performance of representative VLMs on AlignMMBench, offering\ninsights into the capabilities and limitations of different VLM architectures.\nAll evaluation codes and data are available on https://alignmmbench.github.io.\n","authors":["Yuhang Wu","Wenmeng Yu","Yean Cheng","Yan Wang","Xiaohan Zhang","Jiazheng Xu","Ming Ding","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2406.09295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17287v2","updated":"2024-06-14T01:37:57Z","published":"2024-02-27T08:00:52Z","title":"An Interpretable Evaluation of Entropy-based Novelty of Generative\n  Models","summary":"  The massive developments of generative model frameworks require principled\nmethods for the evaluation of a model's novelty compared to a reference\ndataset. While the literature has extensively studied the evaluation of the\nquality, diversity, and generalizability of generative models, the assessment\nof a model's novelty compared to a reference model has not been adequately\nexplored in the machine learning community. In this work, we focus on the\nnovelty assessment for multi-modal distributions and attempt to address the\nfollowing differential clustering task: Given samples of a generative model\n$P_\\mathcal{G}$ and a reference model $P_\\mathrm{ref}$, how can we discover the\nsample types expressed by $P_\\mathcal{G}$ more frequently than in\n$P_\\mathrm{ref}$? We introduce a spectral approach to the differential\nclustering task and propose the Kernel-based Entropic Novelty (KEN) score to\nquantify the mode-based novelty of $P_\\mathcal{G}$ with respect to\n$P_\\mathrm{ref}$. We analyze the KEN score for mixture distributions with\nwell-separable components and develop a kernel-based method to compute the KEN\nscore from empirical data. We support the KEN framework by presenting numerical\nresults on synthetic and real image datasets, indicating the framework's\neffectiveness in detecting novel modes and comparing generative models. The\npaper's code is available at: www.github.com/buyeah1109/KEN\n","authors":["Jingwei Zhang","Cheuk Ting Li","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2402.17287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09656v1","updated":"2024-06-14T01:36:52Z","published":"2024-06-14T01:36:52Z","title":"RSEND: Retinex-based Squeeze and Excitation Network with Dark Region\n  Detection for Efficient Low Light Image Enhancement","summary":"  Images captured under low-light scenarios often suffer from low quality.\nPrevious CNN-based deep learning methods often involve using Retinex theory.\nNevertheless, most of them cannot perform well in more complicated datasets\nlike LOL-v2 while consuming too much computational resources. Besides, some of\nthese methods require sophisticated training at different stages, making the\nprocedure even more time-consuming and tedious. In this paper, we propose a\nmore accurate, concise, and one-stage Retinex theory based framework, RSEND.\nRSEND first divides the low-light image into the illumination map and\nreflectance map, then captures the important details in the illumination map\nand performs light enhancement. After this step, it refines the enhanced\ngray-scale image and does element-wise matrix multiplication with the\nreflectance map. By denoising the output it has from the previous step, it\nobtains the final result. In all the steps, RSEND utilizes Squeeze and\nExcitation network to better capture the details. Comprehensive quantitative\nand qualitative experiments show that our Efficient Retinex model significantly\noutperforms other CNN-based models, achieving a PSNR improvement ranging from\n0.44 dB to 4.2 dB in different datasets and even outperforms transformer-based\nmodels in the LOL-v2-real dataset.\n","authors":["Jingcheng Li","Ye Qiao","Haocheng Xu","Sitao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.09656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04867v2","updated":"2024-06-14T01:11:09Z","published":"2024-06-07T12:07:09Z","title":"Deep learning for precipitation nowcasting: A survey from the\n  perspective of time series forecasting","summary":"  Deep learning-based time series forecasting has dominated the short-term\nprecipitation forecasting field with the help of its ability to estimate motion\nflow in high-resolution datasets. The growing interest in precipitation\nnowcasting offers substantial opportunities for the advancement of current\nforecasting technologies. Nevertheless, there has been a scarcity of in-depth\nsurveys of time series precipitation forecasting using deep learning. Thus,\nthis paper systemically reviews recent progress in time series precipitation\nforecasting models. Specifically, we investigate the following key points\nwithin background components, covering: i) preprocessing, ii) objective\nfunctions, and iii) evaluation metrics. We then categorize forecasting models\ninto \\textit{recursive} and \\textit{multiple} strategies based on their\napproaches to predict future frames, investigate the impacts of models using\nthe strategies, and performance assessments. Finally, we evaluate current deep\nlearning-based models for precipitation forecasting on a public benchmark,\ndiscuss their limitations and challenges, and present some promising research\ndirections. Our contribution lies in providing insights for a better\nunderstanding of time series precipitation forecasting and in aiding the\ndevelopment of robust AI solutions for the future.\n","authors":["Sojung An","Tae-Jin Oh","Eunha Sohn","Donghyun Kim"],"pdf_url":"https://arxiv.org/pdf/2406.04867v2.pdf","comment":"21 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.08487v3","updated":"2024-06-14T00:52:35Z","published":"2024-06-12T17:59:49Z","title":"Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models","summary":"  Seeing clearly with high resolution is a foundation of Large Multimodal\nModels (LMMs), which has been proven to be vital for visual perception and\nreasoning. Existing works usually employ a straightforward resolution upscaling\nmethod, where the image consists of global and local branches, with the latter\nbeing the sliced image patches but resized to the same resolution as the\nformer. This means that higher resolution requires more local patches,\nresulting in exorbitant computational expenses, and meanwhile, the dominance of\nlocal image tokens may diminish the global context. In this paper, we dive into\nthe problems and propose a new framework as well as an elaborate optimization\nstrategy. Specifically, we extract contextual information from the global view\nusing a mixture of adapters, based on the observation that different adapters\nexcel at different tasks. With regard to local patches, learnable query\nembeddings are introduced to reduce image tokens, the most important tokens\naccounting for the user question will be further selected by a similarity-based\nselector. Our empirical results demonstrate a `less is more' pattern, where\n\\textit{utilizing fewer but more informative local image tokens leads to\nimproved performance}. Besides, a significant challenge lies in the training\nstrategy, as simultaneous end-to-end training of the global mining block and\nlocal compression block does not yield optimal results. We thus advocate for an\nalternating training way, ensuring balanced learning between global and local\naspects. Finally, we also introduce a challenging dataset with high\nrequirements for image detail, enhancing the training of the local compression\nlayer. The proposed method, termed LMM with Sophisticated Tasks, Local image\ncompression, and Mixture of global Experts (SliME), achieves leading\nperformance across various benchmarks with only 2 million training data.\n","authors":["Yi-Fan Zhang","Qingsong Wen","Chaoyou Fu","Xue Wang","Zhang Zhang","Liang Wang","Rong Jin"],"pdf_url":"https://arxiv.org/pdf/2406.08487v3.pdf","comment":"Project page: https://github.com/yfzhang114/SliME"},{"id":"http://arxiv.org/abs/2406.04679v2","updated":"2024-06-14T00:41:33Z","published":"2024-06-07T06:50:19Z","title":"XctDiff: Reconstruction of CT Images with Consistent Anatomical\n  Structures from a Single Radiographic Projection Image","summary":"  In this paper, we present XctDiff, an algorithm framework for reconstructing\nCT from a single radiograph, which decomposes the reconstruction process into\ntwo easily controllable tasks: feature extraction and CT reconstruction.\nSpecifically, we first design a progressive feature extraction strategy that is\nable to extract robust 3D priors from radiographs. Then, we use the extracted\nprior information to guide the CT reconstruction in the latent space. Moreover,\nwe design a homogeneous spatial codebook to improve the reconstruction quality\nfurther. The experimental results show that our proposed method achieves\nstate-of-the-art reconstruction performance and overcomes the blurring issue.\nWe also apply XctDiff on self-supervised pre-training task. The effectiveness\nindicates that it has promising additional applications in medical image\nanalysis. The code is available at:https://github.com/qingze-bai/XctDiff\n","authors":["Qingze Bai","Tiange Liu","Zhi Liu","Yubing Tong","Drew Torigian","Jayaram Udupa"],"pdf_url":"https://arxiv.org/pdf/2406.04679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09648v1","updated":"2024-06-14T00:40:31Z","published":"2024-06-14T00:40:31Z","title":"An Intrinsic Vector Heat Network","summary":"  Vector fields are widely used to represent and model flows for many science\nand engineering applications. This paper introduces a novel neural network\narchitecture for learning tangent vector fields that are intrinsically defined\non manifold surfaces embedded in 3D. Previous approaches to learning vector\nfields on surfaces treat vectors as multi-dimensional scalar fields, using\ntraditional scalar-valued architectures to process channels individually, thus\nfail to preserve fundamental intrinsic properties of the vector field. The core\nidea of this work is to introduce a trainable vector heat diffusion module to\nspatially propagate vector-valued feature data across the surface, which we\nincorporate into our proposed architecture that consists of vector-valued\nneurons. Our architecture is invariant to rigid motion of the input, isometric\ndeformation, and choice of local tangent bases, and is robust to\ndiscretizations of the surface. We evaluate our Vector Heat Network on triangle\nmeshes, and empirically validate its invariant properties. We also demonstrate\nthe effectiveness of our method on the useful industrial application of\nquadrilateral mesh generation.\n","authors":["Alexander Gao","Maurice Chu","Mubbasir Kapadia","Ming C. Lin","Hsueh-Ti Derek Liu"],"pdf_url":"https://arxiv.org/pdf/2406.09648v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2309.06908v2","updated":"2024-06-14T16:27:24Z","published":"2023-09-13T12:10:54Z","title":"Towards the TopMost: A Topic Modeling System Toolkit","summary":"  Topic models have a rich history with various applications and have recently\nbeen reinvigorated by neural topic modeling. However, these numerous topic\nmodels adopt totally distinct datasets, implementations, and evaluations. This\nimpedes quick utilization and fair comparisons, and thereby hinders their\nresearch progress and applications. To tackle this challenge, we in this paper\npropose a Topic Modeling System Toolkit (TopMost). Compared to existing\ntoolkits, TopMost stands out by supporting more extensive features. It covers a\nbroader spectrum of topic modeling scenarios with their complete lifecycles,\nincluding datasets, preprocessing, models, training, and evaluations. Thanks to\nits highly cohesive and decoupled modular design, TopMost enables rapid\nutilization, fair comparisons, and flexible extensions of diverse cutting-edge\ntopic models. Our code, tutorials, and documentation are available at\nhttps://github.com/bobxwu/topmost.\n","authors":["Xiaobao Wu","Fengjun Pan","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2309.06908v2.pdf","comment":"Accepted to ACL 2024 System Demonstrations Track"},{"id":"http://arxiv.org/abs/2406.09215v2","updated":"2024-06-14T15:22:58Z","published":"2024-06-13T15:16:11Z","title":"On Softmax Direct Preference Optimization for Recommendation","summary":"  Recommender systems aim to predict personalized rankings based on user\npreference data. With the rise of Language Models (LMs), LM-based recommenders\nhave been widely explored due to their extensive world knowledge and powerful\nreasoning abilities. Most of the LM-based recommenders convert historical\ninteractions into language prompts, pairing with a positive item as the target\nresponse and fine-tuning LM with a language modeling loss. However, the current\nobjective fails to fully leverage preference data and is not optimized for\npersonalized ranking tasks, which hinders the performance of LM-based\nrecommenders. Inspired by the current advancement of Direct Preference\nOptimization (DPO) in human preference alignment and the success of softmax\nloss in recommendations, we propose Softmax-DPO (S-DPO) to instill ranking\ninformation into the LM to help LM-based recommenders distinguish preferred\nitems from negatives, rather than solely focusing on positives. Specifically,\nwe incorporate multiple negatives in user preference data and devise an\nalternative version of DPO loss tailored for LM-based recommenders, connected\nto softmax sampling strategies. Theoretically, we bridge S-DPO with the softmax\nloss over negative sampling and find that it has a side effect of mining hard\nnegatives, which assures its exceptional capabilities in recommendation tasks.\nEmpirically, extensive experiments conducted on three real-world datasets\ndemonstrate the superiority of S-DPO to effectively model user preference and\nfurther boost recommendation performance while mitigating the data likelihood\ndecline issue of DPO. Our codes are available at\nhttps://github.com/chenyuxin1999/S-DPO.\n","authors":["Yuxin Chen","Junfei Tan","An Zhang","Zhengyi Yang","Leheng Sheng","Enzhi Zhang","Xiang Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2406.09215v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06211v2","updated":"2024-06-14T13:07:27Z","published":"2024-05-10T02:48:45Z","title":"A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language\n  Models","summary":"  As one of the most advanced techniques in AI, Retrieval-Augmented Generation\n(RAG) can offer reliable and up-to-date external knowledge, providing huge\nconvenience for numerous tasks. Particularly in the era of AI-Generated Content\n(AIGC), the powerful capacity of retrieval in providing additional knowledge\nenables RAG to assist existing generative AI in producing high-quality outputs.\nRecently, Large Language Models (LLMs) have demonstrated revolutionary\nabilities in language understanding and generation, while still facing inherent\nlimitations, such as hallucinations and out-of-date internal knowledge. Given\nthe powerful abilities of RAG in providing the latest and helpful auxiliary\ninformation, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged\nto harness external and authoritative knowledge bases, rather than solely\nrelying on the model's internal knowledge, to augment the generation quality of\nLLMs. In this survey, we comprehensively review existing research studies in\nRA-LLMs, covering three primary technical perspectives: architectures, training\nstrategies, and applications. As the preliminary knowledge, we briefly\nintroduce the foundations and recent advances of LLMs. Then, to illustrate the\npractical significance of RAG for LLMs, we systematically review mainstream\nrelevant work by their architectures, training strategies, and application\nareas, detailing specifically the challenges of each and the corresponding\ncapabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss\ncurrent limitations and several promising directions for future research.\nUpdated information about this survey can be found at\nhttps://advanced-recommender-systems.github.io/RAG-Meets-LLMs/\n","authors":["Wenqi Fan","Yujuan Ding","Liangbo Ning","Shijie Wang","Hengyun Li","Dawei Yin","Tat-Seng Chua","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2405.06211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09979v1","updated":"2024-06-14T12:41:07Z","published":"2024-06-14T12:41:07Z","title":"HIRO: Hierarchical Information Retrieval Optimization","summary":"  Large Language Models (LLMs) excel in natural language tasks but face\nlimitations due to static training datasets, resulting in outdated or\ncontextually shallow responses. Retrieval-Augmented Generation (RAG) addresses\nthis by integrating real-time external knowledge, enhancing model accuracy and\ncredibility, especially for knowledge-intensive tasks. However, RAG-enhanced\nLLMs struggle with long contexts, causing them to \"choke\" on information\noverload, compromising response quality. Recent RAG applications use\nhierarchical data structures for storing documents, organized at various levels\nof summarization and information density. In this context, we introduce HIRO\n(Hierarchical Information Retrieval Optimization), a novel querying approach\nfor RAG applications using hierarchical structures for storing documents. HIRO\nemploys DFS-based recursive similarity score calculation and branch pruning to\nminimize the context returned to the LLM without informational loss. HIRO\noutperforms existing querying mechanisms on the NarrativeQA dataset by an\nabsolute performance gain of 10.85%.\n","authors":["Krish Goel","Mahek Chandak"],"pdf_url":"https://arxiv.org/pdf/2406.09979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15529v2","updated":"2024-06-14T11:19:26Z","published":"2024-03-22T17:31:43Z","title":"LimGen: Probing the LLMs for Generating Suggestive Limitations of\n  Research Papers","summary":"  Examining limitations is a crucial step in the scholarly research reviewing\nprocess, revealing aspects where a study might lack decisiveness or require\nenhancement. This aids readers in considering broader implications for further\nresearch. In this article, we present a novel and challenging task of\nSuggestive Limitation Generation (SLG) for research papers. We compile a\ndataset called \\textbf{\\textit{LimGen}}, encompassing 4068 research papers and\ntheir associated limitations from the ACL anthology. We investigate several\napproaches to harness large language models (LLMs) for producing suggestive\nlimitations, by thoroughly examining the related challenges, practical\ninsights, and potential opportunities. Our LimGen dataset and code can be\naccessed at \\url{https://github.com/arbmf/LimGen}.\n","authors":["Abdur Rahman Bin Md Faizullah","Ashok Urlana","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2403.15529v2.pdf","comment":"Accepted at ECML-PKDD 2024"},{"id":"http://arxiv.org/abs/2312.15489v2","updated":"2024-06-14T10:37:18Z","published":"2023-12-24T14:21:15Z","title":"Browsing behavior exposes identities on the Web","summary":"  How easy is it to uniquely identify a person based solely on their web\nbrowsing behavior? Here we show that when people navigate the Web, their online\ntraces produce fingerprints that identify them. Merely the four most visited\nweb domains are enough to identify 95% of the individuals. These digital\nfingerprints are stable and render high re-identifiability. We demonstrate that\nwe can re-identify 80% of the individuals in separate time slices of data. Such\na privacy threat persists even with limited information about individuals'\nbrowsing behavior, reinforcing existing concerns around online privacy.\n","authors":["Marcos Oliveira","Junran Yang","Daniel Griffiths","Denis Bonnay","Juhi Kulshrestha"],"pdf_url":"https://arxiv.org/pdf/2312.15489v2.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.09882v1","updated":"2024-06-14T09:52:47Z","published":"2024-06-14T09:52:47Z","title":"Harm Mitigation in Recommender Systems under User Preference Dynamics","summary":"  We consider a recommender system that takes into account the interplay\nbetween recommendations, the evolution of user interests, and harmful content.\nWe model the impact of recommendations on user behavior, particularly the\ntendency to consume harmful content. We seek recommendation policies that\nestablish a tradeoff between maximizing click-through rate (CTR) and mitigating\nharm. We establish conditions under which the user profile dynamics have a\nstationary point, and propose algorithms for finding an optimal recommendation\npolicy at stationarity. We experiment on a semi-synthetic movie recommendation\nsetting initialized with real data and observe that our policies outperform\nbaselines at simultaneously maximizing CTR and mitigating harm.\n","authors":["Jerry Chee","Shankar Kalyanaraman","Sindhu Kiranmai Ernala","Udi Weinsberg","Sarah Dean","Stratis Ioannidis"],"pdf_url":"https://arxiv.org/pdf/2406.09882v1.pdf","comment":"Recommender Systems; Harm Mitigation; Amplification; User Preference\n  Modeling"},{"id":"http://arxiv.org/abs/2406.09825v1","updated":"2024-06-14T08:29:34Z","published":"2024-06-14T08:29:34Z","title":"Unraveling Anomalies in Time: Unsupervised Discovery and Isolation of\n  Anomalous Behavior in Bio-regenerative Life Support System Telemetry","summary":"  The detection of abnormal or critical system states is essential in condition\nmonitoring. While much attention is given to promptly identifying anomalies, a\nretrospective analysis of these anomalies can significantly enhance our\ncomprehension of the underlying causes of observed undesired behavior. This\naspect becomes particularly critical when the monitored system is deployed in a\nvital environment. In this study, we delve into anomalies within the domain of\nBio-Regenerative Life Support Systems (BLSS) for space exploration and analyze\nanomalies found in telemetry data stemming from the EDEN ISS space greenhouse\nin Antarctica. We employ time series clustering on anomaly detection results to\ncategorize various types of anomalies in both uni- and multivariate settings.\nWe then assess the effectiveness of these methods in identifying systematic\nanomalous behavior. Additionally, we illustrate that the anomaly detection\nmethods MDI and DAMP produce complementary results, as previously indicated by\nresearch.\n","authors":["Ferdinand Rewicki","Jakob Gawlikowski","Julia Niebling","Joachim Denzler"],"pdf_url":"https://arxiv.org/pdf/2406.09825v1.pdf","comment":"12 pages, + Supplemental Materials, Accepted at ECML PKDD 2024\n  (European Conference on Machine Learning and Principles and Practice of\n  Knowledge Discovery in Databases)"},{"id":"http://arxiv.org/abs/2406.09818v1","updated":"2024-06-14T08:21:42Z","published":"2024-06-14T08:21:42Z","title":"ClimRetrieve: A Benchmarking Dataset for Information Retrieval from\n  Corporate Climate Disclosures","summary":"  To handle the vast amounts of qualitative data produced in corporate climate\ncommunication, stakeholders increasingly rely on Retrieval Augmented Generation\n(RAG) systems. However, a significant gap remains in evaluating domain-specific\ninformation retrieval - the basis for answer generation. To address this\nchallenge, this work simulates the typical tasks of a sustainability analyst by\nexamining 30 sustainability reports with 16 detailed climate-related questions.\nAs a result, we obtain a dataset with over 8.5K unique question-source-answer\npairs labeled by different levels of relevance. Furthermore, we develop a use\ncase with the dataset to investigate the integration of expert knowledge into\ninformation retrieval with embeddings. Although we show that incorporating\nexpert knowledge works, we also outline the critical limitations of embeddings\nin knowledge-intensive downstream domains like climate change communication.\n","authors":["Tobias Schimanski","Jingwei Ni","Roberto Spacey","Nicola Ranger","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2406.09818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09812v1","updated":"2024-06-14T08:10:44Z","published":"2024-06-14T08:10:44Z","title":"Soil nitrogen forecasting from environmental variables provided by\n  multisensor remote sensing images","summary":"  This study introduces a framework for forecasting soil nitrogen content,\nleveraging multi-modal data, including multi-sensor remote sensing images and\nadvanced machine learning methods. We integrate the Land Use/Land Cover Area\nFrame Survey (LUCAS) database, which covers European and UK territory, with\nenvironmental variables from satellite sensors to create a dataset of novel\nfeatures. We further test a broad range of machine learning algorithms,\nfocusing on tree-based models such as CatBoost, LightGBM, and XGBoost. We test\nthe proposed methods with a variety of land cover classes, including croplands\nand grasslands to ensure the robustness of this approach. Our results\ndemonstrate that the CatBoost model surpasses other methods in accuracy. This\nresearch advances the field of agricultural management and environmental\nmonitoring and demonstrates the significant potential of integrating\nmultisensor remote sensing data with machine learning for environmental\nanalysis.\n","authors":["Weiying Zhao","Ganzorig Chuluunbat","Aleksei Unagaev","Natalia Efremova"],"pdf_url":"https://arxiv.org/pdf/2406.09812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09742v1","updated":"2024-06-14T06:16:03Z","published":"2024-06-14T06:16:03Z","title":"IFA: Interaction Fidelity Attention for Entire Lifelong Behaviour\n  Sequence Modeling","summary":"  The lifelong user behavior sequence provides abundant information of user\npreference and gains impressive improvement in the recommendation task, however\nincreases computational consumption significantly. To meet the severe latency\nrequirement in online service, a short sub-sequence is sampled based on\nsimilarity to the target item. Unfortunately, items not in the sub-sequence are\nabandoned, leading to serious information loss.\n  In this paper, we propose a new efficient paradigm to model the full lifelong\nsequence, which is named as \\textbf{I}nteraction \\textbf{F}idelity\n\\textbf{A}ttention (\\textbf{IFA}). In IFA, we input all target items in the\ncandidate set into the model at once, and leverage linear transformer to reduce\nthe time complexity of the cross attention between the candidate set and the\nsequence without any interaction information loss. We also additionally model\nthe relationship of all target items for optimal set generation, and design\nloss function for better consistency of training and inference. We demonstrate\nthe effectiveness and efficiency of our model by off-line and online\nexperiments in the recommender system of Kuaishou.\n","authors":["Wenhui Yu","Chao Feng","Yanze Zhang","Lantao Hu","Peng Jiang","Han Li"],"pdf_url":"https://arxiv.org/pdf/2406.09742v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2405.10587v2","updated":"2024-06-14T05:07:32Z","published":"2024-05-17T07:22:02Z","title":"RDRec: Rationale Distillation for LLM-based Recommendation","summary":"  Large language model (LLM)-based recommender models that bridge users and\nitems through textual prompts for effective semantic reasoning have gained\nconsiderable attention. However, few methods consider the underlying rationales\nbehind interactions, such as user preferences and item attributes, limiting the\nreasoning capability of LLMs for recommendations. This paper proposes a\nrationale distillation recommender (RDRec), a compact model designed to learn\nrationales generated by a larger language model (LM). By leveraging rationales\nfrom reviews related to users and items, RDRec remarkably specifies their\nprofiles for recommendations. Experiments show that RDRec achieves\nstate-of-the-art (SOTA) performance in both top-N and sequential\nrecommendations. Our source code is released at\nhttps://github.com/WangXFng/RDRec.\n","authors":["Xinfeng Wang","Jin Cui","Yoshimi Suzuki","Fumiyo Fukumoto"],"pdf_url":"https://arxiv.org/pdf/2405.10587v2.pdf","comment":"10 pages. Accepted to ACL 2024 Main as a short paper"},{"id":"http://arxiv.org/abs/2406.09686v1","updated":"2024-06-14T03:13:58Z","published":"2024-06-14T03:13:58Z","title":"Enhancing Text Corpus Exploration with Post Hoc Explanations and\n  Comparative Design","summary":"  Text corpus exploration (TCE) spans the range of exploratory search tasks: it\ngoes beyond simple retrieval to include item discovery and learning about the\ncorpus and topic. Systems support TCE with tools such as similarity-based\nrecommendations and embedding-based spatial maps. However, these tools address\nspecific tasks; current systems lack the flexibility to support the range of\ntasks encountered in practice and the iterative, multiscale, workflows users\nemploy. In this paper, we provide methods that enhance TCE tools with post hoc\nexplanations and multiscale, comparative designs to provide flexible support\nfor user needs. We introduce salience functions as a mechanism to provide post\nhoc explanations of similarity, recommendations, and spatial placement. This\npost hoc strategy allows our approach to complement a variety of underlying\nalgorithms; the salience functions provide both exemplar- and feature-based\nexplanations at scales ranging from individual documents through to the entire\ncorpus. These explanations are incorporated into a set of views that operate at\nmultiple scales. The views use design elements that explicitly support\ncomparison to enable flexible integration. Together, these form an approach\nthat provides a flexible toolset that can address a range of tasks. We\ndemonstrate our approach in a prototype system that enables the exploration of\ncorpora of paper abstracts and newspaper archives. Examples illustrate how our\napproach enables the system to flexibly support a wide range of tasks and\nworkflows that emerge in user scenarios. A user study confirms that researchers\nare able to use our system to achieve a variety of tasks.\n","authors":["Michael Gleicher","Keaton Leppenan","Yunyu Bai"],"pdf_url":"https://arxiv.org/pdf/2406.09686v1.pdf","comment":"The system is available at:\n  https://pages.graphics.cs.wisc.edu/AbstractsViewer. The user guide (including\n  more examples) is at: https://pages.graphics.cs.wisc.edu/AbstractsViewerDocs/"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2406.10229v1","updated":"2024-06-14T17:59:54Z","published":"2024-06-14T17:59:54Z","title":"Quantifying Variance in Evaluation Benchmarks","summary":"  Evaluation benchmarks are the cornerstone of measuring capabilities of large\nlanguage models (LLMs), as well as driving progress in said capabilities.\nOriginally designed to make claims about capabilities (or lack thereof) in\nfully pretrained models, evaluation benchmarks are now also extensively used to\ndecide between various training choices. Despite this widespread usage, we\nrarely quantify the variance in our evaluation benchmarks, which dictates\nwhether differences in performance are meaningful. Here, we define and measure\na range of metrics geared towards measuring variance in evaluation benchmarks,\nincluding seed variance across initialisations, and monotonicity during\ntraining. By studying a large number of models -- both openly available and\npretrained from scratch -- we provide empirical estimates for a variety of\nvariance metrics, with considerations and recommendations for practitioners. We\nalso evaluate the utility and tradeoffs of continuous versus discrete\nperformance measures and explore options for better understanding and reducing\nthis variance. We find that simple changes, such as framing choice tasks (like\nMMLU) as completion tasks, can often reduce variance for smaller scale\n($\\sim$7B) models, while more involved methods inspired from human testing\nliterature (such as item analysis and item response theory) struggle to\nmeaningfully reduce variance. Overall, our work provides insights into variance\nin evaluation benchmarks, suggests LM-specific techniques to reduce variance,\nand more generally encourages practitioners to carefully factor in variance\nwhen comparing models.\n","authors":["Lovish Madaan","Aaditya K. Singh","Rylan Schaeffer","Andrew Poulton","Sanmi Koyejo","Pontus Stenetorp","Sharan Narang","Dieuwke Hupkes"],"pdf_url":"https://arxiv.org/pdf/2406.10229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08813v2","updated":"2024-06-14T17:59:34Z","published":"2024-05-14T17:59:02Z","title":"CinePile: A Long Video Question Answering Dataset and Benchmark","summary":"  Current datasets for long-form video understanding often fall short of\nproviding genuine long-form comprehension challenges, as many tasks derived\nfrom these datasets can be successfully tackled by analyzing just one or a few\nrandom frames from a video. To address this issue, we present a novel dataset\nand benchmark, CinePile, specifically designed for authentic long-form video\nunderstanding. This paper details our innovative approach for creating a\nquestion-answer dataset, utilizing advanced LLMs with human-in-the-loop and\nbuilding upon human-generated raw data. Our comprehensive dataset comprises\n305,000 multiple-choice questions (MCQs), covering various visual and\nmultimodal aspects, including temporal comprehension, understanding\nhuman-object interactions, and reasoning about events or actions within a\nscene. Additionally, we evaluate recent video-centric LLMs, both open-source\nand proprietary, on the test split of our dataset. The findings reveal that\neven state-of-the-art video-centric LLMs significantly lag behind human\nperformance in these tasks, highlighting the complexity and challenge inherent\nin video understanding. The dataset is available at\nhttps://hf.co/datasets/tomg-group-umd/cinepile\n","authors":["Ruchit Rawal","Khalid Saifullah","Ronen Basri","David Jacobs","Gowthami Somepalli","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2405.08813v2.pdf","comment":"Project page with all the artifacts -\n  https://ruchitrawal.github.io/cinepile/. Updated version with results on\n  Gemini Flash model and additional related work"},{"id":"http://arxiv.org/abs/2406.10223v1","updated":"2024-06-14T17:55:55Z","published":"2024-06-14T17:55:55Z","title":"Diffusion Synthesizer for Efficient Multilingual Speech to Speech\n  Translation","summary":"  We introduce DiffuseST, a low-latency, direct speech-to-speech translation\nsystem capable of preserving the input speaker's voice zero-shot while\ntranslating from multiple source languages into English. We experiment with the\nsynthesizer component of the architecture, comparing a Tacotron-based\nsynthesizer to a novel diffusion-based synthesizer. We find the diffusion-based\nsynthesizer to improve MOS and PESQ audio quality metrics by 23\\% each and\nspeaker similarity by 5\\% while maintaining comparable BLEU scores. Despite\nhaving more than double the parameter count, the diffusion synthesizer has\nlower latency, allowing the entire model to run more than 5$\\times$ faster than\nreal-time.\n","authors":["Nameer Hirschkind","Xiao Yu","Mahesh Kumar Nandwana","Joseph Liu","Eloi DuBois","Dao Le","Nicolas Thiebaut","Colin Sinclair","Kyle Spence","Charles Shang","Zoe Abrams","Morgan McGuire"],"pdf_url":"https://arxiv.org/pdf/2406.10223v1.pdf","comment":"Published in Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.10218v1","updated":"2024-06-14T17:53:50Z","published":"2024-06-14T17:53:50Z","title":"Semantic Membership Inference Attack against Large Language Models","summary":"  Membership Inference Attacks (MIAs) determine whether a specific data point\nwas included in the training set of a target model. In this paper, we introduce\nthe Semantic Membership Inference Attack (SMIA), a novel approach that enhances\nMIA performance by leveraging the semantic content of inputs and their\nperturbations. SMIA trains a neural network to analyze the target model's\nbehavior on perturbed inputs, effectively capturing variations in output\nprobability distributions between members and non-members. We conduct\ncomprehensive evaluations on the Pythia and GPT-Neo model families using the\nWikipedia dataset. Our results show that SMIA significantly outperforms\nexisting MIAs; for instance, SMIA achieves an AUC-ROC of 67.39% on Pythia-12B,\ncompared to 58.90% by the second-best attack.\n","authors":["Hamid Mozaffari","Virendra J. Marathe"],"pdf_url":"https://arxiv.org/pdf/2406.10218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10215v1","updated":"2024-06-14T17:49:41Z","published":"2024-06-14T17:49:41Z","title":"DevBench: A multimodal developmental benchmark for language learning","summary":"  How (dis)similar are the learning trajectories of vision-language models and\nchildren? Recent modeling work has attempted to understand the gap between\nmodels' and humans' data efficiency by constructing models trained on less\ndata, especially multimodal naturalistic data. However, such models are often\nevaluated on adult-level benchmarks, with limited breadth in language abilities\ntested, and without direct comparison to behavioral data. We introduce\nDevBench, a multimodal benchmark comprising seven language evaluation tasks\nspanning the domains of lexical, syntactic, and semantic ability, with\nbehavioral data from both children and adults. We evaluate a set of\nvision-language models on these tasks, comparing models and humans not only on\naccuracy but on their response patterns. Across tasks, models exhibit variation\nin their closeness to human response patterns, and models that perform better\non a task also more closely resemble human behavioral responses. We also\nexamine the developmental trajectory of OpenCLIP over training, finding that\ngreater training results in closer approximations to adult response patterns.\nDevBench thus provides a benchmark for comparing models to human language\ndevelopment. These comparisons highlight ways in which model and human language\nlearning processes diverge, providing insight into entry points for improving\nlanguage models.\n","authors":["Alvin Wei Ming Tan","Sunny Yu","Bria Long","Wanjing Anya Ma","Tonya Murray","Rebecca D. Silverman","Jason D. Yeatman","Michael C. Frank"],"pdf_url":"https://arxiv.org/pdf/2406.10215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10214v1","updated":"2024-06-14T17:49:29Z","published":"2024-06-14T17:49:29Z","title":"Universal randomised signatures for generative time series modelling","summary":"  Randomised signature has been proposed as a flexible and easily implementable\nalternative to the well-established path signature. In this article, we employ\nrandomised signature to introduce a generative model for financial time series\ndata in the spirit of reservoir computing. Specifically, we propose a novel\nWasserstein-type distance based on discrete-time randomised signatures. This\nmetric on the space of probability measures captures the distance between\n(conditional) distributions. Its use is justified by our novel universal\napproximation results for randomised signatures on the space of continuous\nfunctions taking the underlying path as an input. We then use our metric as the\nloss function in a non-adversarial generator model for synthetic time series\ndata based on a reservoir neural stochastic differential equation. We compare\nthe results of our model to benchmarks from the existing literature.\n","authors":["Francesca Biagini","Lukas Gonon","Niklas Walter"],"pdf_url":"https://arxiv.org/pdf/2406.10214v1.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2406.10213v1","updated":"2024-06-14T17:49:04Z","published":"2024-06-14T17:49:04Z","title":"Selecting Interpretability Techniques for Healthcare Machine Learning\n  models","summary":"  In healthcare there is a pursuit for employing interpretable algorithms to\nassist healthcare professionals in several decision scenarios. Following the\nPredictive, Descriptive and Relevant (PDR) framework, the definition of\ninterpretable machine learning as a machine-learning model that explicitly and\nin a simple frame determines relationships either contained in data or learned\nby the model that are relevant for its functioning and the categorization of\nmodels by post-hoc, acquiring interpretability after training, or model-based,\nbeing intrinsically embedded in the algorithm design. We overview a selection\nof eight algorithms, both post-hoc and model-based, that can be used for such\npurposes.\n","authors":["Daniel Sierra-Botero","Ana Molina-Taborda","Mario S. Valdés-Tresanco","Alejandro Hernández-Arango","Leonardo Espinosa-Leal","Alexander Karpenko","Olga Lopez-Acevedo"],"pdf_url":"https://arxiv.org/pdf/2406.10213v1.pdf","comment":"26 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.01299v2","updated":"2024-06-14T17:46:02Z","published":"2024-04-01T17:59:53Z","title":"CausalChaos! Dataset for Comprehensive Causal Action Question Answering\n  Over Longer Causal Chains Grounded in Dynamic Visual Scenes","summary":"  Causal video question answering (QA) has garnered increasing interest, yet\nexisting datasets often lack depth in causal reasoning. To address this gap, we\ncapitalize on the unique properties of cartoons and construct CausalChaos!, a\nnovel, challenging causal Why-QA dataset built upon the iconic \"Tom and Jerry\"\ncartoon series. Cartoons use the principles of animation that allow animators\nto create expressive, unambiguous causal relationships between events to form a\ncoherent storyline. Utilizing these properties, along with thought-provoking\nquestions and multi-level answers (answer and detailed causal explanation), our\nquestions involve causal chains that interconnect multiple dynamic interactions\nbetween characters and visual scenes. These factors demand models to solve more\nchallenging, yet well-defined causal relationships. We also introduce hard\nincorrect answer mining, including a causally confusing version that is even\nmore challenging. While models perform well, there is much room for\nimprovement, especially, on open-ended answers. We identify more\nadvanced/explicit causal relationship modeling & joint modeling of vision and\nlanguage as the immediate areas for future efforts to focus upon. Along with\nthe other complementary datasets, our new challenging dataset will pave the way\nfor these developments in the field.\n","authors":["Paritosh Parmar","Eric Peh","Ruirui Chen","Ting En Lam","Yuhan Chen","Elston Tan","Basura Fernando"],"pdf_url":"https://arxiv.org/pdf/2404.01299v2.pdf","comment":"Project Page: https://github.com/LUNAProject22/CausalChaos"},{"id":"http://arxiv.org/abs/2308.14929v2","updated":"2024-06-14T17:40:29Z","published":"2023-08-28T23:08:15Z","title":"Maestro: Uncovering Low-Rank Structures via Trainable Decomposition","summary":"  Deep Neural Networks (DNNs) have been a large driver for AI breakthroughs in\nrecent years. However, these models have been getting increasingly large as\nthey become more accurate and safe. This means that their training becomes\nincreasingly costly and time-consuming and typically yields a single model to\nfit all targets. Various techniques have been proposed in the literature to\nmitigate this, including pruning, sparsification, or quantization of model\nweights and updates. While achieving high compression rates, they often incur\nsignificant computational overheads at training or lead to non-negligible\naccuracy penalty. Alternatively, factorization methods have been leveraged for\nlow-rank compression of DNNs. Similarly, such techniques (e.g., SVD) frequently\nrely on heavy iterative decompositions of layers and are potentially\nsub-optimal for non-linear models, such as DNNs. We take a further step in\ndesigning efficient low-rank models and propose Maestro, a framework for\ntrainable low-rank layers. Instead of iteratively applying a priori\ndecompositions, the low-rank structure is baked into the training process\nthrough LoD, a low-rank ordered decomposition. Not only is this the first time\nimportance ordering via sampling is applied on the decomposed DNN structure,\nbut it also allows selecting ranks at a layer granularity. Our theoretical\nanalysis demonstrates that in special cases LoD recovers the SVD decomposition\nand PCA. Applied to DNNs, Maestro enables the extraction of lower footprint\nmodels that preserve performance. Simultaneously, it enables the graceful\ntrade-off between accuracy-latency for deployment to even more constrained\ndevices without retraining.\n","authors":["Samuel Horvath","Stefanos Laskaridis","Shashank Rajput","Hongyi Wang"],"pdf_url":"https://arxiv.org/pdf/2308.14929v2.pdf","comment":"Accepted at the 41st International Conference on Machine Learning\n  (ICML 2024)"},{"id":"http://arxiv.org/abs/2205.03703v2","updated":"2024-06-14T17:33:28Z","published":"2022-05-07T18:45:06Z","title":"Training from Zero: Radio Frequency Machine Learning Data Quantity\n  Forecasting","summary":"  The data used during training in any given application space is directly tied\nto the performance of the system once deployed. While there are many other\nfactors that go into producing high performance models within machine learning,\nthere is no doubt that the data used to train a system provides the foundation\nfrom which to build. One of the underlying rule of thumb heuristics used within\nthe machine learning space is that more data leads to better models, but there\nis no easy answer for the question, \"How much data is needed?\" This work\nexamines a modulation classification problem in the Radio Frequency domain\nspace, attempting to answer the question of how much training data is required\nto achieve a desired level of performance, but the procedure readily applies to\nclassification problems across modalities. The ultimate goal is determining an\napproach that requires the least amount of data collection to better inform a\nmore thorough collection effort to achieve the desired performance metric.\nWhile this approach will require an initial dataset that is germane to the\nproblem space to act as a \\textit{target} dataset on which metrics are\nextracted, the goal is to allow for the initial data to be orders of magnitude\nsmaller than what is required for delivering a system that achieves the desired\nperformance. An additional benefit of the techniques presented here is that the\nquality of different datasets can be numerically evaluated and tied together\nwith the quantity of data, and ultimately, the performance of the architecture\nin the problem domain.\n","authors":["William H. Clark IV","Alan J. Michaels"],"pdf_url":"https://arxiv.org/pdf/2205.03703v2.pdf","comment":"20 pages, 8 figures, submitted to MDPI Telecom"},{"id":"http://arxiv.org/abs/2402.19226v3","updated":"2024-06-14T17:32:32Z","published":"2024-02-29T14:58:15Z","title":"Investigating Gender Fairness in Machine Learning-driven Personalized\n  Care for Chronic Pain","summary":"  Chronic pain significantly diminishes the quality of life for millions\nworldwide. While psychoeducation and therapy can improve pain outcomes, many\nindividuals experiencing pain lack access to evidence-based treatments or fail\nto complete the necessary number of sessions to achieve benefit. Reinforcement\nlearning (RL) shows potential in tailoring personalized pain management\ninterventions according to patients' individual needs while ensuring the\nefficient use of scarce clinical resources. However, clinicians, patients, and\nhealthcare decision-makers are concerned that RL solutions could exacerbate\ndisparities associated with patient characteristics like race or gender. In\nthis article, we study gender fairness in personalized pain care\nrecommendations using a real-world application of reinforcement learning\n(Piette et al., 2022a). Here, adhering to gender fairness translates to minimal\nor no disparity in the utility received by subpopulations as defined by gender.\nWe investigate whether the selection of relevant patient information (referred\nto as features) used to assist decision-making affects gender fairness. Our\nexperiments, conducted using real-world data Piette, 2022), indicate that\nincluded features can impact gender fairness. Moreover, we propose an RL\nsolution, NestedRecommendation, that demonstrates the ability: i) to adaptively\nlearn to select the features that optimize for utility and fairness, and ii) to\naccelerate feature selection and in turn, improve pain care recommendations\nfrom early on, by leveraging clinicians' domain expertise.\n","authors":["Pratik Gajane","Sean Newman","Mykola Pechenizkiy","John D. Piette"],"pdf_url":"https://arxiv.org/pdf/2402.19226v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10197v1","updated":"2024-06-14T17:31:29Z","published":"2024-06-14T17:31:29Z","title":"Crafting Parts for Expressive Object Composition","summary":"  Text-to-image generation from large generative models like Stable Diffusion,\nDALLE-2, etc., have become a common base for various tasks due to their\nsuperior quality and extensive knowledge bases. As image composition and\ngeneration are creative processes the artists need control over various parts\nof the images being generated. We find that just adding details about parts in\nthe base text prompt either leads to an entirely different image (e.g.,\nmissing/incorrect identity) or the extra part details simply being ignored. To\nmitigate these issues, we introduce PartCraft, which enables image generation\nbased on fine-grained part-level details specified for objects in the base text\nprompt. This allows more control for artists and enables novel object\ncompositions by combining distinctive object parts. PartCraft first localizes\nobject parts by denoising the object region from a specific diffusion process.\nThis enables each part token to be localized to the right object region. After\nobtaining part masks, we run a localized diffusion process in each of the part\nregions based on fine-grained part descriptions and combine them to produce the\nfinal image. All the stages of PartCraft are based on repurposing a pre-trained\ndiffusion model, which enables it to generalize across various domains without\ntraining. We demonstrate the effectiveness of part-level control provided by\nPartCraft qualitatively through visual examples and quantitatively in\ncomparison to the contemporary baselines.\n","authors":["Harsh Rangwani","Aishwarya Agarwal","Kuldeep Kulkarni","R. Venkatesh Babu","Srikrishna Karanam"],"pdf_url":"https://arxiv.org/pdf/2406.10197v1.pdf","comment":"Project Page Will Be Here: https://rangwani-harsh.github.io/PartCraft"},{"id":"http://arxiv.org/abs/2310.03024v2","updated":"2024-06-14T17:19:58Z","published":"2023-10-04T17:59:38Z","title":"AstroCLIP: A Cross-Modal Foundation Model for Galaxies","summary":"  We present AstroCLIP, a single, versatile model that can embed both galaxy\nimages and spectra into a shared, physically meaningful latent space. These\nembeddings can then be used - without any model fine-tuning - for a variety of\ndownstream tasks including (1) accurate in-modality and cross-modality semantic\nsimilarity search, (2) photometric redshift estimation, (3) galaxy property\nestimation from both images and spectra, and (4) morphology classification. Our\napproach to implementing AstroCLIP consists of two parts. First, we embed\ngalaxy images and spectra separately by pretraining separate transformer-based\nimage and spectrum encoders in self-supervised settings. We then align the\nencoders using a contrastive loss. We apply our method to spectra from the Dark\nEnergy Spectroscopic Instrument and images from its corresponding Legacy\nImaging Survey. Overall, we find remarkable performance on all downstream\ntasks, even relative to supervised baselines. For example, for a task like\nphotometric redshift prediction, we find similar performance to a\nspecifically-trained ResNet18, and for additional tasks like physical property\nestimation (stellar mass, age, metallicity, and sSFR), we beat this supervised\nbaseline by 19\\% in terms of $R^2$. We also compare our results to a\nstate-of-the-art self-supervised single-modal model for galaxy images, and find\nthat our approach outperforms this benchmark by roughly a factor of two on\nphotometric redshift estimation and physical property prediction in terms of\n$R^2$, while remaining roughly in-line in terms of morphology classification.\nUltimately, our approach represents the first cross-modal self-supervised model\nfor galaxies, and the first self-supervised transformer-based architectures for\ngalaxy images and spectra.\n","authors":["Liam Parker","Francois Lanusse","Siavash Golkar","Leopoldo Sarra","Miles Cranmer","Alberto Bietti","Michael Eickenberg","Geraud Krawezik","Michael McCabe","Ruben Ohana","Mariel Pettee","Bruno Regaldo-Saint Blancard","Tiberiu Tesileanu","Kyunghyun Cho","Shirley Ho"],"pdf_url":"https://arxiv.org/pdf/2310.03024v2.pdf","comment":"18 pages, accepted in Monthly Notices of the Royal Astronomical\n  Society, Presented at the NeurIPS 2023 AI4Science Workshop"},{"id":"http://arxiv.org/abs/2402.09947v2","updated":"2024-06-14T17:18:11Z","published":"2024-02-15T13:50:00Z","title":"Explaining Probabilistic Models with Distributional Values","summary":"  A large branch of explainable machine learning is grounded in cooperative\ngame theory. However, research indicates that game-theoretic explanations may\nmislead or be hard to interpret. We argue that often there is a critical\nmismatch between what one wishes to explain (e.g. the output of a classifier)\nand what current methods such as SHAP explain (e.g. the scalar probability of a\nclass). This paper addresses such gap for probabilistic models by generalising\ncooperative games and value operators. We introduce the distributional values,\nrandom variables that track changes in the model output (e.g. flipping of the\npredicted class) and derive their analytic expressions for games with Gaussian,\nBernoulli and Categorical payoffs. We further establish several characterising\nproperties, and show that our framework provides fine-grained and insightful\nexplanations with case studies on vision and language models.\n","authors":["Luca Franceschi","Michele Donini","Cédric Archambeau","Matthias Seeger"],"pdf_url":"https://arxiv.org/pdf/2402.09947v2.pdf","comment":"ICML 2024 (spotlight paper). Code:\n  https://github.com/amazon-science/explaining-probabilistic-models-with-distributinal-values"},{"id":"http://arxiv.org/abs/2405.18732v3","updated":"2024-06-14T17:12:17Z","published":"2024-05-29T03:23:34Z","title":"Gemini & Physical World: Large Language Models Can Estimate the\n  Intensity of Earthquake Shaking from Multi-Modal Social Media Posts","summary":"  This paper presents a novel approach to extract scientifically valuable\ninformation about Earth's physical phenomena from unconventional sources, such\nas multi-modal social media posts. Employing a state-of-the-art large language\nmodel (LLM), Gemini 1.5 Pro (Reid et al. 2024), we estimate earthquake ground\nshaking intensity from these unstructured posts. The model's output, in the\nform of Modified Mercalli Intensity (MMI) values, aligns well with independent\nobservational data. Furthermore, our results suggest that LLMs, trained on vast\ninternet data, may have developed a unique understanding of physical phenomena.\nSpecifically, Google's Gemini models demonstrate a simplified understanding of\nthe general relationship between earthquake magnitude, distance, and MMI\nintensity, accurately describing observational data even though it's not\nidentical to established models. These findings raise intriguing questions\nabout the extent to which Gemini's training has led to a broader understanding\nof the physical world and its phenomena. The ability of Generative AI models\nlike Gemini to generate results consistent with established scientific\nknowledge highlights their potential to augment our understanding of complex\nphysical phenomena like earthquakes. The flexible and effective approach\nproposed in this study holds immense potential for enriching our understanding\nof the impact of physical phenomena and improving resilience during natural\ndisasters. This research is a significant step toward harnessing the power of\nsocial media and AI for natural disaster mitigation, opening new avenues for\nunderstanding the emerging capabilities of Generative AI and LLMs for\nscientific applications.\n","authors":["S. Mostafa Mousavi","Marc Stogaitis","Tajinder Gadh","Richard M Allen","Alexei Barski","Robert Bosch","Patrick Robertson","Nivetha Thiruverahan","Youngmin Cho","Aman Raj"],"pdf_url":"https://arxiv.org/pdf/2405.18732v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16325v2","updated":"2024-06-14T16:43:26Z","published":"2024-05-25T18:43:05Z","title":"SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of\n  LLMs","summary":"  We propose SLoPe, a Double-Pruned Sparse Plus Lazy Low-rank Adapter\nPretraining method for LLMs that improves the accuracy of sparse LLMs while\naccelerating their pretraining and inference and reducing their memory\nfootprint. Sparse pretraining of LLMs reduces the accuracy of the model, to\novercome this, prior work uses dense models during fine-tuning. SLoPe improves\nthe accuracy of sparsely pretrained models by adding low-rank adapters in the\nfinal 1% iterations of pretraining without adding significant overheads to the\nmodel pretraining and inference. In addition, SLoPe uses a double-pruned\nbackward pass formulation that prunes the transposed weight matrix using N:M\nsparsity structures to enable an accelerated sparse backward pass. SLoPe\naccelerates the training and inference of models with billions of parameters up\nto $1.14\\times$ and $1.34\\times$ respectively (OPT-33B and OPT-66B) while\nreducing their memory usage by up to $0.77\\times$ and $0.51\\times$ for training\nand inference respectively.\n","authors":["Mohammad Mozaffari","Amir Yazdanbakhsh","Zhao Zhang","Maryam Mehri Dehnavi"],"pdf_url":"https://arxiv.org/pdf/2405.16325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.21070v2","updated":"2024-06-14T16:42:47Z","published":"2024-05-31T17:57:24Z","title":"Generalization Beyond Data Imbalance: A Controlled Study on CLIP for\n  Transferable Insights","summary":"  Severe data imbalance naturally exists among web-scale vision-language\ndatasets. Despite this, we find CLIP pre-trained thereupon exhibits notable\nrobustness to the data imbalance compared to supervised learning, and\ndemonstrates significant effectiveness in learning generalizable\nrepresentations. With an aim to investigate the reasons behind this finding, we\nconduct controlled experiments to study various underlying factors, and reveal\nthat CLIP's pretext task forms a dynamic classification problem wherein only a\nsubset of classes is present in training. This isolates the bias from dominant\nclasses and implicitly balances the learning signal. Furthermore, the\nrobustness and discriminability of CLIP improve with more descriptive language\nsupervision, larger data scale, and broader open-world concepts, which are\ninaccessible to supervised learning. Our study not only uncovers the mechanisms\nbehind CLIP's generalizability beyond data imbalance but also provides\ntransferable insights for the research community. The findings are validated in\nboth supervised and self-supervised learning, enabling models trained on\nimbalanced data to achieve CLIP-level performance on diverse recognition tasks.\nCode and data are available at: https://github.com/CVMI-Lab/clip-beyond-tail.\n","authors":["Xin Wen","Bingchen Zhao","Yilun Chen","Jiangmiao Pang","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2405.21070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10166v1","updated":"2024-06-14T16:36:35Z","published":"2024-06-14T16:36:35Z","title":"Misam: Using ML in Dataflow Selection of Sparse-Sparse Matrix\n  Multiplication","summary":"  Sparse matrix-matrix multiplication (SpGEMM) is a critical operation in\nnumerous fields, including scientific computing, graph analytics, and deep\nlearning. These applications exploit the sparsity of matrices to reduce storage\nand computational demands. However, the irregular structure of sparse matrices\nposes significant challenges for performance optimization. Traditional hardware\naccelerators are tailored for specific sparsity patterns with fixed dataflow\nschemes - inner, outer, and row-wise but often perform suboptimally when the\nactual sparsity deviates from these predetermined patterns. As the use of\nSpGEMM expands across various domains, each with distinct sparsity\ncharacteristics, the demand for hardware accelerators that can efficiently\nhandle a range of sparsity patterns is increasing. This paper presents a\nmachine learning based approach for adaptively selecting the most appropriate\ndataflow scheme for SpGEMM tasks with diverse sparsity patterns. By employing\ndecision trees and deep reinforcement learning, we explore the potential of\nthese techniques to surpass heuristic-based methods in identifying optimal\ndataflow schemes. We evaluate our models by comparing their performance with\nthat of a heuristic, highlighting the strengths and weaknesses of each\napproach. Our findings suggest that using machine learning for dynamic dataflow\nselection in hardware accelerators can provide upto 28 times gains.\n","authors":["Sanjali Yadav","Bahar Asgari"],"pdf_url":"https://arxiv.org/pdf/2406.10166v1.pdf","comment":"Accepted to ISCA 2024 MLArchSys workshop\n  https://openreview.net/forum?id=A1V9FaZRbV"},{"id":"http://arxiv.org/abs/2309.06908v2","updated":"2024-06-14T16:27:24Z","published":"2023-09-13T12:10:54Z","title":"Towards the TopMost: A Topic Modeling System Toolkit","summary":"  Topic models have a rich history with various applications and have recently\nbeen reinvigorated by neural topic modeling. However, these numerous topic\nmodels adopt totally distinct datasets, implementations, and evaluations. This\nimpedes quick utilization and fair comparisons, and thereby hinders their\nresearch progress and applications. To tackle this challenge, we in this paper\npropose a Topic Modeling System Toolkit (TopMost). Compared to existing\ntoolkits, TopMost stands out by supporting more extensive features. It covers a\nbroader spectrum of topic modeling scenarios with their complete lifecycles,\nincluding datasets, preprocessing, models, training, and evaluations. Thanks to\nits highly cohesive and decoupled modular design, TopMost enables rapid\nutilization, fair comparisons, and flexible extensions of diverse cutting-edge\ntopic models. Our code, tutorials, and documentation are available at\nhttps://github.com/bobxwu/topmost.\n","authors":["Xiaobao Wu","Fengjun Pan","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2309.06908v2.pdf","comment":"Accepted to ACL 2024 System Demonstrations Track"},{"id":"http://arxiv.org/abs/2310.14533v2","updated":"2024-06-14T16:21:51Z","published":"2023-10-23T03:36:35Z","title":"Context-Aware Prediction of User Engagement on Online Social Platforms","summary":"  The success of online social platforms hinges on their ability to predict and\nunderstand user behavior at scale. Here, we present data suggesting that\ncontext-aware modeling approaches may offer a holistic yet lightweight and\npotentially privacy-preserving representation of user engagement on online\nsocial platforms. Leveraging deep LSTM neural networks to analyze more than 100\nmillion Snapchat sessions from almost 80.000 users, we demonstrate that\npatterns of active and passive use are predictable from past behavior\n(R2=0.345) and that the integration of context features substantially improves\npredictive performance compared to the behavioral baseline model (R2=0.522).\nFeatures related to smartphone connectivity status, location, temporal context,\nand weather were found to capture non-redundant variance in user engagement\nrelative to features derived from histories of in-app behaviors. Further, we\nshow that a large proportion of variance can be accounted for with minimal\nbehavioral histories if momentary context is considered (R2=0.442). These\nresults indicate the potential of context-aware approaches for making models\nmore efficient and privacy-preserving by reducing the need for long data\nhistories. Finally, we employ model explainability techniques to glean\npreliminary insights into the underlying behavioral mechanisms. Our findings\nare consistent with the notion of context-contingent, habit-driven patterns of\nactive and passive use, underscoring the value of contextualized\nrepresentations of user behavior for predicting user engagement on social\nplatforms.\n","authors":["Heinrich Peters","Yozen Liu","Francesco Barbieri","Raiyan Abdul Baten","Sandra C. Matz","Maarten W. Bos"],"pdf_url":"https://arxiv.org/pdf/2310.14533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12970v2","updated":"2024-06-14T16:21:39Z","published":"2023-08-24T17:59:54Z","title":"NeuralClothSim: Neural Deformation Fields Meet the Thin Shell Theory","summary":"  Despite existing 3D cloth simulators producing realistic results, they\npredominantly operate on discrete surface representations (e.g. points and\nmeshes) with a fixed spatial resolution, which often leads to large memory\nconsumption and resolution-dependent simulations. Moreover, back-propagating\ngradients through the existing solvers is difficult, and they cannot be easily\nintegrated into modern neural architectures. In response, this paper re-thinks\nphysically plausible cloth simulation: We propose NeuralClothSim, i.e., a new\nquasistatic cloth simulator using thin shells, in which surface deformation is\nencoded in neural network weights in the form of a neural field. Our\nmemory-efficient solver operates on a new continuous coordinate-based surface\nrepresentation called neural deformation fields (NDFs); it supervises NDF\nequilibria with the laws of the non-linear Kirchhoff-Love shell theory with a\nnon-linear anisotropic material model. NDFs are adaptive: They 1) allocate\ntheir capacity to the deformation details and 2) allow surface state queries at\narbitrary spatial resolutions without re-training. We show how to train\nNeuralClothSim while imposing hard boundary conditions and demonstrate multiple\napplications, such as material interpolation and simulation editing. The\nexperimental results highlight the effectiveness of our continuous neural\nformulation.\n","authors":["Navami Kairanda","Marc Habermann","Christian Theobalt","Vladislav Golyanik"],"pdf_url":"https://arxiv.org/pdf/2308.12970v2.pdf","comment":"33 pages, 21 figures and 3 tables; project page:\n  https://4dqv.mpi-inf.mpg.de/NeuralClothSim/"},{"id":"http://arxiv.org/abs/2406.10161v1","updated":"2024-06-14T16:20:04Z","published":"2024-06-14T16:20:04Z","title":"On the Computability of Robust PAC Learning","summary":"  We initiate the study of computability requirements for adversarially robust\nlearning. Adversarially robust PAC-type learnability is by now an established\nfield of research. However, the effects of computability requirements in\nPAC-type frameworks are only just starting to emerge. We introduce the problem\nof robust computable PAC (robust CPAC) learning and provide some simple\nsufficient conditions for this. We then show that learnability in this setup is\nnot implied by the combination of its components: classes that are both CPAC\nand robustly PAC learnable are not necessarily robustly CPAC learnable.\nFurthermore, we show that the novel framework exhibits some surprising effects:\nfor robust CPAC learnability it is not required that the robust loss is\ncomputably evaluable! Towards understanding characterizing properties, we\nintroduce a novel dimension, the computable robust shattering dimension. We\nprove that its finiteness is necessary, but not sufficient for robust CPAC\nlearnability. This might yield novel insights for the corresponding phenomenon\nin the context of robust PAC learnability, where insufficiency of the robust\nshattering dimension for learnability has been conjectured, but so far a\nresolution has remained elusive.\n","authors":["Pascale Gourdeau","Tosca Lechner","Ruth Urner"],"pdf_url":"https://arxiv.org/pdf/2406.10161v1.pdf","comment":"To appear in Conference on Learning Theory (COLT) 2024"},{"id":"http://arxiv.org/abs/2406.10154v1","updated":"2024-06-14T16:16:26Z","published":"2024-06-14T16:16:26Z","title":"Automated Design of Linear Bounding Functions for Sigmoidal\n  Nonlinearities in Neural Networks","summary":"  The ubiquity of deep learning algorithms in various applications has\namplified the need for assuring their robustness against small input\nperturbations such as those occurring in adversarial attacks. Existing complete\nverification techniques offer provable guarantees for all robustness queries\nbut struggle to scale beyond small neural networks. To overcome this\ncomputational intractability, incomplete verification methods often rely on\nconvex relaxation to over-approximate the nonlinearities in neural networks.\nProgress in tighter approximations has been achieved for piecewise linear\nfunctions. However, robustness verification of neural networks for general\nactivation functions (e.g., Sigmoid, Tanh) remains under-explored and poses new\nchallenges. Typically, these networks are verified using convex relaxation\ntechniques, which involve computing linear upper and lower bounds of the\nnonlinear activation functions. In this work, we propose a novel parameter\nsearch method to improve the quality of these linear approximations.\nSpecifically, we show that using a simple search method, carefully adapted to\nthe given verification problem through state-of-the-art algorithm configuration\ntechniques, improves the average global lower bound by 25% on average over the\ncurrent state of the art on several commonly used local robustness verification\nbenchmarks.\n","authors":["Matthias König","Xiyue Zhang","Holger H. Hoos","Marta Kwiatkowska","Jan N. van Rijn"],"pdf_url":"https://arxiv.org/pdf/2406.10154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02804v2","updated":"2024-06-14T16:10:33Z","published":"2023-12-05T14:44:58Z","title":"Score-Aware Policy-Gradient Methods and Performance Guarantees using\n  Local Lyapunov Conditions: Applications to Product-Form Stochastic Networks\n  and Queueing Systems","summary":"  In this paper, we introduce a policy-gradient method for model-based\nreinforcement learning (RL) that exploits a type of stationary distributions\ncommonly obtained from Markov decision processes (MDPs) in stochastic networks,\nqueueing systems, and statistical mechanics. Specifically, when the stationary\ndistribution of the MDP belongs to an exponential family that is parametrized\nby policy parameters, we can improve existing policy gradient methods for\naverage-reward RL. Our key identification is a family of gradient estimators,\ncalled score-aware gradient estimators (SAGEs), that enable policy gradient\nestimation without relying on value-function approximation in the\naforementioned setting. This contrasts with other common policy-gradient\nalgorithms such as actor-critic methods. We first show that policy-gradient\nwith SAGE locally converges, including in cases when the objective function is\nnonconvex, presents multiple maximizers, and the state space of the MDP is not\nfinite. Under appropriate assumptions such as starting sufficiently close to a\nmaximizer, the policy under stochastic gradient ascent with SAGE has an\noverwhelming probability of converging to the associated optimal policy. Other\nkey assumptions are that a local Lyapunov function exists, and a nondegeneracy\nproperty of the Hessian of the objective function holds locally around a\nmaximizer. Furthermore, we conduct a numerical comparison between a SAGE-based\npolicy-gradient method and an actor-critic method. We specifically focus on\nseveral examples inspired from stochastic networks, queueing systems, and\nmodels derived from statistical physics, where parametrizable exponential\nfamilies are commonplace. Our results demonstrate that a SAGE-based method\nfinds close-to-optimal policies faster than an actor-critic method.\n","authors":["Céline Comte","Matthieu Jonckheere","Jaron Sanders","Albert Senen-Cerda"],"pdf_url":"https://arxiv.org/pdf/2312.02804v2.pdf","comment":"60 pages, 5 figures. Extended numerical results in section 6 and\n  included sample complexity in section 5"},{"id":"http://arxiv.org/abs/2402.02287v4","updated":"2024-06-14T15:54:12Z","published":"2024-02-03T22:55:31Z","title":"Future Directions in the Theory of Graph Machine Learning","summary":"  Machine learning on graphs, especially using graph neural networks (GNNs),\nhas seen a surge in interest due to the wide availability of graph data across\na broad spectrum of disciplines, from life to social and engineering sciences.\nDespite their practical success, our theoretical understanding of the\nproperties of GNNs remains highly incomplete. Recent theoretical advancements\nprimarily focus on elucidating the coarse-grained expressive power of GNNs,\npredominantly employing combinatorial techniques. However, these studies do not\nperfectly align with practice, particularly in understanding the generalization\nbehavior of GNNs when trained with stochastic first-order optimization\ntechniques. In this position paper, we argue that the graph machine learning\ncommunity needs to shift its attention to developing a balanced theory of graph\nmachine learning, focusing on a more thorough understanding of the interplay of\nexpressive power, generalization, and optimization.\n","authors":["Christopher Morris","Fabrizio Frasca","Nadav Dym","Haggai Maron","İsmail İlkan Ceylan","Ron Levie","Derek Lim","Michael Bronstein","Martin Grohe","Stefanie Jegelka"],"pdf_url":"https://arxiv.org/pdf/2402.02287v4.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2406.10137v1","updated":"2024-06-14T15:47:13Z","published":"2024-06-14T15:47:13Z","title":"Compressed Sensor Caching and Collaborative Sparse Data Recovery with\n  Anchor Alignment","summary":"  This work examines the compressed sensor caching problem in wireless sensor\nnetworks and devises efficient distributed sparse data recovery algorithms to\nenable collaboration among multiple caches. In this problem, each cache is only\nallowed to access measurements from a small subset of sensors within its\nvicinity to reduce both cache size and data acquisition overhead. To enable\nreliable data recovery with limited access to measurements, we propose a\ndistributed sparse data recovery method, called the collaborative sparse\nrecovery by anchor alignment (CoSR-AA) algorithm, where collaboration among\ncaches is enabled by aligning their locally recovered data at a few anchor\nnodes. The proposed algorithm is based on the consensus alternating direction\nmethod of multipliers (ADMM) algorithm but with message exchange that is\nreduced by considering the proposed anchor alignment strategy. Then, by the\ndeep unfolding of the ADMM iterations, we further propose the Deep CoSR-AA\nalgorithm that can be used to significantly reduce the number of iterations. We\nobtain a graph neural network architecture where message exchange is done more\nefficiently by an embedded autoencoder. Simulations are provided to demonstrate\nthe effectiveness of the proposed collaborative recovery algorithms in terms of\nthe improved reconstruction quality and the reduced communication overhead due\nto anchor alignment.\n","authors":["Yi-Jen Yang","Ming-Hsun Yang","Jwo-Yuh Wu","Y. -W. Peter Hong"],"pdf_url":"https://arxiv.org/pdf/2406.10137v1.pdf","comment":"v1 was submitted to IEEE Transactions on Signal Processing on Sept.\n  18, 2023"},{"id":"http://arxiv.org/abs/2305.11616v4","updated":"2024-06-14T15:46:55Z","published":"2023-05-19T11:47:51Z","title":"Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD\n  Detection, Calibration, and Accuracy","summary":"  Deep ensembles are capable of achieving state-of-the-art results in\nclassification and out-of-distribution (OOD) detection. However, their\neffectiveness is limited due to the homogeneity of learned patterns within\nensembles. To overcome this issue, our study introduces Saliency Diversified\nDeep Ensemble (SDDE), a novel approach that promotes diversity among ensemble\nmembers by leveraging saliency maps. Through incorporating saliency map\ndiversification, our method outperforms conventional ensemble techniques and\nimproves calibration in multiple classification and OOD detection tasks. In\nparticular, the proposed method achieves state-of-the-art OOD detection\nquality, calibration, and accuracy on multiple benchmarks, including\nCIFAR10/100 and large-scale ImageNet datasets.\n","authors":["Stanislav Dereka","Ivan Karpukhin","Maksim Zhdanov","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2305.11616v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07200v3","updated":"2024-06-14T15:46:11Z","published":"2024-05-12T07:55:43Z","title":"Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient\n  Architecture for Nonlinear Function Approximation","summary":"  Accurate approximation of complex nonlinear functions is a fundamental\nchallenge across many scientific and engineering domains. Traditional neural\nnetwork architectures, such as Multi-Layer Perceptrons (MLPs), often struggle\nto efficiently capture intricate patterns and irregularities present in\nhigh-dimensional functions. This paper presents the Chebyshev Kolmogorov-Arnold\nNetwork (Chebyshev KAN), a new neural network architecture inspired by the\nKolmogorov-Arnold representation theorem, incorporating the powerful\napproximation capabilities of Chebyshev polynomials. By utilizing learnable\nfunctions parametrized by Chebyshev polynomials on the network's edges,\nChebyshev KANs enhance flexibility, efficiency, and interpretability in\nfunction approximation tasks. We demonstrate the efficacy of Chebyshev KANs\nthrough experiments on digit classification, synthetic function approximation,\nand fractal function generation, highlighting their superiority over\ntraditional MLPs in terms of parameter efficiency and interpretability. Our\ncomprehensive evaluation, including ablation studies, confirms the potential of\nChebyshev KANs to address longstanding challenges in nonlinear function\napproximation, paving the way for further advancements in various scientific\nand engineering applications.\n","authors":["Sidharth SS","Keerthana AR","Gokul R","Anas KP"],"pdf_url":"https://arxiv.org/pdf/2405.07200v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10131v1","updated":"2024-06-14T15:41:21Z","published":"2024-06-14T15:41:21Z","title":"Linear Contextual Bandits with Hybrid Payoff: Revisited","summary":"  We study the Linear Contextual Bandit problem in the hybrid reward setting.\nIn this setting every arm's reward model contains arm specific parameters in\naddition to parameters shared across the reward models of all the arms. We can\nreduce this setting to two closely related settings (a) Shared - no arm\nspecific parameters, and (b) Disjoint - only arm specific parameters, enabling\nthe application of two popular state of the art algorithms - $\\texttt{LinUCB}$\nand $\\texttt{DisLinUCB}$ (Algorithm 1 in (Li et al. 2010)). When the arm\nfeatures are stochastic and satisfy a popular diversity condition, we provide\nnew regret analyses for both algorithms, significantly improving on the known\nregret guarantees of these algorithms. Our novel analysis critically exploits\nthe hybrid reward structure and the diversity condition. Moreover, we introduce\na new algorithm $\\texttt{HyLinUCB}$ that crucially modifies $\\texttt{LinUCB}$\n(using a new exploration coefficient) to account for sparsity in the hybrid\nsetting. Under the same diversity assumptions, we prove that\n$\\texttt{HyLinUCB}$ also incurs only $O(\\sqrt{T})$ regret for $T$ rounds. We\nperform extensive experiments on synthetic and real-world datasets\ndemonstrating strong empirical performance of $\\texttt{HyLinUCB}$.For number of\narm specific parameters much larger than the number of shared parameters, we\nobserve that $\\texttt{DisLinUCB}$ incurs the lowest regret. In this case,\nregret of $\\texttt{HyLinUCB}$ is the second best and extremely competitive to\n$\\texttt{DisLinUCB}$. In all other situations, including our real-world\ndataset, $\\texttt{HyLinUCB}$ has significantly lower regret than\n$\\texttt{LinUCB}$, $\\texttt{DisLinUCB}$ and other SOTA baselines we considered.\nWe also empirically observe that the regret of $\\texttt{HyLinUCB}$ grows much\nslower with the number of arms compared to baselines, making it suitable even\nfor very large action spaces.\n","authors":["Nirjhar Das","Gaurav Sinha"],"pdf_url":"https://arxiv.org/pdf/2406.10131v1.pdf","comment":"Accepted at ECML PKDD 2024 as a Research Track Paper"},{"id":"http://arxiv.org/abs/2406.08310v2","updated":"2024-06-14T15:36:00Z","published":"2024-06-12T15:10:44Z","title":"GraphFM: A Comprehensive Benchmark for Graph Foundation Model","summary":"  Foundation Models (FMs) serve as a general class for the development of\nartificial intelligence systems, offering broad potential for generalization\nacross a spectrum of downstream tasks. Despite extensive research into\nself-supervised learning as the cornerstone of FMs, several outstanding issues\npersist in Graph Foundation Models that rely on graph self-supervised learning,\nnamely: 1) Homogenization. The extent of generalization capability on\ndownstream tasks remains unclear. 2) Scalability. It is unknown how effectively\nthese models can scale to large datasets. 3) Efficiency. The training time and\nmemory usage of these models require evaluation. 4) Training Stop Criteria.\nDetermining the optimal stopping strategy for pre-training across multiple\ntasks to maximize performance on downstream tasks. To address these questions,\nwe have constructed a rigorous benchmark that thoroughly analyzes and studies\nthe generalization and scalability of self-supervised Graph Neural Network\n(GNN) models. Regarding generalization, we have implemented and compared the\nperformance of various self-supervised GNN models, trained to generate node\nrepresentations, across tasks such as node classification, link prediction, and\nnode clustering. For scalability, we have compared the performance of various\nmodels after training using full-batch and mini-batch strategies. Additionally,\nwe have assessed the training efficiency of these models by conducting\nexperiments to test their GPU memory usage and throughput. Through these\nexperiments, we aim to provide insights to motivate future research. The code\nfor this benchmark is publicly available at https://github.com/NYUSHCS/GraphFM.\n","authors":["Yuhao Xu","Xinqi Liu","Keyu Duan","Yi Fang","Yu-Neng Chuang","Daochen Zha","Qiaoyu Tan"],"pdf_url":"https://arxiv.org/pdf/2406.08310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10117v1","updated":"2024-06-14T15:23:27Z","published":"2024-06-14T15:23:27Z","title":"Trustworthy Artificial Intelligence in the Context of Metrology","summary":"  We review research at the National Physical Laboratory (NPL) in the area of\ntrustworthy artificial intelligence (TAI), and more specifically trustworthy\nmachine learning (TML), in the context of metrology, the science of\nmeasurement. We describe three broad themes of TAI: technical, socio-technical\nand social, which play key roles in ensuring that the developed models are\ntrustworthy and can be relied upon to make responsible decisions. From a\nmetrology perspective we emphasise uncertainty quantification (UQ), and its\nimportance within the framework of TAI to enhance transparency and trust in the\noutputs of AI systems. We then discuss three research areas within TAI that we\nare working on at NPL, and examine the certification of AI systems in terms of\nadherence to the characteristics of TAI.\n","authors":["Tameem Adel","Sam Bilson","Mark Levene","Andrew Thompson"],"pdf_url":"https://arxiv.org/pdf/2406.10117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10115v1","updated":"2024-06-14T15:21:57Z","published":"2024-06-14T15:21:57Z","title":"Shelf-Supervised Multi-Modal Pre-Training for 3D Object Detection","summary":"  State-of-the-art 3D object detectors are often trained on massive labeled\ndatasets. However, annotating 3D bounding boxes remains prohibitively expensive\nand time-consuming, particularly for LiDAR. Instead, recent works demonstrate\nthat self-supervised pre-training with unlabeled data can improve detection\naccuracy with limited labels. Contemporary methods adapt best-practices for\nself-supervised learning from the image domain to point clouds (such as\ncontrastive learning). However, publicly available 3D datasets are considerably\nsmaller and less diverse than those used for image-based self-supervised\nlearning, limiting their effectiveness. We do note, however, that such data is\nnaturally collected in a multimodal fashion, often paired with images. Rather\nthan pre-training with only self-supervised objectives, we argue that it is\nbetter to bootstrap point cloud representations using image-based foundation\nmodels trained on internet-scale image data. Specifically, we propose a\nshelf-supervised approach (e.g. supervised with off-the-shelf image foundation\nmodels) for generating zero-shot 3D bounding boxes from paired RGB and LiDAR\ndata. Pre-training 3D detectors with such pseudo-labels yields significantly\nbetter semi-supervised detection accuracy than prior self-supervised pretext\ntasks. Importantly, we show that image-based shelf-supervision is helpful for\ntraining LiDAR-only and multi-modal (RGB + LiDAR) detectors. We demonstrate the\neffectiveness of our approach on nuScenes and WOD, significantly improving over\nprior work in limited data settings.\n","authors":["Mehar Khurana","Neehar Peri","Deva Ramanan","James Hays"],"pdf_url":"https://arxiv.org/pdf/2406.10115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11436v2","updated":"2024-06-14T15:17:20Z","published":"2023-07-21T08:57:16Z","title":"Neural Operators for PDE Backstepping Control of First-Order Hyperbolic\n  PIDE with Recycle and Delay","summary":"  The recently introduced DeepONet operator-learning framework for PDE control\nis extended from the results for basic hyperbolic and parabolic PDEs to an\nadvanced hyperbolic class that involves delays on both the state and the system\noutput or input. The PDE backstepping design produces gain functions that are\noutputs of a nonlinear operator, mapping functions on a spatial domain into\nfunctions on a spatial domain, and where this gain-generating operator's inputs\nare the PDE's coefficients. The operator is approximated with a DeepONet neural\nnetwork to a degree of accuracy that is provably arbitrarily tight. Once we\nproduce this approximation-theoretic result in infinite dimension, with it we\nestablish stability in closed loop under feedback that employs approximate\ngains. In addition to supplying such results under full-state feedback, we also\ndevelop DeepONet-approximated observers and output-feedback laws and prove\ntheir own stabilizing properties under neural operator approximations. With\nnumerical simulations we illustrate the theoretical results and quantify the\nnumerical effort savings, which are of two orders of magnitude, thanks to\nreplacing the numerical PDE solving with the DeepONet.\n","authors":["Jie Qi","Jing Zhang","Miroslav Krstic"],"pdf_url":"https://arxiv.org/pdf/2307.11436v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2406.10108v1","updated":"2024-06-14T15:12:53Z","published":"2024-06-14T15:12:53Z","title":"Precipitation Nowcasting Using Physics Informed Discriminator Generative\n  Models","summary":"  Nowcasting leverages real-time atmospheric conditions to forecast weather\nover short periods. State-of-the-art models, including PySTEPS, encounter\ndifficulties in accurately forecasting extreme weather events because of their\nunpredictable distribution patterns. In this study, we design a\nphysics-informed neural network to perform precipitation nowcasting using the\nprecipitation and meteorological data from the Royal Netherlands Meteorological\nInstitute (KNMI). This model draws inspiration from the novel Physics-Informed\nDiscriminator GAN (PID-GAN) formulation, directly integrating physics-based\nsupervision within the adversarial learning framework. The proposed model\nadopts a GAN structure, featuring a Vector Quantization Generative Adversarial\nNetwork (VQ-GAN) and a Transformer as the generator, with a temporal\ndiscriminator serving as the discriminator. Our findings demonstrate that the\nPID-GAN model outperforms numerical and SOTA deep generative models in terms of\nprecipitation nowcasting downstream metrics.\n","authors":["Junzhe Yin","Cristian Meo","Ankush Roy","Zeineh Bou Cher","Yanbo Wang","Ruben Imhoff","Remko Uijlenhoet","Justin Dauwels"],"pdf_url":"https://arxiv.org/pdf/2406.10108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10098v1","updated":"2024-06-14T14:55:53Z","published":"2024-06-14T14:55:53Z","title":"ECGMamba: Towards Efficient ECG Classification with BiSSM","summary":"  Electrocardiogram (ECG) signal analysis represents a pivotal technique in the\ndiagnosis of cardiovascular diseases. Although transformer-based models have\nmade significant progress in ECG classification, they exhibit inefficiencies in\nthe inference phase. The issue is primarily attributable to the secondary\ncomputational complexity of Transformer's self-attention mechanism.\nparticularly when processing lengthy sequences. To address this issue, we\npropose a novel model, ECGMamba, which employs a bidirectional state-space\nmodel (BiSSM) to enhance classification efficiency. ECGMamba is based on the\ninnovative Mamba-based block, which incorporates a range of time series\nmodeling techniques to enhance performance while maintaining the efficiency of\ninference. The experimental results on two publicly available ECG datasets\ndemonstrate that ECGMamba effectively balances the effectiveness and efficiency\nof classification, achieving competitive performance. This study not only\ncontributes to the body of knowledge in the field of ECG classification but\nalso provides a new research path for efficient and accurate ECG signal\nanalysis. This is of guiding significance for the development of diagnostic\nmodels for cardiovascular diseases.\n","authors":["Yupeng Qiang","Xunde Dong","Xiuling Liu","Yang Yang","Yihai Fang","Jianhong Dou"],"pdf_url":"https://arxiv.org/pdf/2406.10098v1.pdf","comment":"6 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.17858 by other authors"},{"id":"http://arxiv.org/abs/2401.17626v2","updated":"2024-06-14T14:49:12Z","published":"2024-01-31T06:58:26Z","title":"Generative AI to Generate Test Data Generators","summary":"  Generating fake data is an essential dimension of modern software testing, as\ndemonstrated by the number and significance of data faking libraries. Yet,\ndevelopers of faking libraries cannot keep up with the wide range of data to be\ngenerated for different natural languages and domains. In this paper, we assess\nthe ability of generative AI for generating test data in different domains. We\ndesign three types of prompts for Large Language Models (LLMs), which perform\ntest data generation tasks at different levels of integrability: 1) raw test\ndata generation, 2) synthesizing programs in a specific language that generate\nuseful test data, and 3) producing programs that use state-of-the-art faker\nlibraries. We evaluate our approach by prompting LLMs to generate test data for\n11 domains. The results show that LLMs can successfully generate realistic test\ndata generators in a wide range of domains at all three levels of\nintegrability.\n","authors":["Benoit Baudry","Khashayar Etemadi","Sen Fang","Yogya Gamage","Yi Liu","Yuxin Liu","Martin Monperrus","Javier Ron","André Silva","Deepika Tiwari"],"pdf_url":"https://arxiv.org/pdf/2401.17626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10093v1","updated":"2024-06-14T14:49:12Z","published":"2024-06-14T14:49:12Z","title":"BiKC: Keypose-Conditioned Consistency Policy for Bimanual Robotic\n  Manipulation","summary":"  Bimanual manipulation tasks typically involve multiple stages which require\nefficient interactions between two arms, posing step-wise and stage-wise\nchallenges for imitation learning systems. Specifically, failure and delay of\none step will broadcast through time, hinder success and efficiency of each\nsub-stage task, and thereby overall task performance. Although recent works\nhave made strides in addressing certain challenges, few approaches explicitly\nconsider the multi-stage nature of bimanual tasks while simultaneously\nemphasizing the importance of inference speed. In this paper, we introduce a\nnovel keypose-conditioned consistency policy tailored for bimanual\nmanipulation. It is a hierarchical imitation learning framework that consists\nof a high-level keypose predictor and a low-level trajectory generator. The\npredicted keyposes provide guidance for trajectory generation and also mark the\ncompletion of one sub-stage task. The trajectory generator is designed as a\nconsistency model trained from scratch without distillation, which generates\naction sequences conditioning on current observations and predicted keyposes\nwith fast inference speed. Simulated and real-world experimental results\ndemonstrate that the proposed approach surpasses baseline methods in terms of\nsuccess rate and operational efficiency.\n","authors":["Dongjie Yu","Hang Xu","Yizhou Chen","Yi Ren","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2406.10093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10090v1","updated":"2024-06-14T14:47:06Z","published":"2024-06-14T14:47:06Z","title":"Over-parameterization and Adversarial Robustness in Neural Networks: An\n  Overview and Empirical Analysis","summary":"  Thanks to their extensive capacity, over-parameterized neural networks\nexhibit superior predictive capabilities and generalization. However, having a\nlarge parameter space is considered one of the main suspects of the neural\nnetworks' vulnerability to adversarial example -- input samples crafted ad-hoc\nto induce a desired misclassification. Relevant literature has claimed\ncontradictory remarks in support of and against the robustness of\nover-parameterized networks. These contradictory findings might be due to the\nfailure of the attack employed to evaluate the networks' robustness. Previous\nresearch has demonstrated that depending on the considered model, the algorithm\nemployed to generate adversarial examples may not function properly, leading to\noverestimating the model's robustness. In this work, we empirically study the\nrobustness of over-parameterized networks against adversarial examples.\nHowever, unlike the previous works, we also evaluate the considered attack's\nreliability to support the results' veracity. Our results show that\nover-parameterized networks are robust against adversarial attacks as opposed\nto their under-parameterized counterparts.\n","authors":["Zhang Chen","Luca Demetrio","Srishti Gupta","Xiaoyi Feng","Zhaoqiang Xia","Antonio Emanuele Cinà","Maura Pintor","Luca Oneto","Ambra Demontis","Battista Biggio","Fabio Roli"],"pdf_url":"https://arxiv.org/pdf/2406.10090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10087v1","updated":"2024-06-14T14:43:59Z","published":"2024-06-14T14:43:59Z","title":"Biomarker based Cancer Classification using an Ensemble with Pre-trained\n  Models","summary":"  Certain cancer types, namely pancreatic cancer is difficult to detect at an\nearly stage; sparking the importance of discovering the causal relationship\nbetween biomarkers and cancer to identify cancer efficiently. By allowing for\nthe detection and monitoring of specific biomarkers through a non-invasive\nmethod, liquid biopsies enhance the precision and efficacy of medical\ninterventions, advocating the move towards personalized healthcare. Several\nmachine learning algorithms such as Random Forest, SVM are utilized for\nclassification, yet causing inefficiency due to the need for conducting\nhyperparameter tuning. We leverage a meta-trained Hyperfast model for\nclassifying cancer, accomplishing the highest AUC of 0.9929 and simultaneously\nachieving robustness especially on highly imbalanced datasets compared to other\nML algorithms in several binary classification tasks (e.g. breast invasive\ncarcinoma; BRCA vs. non-BRCA). We also propose a novel ensemble model combining\npre-trained Hyperfast model, XGBoost, and LightGBM for multi-class\nclassification tasks, achieving an incremental increase in accuracy (0.9464)\nwhile merely using 500 PCA features; distinguishable from previous studies\nwhere they used more than 2,000 features for similar results.\n","authors":["Chongmin Lee","Jihie Kim"],"pdf_url":"https://arxiv.org/pdf/2406.10087v1.pdf","comment":"Accepted to the AIAA Workshop at IJCAI 2024"},{"id":"http://arxiv.org/abs/2406.09406v2","updated":"2024-06-14T14:43:26Z","published":"2024-06-13T17:59:42Z","title":"4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities","summary":"  Current multimodal and multitask foundation models like 4M or UnifiedIO show\npromising results, but in practice their out-of-the-box abilities to accept\ndiverse inputs and perform diverse tasks are limited by the (usually rather\nsmall) number of modalities and tasks they are trained on. In this paper, we\nexpand upon the capabilities of them by training a single model on tens of\nhighly diverse modalities and by performing co-training on large-scale\nmultimodal datasets and text corpora. This includes training on several\nsemantic and geometric modalities, feature maps from recent state of the art\nmodels like DINOv2 and ImageBind, pseudo labels of specialist models like SAM\nand 4DHumans, and a range of new modalities that allow for novel ways to\ninteract with the model and steer the generation, for example image metadata or\ncolor palettes. A crucial step in this process is performing discrete\ntokenization on various modalities, whether they are image-like, neural network\nfeature maps, vectors, structured data like instance segmentation or human\nposes, or data that can be represented as text. Through this, we expand on the\nout-of-the-box capabilities of multimodal models and specifically show the\npossibility of training one model to solve at least 3x more tasks/modalities\nthan existing ones and doing so without a loss in performance. This enables\nmore fine-grained and controllable multimodal generation capabilities and\nallows us to study the distillation of models trained on diverse data and\nobjectives into a unified model. We successfully scale the training to a three\nbillion parameter model using tens of modalities and different datasets. The\nresulting models and training code are open sourced at 4m.epfl.ch.\n","authors":["Roman Bachmann","Oğuzhan Fatih Kar","David Mizrahi","Ali Garjani","Mingfei Gao","David Griffiths","Jiaming Hu","Afshin Dehghan","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2406.09406v2.pdf","comment":"Project page at 4m.epfl.ch"},{"id":"http://arxiv.org/abs/2312.13327v4","updated":"2024-06-14T14:42:43Z","published":"2023-12-20T16:58:55Z","title":"In-Context Reinforcement Learning for Variable Action Spaces","summary":"  Recently, it has been shown that transformers pre-trained on diverse datasets\nwith multi-episode contexts can generalize to new reinforcement learning tasks\nin-context. A key limitation of previously proposed models is their reliance on\na predefined action space size and structure. The introduction of a new action\nspace often requires data re-collection and model re-training, which can be\ncostly for some applications. In our work, we show that it is possible to\nmitigate this issue by proposing the Headless-AD model that, despite being\ntrained only once, is capable of generalizing to discrete action spaces of\nvariable size, semantic content and order. By experimenting with Bernoulli and\ncontextual bandits, as well as a gridworld environment, we show that\nHeadless-AD exhibits significant capability to generalize to action spaces it\nhas never encountered, even outperforming specialized models trained for a\nspecific set of actions on several environment configurations.\n","authors":["Viacheslav Sinii","Alexander Nikulin","Vladislav Kurenkov","Ilya Zisman","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2312.13327v4.pdf","comment":"Preprint, Under Review; code:\n  https://github.com/corl-team/headless-ad"},{"id":"http://arxiv.org/abs/2406.10086v1","updated":"2024-06-14T14:41:44Z","published":"2024-06-14T14:41:44Z","title":"Discovering influential text using convolutional neural networks","summary":"  Experimental methods for estimating the impacts of text on human evaluation\nhave been widely used in the social sciences. However, researchers in\nexperimental settings are usually limited to testing a small number of\npre-specified text treatments. While efforts to mine unstructured texts for\nfeatures that causally affect outcomes have been ongoing in recent years, these\nmodels have primarily focused on the topics or specific words of text, which\nmay not always be the mechanism of the effect. We connect these efforts with\nNLP interpretability techniques and present a method for flexibly discovering\nclusters of similar text phrases that are predictive of human reactions to\ntexts using convolutional neural networks. When used in an experimental\nsetting, this method can identify text treatments and their effects under\ncertain assumptions. We apply the method to two datasets. The first enables\ndirect validation of the model's ability to detect phrases known to cause the\noutcome. The second demonstrates its ability to flexibly discover text\ntreatments with varying textual structures. In both cases, the model learns a\ngreater variety of text treatments compared to benchmark methods, and these\ntext features quantitatively meet or exceed the ability of benchmark methods to\npredict the outcome.\n","authors":["Megan Ayers","Luke Sanford","Margaret Roberts","Eddie Yang"],"pdf_url":"https://arxiv.org/pdf/2406.10086v1.pdf","comment":"To be published in ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2312.03415v2","updated":"2024-06-14T14:36:45Z","published":"2023-12-06T10:54:34Z","title":"Run LoRA Run: Faster and Lighter LoRA Implementations","summary":"  LoRA is a technique that reduces the number of trainable parameters in a\nneural network by introducing low-rank adapters to linear layers. This\ntechnique is used both for fine-tuning and full training of large language\nmodels. This paper presents the RunLoRA framework for efficient implementations\nof LoRA that significantly improves the speed of neural network training and\nfine-tuning using low-rank adapters. The proposed implementation optimizes the\ncomputation of LoRA operations based on dimensions of corresponding linear\nlayer, layer input dimensions and lora rank by choosing best forward and\nbackward computation graph based on FLOPs and time estimations, resulting in\nfaster training without sacrificing accuracy. The experimental results show up\nto 28\\% speedup on language modeling networks.\n","authors":["Daria Cherniuk","Aleksandr Mikhalev","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2312.03415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10078v1","updated":"2024-06-14T14:35:44Z","published":"2024-06-14T14:35:44Z","title":"D-NPC: Dynamic Neural Point Clouds for Non-Rigid View Synthesis from\n  Monocular Video","summary":"  Dynamic reconstruction and spatiotemporal novel-view synthesis of non-rigidly\ndeforming scenes recently gained increased attention. While existing work\nachieves impressive quality and performance on multi-view or teleporting camera\nsetups, most methods fail to efficiently and faithfully recover motion and\nappearance from casual monocular captures. This paper contributes to the field\nby introducing a new method for dynamic novel view synthesis from monocular\nvideo, such as casual smartphone captures.\n  Our approach represents the scene as a $\\textit{dynamic neural point cloud}$,\nan implicit time-conditioned point distribution that encodes local geometry and\nappearance in separate hash-encoded neural feature grids for static and dynamic\nregions. By sampling a discrete point cloud from our model, we can efficiently\nrender high-quality novel views using a fast differentiable rasterizer and\nneural rendering network. Similar to recent work, we leverage advances in\nneural scene analysis by incorporating data-driven priors like monocular depth\nestimation and object segmentation to resolve motion and depth ambiguities\noriginating from the monocular captures. In addition to guiding the\noptimization process, we show that these priors can be exploited to explicitly\ninitialize our scene representation to drastically improve optimization speed\nand final image quality. As evidenced by our experimental evaluation, our\ndynamic point cloud model not only enables fast optimization and real-time\nframe rates for interactive applications, but also achieves competitive image\nquality on monocular benchmark sequences.\n  Our project page is available at\nhttps://moritzkappel.github.io/projects/dnpc.\n","authors":["Moritz Kappel","Florian Hahlbohm","Timon Scholz","Susana Castillo","Christian Theobalt","Martin Eisemann","Vladislav Golyanik","Marcus Magnor"],"pdf_url":"https://arxiv.org/pdf/2406.10078v1.pdf","comment":"16 pages, 5 figures, 10 tables. Project page:\n  https://moritzkappel.github.io/projects/dnpc"},{"id":"http://arxiv.org/abs/2402.14148v4","updated":"2024-06-14T14:27:40Z","published":"2024-02-21T22:11:01Z","title":"Neural Networks and Friction: Slide, Hold, Learn","summary":"  In this study, it is demonstrated that Recurrent Neural Networks (RNNs),\nspecifically those utilizing Gated Recurrent Unit (GRU) architecture, possess\nthe capability to learn the complex dynamics of rate-and-state friction laws\nfrom synthetic data. The data employed for training the network is generated\nthrough the application of traditional rate-and-state friction equations\ncoupled with the aging law for state evolution. A novel aspect of our approach\nis the formulation of a loss function that explicitly accounts for the direct\neffect by means of automatic differentiation. It is found that the RNN, with\nits GRU architecture, effectively learns to predict changes in the friction\ncoefficient resulting from velocity jumps (with and without noise in the target\ndata), thereby showcasing the potential of machine learning models in\nunderstanding and simulating the physics of frictional processes.\n","authors":["Joaquin Garcia-Suarez"],"pdf_url":"https://arxiv.org/pdf/2402.14148v4.pdf","comment":"12 pages, 12 figures, 4 tables"},{"id":"http://arxiv.org/abs/2404.15081v2","updated":"2024-06-14T14:26:38Z","published":"2024-04-23T14:31:15Z","title":"Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\n  Perturbations That Efficiently Fool Customized Diffusion Models","summary":"  Diffusion models (DMs) embark a new era of generative modeling and offer more\nopportunities for efficient generating high-quality and realistic data samples.\nHowever, their widespread use has also brought forth new challenges in model\nsecurity, which motivates the creation of more effective adversarial attackers\non DMs to understand its vulnerability. We propose CAAT, a simple but generic\nand efficient approach that does not require costly training to effectively\nfool latent diffusion models (LDMs). The approach is based on the observation\nthat cross-attention layers exhibits higher sensitivity to gradient change,\nallowing for leveraging subtle perturbations on published images to\nsignificantly corrupt the generated images. We show that a subtle perturbation\non an image can significantly impact the cross-attention layers, thus changing\nthe mapping between text and image during the fine-tuning of customized\ndiffusion models. Extensive experiments demonstrate that CAAT is compatible\nwith diverse diffusion models and outperforms baseline attack methods in a more\neffective (more noise) and efficient (twice as fast as Anti-DreamBooth and\nMist) manner.\n","authors":["Jingyao Xu","Yuetong Lu","Yandong Li","Siyang Lu","Dongdong Wang","Xiang Wei"],"pdf_url":"https://arxiv.org/pdf/2404.15081v2.pdf","comment":"Published at CVPR 2024, code:https://github.com/CO2-cityao/CAAT"},{"id":"http://arxiv.org/abs/2403.04546v2","updated":"2024-06-14T14:25:29Z","published":"2024-03-07T14:42:33Z","title":"Architectural Blueprint For Heterogeneity-Resilient Federated Learning","summary":"  This paper proposes a novel three tier architecture for federated learning to\noptimize edge computing environments. The proposed architecture addresses the\nchallenges associated with client data heterogeneity and computational\nconstraints. It introduces a scalable, privacy preserving framework that\nenhances the efficiency of distributed machine learning. Through\nexperimentation, the paper demonstrates the architecture capability to manage\nnon IID data sets more effectively than traditional federated learning models.\nAdditionally, the paper highlights the potential of this innovative approach to\nsignificantly improve model accuracy, reduce communication overhead, and\nfacilitate broader adoption of federated learning technologies.\n","authors":["Satwat Bashir","Tasos Dagiuklas","Kasra Kassai","Muddesar Iqbal"],"pdf_url":"https://arxiv.org/pdf/2403.04546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10061v1","updated":"2024-06-14T14:18:38Z","published":"2024-06-14T14:18:38Z","title":"TACCO: Task-guided Co-clustering of Clinical Concepts and Patient Visits\n  for Disease Subtyping based on EHR Data","summary":"  The growing availability of well-organized Electronic Health Records (EHR)\ndata has enabled the development of various machine learning models towards\ndisease risk prediction. However, existing risk prediction methods overlook the\nheterogeneity of complex diseases, failing to model the potential disease\nsubtypes regarding their corresponding patient visits and clinical concept\nsubgroups. In this work, we introduce TACCO, a novel framework that jointly\ndiscovers clusters of clinical concepts and patient visits based on a\nhypergraph modeling of EHR data. Specifically, we develop a novel\nself-supervised co-clustering framework that can be guided by the risk\nprediction task of specific diseases. Furthermore, we enhance the hypergraph\nmodel of EHR data with textual embeddings and enforce the alignment between the\nclusters of clinical concepts and patient visits through a contrastive\nobjective. Comprehensive experiments conducted on the public MIMIC-III dataset\nand Emory internal CRADLE dataset over the downstream clinical tasks of\nphenotype classification and cardiovascular risk prediction demonstrate an\naverage 31.25% performance improvement compared to traditional ML baselines and\na 5.26% improvement on top of the vanilla hypergraph model without our\nco-clustering mechanism. In-depth model analysis, clustering results analysis,\nand clinical case studies further validate the improved utilities and\ninsightful interpretations delivered by TACCO. Code is available at\nhttps://github.com/PericlesHat/TACCO.\n","authors":["Ziyang Zhang","Hejie Cui","Ran Xu","Yuzhang Xie","Joyce C. Ho","Carl Yang"],"pdf_url":"https://arxiv.org/pdf/2406.10061v1.pdf","comment":"11 pages, 5 figures, to be published in Proceedings of the 30th ACM\n  SIGKDD Conference on Knowledge Discovery and Data Mining"},{"id":"http://arxiv.org/abs/2406.10060v1","updated":"2024-06-14T14:16:39Z","published":"2024-06-14T14:16:39Z","title":"PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory\n  Planner","summary":"  In decentralized multiagent trajectory planners, agents need to communicate\nand exchange their positions to generate collision-free trajectories. However,\ndue to localization errors/uncertainties, trajectory deconfliction can fail\neven if trajectories are perfectly shared between agents. To address this\nissue, we first present PARM and PARM*, perception-aware, decentralized,\nasynchronous multiagent trajectory planners that enable a team of agents to\nnavigate uncertain environments while deconflicting trajectories and avoiding\nobstacles using perception information. PARM* differs from PARM as it is less\nconservative, using more computation to find closer-to-optimal solutions. While\nthese methods achieve state-of-the-art performance, they suffer from high\ncomputational costs as they need to solve large optimization problems onboard,\nmaking it difficult for agents to replan at high rates. To overcome this\nchallenge, we present our second key contribution, PRIMER, a learning-based\nplanner trained with imitation learning (IL) using PARM* as the expert\ndemonstrator. PRIMER leverages the low computational requirements at deployment\nof neural networks and achieves a computation speed up to 5500 times faster\nthan optimization-based approaches.\n","authors":["Kota Kondo","Claudius T. Tewari","Andrea Tagliabue","Jesus Tordesillas","Parker C. Lusk","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2406.10060v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2204.02283v2","updated":"2024-06-14T14:09:18Z","published":"2022-04-05T15:31:06Z","title":"Lost in Latent Space: Disentangled Models and the Challenge of\n  Combinatorial Generalisation","summary":"  Recent research has shown that generative models with highly disentangled\nrepresentations fail to generalise to unseen combination of generative factor\nvalues. These findings contradict earlier research which showed improved\nperformance in out-of-training distribution settings when compared to entangled\nrepresentations. Additionally, it is not clear if the reported failures are due\nto (a) encoders failing to map novel combinations to the proper regions of the\nlatent space or (b) novel combinations being mapped correctly but the\ndecoder/downstream process is unable to render the correct output for the\nunseen combinations. We investigate these alternatives by testing several\nmodels on a range of datasets and training settings. We find that (i) when\nmodels fail, their encoders also fail to map unseen combinations to correct\nregions of the latent space and (ii) when models succeed, it is either because\nthe test conditions do not exclude enough examples, or because excluded\ngenerative factors determine independent parts of the output image. Based on\nthese results, we argue that to generalise properly, models not only need to\ncapture factors of variation, but also understand how to invert the generative\nprocess that was used to generate the data.\n","authors":["Milton L. Montero","Jeffrey S. Bowers","Rui Ponte Costa","Casimir J. H. Ludwig","Gaurav Malhotra"],"pdf_url":"https://arxiv.org/pdf/2204.02283v2.pdf","comment":"10 pages and 7 figures in main text (not including references). 27\n  pages and 31 figures in appendix. Updated to match the camera-ready version"},{"id":"http://arxiv.org/abs/2403.02035v2","updated":"2024-06-14T14:02:12Z","published":"2024-03-04T13:39:22Z","title":"Exponential Expressivity of ReLU$^k$ Neural Networks on Gevrey Classes\n  with Point Singularities","summary":"  We analyze deep Neural Network emulation rates of smooth functions with point\nsingularities in bounded, polytopal domains $\\mathrm{D} \\subset \\mathbb{R}^d$,\n$d=2,3$. We prove exponential emulation rates in Sobolev spaces in terms of the\nnumber of neurons and in terms of the number of nonzero coefficients for\nGevrey-regular solution classes defined in terms of weighted Sobolev scales in\n$\\mathrm{D}$, comprising the countably-normed spaces of I.M. Babu\\v{s}ka and\nB.Q. Guo.\n  As intermediate result, we prove that continuous, piecewise polynomial high\norder (``$p$-version'') finite elements with elementwise polynomial degree\n$p\\in\\mathbb{N}$ on arbitrary, regular, simplicial partitions of polyhedral\ndomains $\\mathrm{D} \\subset \\mathbb{R}^d$, $d\\geq 2$ can be exactly emulated by\nneural networks combining ReLU and ReLU$^2$ activations. On shape-regular,\nsimplicial partitions of polytopal domains $\\mathrm{D}$, both the number of\nneurons and the number of nonzero parameters are proportional to the number of\ndegrees of freedom of the finite element space, in particular for the\n$hp$-Finite Element Method of I.M. Babu\\v{s}ka and B.Q. Guo.\n","authors":["Joost A. A. Opschoor","Christoph Schwab"],"pdf_url":"https://arxiv.org/pdf/2403.02035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10050v1","updated":"2024-06-14T14:00:02Z","published":"2024-06-14T14:00:02Z","title":"Comparison of fine-tuning strategies for transfer learning in medical\n  image classification","summary":"  In the context of medical imaging and machine learning, one of the most\npressing challenges is the effective adaptation of pre-trained models to\nspecialized medical contexts. Despite the availability of advanced pre-trained\nmodels, their direct application to the highly specialized and diverse field of\nmedical imaging often falls short due to the unique characteristics of medical\ndata. This study provides a comprehensive analysis on the performance of\nvarious fine-tuning methods applied to pre-trained models across a spectrum of\nmedical imaging domains, including X-ray, MRI, Histology, Dermoscopy, and\nEndoscopic surgery. We evaluated eight fine-tuning strategies, including\nstandard techniques such as fine-tuning all layers or fine-tuning only the\nclassifier layers, alongside methods such as gradually unfreezing layers,\nregularization based fine-tuning and adaptive learning rates. We selected three\nwell-established CNN architectures (ResNet-50, DenseNet-121, and VGG-19) to\ncover a range of learning and feature extraction scenarios. Although our\nresults indicate that the efficacy of these fine-tuning methods significantly\nvaries depending on both the architecture and the medical imaging type,\nstrategies such as combining Linear Probing with Full Fine-tuning resulted in\nnotable improvements in over 50% of the evaluated cases, demonstrating general\neffectiveness across medical domains. Moreover, Auto-RGN, which dynamically\nadjusts learning rates, led to performance enhancements of up to 11% for\nspecific modalities. Additionally, the DenseNet architecture showed more\npronounced benefits from alternative fine-tuning approaches compared to\ntraditional full fine-tuning. This work not only provides valuable insights for\noptimizing pre-trained models in medical image analysis but also suggests the\npotential for future research into more advanced architectures and fine-tuning\nmethods.\n","authors":["Ana Davila","Jacinto Colan","Yasuhisa Hasegawa"],"pdf_url":"https://arxiv.org/pdf/2406.10050v1.pdf","comment":"Accepted at Image and Vision Computing"},{"id":"http://arxiv.org/abs/2403.00252v2","updated":"2024-06-14T13:51:01Z","published":"2024-03-01T03:30:38Z","title":"EUROPA: A Legal Multilingual Keyphrase Generation Dataset","summary":"  Keyphrase generation has primarily been explored within the context of\nacademic research articles, with a particular focus on scientific domains and\nthe English language. In this work, we present EUROPA, a dataset for\nmultilingual keyphrase generation in the legal domain. It is derived from legal\njudgments from the Court of Justice of the European Union (EU), and contains\ninstances in all 24 EU official languages. We run multilingual models on our\ncorpus and analyze the results, showing room for improvement on a\ndomain-specific multilingual corpus such as the one we present.\n","authors":["Olivier Salaün","Frédéric Piedboeuf","Guillaume Le Berre","David Alfonso Hermelo","Philippe Langlais"],"pdf_url":"https://arxiv.org/pdf/2403.00252v2.pdf","comment":"19 pages, 2 figures, accepted at ACL 2024"},{"id":"http://arxiv.org/abs/2406.10043v1","updated":"2024-06-14T13:50:29Z","published":"2024-06-14T13:50:29Z","title":"Bridging the Communication Gap: Artificial Agents Learning Sign Language\n  through Imitation","summary":"  Artificial agents, particularly humanoid robots, interact with their\nenvironment, objects, and people using cameras, actuators, and physical\npresence. Their communication methods are often pre-programmed, limiting their\nactions and interactions. Our research explores acquiring non-verbal\ncommunication skills through learning from demonstrations, with potential\napplications in sign language comprehension and expression. In particular, we\nfocus on imitation learning for artificial agents, exemplified by teaching a\nsimulated humanoid American Sign Language. We use computer vision and deep\nlearning to extract information from videos, and reinforcement learning to\nenable the agent to replicate observed actions. Compared to other methods, our\napproach eliminates the need for additional hardware to acquire information. We\ndemonstrate how the combination of these different techniques offers a viable\nway to learn sign language. Our methodology successfully teaches 5 different\nsigns involving the upper body (i.e., arms and hands). This research paves the\nway for advanced communication skills in artificial agents.\n","authors":["Federico Tavella","Aphrodite Galata","Angelo Cangelosi"],"pdf_url":"https://arxiv.org/pdf/2406.10043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10031v1","updated":"2024-06-14T13:41:21Z","published":"2024-06-14T13:41:21Z","title":"Intepretative Deep Learning using Domain Adaptation for Fluorescence\n  Spectroscopy","summary":"  Fluorescence spectroscopy is a fundamental tool in life sciences and\nchemistry, widely used for applications such as environmental monitoring, food\nquality control, and biomedical diagnostics. However, analysis of spectroscopic\ndata with deep learning, in particular of fluorescence excitation-emission\nmatrices (EEMs), presents significant challenges due mainly to the typically\nsmall and sparse datasets available. Furthermore, the analysis of EEMs is\ndifficult due to their high dimensionality and overlapping spectral features.\nThis study proposes a new approach that exploits domain adaptation with\npretrained vision models, alongside a novel interpretability algorithm to\naddress these challenges. Thanks to specialised feature engineering of the\nneural networks described in this work, we are now able to provide deeper and\nmeaningful insights into the physico-chemical processes underlying the data.\nThe proposed approach is demonstrated through the analysis of the oxidation\nprocess in extra virgin olive oil (EVOO), showing its effectiveness in\npredicting quality indicators and identifying relevant spectral bands. This\nwork describes significantly innovative results in the use of deep learning for\nspectroscopy, transforming it from a black box into a tool for understanding\ncomplex biological and chemical processes.\n","authors":["Umberto Michelucci","Francesca Venturini"],"pdf_url":"https://arxiv.org/pdf/2406.10031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10030v1","updated":"2024-06-14T13:38:18Z","published":"2024-06-14T13:38:18Z","title":"Off-Policy Evaluation from Logged Human Feedback","summary":"  Learning from human feedback has been central to recent advances in\nartificial intelligence and machine learning. Since the collection of human\nfeedback is costly, a natural question to ask is if the new feedback always\nneeds to collected. Or could we evaluate a new model with the human feedback on\nresponses of another model? This motivates us to study off-policy evaluation\nfrom logged human feedback. We formalize the problem, propose both model-based\nand model-free estimators for policy values, and show how to optimize them. We\nanalyze unbiasedness of our estimators and evaluate them empirically. Our\nestimators can predict the absolute values of evaluated policies, rank them,\nand be optimized.\n","authors":["Aniruddha Bhargava","Lalit Jain","Branislav Kveton","Ge Liu","Subhojyoti Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2406.10030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10025v1","updated":"2024-06-14T13:36:30Z","published":"2024-06-14T13:36:30Z","title":"ProtoS-ViT: Visual foundation models for sparse self-explainable\n  classifications","summary":"  Prototypical networks aim to build intrinsically explainable models based on\nthe linear summation of concepts. However, important challenges remain in the\ntransparency, compactness, and meaningfulness of the explanations provided by\nthese models. This work demonstrates how frozen pre-trained ViT backbones can\nbe effectively turned into prototypical models for both general and\ndomain-specific tasks, in our case biomedical image classifiers. By leveraging\nstrong spatial features combined with a novel prototypical head, ProtoS-ViT\nsurpasses existing prototypical models showing strong performance in terms of\naccuracy, compactness, and explainability. Model explainability is evaluated\nthrough an extensive set of quantitative and qualitative metrics which serve as\na general benchmark for the development of prototypical models. Code is\navailable at https://github.com/hturbe/protosvit.\n","authors":["Hugues Turbé","Mina Bjelogrlic","Gianmarco Mengaldo","Christian Lovis"],"pdf_url":"https://arxiv.org/pdf/2406.10025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10023v1","updated":"2024-06-14T13:32:43Z","published":"2024-06-14T13:32:43Z","title":"Deep Bayesian Active Learning for Preference Modeling in Large Language\n  Models","summary":"  Leveraging human preferences for steering the behavior of Large Language\nModels (LLMs) has demonstrated notable success in recent years. Nonetheless,\ndata selection and labeling are still a bottleneck for these systems,\nparticularly at large scale. Hence, selecting the most informative points for\nacquiring human feedback may considerably reduce the cost of preference\nlabeling and unleash the further development of LLMs. Bayesian Active Learning\nprovides a principled framework for addressing this challenge and has\ndemonstrated remarkable success in diverse settings. However, previous attempts\nto employ it for Preference Modeling did not meet such expectations. In this\nwork, we identify that naive epistemic uncertainty estimation leads to the\nacquisition of redundant samples. We address this by proposing the Bayesian\nActive Learner for Preference Modeling (BAL-PM), a novel stochastic acquisition\npolicy that not only targets points of high epistemic uncertainty according to\nthe preference model but also seeks to maximize the entropy of the acquired\nprompt distribution in the feature space spanned by the employed LLM. Notably,\nour experiments demonstrate that BAL-PM requires 33% to 68% fewer preference\nlabels in two popular human preference datasets and exceeds previous stochastic\nBayesian acquisition policies.\n","authors":["Luckeciano C. Melo","Panagiotis Tigas","Alessandro Abate","Yarin Gal"],"pdf_url":"https://arxiv.org/pdf/2406.10023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10019v1","updated":"2024-06-14T13:29:36Z","published":"2024-06-14T13:29:36Z","title":"Group and Shuffle: Efficient Structured Orthogonal Parametrization","summary":"  The increasing size of neural networks has led to a growing demand for\nmethods of efficient fine-tuning. Recently, an orthogonal fine-tuning paradigm\nwas introduced that uses orthogonal matrices for adapting the weights of a\npretrained model. In this paper, we introduce a new class of structured\nmatrices, which unifies and generalizes structured classes from previous works.\nWe examine properties of this class and build a structured orthogonal\nparametrization upon it. We then use this parametrization to modify the\northogonal fine-tuning framework, improving parameter and computational\nefficiency. We empirically validate our method on different domains, including\nadapting of text-to-image diffusion models and downstream task fine-tuning in\nlanguage modeling. Additionally, we adapt our construction for orthogonal\nconvolutions and conduct experiments with 1-Lipschitz neural networks.\n","authors":["Mikhail Gorbunov","Nikolay Yudin","Vera Soboleva","Aibek Alanov","Alexey Naumov","Maxim Rakhuba"],"pdf_url":"https://arxiv.org/pdf/2406.10019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19202v3","updated":"2024-06-14T13:28:43Z","published":"2024-05-29T15:42:10Z","title":"Vulnerable Road User Detection and Safety Enhancement: A Comprehensive\n  Survey","summary":"  Traffic incidents involving vulnerable road users (VRUs) constitute a\nsignificant proportion of global road accidents. Advances in traffic\ncommunication ecosystems, coupled with sophisticated signal processing and\nmachine learning techniques, have facilitated the utilization of data from\ndiverse sensors. Despite these advancements and the availability of extensive\ndatasets, substantial progress is required to mitigate traffic casualties. This\npaper provides a comprehensive survey of state-of-the-art technologies and\nmethodologies to enhance the safety of VRUs. The study delves into the\ncommunication networks between vehicles and VRUs, emphasizing the integration\nof advanced sensors and the availability of relevant datasets. It explores\npreprocessing techniques and data fusion methods to enhance sensor data\nquality. Furthermore, our study assesses critical simulation environments\nessential for developing and testing VRU safety systems. Our research also\nhighlights recent advances in VRU detection and classification algorithms,\naddressing challenges such as variable environmental conditions. Additionally,\nwe cover cutting-edge research in predicting VRU intentions and behaviors,\nwhich is crucial for proactive collision avoidance strategies. Through this\nsurvey, we aim to provide a comprehensive understanding of the current\nlandscape of VRU safety technologies, identifying areas of progress and areas\nneeding further research and development.\n","authors":["Renato M. Silva","Gregório F. Azevedo","Matheus V. V. Berto","Jean R. Rocha","Eduardo C. Fidelis","Matheus V. Nogueira","Pedro H. Lisboa","Tiago A. Almeida"],"pdf_url":"https://arxiv.org/pdf/2405.19202v3.pdf","comment":"46 pages, 8 figures, citing 337 (up-to-date) papers, preprint\n  submitted to Expert Systems with Applications (Elsevier)"},{"id":"http://arxiv.org/abs/2406.10015v1","updated":"2024-06-14T13:26:36Z","published":"2024-06-14T13:26:36Z","title":"Gradient-based Learning in State-based Potential Games for Self-Learning\n  Production Systems","summary":"  In this paper, we introduce novel gradient-based optimization methods for\nstate-based potential games (SbPGs) within self-learning distributed production\nsystems. SbPGs are recognised for their efficacy in enabling self-optimizing\ndistributed multi-agent systems and offer a proven convergence guarantee, which\nfacilitates collaborative player efforts towards global objectives. Our study\nstrives to replace conventional ad-hoc random exploration-based learning in\nSbPGs with contemporary gradient-based approaches, which aim for faster\nconvergence and smoother exploration dynamics, thereby shortening training\nduration while upholding the efficacy of SbPGs. Moreover, we propose three\ndistinct variants for estimating the objective function of gradient-based\nlearning, each developed to suit the unique characteristics of the systems\nunder consideration. To validate our methodology, we apply it to a laboratory\ntestbed, namely Bulk Good Laboratory Plant, which represents a smart and\nflexible distributed multi-agent production system. The incorporation of\ngradient-based learning in SbPGs reduces training times and achieves more\noptimal policies than its baseline.\n","authors":["Steve Yuwono","Marlon Löppenberg","Dorothea Schwung","Andreas Schwung"],"pdf_url":"https://arxiv.org/pdf/2406.10015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10011v1","updated":"2024-06-14T13:24:07Z","published":"2024-06-14T13:24:07Z","title":"Beyond Slow Signs in High-fidelity Model Extraction","summary":"  Deep neural networks, costly to train and rich in intellectual property\nvalue, are increasingly threatened by model extraction attacks that compromise\ntheir confidentiality. Previous attacks have succeeded in reverse-engineering\nmodel parameters up to a precision of float64 for models trained on random data\nwith at most three hidden layers using cryptanalytical techniques. However, the\nprocess was identified to be very time consuming and not feasible for larger\nand deeper models trained on standard benchmarks. Our study evaluates the\nfeasibility of parameter extraction methods of Carlini et al. [1] further\nenhanced by Canales-Mart\\'inez et al. [2] for models trained on standard\nbenchmarks. We introduce a unified codebase that integrates previous methods\nand reveal that computational tools can significantly influence performance. We\ndevelop further optimisations to the end-to-end attack and improve the\nefficiency of extracting weight signs by up to 14.8 times compared to former\nmethods through the identification of easier and harder to extract neurons.\nContrary to prior assumptions, we identify extraction of weights, not\nextraction of weight signs, as the critical bottleneck. With our improvements,\na 16,721 parameter model with 2 hidden layers trained on MNIST is extracted\nwithin only 98 minutes compared to at least 150 minutes previously. Finally,\naddressing methodological deficiencies observed in previous studies, we propose\nnew ways of robust benchmarking for future model extraction attacks.\n","authors":["Hanna Foerster","Robert Mullins","Ilia Shumailov","Jamie Hayes"],"pdf_url":"https://arxiv.org/pdf/2406.10011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04193v3","updated":"2024-06-14T13:22:31Z","published":"2024-02-06T17:49:02Z","title":"Gradient Coding in Decentralized Learning for Evading Stragglers","summary":"  In this paper, we consider a decentralized learning problem in the presence\nof stragglers. Although gradient coding techniques have been developed for\ndistributed learning to evade stragglers, where the devices send encoded\ngradients with redundant training data, it is difficult to apply those\ntechniques directly to decentralized learning scenarios. To deal with this\nproblem, we propose a new gossip-based decentralized learning method with\ngradient coding (GOCO). In the proposed method, to avoid the negative impact of\nstragglers, the parameter vectors are updated locally using encoded gradients\nbased on the framework of stochastic gradient coding and then averaged in a\ngossip-based manner. We analyze the convergence performance of GOCO for\nstrongly convex loss functions. And we also provide simulation results to\ndemonstrate the superiority of the proposed method in terms of learning\nperformance compared with the baseline methods.\n","authors":["Chengxi Li","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2402.04193v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15767v2","updated":"2024-06-14T13:20:06Z","published":"2024-05-24T17:59:06Z","title":"Improved Particle Approximation Error for Mean Field Neural Networks","summary":"  Mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized\nnonlinear convex functional defined over the space of probability\ndistributions. MFLD has gained attention due to its connection with noisy\ngradient descent for mean-field two-layer neural networks. Unlike standard\nLangevin dynamics, the nonlinearity of the objective functional induces\nparticle interactions, necessitating multiple particles to approximate the\ndynamics in a finite-particle setting. Recent works (Chen et al., 2022; Suzuki\net al., 2023b) have demonstrated the uniform-in-time propagation of chaos for\nMFLD, showing that the gap between the particle system and its mean-field limit\nuniformly shrinks over time as the number of particles increases. In this work,\nwe improve the dependence on logarithmic Sobolev inequality (LSI) constants in\ntheir particle approximation errors, which can exponentially deteriorate with\nthe regularization coefficient. Specifically, we establish an LSI-constant-free\nparticle approximation error concerning the objective gap by leveraging the\nproblem structure in risk minimization. As the application, we demonstrate\nimproved convergence of MFLD, sampling guarantee for the mean-field stationary\ndistribution, and uniform-in-time Wasserstein propagation of chaos in terms of\nparticle complexity.\n","authors":["Atsushi Nitanda"],"pdf_url":"https://arxiv.org/pdf/2405.15767v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2406.10002v1","updated":"2024-06-14T13:16:48Z","published":"2024-06-14T13:16:48Z","title":"An elementary proof of a universal approximation theorem","summary":"  In this short note, we give an elementary proof of a universal approximation\ntheorem for neural networks with three hidden layers and increasing,\ncontinuous, bounded activation function. The result is weaker than the best\nknown results, but the proof is elementary in the sense that no machinery\nbeyond undergraduate analysis is used.\n","authors":["Chris Monico"],"pdf_url":"https://arxiv.org/pdf/2406.10002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11898v2","updated":"2024-06-14T13:16:00Z","published":"2023-06-20T21:19:57Z","title":"Relating tSNE and UMAP to Classical Dimensionality Reduction","summary":"  It has become standard to use gradient-based dimensionality reduction (DR)\nmethods like tSNE and UMAP when explaining what AI models have learned. This\nmakes sense: these methods are fast, robust, and have an uncanny ability to\nfind semantic patterns in high-dimensional data without supervision. Despite\nthis, gradient-based DR methods lack the most important quality that an\nexplainability method should possess: themselves being explainable. That is,\ngiven a UMAP output, it is currently unclear what one can say about the\ncorresponding input. We work towards closing this question by relating UMAP to\nclassical DR techniques. Specifically, we show that one can fully recover\nmethods like PCA, MDS, and ISOMAP in the modern DR paradigm: by applying\nattractions and repulsions onto a randomly initialized dataset. We also show\nthat, with a small change, Locally Linear Embeddings (LLE) can\nindistinguishably reproduce UMAP outputs. This implies that the UMAP effective\nobjective is minimized by this modified version of LLE (and vice versa). Given\nthis, we discuss what must be true of UMAP emebddings and present avenues for\nfuture work.\n","authors":["Andrew Draganov","Simon Dohn"],"pdf_url":"https://arxiv.org/pdf/2306.11898v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09998v1","updated":"2024-06-14T13:15:18Z","published":"2024-06-14T13:15:18Z","title":"Understanding Pedestrian Movement Using Urban Sensing Technologies: The\n  Promise of Audio-based Sensors","summary":"  While various sensors have been deployed to monitor vehicular flows, sensing\npedestrian movement is still nascent. Yet walking is a significant mode of\ntravel in many cities, especially those in Europe, Africa, and Asia.\nUnderstanding pedestrian volumes and flows is essential for designing safer and\nmore attractive pedestrian infrastructure and for controlling periodic\novercrowding. This study discusses a new approach to scale up urban sensing of\npeople with the help of novel audio-based technology. It assesses the benefits\nand limitations of microphone-based sensors as compared to other forms of\npedestrian sensing. A large-scale dataset called ASPED is presented, which\nincludes high-quality audio recordings along with video recordings used for\nlabeling the pedestrian count data. The baseline analyses highlight the promise\nof using audio sensors for pedestrian tracking, although algorithmic and\ntechnological improvements to make the sensors practically usable continue.\nThis study also demonstrates how the data can be leveraged to predict\npedestrian trajectories. Finally, it discusses the use cases and scenarios\nwhere audio-based pedestrian sensing can support better urban and\ntransportation planning.\n","authors":["Chaeyeon Han","Pavan Seshadri","Yiwei Ding","Noah Posner","Bon Woo Koo","Animesh Agrawal","Alexander Lerch","Subhrajit Guhathakurta"],"pdf_url":"https://arxiv.org/pdf/2406.09998v1.pdf","comment":"submitted to Urban Informatics"},{"id":"http://arxiv.org/abs/2406.09997v1","updated":"2024-06-14T13:12:07Z","published":"2024-06-14T13:12:07Z","title":"Towards Scalable and Versatile Weight Space Learning","summary":"  Learning representations of well-trained neural network models holds the\npromise to provide an understanding of the inner workings of those models.\nHowever, previous work has either faced limitations when processing larger\nnetworks or was task-specific to either discriminative or generative tasks.\nThis paper introduces the SANE approach to weight-space learning. SANE\novercomes previous limitations by learning task-agnostic representations of\nneural networks that are scalable to larger models of varying architectures and\nthat show capabilities beyond a single task. Our method extends the idea of\nhyper-representations towards sequential processing of subsets of neural\nnetwork weights, thus allowing one to embed larger neural networks as a set of\ntokens into the learned representation space. SANE reveals global model\ninformation from layer-wise embeddings, and it can sequentially generate unseen\nneural network models, which was unattainable with previous\nhyper-representation learning methods. Extensive empirical evaluation\ndemonstrates that SANE matches or exceeds state-of-the-art performance on\nseveral weight representation learning benchmarks, particularly in\ninitialization for new tasks and larger ResNet architectures.\n","authors":["Konstantin Schürholt","Michael W. Mahoney","Damian Borth"],"pdf_url":"https://arxiv.org/pdf/2406.09997v1.pdf","comment":"Accepted at ICML 2024"},{"id":"http://arxiv.org/abs/2402.10998v2","updated":"2024-06-14T13:05:01Z","published":"2024-02-16T16:15:25Z","title":"Provably Safe Neural Network Controllers via Differential Dynamic Logic","summary":"  While neural networks (NNs) have potential as autonomous controllers for\nCyber-Physical Systems, verifying the safety of NN based control systems\n(NNCSs) poses significant challenges for the practical use of NNs, especially\nwhen safety is needed for unbounded time horizons. One reason is the\nintractability of analyzing NNs, ODEs and hybrid systems. To this end, we\nintroduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The\nfirst general approach that allows reusing control theory results for NNCS\nverification. By joining forces, we exploit the efficiency of NN verification\ntools while retaining the rigor of differential dynamic logic (dL). Based on\nprovably safe control envelopes in dL, we derive specifications for the NN\nwhich is proven via NN verification. We show that a proof of the NN adhering to\nthe specification is mirrored by a dL proof on the infinite-time safety of the\nNNCS.\n  The NN verification properties resulting from hybrid systems typically\ncontain nonlinear arithmetic and arbitrary logical structures while efficient\nNN verification merely supports linear constraints. To overcome this divide, we\npresent Mosaic: An efficient, sound and complete verification approach for\npolynomial real arithmetic properties on piece-wise linear NNs. Mosaic\npartitions complex verification queries into simple queries and lifts\noff-the-shelf linear constraint tools to the nonlinear setting in a\ncompleteness-preserving manner by combining approximation with exact reasoning\nfor counterexample regions. Our evaluation demonstrates the versatility of\nVerSAILLE and Mosaic: We prove infinite-time safety on the classical Vertical\nAirborne Collision Avoidance NNCS verification benchmark for two scenarios\nwhile (exhaustively) enumerating counterexample regions in unsafe scenarios. We\nalso show that our approach significantly outperforms State-of-the-Art tools in\nclosed-loop NNV.\n","authors":["Samuel Teuber","Stefan Mitsch","André Platzer"],"pdf_url":"https://arxiv.org/pdf/2402.10998v2.pdf","comment":"35 pages (main paper has 9 pages), 12 figures"},{"id":"http://arxiv.org/abs/2212.10430v2","updated":"2024-06-14T13:04:54Z","published":"2022-12-20T17:09:08Z","title":"Walking Noise: On Layer-Specific Robustness of Neural Architectures\n  against Noisy Computations and Associated Characteristic Learning Dynamics","summary":"  Deep neural networks are extremely successful in various applications,\nhowever they exhibit high computational demands and energy consumption. This is\nexacerbated by stuttering technology scaling, prompting the need for novel\napproaches to handle increasingly complex neural architectures. At the same\ntime, alternative computing technologies such as analog computing, which\npromise groundbreaking improvements in energy efficiency, are inevitably\nfraught with noise and inaccurate calculations. Such noisy computations are\nmore energy efficient, and, given a fixed power budget, also more time\nefficient. However, like any kind of unsafe optimization, they require\ncountermeasures to ensure functionally correct results.\n  This work considers noisy computations in an abstract form, and gears to\nunderstand the implications of such noise on the accuracy of neural network\nclassifiers as an exemplary workload. We propose a methodology called Walking\nNoise which injects layer-specific noise to measure the robustness and to\nprovide insights on the learning dynamics. In more detail, we investigate the\nimplications of additive, multiplicative and mixed noise for different\nclassification tasks and model architectures. While noisy training\nsignificantly increases robustness for all noise types, we observe in\nparticular that it results in increased weight magnitudes and thus inherently\nimproves the signal-to-noise ratio for additive noise injection. Contrarily,\ntraining with multiplicative noise can lead to a form of self-binarization of\nthe model parameters, leading to extreme robustness. We conclude with a\ndiscussion of the use of this methodology in practice, among others, discussing\nits use for tailored multi-execution in noisy environments.\n","authors":["Hendrik Borras","Bernhard Klein","Holger Fröning"],"pdf_url":"https://arxiv.org/pdf/2212.10430v2.pdf","comment":"24 pages, 11 figures, To be published at the European Conference on\n  Machine Learning and Data Mining (ECML PKKD) 2024"},{"id":"http://arxiv.org/abs/2406.09984v1","updated":"2024-06-14T12:48:26Z","published":"2024-06-14T12:48:26Z","title":"Self-Supervised and Few-Shot Learning for Robust Bioaerosol Monitoring","summary":"  Real-time bioaerosol monitoring is improving the quality of life for people\naffected by allergies, but it often relies on deep-learning models which pose\nchallenges for widespread adoption. These models are typically trained in a\nsupervised fashion and require considerable effort to produce large amounts of\nannotated data, an effort that must be repeated for new particles, geographical\nregions, or measurement systems. In this work, we show that self-supervised\nlearning and few-shot learning can be combined to classify holographic images\nof bioaerosol particles using a large collection of unlabelled data and only a\nfew examples for each particle type. We first demonstrate that self-supervision\non pictures of unidentified particles from ambient air measurements enhances\nidentification even when labelled data is abundant. Most importantly, it\ngreatly improves few-shot classification when only a handful of labelled images\nare available. Our findings suggest that real-time bioaerosol monitoring\nworkflows can be substantially optimized, and the effort required to adapt\nmodels for different situations considerably reduced.\n","authors":["Adrian Willi","Pascal Baumann","Sophie Erb","Fabian Gröger","Yanick Zeder","Simone Lionetti"],"pdf_url":"https://arxiv.org/pdf/2406.09984v1.pdf","comment":"Short communication, 8 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2406.09981v1","updated":"2024-06-14T12:44:04Z","published":"2024-06-14T12:44:04Z","title":"Challenges in explaining deep learning models for data with biological\n  variation","summary":"  Much machine learning research progress is based on developing models and\nevaluating them on a benchmark dataset (e.g., ImageNet for images). However,\napplying such benchmark-successful methods to real-world data often does not\nwork as expected. This is particularly the case for biological data where we\nexpect variability at multiple time and spatial scales. In this work, we are\nusing grain data and the goal is to detect diseases and damages. Pink fusarium,\nskinned grains, and other diseases and damages are key factors in setting the\nprice of grains or excluding dangerous grains from food production. Apart from\nchallenges stemming from differences of the data from the standard toy\ndatasets, we also present challenges that need to be overcome when explaining\ndeep learning models. For example, explainability methods have many\nhyperparameters that can give different results, and the ones published in the\npapers do not work on dissimilar images. Other challenges are more general:\nproblems with visualization of the explanations and their comparison since the\nmagnitudes of their values differ from method to method. An open fundamental\nquestion also is: How to evaluate explanations? It is a non-trivial task\nbecause the \"ground truth\" is usually missing or ill-defined. Also, human\nannotators may create what they think is an explanation of the task at hand,\nyet the machine learning model might solve it in a different and perhaps\ncounter-intuitive way. We discuss several of these challenges and evaluate\nvarious post-hoc explainability methods on grain data. We focus on robustness,\nquality of explanations, and similarity to particular \"ground truth\"\nannotations made by experts. The goal is to find the methods that overall\nperform well and could be used in this challenging task. We hope the proposed\npipeline will be used as a framework for evaluating explainability methods in\nspecific use cases.\n","authors":["Lenka Tětková","Erik Schou Dreier","Robin Malm","Lars Kai Hansen"],"pdf_url":"https://arxiv.org/pdf/2406.09981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09976v1","updated":"2024-06-14T12:37:08Z","published":"2024-06-14T12:37:08Z","title":"Robust Model-Based Reinforcement Learning with an Adversarial Auxiliary\n  Model","summary":"  Reinforcement learning has demonstrated impressive performance in various\nchallenging problems such as robotics, board games, and classical arcade games.\nHowever, its real-world applications can be hindered by the absence of\nrobustness and safety in the learned policies. More specifically, an RL agent\nthat trains in a certain Markov decision process (MDP) often struggles to\nperform well in nearly identical MDPs. To address this issue, we employ the\nframework of Robust MDPs (RMDPs) in a model-based setting and introduce a novel\nlearned transition model. Our method specifically incorporates an auxiliary\npessimistic model, updated adversarially, to estimate the worst-case MDP within\na Kullback-Leibler uncertainty set. In comparison to several existing works,\nour work does not impose any additional conditions on the training environment,\nsuch as the need for a parametric simulator. To test the effectiveness of the\nproposed pessimistic model in enhancing policy robustness, we integrate it into\na practical RL algorithm, called Robust Model-Based Policy Optimization\n(RMBPO). Our experimental results indicate a notable improvement in policy\nrobustness on high-dimensional MuJoCo control tasks, with the auxiliary model\nenhancing the performance of the learned policy in distorted MDPs. We further\nexplore the learned deviation between the proposed auxiliary world model and\nthe nominal model, to examine how pessimism is achieved. By learning a\npessimistic world model and demonstrating its role in improving policy\nrobustness, our research contributes towards making (model-based) RL more\nrobust.\n","authors":["Siemen Herremans","Ali Anwar","Siegfried Mercelis"],"pdf_url":"https://arxiv.org/pdf/2406.09976v1.pdf","comment":"Will be presented at the RL Safety Workshop at RLC 2024"},{"id":"http://arxiv.org/abs/2406.09014v2","updated":"2024-06-14T12:24:54Z","published":"2024-06-13T11:38:58Z","title":"Deep learning empowered sensor fusion to improve infant movement\n  classification","summary":"  There is a recent boom in the development of AI solutions to facilitate and\nenhance diagnostic procedures for established clinical tools. To assess the\nintegrity of the developing nervous system, the Prechtl general movement\nassessment (GMA) is recognized for its clinical value in diagnosing\nneurological impairments in early infancy. GMA has been increasingly augmented\nthrough machine learning approaches intending to scale-up its application,\ncircumvent costs in the training of human assessors and further standardize\nclassification of spontaneous motor patterns. Available deep learning tools,\nall of which are based on single sensor modalities, are however still\nconsiderably inferior to that of well-trained human assessors. These approaches\nare hardly comparable as all models are designed, trained and evaluated on\nproprietary/silo-data sets. With this study we propose a sensor fusion approach\nfor assessing fidgety movements (FMs) comparing three different sensor\nmodalities (pressure, inertial, and visual sensors). Various combinations and\ntwo sensor fusion approaches (late and early fusion) for infant movement\nclassification were tested to evaluate whether a multi-sensor system\noutperforms single modality assessments. The performance of the three-sensor\nfusion (classification accuracy of 94.5\\%) was significantly higher than that\nof any single modality evaluated, suggesting the sensor fusion approach is a\npromising avenue for automated classification of infant motor patterns. The\ndevelopment of a robust sensor fusion system may significantly enhance AI-based\nearly recognition of neurofunctions, ultimately facilitating automated early\ndetection of neurodevelopmental conditions.\n","authors":["Tomas Kulvicius","Dajie Zhang","Luise Poustka","Sven Bölte","Lennart Jahn","Sarah Flügge","Marc Kraft","Markus Zweckstetter","Karin Nielsen-Saines","Florentin Wörgötter","Peter B Marschik"],"pdf_url":"https://arxiv.org/pdf/2406.09014v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00585v2","updated":"2024-06-14T12:19:30Z","published":"2023-12-01T13:50:15Z","title":"Adaptive Robust Learning using Latent Bernoulli Variables","summary":"  We present an adaptive approach for robust learning from corrupted training\nsets. We identify corrupted and non-corrupted samples with latent Bernoulli\nvariables and thus formulate the learning problem as maximization of the\nlikelihood where latent variables are marginalized. The resulting problem is\nsolved via variational inference, using an efficient Expectation-Maximization\nbased method. The proposed approach improves over the state-of-the-art by\nautomatically inferring the corruption level, while adding minimal\ncomputational overhead. We demonstrate our robust learning method and its\nparameter-free nature on a wide variety of machine learning tasks including\nonline learning and deep learning where it adapts to different levels of noise\nand maintains high prediction accuracy.\n","authors":["Aleksandr Karakulev","Dave Zachariah","Prashant Singh"],"pdf_url":"https://arxiv.org/pdf/2312.00585v2.pdf","comment":"Accepted at ICML 2024"},{"id":"http://arxiv.org/abs/2406.09968v1","updated":"2024-06-14T12:19:18Z","published":"2024-06-14T12:19:18Z","title":"Impact of Speech Mode in Automatic Pathological Speech Detection","summary":"  Automatic pathological speech detection approaches yield promising results in\nidentifying various pathologies. These approaches are typically designed and\nevaluated for phonetically-controlled speech scenarios, where speakers are\nprompted to articulate identical phonetic content. While gathering controlled\nspeech recordings can be laborious, spontaneous speech can be conveniently\nacquired as potential patients navigate their daily routines. Further,\nspontaneous speech can be valuable in detecting subtle and abstract cues of\npathological speech. Nonetheless, the efficacy of automatic pathological speech\ndetection for spontaneous speech remains unexplored. This paper analyzes the\ninfluence of speech mode on pathological speech detection approaches, examining\ntwo distinct categories of approaches, i.e., classical machine learning and\ndeep learning. Results indicate that classical approaches may struggle to\ncapture pathology-discriminant cues in spontaneous speech. In contrast, deep\nlearning approaches demonstrate superior performance, managing to extract\nadditional cues that were previously inaccessible in non-spontaneous speech\n","authors":["Shakeel A. Sheikh","Ina Kodrasi"],"pdf_url":"https://arxiv.org/pdf/2406.09968v1.pdf","comment":"Accepted in EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2406.09966v1","updated":"2024-06-14T12:15:15Z","published":"2024-06-14T12:15:15Z","title":"Outlier detection in maritime environments using AIS data and deep\n  recurrent architectures","summary":"  A methodology based on deep recurrent models for maritime surveillance, over\npublicly available Automatic Identification System (AIS) data, is presented in\nthis paper. The setup employs a deep Recurrent Neural Network (RNN)-based\nmodel, for encoding and reconstructing the observed ships' motion patterns. Our\napproach is based on a thresholding mechanism, over the calculated errors\nbetween observed and reconstructed motion patterns of maritime vessels.\nSpecifically, a deep-learning framework, i.e. an encoder-decoder architecture,\nis trained using the observed motion patterns, enabling the models to learn and\npredict the expected trajectory, which will be compared to the effective ones.\nOur models, particularly the bidirectional GRU with recurrent dropouts,\nshowcased superior performance in capturing the temporal dynamics of maritime\ndata, illustrating the potential of deep learning to enhance maritime\nsurveillance capabilities. Our work lays a solid foundation for future research\nin this domain, highlighting a path toward improved maritime safety through the\ninnovative application of technology.\n","authors":["Constantine Maganaris","Eftychios Protopapadakis","Nikolaos Doulamis"],"pdf_url":"https://arxiv.org/pdf/2406.09966v1.pdf","comment":"Presented in PETRA '24 The PErvasive Technologies Related to\n  Assistive Environments Conference June 26--28, 2024 Crete, Greece"},{"id":"http://arxiv.org/abs/2406.09958v1","updated":"2024-06-14T12:05:17Z","published":"2024-06-14T12:05:17Z","title":"H-Fac: Memory-Efficient Optimization with Factorized Hamiltonian Descent","summary":"  In this study, we introduce a novel adaptive optimizer, H-Fac, which\nincorporates a factorized approach to momentum and scaling parameters. Our\nalgorithm demonstrates competitive performances on both ResNets and Vision\nTransformers, while achieving sublinear memory costs through the use of rank-1\nparameterizations for moment estimators. We develop our algorithms based on\nprinciples derived from Hamiltonian dynamics, providing robust theoretical\nunderpinnings. These optimization algorithms are designed to be both\nstraightforward and adaptable, facilitating easy implementation in diverse\nsettings.\n","authors":["Son Nguyen","Lizhang Chen","Bo Liu","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.09958v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.09954v1","updated":"2024-06-14T12:01:18Z","published":"2024-06-14T12:01:18Z","title":"Rule Based Learning with Dynamic (Graph) Neural Networks","summary":"  A common problem of classical neural network architectures is that additional\ninformation or expert knowledge cannot be naturally integrated into the\nlearning process. To overcome this limitation, we propose a two-step approach\nconsisting of (1) generating rule functions from knowledge and (2) using these\nrules to define rule based layers -- a new type of dynamic neural network\nlayer. The focus of this work is on the second step, i.e., rule based layers\nthat are designed to dynamically arrange learnable parameters in the weight\nmatrices and bias vectors depending on the input samples. Indeed, we prove that\nour approach generalizes classical feed-forward layers such as fully connected\nand convolutional layers by choosing appropriate rules. As a concrete\napplication we present rule based graph neural networks (RuleGNNs) that\novercome some limitations of ordinary graph neural networks. Our experiments\nshow that the predictive performance of RuleGNNs is comparable to\nstate-of-the-art graph classifiers using simple rules based on Weisfeiler-Leman\nlabeling and pattern counting. Moreover, we introduce new synthetic benchmark\ngraph datasets to show how to integrate expert knowledge into RuleGNNs making\nthem more powerful than ordinary graph neural networks.\n","authors":["Florian Seiffarth"],"pdf_url":"https://arxiv.org/pdf/2406.09954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09952v1","updated":"2024-06-14T11:58:49Z","published":"2024-06-14T11:58:49Z","title":"BiVLC: Extending Vision-Language Compositionality Evaluation with\n  Text-to-Image Retrieval","summary":"  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe\nare formulated as image-to-text retrieval problems, where, given an image, the\nmodels need to select between the correct textual description and a synthetic\nhard negative text. In this work we present the Bidirectional Vision-Language\nCompositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic\nhard negative image generated from the synthetic text, resulting in two\nimage-to-text retrieval examples (one for each image) and, more importantly,\ntwo text-to-image retrieval examples (one for each text). Human annotators\nfilter out ill-formed examples ensuring the validity of the benchmark. The\nexperiments on BiVLC uncover a weakness of current multimodal models, as they\nperform poorly in the text-to-image direction. In fact, when considering both\nretrieval directions, the conclusions obtained in previous works change\nsignificantly. In addition to the benchmark, we show that a contrastive model\ntrained using synthetic images and texts improves the state of the art in\nSugarCrepe and in BiVLC for both retrieval directions. The gap to human\nperformance in BiVLC confirms that Vision-Language Compositionality is still a\nchallenging problem. BiVLC and code are available at\nhttps://imirandam.github.io/BiVLC_project_page.\n","authors":["Imanol Miranda","Ander Salaberria","Eneko Agirre","Gorka Azkune"],"pdf_url":"https://arxiv.org/pdf/2406.09952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17775v2","updated":"2024-06-14T11:57:53Z","published":"2024-03-26T15:07:58Z","title":"Secure Aggregation is Not Private Against Membership Inference Attacks","summary":"  Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in\nfederated learning, affording the server access only to the aggregate of model\nupdates while safeguarding the confidentiality of individual updates. Despite\nwidespread claims regarding SecAgg's privacy-preserving capabilities, a formal\nanalysis of its privacy is lacking, making such presumptions unjustified. In\nthis paper, we delve into the privacy implications of SecAgg by treating it as\na local differential privacy (LDP) mechanism for each local update. We design a\nsimple attack wherein an adversarial server seeks to discern which update\nvector a client submitted, out of two possible ones, in a single training round\nof federated learning under SecAgg. By conducting privacy auditing, we assess\nthe success probability of this attack and quantify the LDP guarantees provided\nby SecAgg. Our numerical results unveil that, contrary to prevailing claims,\nSecAgg offers weak privacy against membership inference attacks even in a\nsingle training round. Indeed, it is difficult to hide a local update by adding\nother independent local updates when the updates are of high dimension. Our\nfindings underscore the imperative for additional privacy-enhancing mechanisms,\nsuch as noise injection, in federated learning.\n","authors":["Khac-Hoang Ngo","Johan Östman","Giuseppe Durisi","Alexandre Graell i Amat"],"pdf_url":"https://arxiv.org/pdf/2403.17775v2.pdf","comment":"accepted to the European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) 2024"},{"id":"http://arxiv.org/abs/2406.09949v1","updated":"2024-06-14T11:52:09Z","published":"2024-06-14T11:52:09Z","title":"Neural Concept Binder","summary":"  The challenge in object-based visual reasoning lies in generating descriptive\nyet distinct concept representations. Moreover, doing this in an unsupervised\nfashion requires human users to understand a model's learned concepts and\npotentially revise false concepts. In addressing this challenge, we introduce\nthe Neural Concept Binder, a new framework for deriving discrete concept\nrepresentations resulting in what we term \"concept-slot encodings\". These\nencodings leverage both \"soft binding\" via object-centric block-slot encodings\nand \"hard binding\" via retrieval-based inference. The Neural Concept Binder\nfacilitates straightforward concept inspection and direct integration of\nexternal knowledge, such as human input or insights from other AI models like\nGPT-4. Additionally, we demonstrate that incorporating the hard binding\nmechanism does not compromise performance; instead, it enables seamless\nintegration into both neural and symbolic modules for intricate reasoning\ntasks, as evidenced by evaluations on our newly introduced CLEVR-Sudoku\ndataset.\n","authors":["Wolfgang Stammer","Antonia Wüst","David Steinmann","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2406.09949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09946v1","updated":"2024-06-14T11:47:25Z","published":"2024-06-14T11:47:25Z","title":"Finite-Time Analysis of Simultaneous Double Q-learning","summary":"  $Q$-learning is one of the most fundamental reinforcement learning (RL)\nalgorithms. Despite its widespread success in various applications, it is prone\nto overestimation bias in the $Q$-learning update. To address this issue,\ndouble $Q$-learning employs two independent $Q$-estimators which are randomly\nselected and updated during the learning process. This paper proposes a\nmodified double $Q$-learning, called simultaneous double $Q$-learning (SDQ),\nwith its finite-time analysis. SDQ eliminates the need for random selection\nbetween the two $Q$-estimators, and this modification allows us to analyze\ndouble $Q$-learning through the lens of a novel switching system framework\nfacilitating efficient finite-time analysis. Empirical studies demonstrate that\nSDQ converges faster than double $Q$-learning while retaining the ability to\nmitigate the maximization bias. Finally, we derive a finite-time expected error\nbound for SDQ.\n","authors":["Hyunjun Na","Donghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2406.09946v1.pdf","comment":"25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2402.07417v2","updated":"2024-06-14T11:40:31Z","published":"2024-02-12T05:44:10Z","title":"An Empirical Study Into What Matters for Calibrating Vision-Language\n  Models","summary":"  Vision-Language Models (VLMs) have emerged as the dominant approach for\nzero-shot recognition, adept at handling diverse scenarios and significant\ndistribution changes. However, their deployment in risk-sensitive areas\nrequires a deeper understanding of their uncertainty estimation capabilities, a\nrelatively uncharted area. In this study, we explore the calibration properties\nof VLMs across different architectures, datasets, and training strategies. In\nparticular, we analyze the uncertainty estimation performance of VLMs when\ncalibrated in one domain, label set or hierarchy level, and tested in a\ndifferent one. Our findings reveal that while VLMs are not inherently\ncalibrated for uncertainty, temperature scaling significantly and consistently\nimproves calibration, even across shifts in distribution and changes in label\nset. Moreover, VLMs can be calibrated with a very small set of examples.\nThrough detailed experimentation, we highlight the potential applications and\nimportance of our insights, aiming for more reliable and effective use of VLMs\nin critical, real-world scenarios.\n","authors":["Weijie Tu","Weijian Deng","Dylan Campbell","Stephen Gould","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2402.07417v2.pdf","comment":"ICML 2024 Camera Ready"},{"id":"http://arxiv.org/abs/2406.09935v1","updated":"2024-06-14T11:31:12Z","published":"2024-06-14T11:31:12Z","title":"Forgetting Order of Continual Learning: Examples That are Learned First\n  are Forgotten Last","summary":"  Catastrophic forgetting poses a significant challenge in continual learning,\nwhere models often forget previous tasks when trained on new data. Our\nempirical analysis reveals a strong correlation between catastrophic forgetting\nand the learning speed of examples: examples learned early are rarely\nforgotten, while those learned later are more susceptible to forgetting. We\ndemonstrate that replay-based continual learning methods can leverage this\nphenomenon by focusing on mid-learned examples for rehearsal. We introduce\nGoldilocks, a novel replay buffer sampling method that filters out examples\nlearned too quickly or too slowly, keeping those learned at an intermediate\nspeed. Goldilocks improves existing continual learning algorithms, leading to\nstate-of-the-art performance across several image classification tasks.\n","authors":["Guy Hacohen","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2406.09935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16888v3","updated":"2024-06-14T11:30:25Z","published":"2023-09-28T23:03:12Z","title":"Beyond Gut Feel: Using Time Series Transformers to Find Investment Gems","summary":"  This paper addresses the growing application of data-driven approaches within\nthe Private Equity (PE) industry, particularly in sourcing investment targets\n(i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present\na comprehensive review of the relevant approaches and propose a novel approach\nleveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for\npredicting the success likelihood of any candidate company. The objective of\nour research is to optimize sourcing performance for VC and GC investments by\nformally defining the sourcing problem as a multivariate time series\nclassification task. We consecutively introduce the key components of our\nimplementation which collectively contribute to the successful application of\nTMTSC in VC/GC sourcing: input features, model architecture, optimization\ntarget, and investor-centric data processing. Our extensive experiments on two\nreal-world investment tasks, benchmarked towards three popular baselines,\ndemonstrate the effectiveness of our approach in improving decision making\nwithin the VC and GC industry.\n","authors":["Lele Cao","Gustaf Halvardsson","Andrew McCornack","Vilhelm von Ehrenheim","Pawel Herman"],"pdf_url":"https://arxiv.org/pdf/2309.16888v3.pdf","comment":"Published by ICANN (33rd International Conference on Artificial\n  Neural Networks) 2024 as full paper (15 pages and 7 figures)"},{"id":"http://arxiv.org/abs/2406.09933v1","updated":"2024-06-14T11:27:19Z","published":"2024-06-14T11:27:19Z","title":"What Does it Take to Generalize SER Model Across Datasets? A\n  Comprehensive Benchmark","summary":"  Speech emotion recognition (SER) is essential for enhancing human-computer\ninteraction in speech-based applications. Despite improvements in specific\nemotional datasets, there is still a research gap in SER's capability to\ngeneralize across real-world situations. In this paper, we investigate\napproaches to generalize the SER system across different emotion datasets. In\nparticular, we incorporate 11 emotional speech datasets and illustrate a\ncomprehensive benchmark on the SER task. We also address the challenge of\nimbalanced data distribution using over-sampling methods when combining SER\ndatasets for training. Furthermore, we explore various evaluation protocols for\nadeptness in the generalization of SER. Building on this, we explore the\npotential of Whisper for SER, emphasizing the importance of thorough\nevaluation. Our approach is designed to advance SER technology by integrating\nspeaker-independent methods.\n","authors":["Adham Ibrahim","Shady Shehata","Ajinkya Kulkarni","Mukhtar Mohamed","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2406.09933v1.pdf","comment":"ACCEPTED AT INTERSPEECH 2024, GREECE"},{"id":"http://arxiv.org/abs/2406.09931v1","updated":"2024-06-14T11:25:53Z","published":"2024-06-14T11:25:53Z","title":"SCKansformer: Fine-Grained Classification of Bone Marrow Cells via\n  Kansformer Backbone and Hierarchical Attention Mechanisms","summary":"  The incidence and mortality rates of malignant tumors, such as acute\nleukemia, have risen significantly. Clinically, hospitals rely on cytological\nexamination of peripheral blood and bone marrow smears to diagnose malignant\ntumors, with accurate blood cell counting being crucial. Existing automated\nmethods face challenges such as low feature expression capability, poor\ninterpretability, and redundant feature extraction when processing\nhigh-dimensional microimage data. We propose a novel fine-grained\nclassification model, SCKansformer, for bone marrow blood cells, which\naddresses these challenges and enhances classification accuracy and efficiency.\nThe model integrates the Kansformer Encoder, SCConv Encoder, and Global-Local\nAttention Encoder. The Kansformer Encoder replaces the traditional MLP layer\nwith the KAN, improving nonlinear feature representation and interpretability.\nThe SCConv Encoder, with its Spatial and Channel Reconstruction Units, enhances\nfeature representation and reduces redundancy. The Global-Local Attention\nEncoder combines Multi-head Self-Attention with a Local Part module to capture\nboth global and local features. We validated our model using the Bone Marrow\nBlood Cell Fine-Grained Classification Dataset (BMCD-FGCD), comprising over\n10,000 samples and nearly 40 classifications, developed with a partner\nhospital. Comparative experiments on our private dataset, as well as the\npublicly available PBC and ALL-IDB datasets, demonstrate that SCKansformer\noutperforms both typical and advanced microcell classification methods across\nall datasets. Our source code and private BMCD-FGCD dataset are available at\nhttps://github.com/JustlfC03/SCKansformer.\n","authors":["Yifei Chen","Zhu Zhu","Shenghao Zhu","Linwei Qiu","Binfeng Zou","Fan Jia","Yunpeng Zhu","Chenyan Zhang","Zhaojie Fang","Feiwei Qin","Jin Fan","Changmiao Wang","Yu Gao","Gang Yu"],"pdf_url":"https://arxiv.org/pdf/2406.09931v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.09928v1","updated":"2024-06-14T11:16:46Z","published":"2024-06-14T11:16:46Z","title":"Personalized Speech Enhancement Without a Separate Speaker Embedding\n  Model","summary":"  Personalized speech enhancement (PSE) models can improve the audio quality of\nteleconferencing systems by adapting to the characteristics of a speaker's\nvoice. However, most existing methods require a separate speaker embedding\nmodel to extract a vector representation of the speaker from enrollment audio,\nwhich adds complexity to the training and deployment process. We propose to use\nthe internal representation of the PSE model itself as the speaker embedding,\nthereby avoiding the need for a separate model. We show that our approach\nperforms equally well or better than the standard method of using a pre-trained\nspeaker embedding model on noise suppression and echo cancellation tasks.\nMoreover, our approach surpasses the ICASSP 2023 Deep Noise Suppression\nChallenge winner by 0.15 in Mean Opinion Score.\n","authors":["Tanel Pärnamaa","Ando Saabas"],"pdf_url":"https://arxiv.org/pdf/2406.09928v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.09926v1","updated":"2024-06-14T11:14:01Z","published":"2024-06-14T11:14:01Z","title":"POWN: Prototypical Open-World Node Classification","summary":"  We consider the problem of \\textit{true} open-world semi-supervised node\nclassification, in which nodes in a graph either belong to known or new\nclasses, with the latter not present during training. Existing methods detect\nand reject new classes but fail to distinguish between different new classes.\nWe adapt existing methods and show they do not solve the problem sufficiently.\nWe introduce a novel end-to-end approach for classification into known classes\nand new classes based on class prototypes, which we call Prototypical\nOpen-World Learning for Node Classification (POWN). Our method combines graph\nsemi-supervised learning, self-supervised learning, and pseudo-labeling to\nlearn prototype representations of new classes in a zero-shot way. In contrast\nto existing solutions from the vision domain, POWN does not require data\naugmentation techniques for node classification. Experiments on benchmark\ndatasets demonstrate the effectiveness of POWN, where it outperforms baselines\nby up to $20\\%$ accuracy on the small and up to $30\\%$ on the large datasets.\nSource code is available at https://github.com/Bobowner/POWN.\n","authors":["Marcel Hoffmann","Lukas Galke","Ansgar Scherp"],"pdf_url":"https://arxiv.org/pdf/2406.09926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09924v1","updated":"2024-06-14T11:12:00Z","published":"2024-06-14T11:12:00Z","title":"Fundamental operating regimes, hyper-parameter fine-tuning and\n  glassiness: towards an interpretable replica-theory for trained restricted\n  Boltzmann machines","summary":"  We consider restricted Boltzmann machines with a binary visible layer and a\nGaussian hidden layer trained by an unlabelled dataset composed of noisy\nrealizations of a single ground pattern. We develop a statistical mechanics\nframework to describe the network generative capabilities, by exploiting the\nreplica trick and assuming self-averaging of the underlying order parameters\n(i.e., replica symmetry). In particular, we outline the effective control\nparameters (e.g., the relative number of weights to be trained, the\nregularization parameter), whose tuning can yield qualitatively-different\noperative regimes. Further, we provide analytical and numerical evidence for\nthe existence of a sub-region in the space of the hyperparameters where\nreplica-symmetry breaking occurs.\n","authors":["Alberto Fachechi","Elena Agliari","Miriam Aquaro","Anthony Coolen","Menno Mulder"],"pdf_url":"https://arxiv.org/pdf/2406.09924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09923v1","updated":"2024-06-14T11:10:17Z","published":"2024-06-14T11:10:17Z","title":"CliBench: Multifaceted Evaluation of Large Language Models in Clinical\n  Decisions on Diagnoses, Procedures, Lab Tests Orders and Prescriptions","summary":"  The integration of Artificial Intelligence (AI), especially Large Language\nModels (LLMs), into the clinical diagnosis process offers significant potential\nto improve the efficiency and accessibility of medical care. While LLMs have\nshown some promise in the medical domain, their application in clinical\ndiagnosis remains underexplored, especially in real-world clinical practice,\nwhere highly sophisticated, patient-specific decisions need to be made. Current\nevaluations of LLMs in this field are often narrow in scope, focusing on\nspecific diseases or specialties and employing simplified diagnostic tasks. To\nbridge this gap, we introduce CliBench, a novel benchmark developed from the\nMIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs'\ncapabilities in clinical diagnosis. This benchmark not only covers diagnoses\nfrom a diverse range of medical cases across various specialties but also\nincorporates tasks of clinical significance: treatment procedure\nidentification, lab test ordering and medication prescriptions. Supported by\nstructured output ontologies, CliBench enables a precise and multi-granular\nevaluation, offering an in-depth understanding of LLM's capability on diverse\nclinical tasks of desired granularity. We conduct a zero-shot evaluation of\nleading LLMs to assess their proficiency in clinical decision-making. Our\npreliminary results shed light on the potential and limitations of current LLMs\nin clinical settings, providing valuable insights for future advancements in\nLLM-powered healthcare.\n","authors":["Mingyu Derek Ma","Chenchen Ye","Yu Yan","Xiaoxuan Wang","Peipei Ping","Timothy S Chang","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09923v1.pdf","comment":"Project page: https://clibench.github.io"},{"id":"http://arxiv.org/abs/2311.13261v3","updated":"2024-06-14T11:04:12Z","published":"2023-11-22T09:25:08Z","title":"Immunohistochemistry guided segmentation of benign epithelial cells, in\n  situ lesions, and invasive epithelial cells in breast cancer slides","summary":"  Digital pathology enables automatic analysis of histopathological sections\nusing artificial intelligence (AI). Automatic evaluation could improve\ndiagnostic efficiency and help find associations between morphological features\nand clinical outcome. For development of such prediction models, identifying\ninvasive epithelial cells, and separating these from benign epithelial cells\nand in situ lesions would be the first step. In this study, we aimed to develop\nan AI model for segmentation of epithelial cells in sections from breast\ncancer. We generated epithelial ground truth masks by restaining hematoxylin\nand eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists'\nannotations. HE/CK image pairs were used to train a convolutional neural\nnetwork, and data augmentation was used to make the model more robust. Tissue\nmicroarrays (TMAs) from 839 patients, and whole slide images from two patients\nwere used for training and evaluation of the models. The sections were derived\nfrom four cohorts of breast cancer patients. TMAs from 21 patients from a fifth\ncohort was used as a second test set. In quantitative evaluation, a mean Dice\nscore of 0.70, 0.79, and 0.75 for invasive epithelial cells, benign epithelial\ncells, and in situ lesions, respectively, were achieved. In qualitative scoring\n(0-5) by pathologists, results were best for all epithelium and invasive\nepithelium, with scores of 4.7 and 4.4. Scores for benign epithelium and in\nsitu lesions were 3.7 and 2.0. The proposed model segmented epithelial cells in\nHE stained breast cancer slides well, but further work is needed for accurate\ndivision between the classes. Immunohistochemistry, together with pathologists'\nannotations, enabled the creation of accurate ground truths. The model is made\nfreely available in FastPathology and the code is available at\nhttps://github.com/AICAN-Research/breast-epithelium-segmentation\n","authors":["Maren Høibø","André Pedersen","Vibeke Grotnes Dale","Sissel Marie Berget","Borgny Ytterhus","Cecilia Lindskog","Elisabeth Wik","Lars A. Akslen","Ingerid Reinertsen","Erik Smistad","Marit Valla"],"pdf_url":"https://arxiv.org/pdf/2311.13261v3.pdf","comment":"19 pages, 6 figures. Submitted to a scientific journal"},{"id":"http://arxiv.org/abs/2406.09908v1","updated":"2024-06-14T10:36:26Z","published":"2024-06-14T10:36:26Z","title":"What Does Softmax Probability Tell Us about Classifiers Ranking Across\n  Diverse Test Conditions?","summary":"  This work aims to develop a measure that can accurately rank the performance\nof various classifiers when they are tested on unlabeled data from\nout-of-distribution (OOD) distributions. We commence by demonstrating that\nconventional uncertainty metrics, notably the maximum Softmax prediction\nprobability, possess inherent utility in forecasting model generalization\nacross certain OOD contexts. Building on this insight, we introduce a new\nmeasure called Softmax Correlation (SoftmaxCorr). It calculates the cosine\nsimilarity between a class-class correlation matrix, constructed from Softmax\noutput vectors across an unlabeled test dataset, and a predefined reference\nmatrix that embodies ideal class correlations. A high resemblance of\npredictions to the reference matrix signals that the model delivers confident\nand uniform predictions across all categories, reflecting minimal uncertainty\nand confusion. Through rigorous evaluation across a suite of datasets,\nincluding ImageNet, CIFAR-10, and WILDS, we affirm the predictive validity of\nSoftmaxCorr in accurately forecasting model performance within both\nin-distribution (ID) and OOD settings. Furthermore, we discuss the limitations\nof our proposed measure and suggest avenues for future research.\n","authors":["Weijie Tu","Weijian Deng","Liang Zheng","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2406.09908v1.pdf","comment":"TMLR 2024 (https://openreview.net/forum?id=vtiDUgGjyx)"},{"id":"http://arxiv.org/abs/2406.09904v1","updated":"2024-06-14T10:23:45Z","published":"2024-06-14T10:23:45Z","title":"QQQ: Quality Quattuor-Bit Quantization for Large Language Models","summary":"  Quantization is a proven effective method for compressing large language\nmodels. Although popular techniques like W8A8 and W4A16 effectively maintain\nmodel performance, they often fail to concurrently speed up the prefill and\ndecoding stages of inference. W4A8 is a promising strategy to accelerate both\nof them while usually leads to a significant performance degradation. To\naddress these issues, we present QQQ, a Quality Quattuor-bit Quantization\nmethod with 4-bit weights and 8-bit activations. QQQ employs adaptive smoothing\nand Hessian-based compensation, significantly enhancing the performance of\nquantized models without extensive training. Furthermore, we meticulously\nengineer W4A8 GEMM kernels to increase inference speed. Our specialized\nper-channel W4A8 GEMM and per-group W4A8 GEMM achieve impressive speed\nincreases of 3.67$\\times$ and 3.29 $\\times$ over FP16 GEMM. Our extensive\nexperiments show that QQQ achieves performance on par with existing\nstate-of-the-art LLM quantization methods while significantly accelerating\ninference, achieving speed boosts up to 2.24 $\\times$, 2.10$\\times$, and\n1.25$\\times$ compared to FP16, W8A8, and W4A16, respectively.\n","authors":["Ying Zhang","Peng Zhang","Mincong Huang","Jingyang Xiang","Yujie Wang","Chao Wang","Yineng Zhang","Lei Yu","Chuan Liu","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2406.09904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09899v1","updated":"2024-06-14T10:15:03Z","published":"2024-06-14T10:15:03Z","title":"Learning Solution-Aware Transformers for Efficiently Solving Quadratic\n  Assignment Problem","summary":"  Recently various optimization problems, such as Mixed Integer Linear\nProgramming Problems (MILPs), have undergone comprehensive investigation,\nleveraging the capabilities of machine learning. This work focuses on\nlearning-based solutions for efficiently solving the Quadratic Assignment\nProblem (QAPs), which stands as a formidable challenge in combinatorial\noptimization. While many instances of simpler problems admit fully\npolynomial-time approximate solution (FPTAS), QAP is shown to be strongly\nNP-hard. Even finding a FPTAS for QAP is difficult, in the sense that the\nexistence of a FPTAS implies $P = NP$. Current research on QAPs suffer from\nlimited scale and computational inefficiency. To attack the aforementioned\nissues, we here propose the first solution of its kind for QAP in the\nlearn-to-improve category. This work encodes facility and location nodes\nseparately, instead of forming computationally intensive association graphs\nprevalent in current approaches. This design choice enables scalability to\nlarger problem sizes. Furthermore, a \\textbf{S}olution \\textbf{AW}are\n\\textbf{T}ransformer (SAWT) architecture integrates the incumbent solution\nmatrix with the attention score to effectively capture higher-order information\nof the QAPs. Our model's effectiveness is validated through extensive\nexperiments on self-generated QAP instances of varying sizes and the QAPLIB\nbenchmark.\n","authors":["Zhentao Tan","Yadong Mu"],"pdf_url":"https://arxiv.org/pdf/2406.09899v1.pdf","comment":"Accepted by ICML 2024"},{"id":"http://arxiv.org/abs/2406.09898v1","updated":"2024-06-14T10:14:01Z","published":"2024-06-14T10:14:01Z","title":"Positive-Unlabelled Learning for Identifying New Candidate Dietary\n  Restriction-related Genes among Ageing-related Genes","summary":"  Dietary Restriction (DR) is one of the most popular anti-ageing\ninterventions, prompting exhaustive research into genes associated with its\nmechanisms. Recently, Machine Learning (ML) has been explored to identify\npotential DR-related genes among ageing-related genes, aiming to minimize\ncostly wet lab experiments needed to expand our knowledge on DR. However, to\ntrain a model from positive (DR-related) and negative (non-DR-related)\nexamples, existing ML methods naively label genes without known DR relation as\nnegative examples, assuming that lack of DR-related annotation for a gene\nrepresents evidence of absence of DR-relatedness, rather than absence of\nevidence; this hinders the reliability of the negative examples (non-DR-related\ngenes) and the method's ability to identify novel DR-related genes. This work\nintroduces a novel gene prioritization method based on the two-step\nPositive-Unlabelled (PU) Learning paradigm: using a similarity-based,\nKNN-inspired approach, our method first selects reliable negative examples\namong the genes without known DR associations. Then, these reliable negatives\nand all known positives are used to train a classifier that effectively\ndifferentiates DR-related and non-DR-related genes, which is finally employed\nto generate a more reliable ranking of promising genes for novel\nDR-relatedness. Our method significantly outperforms the existing\nstate-of-the-art non-PU approach for DR-relatedness prediction in three\nrelevant performance metrics. In addition, curation of existing literature\nfinds support for the top-ranked candidate DR-related genes identified by our\nmodel.\n","authors":["Jorge Paz-Ruza","Alex A. Freitas","Amparo Alonso-Betanzos","Bertha Guijarro-Berdiñas"],"pdf_url":"https://arxiv.org/pdf/2406.09898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03210v2","updated":"2024-06-14T10:13:02Z","published":"2023-07-06T14:10:02Z","title":"Sparse Graphical Linear Dynamical Systems","summary":"  Time-series datasets are central in machine learning with applications in\nnumerous fields of science and engineering, such as biomedicine, Earth\nobservation, and network analysis. Extensive research exists on state-space\nmodels (SSMs), which are powerful mathematical tools that allow for\nprobabilistic and interpretable learning on time series. Learning the model\nparameters in SSMs is arguably one of the most complicated tasks, and the\ninclusion of prior knowledge is known to both ease the interpretation but also\nto complicate the inferential tasks. Very recent works have attempted to\nincorporate a graphical perspective on some of those model parameters, but they\npresent notable limitations that this work addresses. More generally, existing\ngraphical modeling tools are designed to incorporate either static information,\nfocusing on statistical dependencies among independent random variables (e.g.,\ngraphical Lasso approach), or dynamic information, emphasizing causal\nrelationships among time series samples (e.g., graphical Granger approaches).\nHowever, there are no joint approaches combining static and dynamic graphical\nmodeling within the context of SSMs. This work proposes a novel approach to\nfill this gap by introducing a joint graphical modeling framework that bridges\nthe graphical Lasso model and a causal-based graphical approach for the\nlinear-Gaussian SSM. We present DGLASSO (Dynamic Graphical Lasso), a new\ninference method within this framework that implements an efficient block\nalternating majorization-minimization algorithm. The algorithm's convergence is\nestablished by departing from modern tools from nonlinear analysis.\nExperimental validation on various synthetic data showcases the effectiveness\nof the proposed model and inference algorithm.\n","authors":["Emilie Chouzenoux","Victor Elvira"],"pdf_url":"https://arxiv.org/pdf/2307.03210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17170v2","updated":"2024-06-14T10:10:13Z","published":"2024-05-27T13:49:24Z","title":"Forecasting Four Business Cycle Phases Using Machine Learning: A Case\n  Study of US and EuroZone","summary":"  Understanding the business cycle is crucial for building economic stability,\nguiding business planning, and informing investment decisions. The business\ncycle refers to the recurring pattern of expansion and contraction in economic\nactivity over time. Economic analysis is inherently complex, incorporating a\nmyriad of factors (such as macroeconomic indicators, political decisions). This\ncomplexity makes it challenging to fully account for all variables when\ndetermining the current state of the economy and predicting its future\ntrajectory in the upcoming months. The objective of this study is to\ninvestigate the capacity of machine learning models in automatically analyzing\nthe state of the economic, with the goal of forecasting business phases\n(expansion, slowdown, recession and recovery) in the United States and the\nEuroZone. We compared three different machine learning approaches to classify\nthe phases of the business cycle, and among them, the Multinomial Logistic\nRegression (MLR) achieved the best results. Specifically, MLR got the best\nresults by achieving the accuracy of 65.25% (Top1) and 84.74% (Top2) for the\nEuroZone and 75% (Top1) and 92.14% (Top2) for the United States. These results\ndemonstrate the potential of machine learning techniques to predict business\ncycles accurately, which can aid in making informed decisions in the fields of\neconomics and finance.\n","authors":["Elvys Linhares Pontes","Mohamed Benjannet","Raymond Yung"],"pdf_url":"https://arxiv.org/pdf/2405.17170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07979v2","updated":"2024-06-14T10:06:38Z","published":"2024-06-12T08:05:45Z","title":"Heuristic Learning with Graph Neural Networks: A Unified Framework for\n  Link Prediction","summary":"  Link prediction is a fundamental task in graph learning, inherently shaped by\nthe topology of the graph. While traditional heuristics are grounded in graph\ntopology, they encounter challenges in generalizing across diverse graphs.\nRecent research efforts have aimed to leverage the potential of heuristics, yet\na unified formulation accommodating both local and global heuristics remains\nundiscovered. Drawing insights from the fact that both local and global\nheuristics can be represented by adjacency matrix multiplications, we propose a\nunified matrix formulation to accommodate and generalize various heuristics. We\nfurther propose the Heuristic Learning Graph Neural Network (HL-GNN) to\nefficiently implement the formulation. HL-GNN adopts intra-layer propagation\nand inter-layer connections, allowing it to reach a depth of around 20 layers\nwith lower time complexity than GCN. Extensive experiments on the Planetoid,\nAmazon, and OGB datasets underscore the effectiveness and efficiency of HL-GNN.\nIt outperforms existing methods by a large margin in prediction performance.\nAdditionally, HL-GNN is several orders of magnitude faster than\nheuristic-inspired methods while requiring only a few trainable parameters. The\ncase study further demonstrates that the generalized heuristics and learned\nweights are highly interpretable.\n","authors":["Juzheng Zhang","Lanning Wei","Zhen Xu","Quanming Yao"],"pdf_url":"https://arxiv.org/pdf/2406.07979v2.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2402.12269v3","updated":"2024-06-14T10:06:23Z","published":"2024-02-19T16:30:35Z","title":"Any2Graph: Deep End-To-End Supervised Graph Prediction With An Optimal\n  Transport Loss","summary":"  We propose Any2graph, a generic framework for end-to-end Supervised Graph\nPrediction (SGP) i.e. a deep learning model that predicts an entire graph for\nany kind of input. The framework is built on a novel Optimal Transport loss,\nthe Partially-Masked Fused Gromov-Wasserstein, that exhibits all necessary\nproperties (permutation invariance, differentiability and scalability) and is\ndesigned to handle any-sized graphs. Numerical experiments showcase the\nversatility of the approach that outperform existing competitors on a novel\nchallenging synthetic dataset and a variety of real-world tasks such as map\nconstruction from satellite image (Sat2Graph) or molecule prediction from\nfingerprint (Fingerprint2Graph).\n","authors":["Paul Krzakala","Junjie Yang","Rémi Flamary","Florence d'Alché-Buc","Charlotte Laclau","Matthieu Labeau"],"pdf_url":"https://arxiv.org/pdf/2402.12269v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.14402v4","updated":"2024-06-14T09:54:58Z","published":"2022-09-28T20:03:57Z","title":"L2XGNN: Learning to Explain Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) are a popular class of machine learning models.\nInspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a\nframework for explainable GNNs which provides faithful explanations by design.\nL2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which\nare exclusively used in the GNNs message-passing operations. L2XGNN is able to\nselect, for each input graph, a subgraph with specific properties such as being\nsparse and connected. Imposing such constraints on the motifs often leads to\nmore interpretable and effective explanations. Experiments on several datasets\nsuggest that L2XGNN achieves the same classification accuracy as baseline\nmethods using the entire input graph while ensuring that only the provided\nexplanations are used to make predictions. Moreover, we show that L2XGNN is\nable to identify motifs responsible for the graph's properties it is intended\nto predict.\n","authors":["Giuseppe Serra","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2209.14402v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09882v1","updated":"2024-06-14T09:52:47Z","published":"2024-06-14T09:52:47Z","title":"Harm Mitigation in Recommender Systems under User Preference Dynamics","summary":"  We consider a recommender system that takes into account the interplay\nbetween recommendations, the evolution of user interests, and harmful content.\nWe model the impact of recommendations on user behavior, particularly the\ntendency to consume harmful content. We seek recommendation policies that\nestablish a tradeoff between maximizing click-through rate (CTR) and mitigating\nharm. We establish conditions under which the user profile dynamics have a\nstationary point, and propose algorithms for finding an optimal recommendation\npolicy at stationarity. We experiment on a semi-synthetic movie recommendation\nsetting initialized with real data and observe that our policies outperform\nbaselines at simultaneously maximizing CTR and mitigating harm.\n","authors":["Jerry Chee","Shankar Kalyanaraman","Sindhu Kiranmai Ernala","Udi Weinsberg","Sarah Dean","Stratis Ioannidis"],"pdf_url":"https://arxiv.org/pdf/2406.09882v1.pdf","comment":"Recommender Systems; Harm Mitigation; Amplification; User Preference\n  Modeling"},{"id":"http://arxiv.org/abs/2406.09877v1","updated":"2024-06-14T09:44:46Z","published":"2024-06-14T09:44:46Z","title":"Federated Learning with Flexible Architectures","summary":"  Traditional federated learning (FL) methods have limited support for clients\nwith varying computational and communication abilities, leading to\ninefficiencies and potential inaccuracies in model training. This limitation\nhinders the widespread adoption of FL in diverse and resource-constrained\nenvironments, such as those with client devices ranging from powerful servers\nto mobile devices. To address this need, this paper introduces Federated\nLearning with Flexible Architectures (FedFA), an FL training algorithm that\nallows clients to train models of different widths and depths. Each client can\nselect a network architecture suitable for its resources, with shallower and\nthinner networks requiring fewer computing resources for training. Unlike prior\nwork in this area, FedFA incorporates the layer grafting technique to align\nclients' local architectures with the largest network architecture in the FL\nsystem during model aggregation. Layer grafting ensures that all client\ncontributions are uniformly integrated into the global model, thereby\nminimizing the risk of any individual client's data skewing the model's\nparameters disproportionately and introducing security benefits. Moreover,\nFedFA introduces the scalable aggregation method to manage scale variations in\nweights among different network architectures. Experimentally, FedFA\noutperforms previous width and depth flexible aggregation strategies.\nFurthermore, FedFA demonstrates increased robustness against performance\ndegradation in backdoor attack scenarios compared to earlier strategies.\n","authors":["Jong-Ik Park","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2406.09877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09876v1","updated":"2024-06-14T09:44:06Z","published":"2024-06-14T09:44:06Z","title":"Sailing in high-dimensional spaces: Low-dimensional embeddings through\n  angle preservation","summary":"  Low-dimensional embeddings (LDEs) of high-dimensional data are ubiquitous in\nscience and engineering. They allow us to quickly understand the main\nproperties of the data, identify outliers and processing errors, and inform the\nnext steps of data analysis. As such, LDEs have to be faithful to the original\nhigh-dimensional data, i.e., they should represent the relationships that are\nencoded in the data, both at a local as well as global scale. The current\ngeneration of LDE approaches focus on reconstructing local distances between\nany pair of samples correctly, often out-performing traditional approaches\naiming at all distances. For these approaches, global relationships are,\nhowever, usually strongly distorted, often argued to be an inherent trade-off\nbetween local and global structure learning for embeddings. We suggest a new\nperspective on LDE learning, reconstructing angles between data points. We show\nthat this approach, Mercat, yields good reconstruction across a diverse set of\nexperiments and metrics, and preserve structures well across all scales.\nCompared to existing work, our approach also has a simple formulation,\nfacilitating future theoretical analysis and algorithmic improvements.\n","authors":["Jonas Fischer","Rong Ma"],"pdf_url":"https://arxiv.org/pdf/2406.09876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09870v1","updated":"2024-06-14T09:30:18Z","published":"2024-06-14T09:30:18Z","title":"IGL-Bench: Establishing the Comprehensive Benchmark for Imbalanced Graph\n  Learning","summary":"  Deep graph learning has gained grand popularity over the past years due to\nits versatility and success in representing graph data across a wide range of\ndomains. However, the pervasive issue of imbalanced graph data distributions,\nwhere certain parts exhibit disproportionally abundant data while others remain\nsparse, undermines the efficacy of conventional graph learning algorithms,\nleading to biased outcomes. To address this challenge, Imbalanced Graph\nLearning (IGL) has garnered substantial attention, enabling more balanced data\ndistributions and better task performance. Despite the proliferation of IGL\nalgorithms, the absence of consistent experimental protocols and fair\nperformance comparisons pose a significant barrier to comprehending\nadvancements in this field. To bridge this gap, we introduce IGL-Bench, a\nfoundational comprehensive benchmark for imbalanced graph learning, embarking\non 16 diverse graph datasets and 24 distinct IGL algorithms with uniform data\nprocessing and splitting strategies. Specifically, IGL-Bench systematically\ninvestigates state-of-the-art IGL algorithms in terms of effectiveness,\nrobustness, and efficiency on node-level and graph-level tasks, with the scope\nof class-imbalance and topology-imbalance. Extensive experiments demonstrate\nthe potential benefits of IGL algorithms on various imbalanced conditions,\noffering insights and opportunities in the IGL field. Further, we have\ndeveloped an open-sourced and unified package to facilitate reproducible\nevaluation and inspire further innovative research, which is available at\nhttps://github.com/RingBDStack/IGL-Bench.\n","authors":["Jiawen Qin","Haonan Yuan","Qingyun Sun","Lyujin Xu","Jiaqi Yuan","Pengfeng Huang","Zhaonan Wang","Xingcheng Fu","Hao Peng","Jianxin Li","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2406.09870v1.pdf","comment":"The Thirty-eight Conference on Neural Information Processing Systems\n  Datasets and Benchmarks Track (Preprint, under review)"},{"id":"http://arxiv.org/abs/2406.09864v1","updated":"2024-06-14T09:22:07Z","published":"2024-06-14T09:22:07Z","title":"LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data","summary":"  Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We introduce LUMA, a unique benchmark\ndataset, featuring audio, image, and textual data from 50 classes, for learning\nfrom uncertain and multimodal data. It extends the well-known CIFAR 10/100\ndataset with audio samples extracted from three audio corpora, and text data\ngenerated using the Gemma-7B Large Language Model (LLM). The LUMA dataset\nenables the controlled injection of varying types and degrees of uncertainty to\nachieve and tailor specific experiments and benchmarking initiatives. LUMA is\nalso available as a Python package including the functions for generating\nmultiple variants of the dataset with controlling the diversity of the data,\nthe amount of noise for each modality, and adding out-of-distribution samples.\nA baseline pre-trained model is also provided alongside three uncertainty\nquantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable\nConflictive Multi-View Learning. This comprehensive dataset and its tools are\nintended to promote and support the development and benchmarking of trustworthy\nand robust multimodal deep learning approaches.\n","authors":["Grigor Bezirganyan","Sana Sellami","Laure Berti-Équille","Sébastien Fournier"],"pdf_url":"https://arxiv.org/pdf/2406.09864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09860v1","updated":"2024-06-14T09:20:44Z","published":"2024-06-14T09:20:44Z","title":"Dataset Condensation with Latent Quantile Matching","summary":"  Dataset condensation (DC) methods aim to learn a smaller synthesized dataset\nwith informative data records to accelerate the training of machine learning\nmodels. Current distribution matching (DM) based DC methods learn a synthesized\ndataset by matching the mean of the latent embeddings between the synthetic and\nthe real dataset. However two distributions with the same mean can still be\nvastly different. In this work we demonstrate the shortcomings of using Maximum\nMean Discrepancy to match latent distributions i.e. the weak matching power and\nlack of outlier regularization. To alleviate these shortcomings we propose our\nnew method: Latent Quantile Matching (LQM) which matches the quantiles of the\nlatent embeddings to minimize the goodness of fit test statistic between two\ndistributions. Empirical experiments on both image and graph-structured\ndatasets show that LQM matches or outperforms previous state of the art in\ndistribution matching based DC. Moreover we show that LQM improves the\nperformance in continual graph learning (CGL) setting where memory efficiency\nand privacy can be important. Our work sheds light on the application of DM\nbased DC for CGL.\n","authors":["Wei Wei","Tom De Schepper","Kevin Mets"],"pdf_url":"https://arxiv.org/pdf/2406.09860v1.pdf","comment":"Accepted by CVPR Workshop 2024: 1st Workshop on Dataset Distillation\n  for Computer Vision"},{"id":"http://arxiv.org/abs/2406.09841v1","updated":"2024-06-14T08:48:10Z","published":"2024-06-14T08:48:10Z","title":"Learning Multi-view Molecular Representations with Structured and\n  Unstructured Knowledge","summary":"  Capturing molecular knowledge with representation learning approaches holds\nsignificant potential in vast scientific fields such as chemistry and life\nscience. An effective and generalizable molecular representation is expected to\ncapture the consensus and complementary molecular expertise from diverse views\nand perspectives. However, existing works fall short in learning multi-view\nmolecular representations, due to challenges in explicitly incorporating view\ninformation and handling molecular knowledge from heterogeneous sources. To\naddress these issues, we present MV-Mol, a molecular representation learning\nmodel that harvests multi-view molecular expertise from chemical structures,\nunstructured knowledge from biomedical texts, and structured knowledge from\nknowledge graphs. We utilize text prompts to model view information and design\na fusion architecture to extract view-based molecular representations. We\ndevelop a two-stage pre-training procedure, exploiting heterogeneous data of\nvarying quality and quantity. Through extensive experiments, we show that\nMV-Mol provides improved representations that substantially benefit molecular\nproperty prediction. Additionally, MV-Mol exhibits state-of-the-art performance\nin multi-modal comprehension of molecular structures and texts. Code and data\nare available at https://github.com/PharMolix/OpenBioMed.\n","authors":["Yizhen Luo","Kai Yang","Massimo Hong","Xing Yi Liu","Zikun Nie","Hao Zhou","Zaiqing Nie"],"pdf_url":"https://arxiv.org/pdf/2406.09841v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.09837v1","updated":"2024-06-14T08:46:33Z","published":"2024-06-14T08:46:33Z","title":"TabularFM: An Open Framework For Tabular Foundational Models","summary":"  Foundational models (FMs), pretrained on extensive datasets using\nself-supervised techniques, are capable of learning generalized patterns from\nlarge amounts of data. This reduces the need for extensive labeled datasets for\neach new task, saving both time and resources by leveraging the broad knowledge\nbase established during pretraining. Most research on FMs has primarily focused\non unstructured data, such as text and images, or semi-structured data, like\ntime-series. However, there has been limited attention to structured data, such\nas tabular data, which, despite its prevalence, remains under-studied due to a\nlack of clean datasets and insufficient research on the transferability of FMs\nfor various tabular data tasks. In response to this gap, we introduce a\nframework called TabularFM (\\url{https://tabularfm.github.io/}), which\nincorporates state-of-the-art methods for developing FMs specifically for\ntabular data. This includes variations of neural architectures such as GANs,\nVAEs, and Transformers. We have curated a million of tabular datasets and\nreleased cleaned versions to facilitate the development of tabular FMs. We\npretrained FMs on this curated data, benchmarked various learning methods on\nthese datasets, and released the pretrained models along with leaderboards for\nfuture comparative studies. Our fully open-sourced system provides a\ncomprehensive analysis of the transferability of tabular FMs. By releasing\nthese datasets, pretrained models, and leaderboards, we aim to enhance the\nvalidity and usability of tabular FMs in the near future.\n","authors":["Quan M. Tran","Suong N. Hoang","Lam M. Nguyen","Dzung Phan","Hoang Thanh Lam"],"pdf_url":"https://arxiv.org/pdf/2406.09837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09836v1","updated":"2024-06-14T08:46:26Z","published":"2024-06-14T08:46:26Z","title":"Robustness-Inspired Defense Against Backdoor Attacks on Graph Neural\n  Networks","summary":"  Graph Neural Networks (GNNs) have achieved promising results in tasks such as\nnode classification and graph classification. However, recent studies reveal\nthat GNNs are vulnerable to backdoor attacks, posing a significant threat to\ntheir real-world adoption. Despite initial efforts to defend against specific\ngraph backdoor attacks, there is no work on defending against various types of\nbackdoor attacks where generated triggers have different properties. Hence, we\nfirst empirically verify that prediction variance under edge dropping is a\ncrucial indicator for identifying poisoned nodes. With this observation, we\npropose using random edge dropping to detect backdoors and theoretically show\nthat it can efficiently distinguish poisoned nodes from clean ones.\nFurthermore, we introduce a novel robust training strategy to efficiently\ncounteract the impact of the triggers. Extensive experiments on real-world\ndatasets show that our framework can effectively identify poisoned nodes,\nsignificantly degrade the attack success rate, and maintain clean accuracy when\ndefending against various types of graph backdoor attacks with different\nproperties.\n","authors":["Zhiwei Zhang","Minhua Lin","Junjie Xu","Zongyu Wu","Enyan Dai","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09835v1","updated":"2024-06-14T08:44:51Z","published":"2024-06-14T08:44:51Z","title":"I Know How: Combining Prior Policies to Solve New Tasks","summary":"  Multi-Task Reinforcement Learning aims at developing agents that are able to\ncontinually evolve and adapt to new scenarios. However, this goal is\nchallenging to achieve due to the phenomenon of catastrophic forgetting and the\nhigh demand of computational resources. Learning from scratch for each new task\nis not a viable or sustainable option, and thus agents should be able to\ncollect and exploit prior knowledge while facing new problems. While several\nmethodologies have attempted to address the problem from different\nperspectives, they lack a common structure. In this work, we propose a new\nframework, I Know How (IKH), which provides a common formalization. Our\nmethodology focuses on modularity and compositionality of knowledge in order to\nachieve and enhance agent's ability to learn and adapt efficiently to dynamic\nenvironments. To support our framework definition, we present a simple\napplication of it in a simulated driving environment and compare its\nperformance with that of state-of-the-art approaches.\n","authors":["Malio Li","Elia Piccoli","Vincenzo Lomonaco","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2406.09835v1.pdf","comment":"7 pages, Conference on Games (CoG) 2024"},{"id":"http://arxiv.org/abs/2406.09831v1","updated":"2024-06-14T08:40:58Z","published":"2024-06-14T08:40:58Z","title":"Federated Learning driven Large Language Models for Swarm Intelligence:\n  A Survey","summary":"  Federated learning (FL) offers a compelling framework for training large\nlanguage models (LLMs) while addressing data privacy and decentralization\nchallenges. This paper surveys recent advancements in the federated learning of\nlarge language models, with a particular focus on machine unlearning, a crucial\naspect for complying with privacy regulations like the Right to be Forgotten.\nMachine unlearning in the context of federated LLMs involves systematically and\nsecurely removing individual data contributions from the learned model without\nretraining from scratch. We explore various strategies that enable effective\nunlearning, such as perturbation techniques, model decomposition, and\nincremental learning, highlighting their implications for maintaining model\nperformance and data privacy. Furthermore, we examine case studies and\nexperimental results from recent literature to assess the effectiveness and\nefficiency of these approaches in real-world scenarios. Our survey reveals a\ngrowing interest in developing more robust and scalable federated unlearning\nmethods, suggesting a vital area for future research in the intersection of AI\nethics and distributed machine learning technologies.\n","authors":["Youyang Qu"],"pdf_url":"https://arxiv.org/pdf/2406.09831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09143v2","updated":"2024-06-14T08:33:11Z","published":"2024-06-13T14:11:19Z","title":"Generative AI-based Prompt Evolution Engineering Design Optimization\n  With Vision-Language Model","summary":"  Engineering design optimization requires an efficient combination of a 3D\nshape representation, an optimization algorithm, and a design performance\nevaluation method, which is often computationally expensive. We present a\nprompt evolution design optimization (PEDO) framework contextualized in a\nvehicle design scenario that leverages a vision-language model for penalizing\nimpractical car designs synthesized by a generative model. The backbone of our\nframework is an evolutionary strategy coupled with an optimization objective\nfunction that comprises a physics-based solver and a vision-language model for\npractical or functional guidance in the generated car designs. In the prompt\nevolutionary search, the optimizer iteratively generates a population of text\nprompts, which embed user specifications on the aerodynamic performance and\nvisual preferences of the 3D car designs. Then, in addition to the\ncomputational fluid dynamics simulations, the pre-trained vision-language model\nis used to penalize impractical designs and, thus, foster the evolutionary\nalgorithm to seek more viable designs. Our investigations on a car design\noptimization problem show a wide spread of potential car designs generated at\nthe early phase of the search, which indicates a good diversity of designs in\nthe initial populations, and an increase of over 20\\% in the probability of\ngenerating practical designs compared to a baseline framework without using a\nvision-language model. Visual inspection of the designs against the performance\nresults demonstrates prompt evolution as a very promising paradigm for finding\nnovel designs with good optimization performance while providing ease of use in\nspecifying design specifications and preferences via a natural language\ninterface.\n","authors":["Melvin Wong","Thiago Rios","Stefan Menzel","Yew Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2406.09143v2.pdf","comment":"Accepted and to be published in IEEE Congress on Evolutionary\n  Computation (CEC) 2024. Copyright 2024 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses"},{"id":"http://arxiv.org/abs/2406.09827v1","updated":"2024-06-14T08:32:45Z","published":"2024-06-14T08:32:45Z","title":"HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical\n  Attention Pruning","summary":"  In modern large language models (LLMs), increasing sequence lengths is a\ncrucial challenge for enhancing their comprehension and coherence in handling\ncomplex tasks such as multi-modal question answering. However, handling long\ncontext sequences with LLMs is prohibitively costly due to the conventional\nattention mechanism's quadratic time and space complexity, and the context\nwindow size is limited by the GPU memory. Although recent works have proposed\nlinear and sparse attention mechanisms to address this issue, their real-world\napplicability is often limited by the need to re-train pre-trained models. In\nresponse, we propose a novel approach, Hierarchically Pruned Attention (HiP),\nwhich simultaneously reduces the training and inference time complexity from\n$O(T^2)$ to $O(T \\log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To\nthis end, we devise a dynamic sparse attention mechanism that generates an\nattention mask through a novel tree-search-like algorithm for a given query on\nthe fly. HiP is training-free as it only utilizes the pre-trained attention\nscores to spot the positions of the top-$k$ most significant elements for each\nquery. Moreover, it ensures that no token is overlooked, unlike the sliding\nwindow-based sub-quadratic attention methods, such as StreamingLLM. Extensive\nexperiments on diverse real-world benchmarks demonstrate that HiP significantly\nreduces prompt (i.e., prefill) and decoding latency and memory usage while\nmaintaining high generation performance with little or no degradation. As HiP\nallows pretrained LLMs to scale to millions of tokens on commodity GPUs with no\nadditional engineering due to its easy plug-and-play deployment, we believe\nthat our work will have a large practical impact, opening up the possibility to\nmany long-context LLM applications previously infeasible.\n","authors":["Heejun Lee","Geon Park","Youngwan Lee","Jina Kim","Wonyoung Jeong","Myeongjae Jeon","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2406.09827v1.pdf","comment":"26 pages, 15 figures"},{"id":"http://arxiv.org/abs/2406.09825v1","updated":"2024-06-14T08:29:34Z","published":"2024-06-14T08:29:34Z","title":"Unraveling Anomalies in Time: Unsupervised Discovery and Isolation of\n  Anomalous Behavior in Bio-regenerative Life Support System Telemetry","summary":"  The detection of abnormal or critical system states is essential in condition\nmonitoring. While much attention is given to promptly identifying anomalies, a\nretrospective analysis of these anomalies can significantly enhance our\ncomprehension of the underlying causes of observed undesired behavior. This\naspect becomes particularly critical when the monitored system is deployed in a\nvital environment. In this study, we delve into anomalies within the domain of\nBio-Regenerative Life Support Systems (BLSS) for space exploration and analyze\nanomalies found in telemetry data stemming from the EDEN ISS space greenhouse\nin Antarctica. We employ time series clustering on anomaly detection results to\ncategorize various types of anomalies in both uni- and multivariate settings.\nWe then assess the effectiveness of these methods in identifying systematic\nanomalous behavior. Additionally, we illustrate that the anomaly detection\nmethods MDI and DAMP produce complementary results, as previously indicated by\nresearch.\n","authors":["Ferdinand Rewicki","Jakob Gawlikowski","Julia Niebling","Joachim Denzler"],"pdf_url":"https://arxiv.org/pdf/2406.09825v1.pdf","comment":"12 pages, + Supplemental Materials, Accepted at ECML PKDD 2024\n  (European Conference on Machine Learning and Principles and Practice of\n  Knowledge Discovery in Databases)"},{"id":"http://arxiv.org/abs/2406.09822v1","updated":"2024-06-14T08:24:52Z","published":"2024-06-14T08:24:52Z","title":"An I2I Inpainting Approach for Efficient Channel Knowledge Map\n  Construction","summary":"  Channel knowledge map (CKM) has received widespread attention as an emerging\nenabling technology for environment-aware wireless communications. It involves\nthe construction of databases containing location-specific channel knowledge,\nwhich are then leveraged to facilitate channel state information (CSI)\nacquisition and transceiver design. In this context, a fundamental challenge\nlies in efficiently constructing the CKM based on a given wireless propagation\nenvironment. Most existing methods are based on stochastic modeling and\nsequence prediction, which do not fully exploit the inherent physical\ncharacteristics of the propagation environment, resulting in low accuracy and\nhigh computational complexity. To address these limitations, we propose a\nLaplacian pyramid (LP)-based CKM construction scheme to predict the channel\nknowledge at arbitrary locations in a targeted area. Specifically, we first\nview the channel knowledge as a 2-D image and transform the CKM construction\nproblem into an image-to-image (I2I) inpainting task, which predicts the\nchannel knowledge at a specific location by recovering the corresponding pixel\nvalue in the image matrix. Then, inspired by the reversible and closed-form\nstructure of the LP, we show its natural suitability for our task in designing\na fast I2I mapping network. For different frequency components of LP\ndecomposition, we design tailored networks accordingly. Besides, to encode the\nglobal structural information of the propagation environment, we introduce\nself-attention and cross-covariance attention mechanisms in different layers,\nrespectively. Finally, experimental results show that the proposed scheme\noutperforms the benchmark, achieving higher reconstruction accuracy while with\nlower computational complexity. Moreover, the proposed approach has a strong\ngeneralization ability and can be implemented in different wireless\ncommunication scenarios.\n","authors":["Zhenzhou Jin","Li You","Jue Wang","Xiang-Gen Xia","Xiqi Gao"],"pdf_url":"https://arxiv.org/pdf/2406.09822v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.08528v2","updated":"2024-06-14T08:19:28Z","published":"2024-06-12T08:51:08Z","title":"Adaptive Teaching with Shared Classifier for Knowledge Distillation","summary":"  Knowledge distillation (KD) is a technique used to transfer knowledge from an\noverparameterized teacher network to a less-parameterized student network,\nthereby minimizing the incurred performance loss. KD methods can be categorized\ninto offline and online approaches. Offline KD leverages a powerful pretrained\nteacher network, while online KD allows the teacher network to be adjusted\ndynamically to enhance the learning effectiveness of the student network.\nRecently, it has been discovered that sharing the classifier of the teacher\nnetwork can significantly boost the performance of the student network with\nonly a minimal increase in the number of network parameters. Building on these\ninsights, we propose adaptive teaching with a shared classifier (ATSC). In\nATSC, the pretrained teacher network self-adjusts to better align with the\nlearning needs of the student network based on its capabilities, and the\nstudent network benefits from the shared classifier, enhancing its performance.\nAdditionally, we extend ATSC to environments with multiple teachers. We conduct\nextensive experiments, demonstrating the effectiveness of the proposed KD\nmethod. Our approach achieves state-of-the-art results on the CIFAR-100 and\nImageNet datasets in both single-teacher and multiteacher scenarios, with only\na modest increase in the number of required model parameters. The source code\nis publicly available at https://github.com/random2314235/ATSC.\n","authors":["Jaeyeon Jang","Young-Ik Kim","Jisu Lim","Hyeonseong Lee"],"pdf_url":"https://arxiv.org/pdf/2406.08528v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06831v3","updated":"2024-06-14T08:11:11Z","published":"2024-04-10T08:47:57Z","title":"Generalized Linear Bandits with Limited Adaptivity","summary":"  We study the generalized linear contextual bandit problem within the\nconstraints of limited adaptivity. In this paper, we present two algorithms,\n$\\texttt{B-GLinCB}$ and $\\texttt{RS-GLinCB}$, that address, respectively, two\nprevalent limited adaptivity settings. Given a budget $M$ on the number of\npolicy updates, in the first setting, the algorithm needs to decide upfront $M$\nrounds at which it will update its policy, while in the second setting it can\nadaptively perform $M$ policy updates during its course. For the first setting,\nwe design an algorithm $\\texttt{B-GLinCB}$, that incurs $\\tilde{O}(\\sqrt{T})$\nregret when $M = \\Omega\\left( \\log{\\log T} \\right)$ and the arm feature vectors\nare generated stochastically. For the second setting, we design an algorithm\n$\\texttt{RS-GLinCB}$ that updates its policy $\\tilde{O}(\\log^2 T)$ times and\nachieves a regret of $\\tilde{O}(\\sqrt{T})$ even when the arm feature vectors\nare adversarially generated. Notably, in these bounds, we manage to eliminate\nthe dependence on a key instance dependent parameter $\\kappa$, that captures\nnon-linearity of the underlying reward model. Our novel approach for removing\nthis dependence for generalized linear contextual bandits might be of\nindependent interest.\n","authors":["Ayush Sawarni","Nirjhar Das","Siddharth Barman","Gaurav Sinha"],"pdf_url":"https://arxiv.org/pdf/2404.06831v3.pdf","comment":"Reorganization; New Experiments"},{"id":"http://arxiv.org/abs/2405.13372v3","updated":"2024-06-14T08:01:09Z","published":"2024-05-22T06:15:50Z","title":"Ada-HGNN: Adaptive Sampling for Scalable Hypergraph Neural Networks","summary":"  Hypergraphs serve as an effective model for depicting complex connections in\nvarious real-world scenarios, from social to biological networks. The\ndevelopment of Hypergraph Neural Networks (HGNNs) has emerged as a valuable\nmethod to manage the intricate associations in data, though scalability is a\nnotable challenge due to memory limitations. In this study, we introduce a new\nadaptive sampling strategy specifically designed for hypergraphs, which tackles\ntheir unique complexities in an efficient manner. We also present a Random\nHyperedge Augmentation (RHA) technique and an additional Multilayer Perceptron\n(MLP) module to improve the robustness and generalization capabilities of our\napproach. Thorough experiments with real-world datasets have proven the\neffectiveness of our method, markedly reducing computational and memory demands\nwhile maintaining performance levels akin to conventional HGNNs and other\nbaseline models. This research paves the way for improving both the scalability\nand efficacy of HGNNs in extensive applications. We will also make our codebase\npublicly accessible.\n","authors":["Shuai Wang","David W. Zhang","Jia-Hong Huang","Stevan Rudinac","Monika Kackovic","Nachoem Wijnberg","Marcel Worring"],"pdf_url":"https://arxiv.org/pdf/2405.13372v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12768v2","updated":"2024-06-14T07:59:15Z","published":"2023-10-19T14:13:12Z","title":"SemantIC: Semantic Interference Cancellation Towards 6G Wireless\n  Communications","summary":"  This letter proposes a novel anti-interference technique, semantic\ninterference cancellation (SemantIC), for enhancing information quality towards\nthe sixth-generation (6G) wireless networks. SemantIC only requires the\nreceiver to concatenate the channel decoder with a semantic auto-encoder. This\nconstructs a turbo loop which iteratively and alternately eliminates noise in\nthe signal domain and the semantic domain. From the viewpoint of network\ninformation theory, the neural network of the semantic auto-encoder stores side\ninformation by training, and provides side information in iterative decoding,\nas an implementation of the Wyner-Ziv theorem. Simulation results verify the\nperformance improvement by SemantIC without extra channel resource cost.\n","authors":["Wensheng Lin","Yuna Yan","Lixin Li","Zhu Han","Tad Matsumoto"],"pdf_url":"https://arxiv.org/pdf/2310.12768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09795v1","updated":"2024-06-14T07:45:07Z","published":"2024-06-14T07:45:07Z","title":"DeltaPhi: Learning Physical Trajectory Residual for PDE Solving","summary":"  Although neural operator networks theoretically approximate any operator\nmapping, the limited generalization capability prevents them from learning\ncorrect physical dynamics when potential data biases exist, particularly in the\npractical PDE solving scenario where the available data amount is restricted or\nthe resolution is extremely low. To address this issue, we propose and\nformulate the Physical Trajectory Residual Learning (DeltaPhi), which learns to\npredict the physical residuals between the pending solved trajectory and a\nknown similar auxiliary trajectory. First, we transform the direct operator\nmapping between input-output function fields in original training data to\nresidual operator mapping between input function pairs and output function\nresiduals. Next, we learn the surrogate model for the residual operator mapping\nbased on existing neural operator networks. Additionally, we design helpful\ncustomized auxiliary inputs for efficient optimization. Through extensive\nexperiments, we conclude that, compared to direct learning, physical residual\nlearning is preferred for PDE solving.\n","authors":["Xihang Yue","Linchao Zhu","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2406.09795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09776v1","updated":"2024-06-14T07:22:39Z","published":"2024-06-14T07:22:39Z","title":"Faster Convergence on Heterogeneous Federated Edge Learning: An Adaptive\n  Sidelink-Assisted Data Multicasting Approach","summary":"  Federated Edge Learning (FEEL) emerges as a pioneering distributed machine\nlearning paradigm for the 6G Hyper-Connectivity, harnessing data from the\nInternet of Things (IoT) devices while upholding data privacy. However, current\nFEEL algorithms struggle with non-independent and non-identically distributed\n(non-IID) data, leading to elevated communication costs and compromised model\naccuracy. To address these statistical imbalances within FEEL, we introduce a\nclustered data sharing framework, mitigating data heterogeneity by selectively\nsharing partial data from cluster heads to trusted associates through\nsidelink-aided multicasting. The collective communication pattern is integral\nto FEEL training, where both cluster formation and the efficiency of\ncommunication and computation impact training latency and accuracy\nsimultaneously. To tackle the strictly coupled data sharing and resource\noptimization, we decompose the overall optimization problem into the clients\nclustering and effective data sharing subproblems. Specifically, a\ndistribution-based adaptive clustering algorithm (DACA) is devised basing on\nthree deductive cluster forming conditions, which ensures the maximum sharing\nyield. Meanwhile, we design a stochastic optimization based joint computed\nfrequency and shared data volume optimization (JFVO) algorithm, determining the\noptimal resource allocation with an uncertain objective function. The\nexperiments show that the proposed framework facilitates FEEL on non-IID\ndatasets with faster convergence rate and higher model accuracy in a limited\ncommunication environment.\n","authors":["Gang Hu","Yinglei Teng","Nan Wang","Zhu Han"],"pdf_url":"https://arxiv.org/pdf/2406.09776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03280v3","updated":"2024-06-14T07:19:51Z","published":"2024-06-05T13:54:28Z","title":"FusionBench: A Comprehensive Benchmark of Deep Model Fusion","summary":"  Deep model fusion is an emerging technique that unifies the predictions or\nparameters of several deep neural networks into a single model in a\ncost-effective and data-efficient manner. This enables the unified model to\ntake advantage of the original models' strengths, potentially exceeding their\nperformance. Although a variety of deep model fusion techniques have been\nintroduced, their evaluations tend to be inconsistent and often inadequate to\nvalidate their effectiveness and robustness against distribution shifts. To\naddress this issue, we introduce FusionBench, which is the first comprehensive\nbenchmark dedicated to deep model fusion. FusionBench covers a wide range of\ntasks, including open-vocabulary image classification, text classification, and\ntext-to-text generation. Each category includes up to eight tasks with\ncorresponding task-specific models, featuring both full fine-tuning and LoRA\nfine-tuning, as well as models of different sizes, to ensure fair and balanced\ncomparisons of various multi-task model fusion techniques across different\ntasks, model scales, and fine-tuning strategies. We implement and evaluate a\nbroad spectrum of deep model fusion techniques. These techniques range from\nmodel ensemble methods, which combine the predictions to improve the overall\nperformance, to model merging, which integrates different models into a single\none, and model mixing methods, which upscale or recombine the components of the\noriginal models. FusionBench now contains 26 distinct tasks, 74 fine-tuned\nmodels, and 16 fusion techniques, and we are committed to consistently\nexpanding the benchmark with more tasks, models, and fusion techniques. In\naddition, we offer a well-documented set of resources and guidelines to aid\nresearchers in understanding and replicating the benchmark results. Homepage\nhttps://github.com/tanganke/fusion_bench\n","authors":["Anke Tang","Li Shen","Yong Luo","Han Hu","Bo Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2406.03280v3.pdf","comment":"Project homepage: https://github.com/tanganke/fusion_bench"},{"id":"http://arxiv.org/abs/2406.09770v1","updated":"2024-06-14T07:16:18Z","published":"2024-06-14T07:16:18Z","title":"Towards Efficient Pareto Set Approximation via Mixture of Experts Based\n  Model Fusion","summary":"  Solving multi-objective optimization problems for large deep neural networks\nis a challenging task due to the complexity of the loss landscape and the\nexpensive computational cost of training and evaluating models. Efficient\nPareto front approximation of large models enables multi-objective optimization\nfor various tasks such as multi-task learning and trade-off analysis. Existing\nalgorithms for learning Pareto set, including (1) evolutionary, hypernetworks,\nand hypervolume-maximization methods, are computationally expensive and have\nrestricted scalability to large models; (2) Scalarization algorithms, where a\nseparate model is trained for each objective ray, which is inefficient for\nlearning the entire Pareto set and fails to capture the objective trade-offs\neffectively. Inspired by the recent success of model merging, we propose a\npractical and scalable approach to Pareto set learning problem via mixture of\nexperts (MoE) based model fusion. By ensembling the weights of specialized\nsingle-task models, the MoE module can effectively capture the trade-offs\nbetween multiple objectives and closely approximate the entire Pareto set of\nlarge neural networks. Once the routers are learned and a preference vector is\nset, the MoE module can be unloaded, thus no additional computational cost is\nintroduced during inference. We conduct extensive experiments on vision and\nlanguage tasks using large-scale models such as CLIP-ViT and GPT-2. The\nexperimental results demonstrate that our method efficiently approximates the\nentire Pareto front of large models. Using only hundreds of trainable\nparameters of the MoE routers, our method even has lower memory usage compared\nto linear scalarization and algorithms that learn a single Pareto optimal\nsolution, and are scalable to both the number of objectives and the size of the\nmodel.\n","authors":["Anke Tang","Li Shen","Yong Luo","Shiwei Liu","Han Hu","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2406.09770v1.pdf","comment":"code is available at https://github.com/tanganke/pareto_set_learning"},{"id":"http://arxiv.org/abs/2406.09768v1","updated":"2024-06-14T07:13:03Z","published":"2024-06-14T07:13:03Z","title":"Bayesian Conditioned Diffusion Models for Inverse Problems","summary":"  Diffusion models have recently been shown to excel in many image\nreconstruction tasks that involve inverse problems based on a forward\nmeasurement operator. A common framework uses task-agnostic unconditional\nmodels that are later post-conditioned for reconstruction, an approach that\ntypically suffers from suboptimal task performance. While task-specific\nconditional models have also been proposed, current methods heuristically\ninject measured data as a naive input channel that elicits sampling\ninaccuracies. Here, we address the optimal conditioning of diffusion models for\nsolving challenging inverse problems that arise during image reconstruction.\nSpecifically, we propose a novel Bayesian conditioning technique for diffusion\nmodels, BCDM, based on score-functions associated with the conditional\ndistribution of desired images given measured data. We rigorously derive the\ntheory to express and train the conditional score-function. Finally, we show\nstate-of-the-art performance in image dealiasing, deblurring, super-resolution,\nand inpainting with the proposed technique.\n","authors":["Alper Güngör","Bahri Batuhan Bilecen","Tolga Çukur"],"pdf_url":"https://arxiv.org/pdf/2406.09768v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2403.07362v2","updated":"2024-06-14T07:03:36Z","published":"2024-03-12T06:50:32Z","title":"Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine\n  Unlearning","summary":"  The trustworthy machine learning (ML) community is increasingly recognizing\nthe crucial need for models capable of selectively 'unlearning' data points\nafter training. This leads to the problem of machine unlearning (MU), aiming to\neliminate the influence of chosen data points on model performance, while still\nmaintaining the model's utility post-unlearning. Despite various MU methods for\ndata influence erasure, evaluations have largely focused on random data\nforgetting, ignoring the vital inquiry into which subset should be chosen to\ntruly gauge the authenticity of unlearning performance. To tackle this issue,\nwe introduce a new evaluative angle for MU from an adversarial viewpoint. We\npropose identifying the data subset that presents the most significant\nchallenge for influence erasure, i.e., pinpointing the worst-case forget set.\nUtilizing a bi-level optimization principle, we amplify unlearning challenges\nat the upper optimization level to emulate worst-case scenarios, while\nsimultaneously engaging in standard training and unlearning at the lower level,\nachieving a balance between data influence erasure and model utility. Our\nproposal offers a worst-case evaluation of MU's resilience and effectiveness.\nThrough extensive experiments across different datasets (including CIFAR-10,\n100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image\nclassifiers and generative models), we expose critical pros and cons in\nexisting (approximate) unlearning strategies. Our results illuminate the\ncomplex challenges of MU in practice, guiding the future development of more\naccurate and robust unlearning algorithms. The code is available at\nhttps://github.com/OPTML-Group/Unlearn-WorstCase.\n","authors":["Chongyu Fan","Jiancheng Liu","Alfred Hero","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07362v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08550v3","updated":"2024-06-14T07:03:08Z","published":"2023-12-13T22:42:55Z","title":"Harmonics of Learning: Universal Fourier Features Emerge in Invariant\n  Networks","summary":"  In this work, we formally prove that, under certain conditions, if a neural\nnetwork is invariant to a finite group then its weights recover the Fourier\ntransform on that group. This provides a mathematical explanation for the\nemergence of Fourier features -- a ubiquitous phenomenon in both biological and\nartificial learning systems. The results hold even for non-commutative groups,\nin which case the Fourier transform encodes all the irreducible unitary group\nrepresentations. Our findings have consequences for the problem of symmetry\ndiscovery. Specifically, we demonstrate that the algebraic structure of an\nunknown group can be recovered from the weights of a network that is at least\napproximately invariant within certain bounds. Overall, this work contributes\nto a foundation for an algebraic learning theory of invariant neural network\nrepresentations.\n","authors":["Giovanni Luca Marchetti","Christopher Hillar","Danica Kragic","Sophia Sanborn"],"pdf_url":"https://arxiv.org/pdf/2312.08550v3.pdf","comment":"Accepted at the Conference on Learning Theory (COLT) 2024"},{"id":"http://arxiv.org/abs/2406.09761v1","updated":"2024-06-14T06:59:37Z","published":"2024-06-14T06:59:37Z","title":"Towards Full Integration of Artificial Intelligence in Colon Capsule\n  Endoscopy's Pathway","summary":"  Despite recent surge of interest in deploying colon capsule endoscopy (CCE)\nfor early diagnosis of colorectal diseases, there remains a large gap between\nthe current state of CCE in clinical practice, and the state of its counterpart\noptical colonoscopy (OC). Our study is aimed at closing this gap, by focusing\non the full integration of AI in CCE's pathway, where image processing steps\nlinked to the detection, localization and characterisation of important\nfindings are carried out autonomously using various AI algorithms. We developed\na recognition network, that with an impressive sensitivity of 99.9%, a\nspecificity of 99.4%, and a negative predictive value (NPV) of 99.8%, detected\ncolorectal polyps. After recognising a polyp within a sequence of images, only\nthose images containing polyps were fed into two parallel independent networks\nfor characterisation, and estimation of the size of those important findings.\nThe characterisation network reached a sensitivity of 82% and a specificity of\n80% in classifying polyps to two groups, namely neoplastic vs. non-neoplastic.\nThe size estimation network reached an accuracy of 88% in correctly segmenting\nthe polyps. By automatically incorporating this crucial information into CCE's\npathway, we moved a step closer towards the full integration of AI in CCE's\nroutine clinical practice.\n","authors":["Esmaeil S. Nadimi","Jan-Matthias Braun","Benedicte Schelde-Olesen","Emile Prudhomme","Victoria Blanes-Vidal","Gunnar Baatrup"],"pdf_url":"https://arxiv.org/pdf/2406.09761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09760v1","updated":"2024-06-14T06:57:18Z","published":"2024-06-14T06:57:18Z","title":"Bootstrapping Language Models with DPO Implicit Rewards","summary":"  Human alignment in large language models (LLMs) is an active area of\nresearch. A recent groundbreaking work, direct preference optimization (DPO),\nhas greatly simplified the process from past work in reinforcement learning\nfrom human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO,\nafter training, provides an implicit reward model. In this work, we make a\nnovel observation that this implicit reward model can by itself be used in a\nbootstrapping fashion to further align the LLM. Our approach is to use the\nrewards from a current LLM model to construct a preference dataset, which is\nthen used in subsequent DPO rounds. We incorporate refinements that debias the\nlength of the responses and improve the quality of the preference dataset to\nfurther improve our approach. Our approach, named self-alignment with DPO\nImpliCit rEwards (DICE), shows great improvements in alignment and achieves\nsuperior performance than Gemini Pro on AlpacaEval 2, reaching 27.55%\nlength-controlled win rate against GPT-4 Turbo, but with only 8B parameters and\nno external feedback. Our code is available at https://github.com/sail-sg/dice.\n","authors":["Changyu Chen","Zichen Liu","Chao Du","Tianyu Pang","Qian Liu","Arunesh Sinha","Pradeep Varakantham","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2406.09760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09757v1","updated":"2024-06-14T06:52:08Z","published":"2024-06-14T06:52:08Z","title":"Evaluating LLM-driven User-Intent Formalization for Verification-Aware\n  Languages","summary":"  Verification-aware programming languages such as Dafny and F* provide means\nto formally specify and prove properties of programs. Although the problem of\nchecking an implementation against a specification can be defined mechanically,\nthere is no algorithmic way of ensuring the correctness of the user-intent\nformalization for programs -- that a specification adheres to the user's intent\nbehind the program. The intent or requirement is expressed informally in\nnatural language and the specification is a formal artefact. The advent of\nlarge language models (LLMs) has made strides bridging the gap between informal\nintent and formal program implementations recently, driven in large parts due\nto benchmarks and automated metrics for evaluation.\n  Recent work has proposed evaluating {\\it user-intent formalization} problem\nfor mainstream programming languages~\\cite{endres-fse24}. However, such an\napproach does not readily extend to verification-aware languages that support\nrich specifications (containing quantifiers and ghost variables) that cannot be\nevaluated through dynamic execution. Previous work also required generating\nprogram mutants using LLMs to create the benchmark. We advocate an alternate\napproach of {\\it symbolically testing specifications} to provide an intuitive\nmetric for evaluating the quality of specifications for verification-aware\nlanguages. We demonstrate that our automated metric agrees closely with mostly\nGPT-4 generated and human-labeled dataset of roughly 150 Dafny specifications\nfor the popular MBPP code-generation benchmark, yet demonstrates cases where\nthe human labeling is not perfect. We believe our work provides a stepping\nstone to enable the establishment of a benchmark and research agenda for the\nproblem of user-intent formalization for programs.\n","authors":["Shuvendu K. Lahiri"],"pdf_url":"https://arxiv.org/pdf/2406.09757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05855v2","updated":"2024-06-14T06:30:22Z","published":"2024-06-09T16:58:19Z","title":"Self-Distilled Disentangled Learning for Counterfactual Prediction","summary":"  The advancements in disentangled representation learning significantly\nenhance the accuracy of counterfactual predictions by granting precise control\nover instrumental variables, confounders, and adjustable variables. An\nappealing method for achieving the independent separation of these factors is\nmutual information minimization, a task that presents challenges in numerous\nmachine learning scenarios, especially within high-dimensional spaces. To\ncircumvent this challenge, we propose the Self-Distilled Disentanglement\nframework, referred to as $SD^2$. Grounded in information theory, it ensures\ntheoretically sound independent disentangled representations without intricate\nmutual information estimator designs for high-dimensional representations. Our\ncomprehensive experiments, conducted on both synthetic and real-world datasets,\nconfirms the effectiveness of our approach in facilitating counterfactual\ninference in the presence of both observed and unobserved confounders.\n","authors":["Xinshu Li","Mingming Gong","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2406.05855v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09745v1","updated":"2024-06-14T06:28:17Z","published":"2024-06-14T06:28:17Z","title":"How Does Distribution Matching Help Domain Generalization: An\n  Information-theoretic Analysis","summary":"  Domain generalization aims to learn invariance across multiple training\ndomains, thereby enhancing generalization against out-of-distribution data.\nWhile gradient or representation matching algorithms have achieved remarkable\nsuccess, these methods generally lack generalization guarantees or depend on\nstrong assumptions, leaving a gap in understanding the underlying mechanism of\ndistribution matching. In this work, we formulate domain generalization from a\nnovel probabilistic perspective, ensuring robustness while avoiding overly\nconservative solutions. Through comprehensive information-theoretic analysis,\nwe provide key insights into the roles of gradient and representation matching\nin promoting generalization. Our results reveal the complementary relationship\nbetween these two components, indicating that existing works focusing solely on\neither gradient or representation alignment are insufficient to solve the\ndomain generalization problem. In light of these theoretical findings, we\nintroduce IDM to simultaneously align the inter-domain gradients and\nrepresentations. Integrated with the proposed PDM method for complex\ndistribution matching, IDM achieves superior performance over various baseline\nmethods.\n","authors":["Yuxin Dong","Tieliang Gong","Hong Chen","Shuangyong Song","Weizhan Zhang","Chen Li"],"pdf_url":"https://arxiv.org/pdf/2406.09745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01055v3","updated":"2024-06-14T06:26:48Z","published":"2023-10-02T10:05:30Z","title":"Improved Crop and Weed Detection with Diverse Data Ensemble Learning","summary":"  Modern agriculture heavily relies on Site-Specific Farm Management practices,\nnecessitating accurate detection, localization, and quantification of crops and\nweeds in the field, which can be achieved using deep learning techniques. In\nthis regard, crop and weed-specific binary segmentation models have shown\npromise. However, uncontrolled field conditions limit their performance from\none field to the other. To improve semantic model generalization, existing\nmethods augment and synthesize agricultural data to account for uncontrolled\nfield conditions. However, given highly varied field conditions, these methods\nhave limitations. To overcome the challenges of model deterioration in such\nconditions, we propose utilizing data specific to other crops and weeds for our\nspecific target problem. To achieve this, we propose a novel ensemble\nframework. Our approach involves utilizing different crop and weed models\ntrained on diverse datasets and employing a teacher-student configuration. By\nusing homogeneous stacking of base models and a trainable meta-architecture to\ncombine their outputs, we achieve significant improvements for Canola crops and\nKochia weeds on unseen test data, surpassing the performance of single semantic\nsegmentation models. We identify the UNET meta-architecture as the most\neffective in this context. Finally, through ablation studies, we demonstrate\nand validate the effectiveness of our proposed model. We observe that including\nbase models trained on other target crops and weeds can help generalize the\nmodel to capture varied field conditions. Lastly, we propose two novel datasets\nwith varied conditions for comparisons.\n","authors":["Muhammad Hamza Asad","Saeed Anwar","Abdul Bais"],"pdf_url":"https://arxiv.org/pdf/2310.01055v3.pdf","comment":"Accepted in CVPR Workshop as an Oral"},{"id":"http://arxiv.org/abs/2406.08287v2","updated":"2024-06-14T06:25:36Z","published":"2024-06-12T14:53:23Z","title":"Pre-Training Identification of Graph Winning Tickets in Adaptive\n  Spatial-Temporal Graph Neural Networks","summary":"  In this paper, we present a novel method to significantly enhance the\ncomputational efficiency of Adaptive Spatial-Temporal Graph Neural Networks\n(ASTGNNs) by introducing the concept of the Graph Winning Ticket (GWT), derived\nfrom the Lottery Ticket Hypothesis (LTH). By adopting a pre-determined star\ntopology as a GWT prior to training, we balance edge reduction with efficient\ninformation propagation, reducing computational demands while maintaining high\nmodel performance. Both the time and memory computational complexity of\ngenerating adaptive spatial-temporal graphs is significantly reduced from\n$\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$. Our approach streamlines the ASTGNN\ndeployment by eliminating the need for exhaustive training, pruning, and\nretraining cycles, and demonstrates empirically across various datasets that it\nis possible to achieve comparable performance to full models with substantially\nlower computational costs. Specifically, our approach enables training ASTGNNs\non the largest scale spatial-temporal dataset using a single A6000 equipped\nwith 48 GB of memory, overcoming the out-of-memory issue encountered during\noriginal training and even achieving state-of-the-art performance. Furthermore,\nwe delve into the effectiveness of the GWT from the perspective of spectral\ngraph theory, providing substantial theoretical support. This advancement not\nonly proves the existence of efficient sub-networks within ASTGNNs but also\nbroadens the applicability of the LTH in resource-constrained settings, marking\na significant step forward in the field of graph neural networks. Code is\navailable at https://anonymous.4open.science/r/paper-1430.\n","authors":["Wenying Duan","Tianxiang Fang","Hong Rao","Xiaoxi He"],"pdf_url":"https://arxiv.org/pdf/2406.08287v2.pdf","comment":"Conference paper, accepted by KDD' 24"},{"id":"http://arxiv.org/abs/2406.09740v1","updated":"2024-06-14T06:02:14Z","published":"2024-06-14T06:02:14Z","title":"Deep Symbolic Optimization for Combinatorial Optimization: Accelerating\n  Node Selection by Discovering Potential Heuristics","summary":"  Combinatorial optimization (CO) is one of the most fundamental mathematical\nmodels in real-world applications. Traditional CO solvers, such as\nBranch-and-Bound (B&B) solvers, heavily rely on expert-designed heuristics,\nwhich are reliable but require substantial manual tuning. Recent studies have\nleveraged deep learning (DL) models as an alternative to capture rich feature\npatterns for improved performance on GPU machines. Nonetheless, the drawbacks\nof high training and inference costs, as well as limited interpretability,\nseverely hinder the adoption of DL methods in real-world applications. To\naddress these challenges, we propose a novel deep symbolic optimization\nlearning framework that combines their advantages. Specifically, we focus on\nthe node selection module within B&B solvers -- namely, deep symbolic\noptimization for node selection (Dso4NS). With data-driven approaches, Dso4NS\nguides the search for mathematical expressions within the high-dimensional\ndiscrete symbolic space and then incorporates the highest-performing\nmathematical expressions into a solver. The data-driven model captures the rich\nfeature information in the input data and generates symbolic expressions, while\nthe expressions deployed in solvers enable fast inference with high\ninterpretability. Experiments demonstrate the effectiveness of Dso4NS in\nlearning high-quality expressions, outperforming existing approaches on a CPU\nmachine. Encouragingly, the learned CPU-based policies consistently achieve\nperformance comparable to state-of-the-art GPU-based approaches.\n","authors":["Hongyu Liu","Haoyang Liu","Yufei Kuang","Jie Wang","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2406.09740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00675v4","updated":"2024-06-14T05:57:01Z","published":"2024-05-01T17:59:20Z","title":"Self-Play Preference Optimization for Language Model Alignment","summary":"  Traditional reinforcement learning from human feedback (RLHF) approaches\nrelying on parametric models like the Bradley-Terry model fall short in\ncapturing the intransitivity and irrationality in human preferences. Recent\nadvancements suggest that directly working with preference probabilities can\nyield a more accurate reflection of human preferences, enabling more flexible\nand accurate language model alignment. In this paper, we propose a\nself-play-based method for language model alignment, which treats the problem\nas a constant-sum two-player game aimed at identifying the Nash equilibrium\npolicy. Our approach, dubbed Self-Play Preference Optimization (SPPO),\napproximates the Nash equilibrium through iterative policy updates and enjoys a\ntheoretical convergence guarantee. Our method can effectively increase the\nlog-likelihood of the chosen response and decrease that of the rejected\nresponse, which cannot be trivially achieved by symmetric pairwise loss such as\nDirect Preference Optimization (DPO) and Identity Preference Optimization\n(IPO). In our experiments, using only 60k prompts (without responses) from the\nUltraFeedback dataset and without any prompt augmentation, by leveraging a\npre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain\na model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the\nstate-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on\nAlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and\nthe Open LLM Leaderboard. Starting from a stronger base model\nLlama-3-8B-Instruct, we are able to achieve a length-controlled win rate of\n38.77%. Notably, the strong performance of SPPO is achieved without additional\nexternal supervision (e.g., responses, preferences, etc.) from GPT-4 or other\nstronger language models. Codes are available at\nhttps://github.com/uclaml/SPPO.\n","authors":["Yue Wu","Zhiqing Sun","Huizhuo Yuan","Kaixuan Ji","Yiming Yang","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2405.00675v4.pdf","comment":"27 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2311.12359v2","updated":"2024-06-14T05:39:24Z","published":"2023-11-21T05:27:16Z","title":"Shedding the Bits: Pushing the Boundaries of Quantization with\n  Minifloats on FPGAs","summary":"  Post-training quantization (PTQ) is a powerful technique for model\ncompression, reducing the numerical precision in neural networks without\nadditional training overhead. Recent works have investigated adopting 8-bit\nfloating-point formats(FP8) in the context of PTQ for model inference. However,\nfloating-point formats smaller than 8 bits and their relative comparison in\nterms of accuracy-hardware cost with integers remains unexplored on FPGAs. In\nthis work, we present minifloats, which are reduced-precision floating-point\nformats capable of further reducing the memory footprint, latency, and energy\ncost of a model while approaching full-precision model accuracy. We implement a\ncustom FPGA-based multiply-accumulate operator library and explore the vast\ndesign space, comparing minifloat and integer representations across 3 to 8\nbits for both weights and activations. We also examine the applicability of\nvarious integerbased quantization techniques to minifloats. Our experiments\nshow that minifloats offer a promising alternative for emerging workloads such\nas vision transformers.\n","authors":["Shivam Aggarwal","Hans Jakob Damsgaard","Alessandro Pappalardo","Giuseppe Franco","Thomas B. Preußer","Michaela Blott","Tulika Mitra"],"pdf_url":"https://arxiv.org/pdf/2311.12359v2.pdf","comment":"Accepted in FPL (International Conference on Field-Programmable Logic\n  and Applications) 2024 conference. Revised with updated results"},{"id":"http://arxiv.org/abs/2406.09723v1","updated":"2024-06-14T05:17:39Z","published":"2024-06-14T05:17:39Z","title":"When Will Gradient Regularization Be Harmful?","summary":"  Gradient regularization (GR), which aims to penalize the gradient norm atop\nthe loss function, has shown promising results in training modern\nover-parameterized deep neural networks. However, can we trust this powerful\ntechnique? This paper reveals that GR can cause performance degeneration in\nadaptive optimization scenarios, particularly with learning rate warmup. Our\nempirical and theoretical analyses suggest this is due to GR inducing\ninstability and divergence in gradient statistics of adaptive optimizers at the\ninitial training stage. Inspired by the warmup heuristic, we propose three GR\nwarmup strategies, each relaxing the regularization effect to a certain extent\nduring the warmup course to ensure the accurate and stable accumulation of\ngradients. With experiments on Vision Transformer family, we confirm the three\nGR warmup strategies can effectively circumvent these issues, thereby largely\nimproving the model performance. Meanwhile, we note that scalable models tend\nto rely more on the GR warmup, where the performance can be improved by up to\n3\\% on Cifar10 compared to baseline GR. Code is available at\n\\href{https://github.com/zhaoyang-0204/gnp}{https://github.com/zhaoyang-0204/gnp}.\n","authors":["Yang Zhao","Hao Zhang","Xiuyuan Hu"],"pdf_url":"https://arxiv.org/pdf/2406.09723v1.pdf","comment":"ICML 2024 paper"},{"id":"http://arxiv.org/abs/2406.09722v1","updated":"2024-06-14T05:14:54Z","published":"2024-06-14T05:14:54Z","title":"Cross-view geo-localization: a survey","summary":"  Cross-view geo-localization has garnered notable attention in the realm of\ncomputer vision, spurred by the widespread availability of copious geotagged\ndatasets and the advancements in machine learning techniques. This paper\nprovides a thorough survey of cutting-edge methodologies, techniques, and\nassociated challenges that are integral to this domain, with a focus on\nfeature-based and deep learning strategies. Feature-based methods capitalize on\nunique features to establish correspondences across disparate viewpoints,\nwhereas deep learning-based methodologies deploy convolutional neural networks\nto embed view-invariant attributes. This work also delineates the multifaceted\nchallenges encountered in cross-view geo-localization, such as variations in\nviewpoints and illumination, the occurrence of occlusions, and it elucidates\ninnovative solutions that have been formulated to tackle these issues.\nFurthermore, we delineate benchmark datasets and relevant evaluation metrics,\nand also perform a comparative analysis of state-of-the-art techniques.\nFinally, we conclude the paper with a discussion on prospective avenues for\nfuture research and the burgeoning applications of cross-view geo-localization\nin an intricately interconnected global landscape.\n","authors":["Abhilash Durgam","Sidike Paheding","Vikas Dhiman","Vijay Devabhaktuni"],"pdf_url":"https://arxiv.org/pdf/2406.09722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05175v3","updated":"2024-06-14T05:10:07Z","published":"2023-06-08T13:14:35Z","title":"Large-scale Dataset Pruning with Dynamic Uncertainty","summary":"  The state of the art of many learning tasks, e.g., image classification, is\nadvanced by collecting larger datasets and then training larger models on them.\nAs the outcome, the increasing computational cost is becoming unaffordable. In\nthis paper, we investigate how to prune the large-scale datasets, and thus\nproduce an informative subset for training sophisticated deep models with\nnegligible performance drop. We propose a simple yet effective dataset pruning\nmethod by exploring both the prediction uncertainty and training dynamics. We\nstudy dataset pruning by measuring the variation of predictions during the\nwhole training process on large-scale datasets, i.e., ImageNet-1K and\nImageNet-21K, and advanced models, i.e., Swin Transformer and ConvNeXt.\nExtensive experimental results indicate that our method outperforms the state\nof the art and achieves 25% lossless pruning ratio on both ImageNet-1K and\nImageNet-21K. The code and pruned datasets are available at\nhttps://github.com/BAAI-DCAI/Dataset-Pruning.\n","authors":["Muyang He","Shuo Yang","Tiejun Huang","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2306.05175v3.pdf","comment":"1st Workshop on Dataset Distillation for Computer Vision, CVPR2024,\n  see\n  https://openaccess.thecvf.com/content/CVPR2024W/DDCV/html/He_Large-scale_Dataset_Pruning_with_Dynamic_Uncertainty_CVPRW_2024_paper.html"},{"id":"http://arxiv.org/abs/2402.10228v5","updated":"2024-06-14T04:51:07Z","published":"2024-02-05T07:07:30Z","title":"Q-Star Meets Scalable Posterior Sampling: Bridging Theory and Practice\n  via HyperAgent","summary":"  We propose HyperAgent, a reinforcement learning (RL) algorithm based on the\nhypermodel framework for exploration in RL. HyperAgent allows for the efficient\nincremental approximation of posteriors associated with an optimal action-value\nfunction ($Q^\\star$) without the need for conjugacy and follows the greedy\npolicies w.r.t. these approximate posterior samples. We demonstrate that\nHyperAgent offers robust performance in large-scale deep RL benchmarks. It can\nsolve Deep Sea hard exploration problems with episodes that optimally scale\nwith problem size and exhibits significant efficiency gains in the Atari suite.\nImplementing HyperAgent requires minimal code addition to well-established deep\nRL frameworks like DQN. We theoretically prove that, under tabular assumptions,\nHyperAgent achieves logarithmic per-step computational complexity while\nattaining sublinear regret, matching the best known randomized tabular RL\nalgorithm.\n","authors":["Yingru Li","Jiawei Xu","Lei Han","Zhi-Quan Luo"],"pdf_url":"https://arxiv.org/pdf/2402.10228v5.pdf","comment":"Proceedings of the $\\mathit{41}^{st}$ International Conference on\n  Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the\n  author(s). Invited talk in Informs Optimization Conference 2024 and\n  International Symposium on Mathematical Programming 2024"},{"id":"http://arxiv.org/abs/2406.09716v1","updated":"2024-06-14T04:49:40Z","published":"2024-06-14T04:49:40Z","title":"Speed-up of Data Analysis with Kernel Trick in Encrypted Domain","summary":"  Homomorphic encryption (HE) is pivotal for secure computation on encrypted\ndata, crucial in privacy-preserving data analysis. However, efficiently\nprocessing high-dimensional data in HE, especially for machine learning and\nstatistical (ML/STAT) algorithms, poses a challenge. In this paper, we present\nan effective acceleration method using the kernel method for HE schemes,\nenhancing time performance in ML/STAT algorithms within encrypted domains. This\ntechnique, independent of underlying HE mechanisms and complementing existing\noptimizations, notably reduces costly HE multiplications, offering near\nconstant time complexity relative to data dimension. Aimed at accessibility,\nthis method is tailored for data scientists and developers with limited\ncryptography background, facilitating advanced data analysis in secure\nenvironments.\n","authors":["Joon Soo Yoo","Baek Kyung Song","Tae Min Ahn","Ji Won Heo","Ji Won Yoon"],"pdf_url":"https://arxiv.org/pdf/2406.09716v1.pdf","comment":"Submitted as a preprint"},{"id":"http://arxiv.org/abs/2406.09714v1","updated":"2024-06-14T04:46:39Z","published":"2024-06-14T04:46:39Z","title":"Large language model validity via enhanced conformal prediction methods","summary":"  We develop new conformal inference methods for obtaining validity guarantees\non the output of large language models (LLMs). Prior work in conformal language\nmodeling identifies a subset of the text that satisfies a high-probability\nguarantee of correctness. These methods work by filtering claims from the LLM's\noriginal response if a scoring function evaluated on the claim fails to exceed\na threshold calibrated via split conformal prediction. Existing methods in this\narea suffer from two deficiencies. First, the guarantee stated is not\nconditionally valid. The trustworthiness of the filtering step may vary based\non the topic of the response. Second, because the scoring function is\nimperfect, the filtering step can remove many valuable and accurate claims. We\naddress both of these challenges via two new conformal methods. First, we\ngeneralize the conditional conformal procedure of Gibbs et al. (2023) in order\nto adaptively issue weaker guarantees when they are required to preserve the\nutility of the output. Second, we show how to systematically improve the\nquality of the scoring function via a novel algorithm for differentiating\nthrough the conditional conformal procedure. We demonstrate the efficacy of our\napproach on both synthetic and real-world datasets.\n","authors":["John J. Cherian","Isaac Gibbs","Emmanuel J. Candès"],"pdf_url":"https://arxiv.org/pdf/2406.09714v1.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.09713v1","updated":"2024-06-14T04:46:14Z","published":"2024-06-14T04:46:14Z","title":"Meta-Learning Loss Functions for Deep Neural Networks","summary":"  Humans can often quickly and efficiently solve complex new learning tasks\ngiven only a small set of examples. In contrast, modern artificially\nintelligent systems often require thousands or millions of observations in\norder to solve even the most basic tasks. Meta-learning aims to resolve this\nissue by leveraging past experiences from similar learning tasks to embed the\nappropriate inductive biases into the learning system. Historically methods for\nmeta-learning components such as optimizers, parameter initializations, and\nmore have led to significant performance increases. This thesis aims to explore\nthe concept of meta-learning to improve performance, through the\noften-overlooked component of the loss function. The loss function is a vital\ncomponent of a learning system, as it represents the primary learning\nobjective, where success is determined and quantified by the system's ability\nto optimize for that objective successfully.\n","authors":["Christian Raymond"],"pdf_url":"https://arxiv.org/pdf/2406.09713v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2406.09009v2","updated":"2024-06-14T04:41:22Z","published":"2024-06-13T11:29:21Z","title":"Fredformer: Frequency Debiased Transformer for Time Series Forecasting","summary":"  The Transformer model has shown leading performance in time series\nforecasting. Nevertheless, in some complex scenarios, it tends to learn\nlow-frequency features in the data and overlook high-frequency features,\nshowing a frequency bias. This bias prevents the model from accurately\ncapturing important high-frequency data features. In this paper, we undertook\nempirical analyses to understand this bias and discovered that frequency bias\nresults from the model disproportionately focusing on frequency features with\nhigher energy. Based on our analysis, we formulate this bias and propose\nFredformer, a Transformer-based framework designed to mitigate frequency bias\nby learning features equally across different frequency bands. This approach\nprevents the model from overlooking lower amplitude features important for\naccurate forecasting. Extensive experiments show the effectiveness of our\nproposed approach, which can outperform other baselines in different real-world\ntime-series datasets. Furthermore, we introduce a lightweight variant of the\nFredformer with an attention matrix approximation, which achieves comparable\nperformance but with much fewer parameters and lower computation costs. The\ncode is available at: https://github.com/chenzRG/Fredformer\n","authors":["Xihao Piao","Zheng Chen","Taichi Murayama","Yasuko Matsubara","Yasushi Sakurai"],"pdf_url":"https://arxiv.org/pdf/2406.09009v2.pdf","comment":"This paper has been accepted by SIGKDD2024"},{"id":"http://arxiv.org/abs/2403.06064v3","updated":"2024-06-14T04:15:20Z","published":"2024-03-10T02:16:13Z","title":"L^2GC:Lorentzian Linear Graph Convolutional Networks for Node\n  Classification","summary":"  Linear Graph Convolutional Networks (GCNs) are used to classify the node in\nthe graph data. However, we note that most existing linear GCN models perform\nneural network operations in Euclidean space, which do not explicitly capture\nthe tree-like hierarchical structure exhibited in real-world datasets that\nmodeled as graphs. In this paper, we attempt to introduce hyperbolic space into\nlinear GCN and propose a novel framework for Lorentzian linear GCN.\nSpecifically, we map the learned features of graph nodes into hyperbolic space,\nand then perform a Lorentzian linear feature transformation to capture the\nunderlying tree-like structure of data. Experimental results on standard\ncitation networks datasets with semi-supervised learning show that our approach\nyields new state-of-the-art results of accuracy 74.7$\\%$ on Citeseer and\n81.3$\\%$ on PubMed datasets. Furthermore, we observe that our approach can be\ntrained up to two orders of magnitude faster than other nonlinear GCN models on\nPubMed dataset. Our code is publicly available at\nhttps://github.com/llqy123/LLGC-master.\n","authors":["Qiuyu Liang","Weihua Wang","Feilong Bao","Guanglai Gao"],"pdf_url":"https://arxiv.org/pdf/2403.06064v3.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2406.06792v2","updated":"2024-06-14T03:59:05Z","published":"2024-06-10T20:59:52Z","title":"Reinforced Compressive Neural Architecture Search for Versatile\n  Adversarial Robustness","summary":"  Prior neural architecture search (NAS) for adversarial robustness works have\ndiscovered that a lightweight and adversarially robust neural network\narchitecture could exist in a non-robust large teacher network, generally\ndisclosed by heuristic rules through statistical analysis and neural\narchitecture search, generally disclosed by heuristic rules from neural\narchitecture search. However, heuristic methods cannot uniformly handle\ndifferent adversarial attacks and \"teacher\" network capacity. To solve this\nchallenge, we propose a Reinforced Compressive Neural Architecture Search\n(RC-NAS) for Versatile Adversarial Robustness. Specifically, we define task\nsettings that compose datasets, adversarial attacks, and teacher network\ninformation. Given diverse tasks, we conduct a novel dual-level training\nparadigm that consists of a meta-training and a fine-tuning phase to\neffectively expose the RL agent to diverse attack scenarios (in meta-training),\nand making it adapt quickly to locate a sub-network (in fine-tuning) for any\npreviously unseen scenarios. Experiments show that our framework could achieve\nadaptive compression towards different initial teacher networks, datasets, and\nadversarial attacks, resulting in more lightweight and adversarially robust\narchitectures.\n","authors":["Dingrong Wang","Hitesh Sapkota","Zhiqiang Tao","Qi Yu"],"pdf_url":"https://arxiv.org/pdf/2406.06792v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2406.09694v1","updated":"2024-06-14T03:38:40Z","published":"2024-06-14T03:38:40Z","title":"An Efficient Approach to Regression Problems with Tensor Neural Networks","summary":"  This paper introduces a tensor neural network (TNN) to address nonparametric\nregression problems. Characterized by its distinct sub-network structure, the\nTNN effectively facilitates variable separation, thereby enhancing the\napproximation of complex, unknown functions. Our comparative analysis reveals\nthat the TNN outperforms conventional Feed-Forward Networks (FFN) and Radial\nBasis Function Networks (RBN) in terms of both approximation accuracy and\ngeneralization potential, despite a similar scale of parameters. A key\ninnovation of our approach is the integration of statistical regression and\nnumerical integration within the TNN framework. This integration allows for the\nefficient computation of high-dimensional integrals associated with the\nregression function. The implications of this advancement extend to a broader\nrange of applications, particularly in scenarios demanding precise\nhigh-dimensional data analysis and prediction.\n","authors":["Yongxin Li"],"pdf_url":"https://arxiv.org/pdf/2406.09694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06255v4","updated":"2024-06-14T03:37:46Z","published":"2023-09-12T14:16:34Z","title":"Enhancing multimodal cooperation via sample-level modality valuation","summary":"  One primary topic of multimodal learning is to jointly incorporate\nheterogeneous information from different modalities. However most models often\nsuffer from unsatisfactory multimodal cooperation which cannot jointly utilize\nall modalities well. Some methods are proposed to identify and enhance the\nworse learnt modality but they are often hard to provide the fine-grained\nobservation of multimodal cooperation at sample-level with theoretical support.\nHence it is essential to reasonably observe and improve the fine-grained\ncooperation between modalities especially when facing realistic scenarios where\nthe modality discrepancy could vary across different samples. To this end we\nintroduce a sample-level modality valuation metric to evaluate the contribution\nof each modality for each sample. Via modality valuation we observe that\nmodality discrepancy indeed could be different at sample-level beyond the\nglobal contribution discrepancy at dataset-level. We further analyze this issue\nand improve cooperation between modalities at sample-level by enhancing the\ndiscriminative ability of low-contributing modalities in a targeted manner.\nOverall our methods reasonably observe the fine-grained uni-modal contribution\nand achieve considerable improvement. The source code and dataset are available\nat https://github.com/GeWu-Lab/Valuate-and-Enhance-Multimodal-Cooperation.\n","authors":["Yake Wei","Ruoxuan Feng","Zihe Wang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2309.06255v4.pdf","comment":"Accepted by CVPR 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2405.08813v2","updated":"2024-06-14T17:59:34Z","published":"2024-05-14T17:59:02Z","title":"CinePile: A Long Video Question Answering Dataset and Benchmark","summary":"  Current datasets for long-form video understanding often fall short of\nproviding genuine long-form comprehension challenges, as many tasks derived\nfrom these datasets can be successfully tackled by analyzing just one or a few\nrandom frames from a video. To address this issue, we present a novel dataset\nand benchmark, CinePile, specifically designed for authentic long-form video\nunderstanding. This paper details our innovative approach for creating a\nquestion-answer dataset, utilizing advanced LLMs with human-in-the-loop and\nbuilding upon human-generated raw data. Our comprehensive dataset comprises\n305,000 multiple-choice questions (MCQs), covering various visual and\nmultimodal aspects, including temporal comprehension, understanding\nhuman-object interactions, and reasoning about events or actions within a\nscene. Additionally, we evaluate recent video-centric LLMs, both open-source\nand proprietary, on the test split of our dataset. The findings reveal that\neven state-of-the-art video-centric LLMs significantly lag behind human\nperformance in these tasks, highlighting the complexity and challenge inherent\nin video understanding. The dataset is available at\nhttps://hf.co/datasets/tomg-group-umd/cinepile\n","authors":["Ruchit Rawal","Khalid Saifullah","Ronen Basri","David Jacobs","Gowthami Somepalli","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2405.08813v2.pdf","comment":"Project page with all the artifacts -\n  https://ruchitrawal.github.io/cinepile/. Updated version with results on\n  Gemini Flash model and additional related work"},{"id":"http://arxiv.org/abs/2406.10200v1","updated":"2024-06-14T17:33:11Z","published":"2024-06-14T17:33:11Z","title":"SSTFB: Leveraging self-supervised pretext learning and temporal\n  self-attention with feature branching for real-time video polyp segmentation","summary":"  Polyps are early cancer indicators, so assessing occurrences of polyps and\ntheir removal is critical. They are observed through a colonoscopy screening\nprocedure that generates a stream of video frames. Segmenting polyps in their\nnatural video screening procedure has several challenges, such as the\nco-existence of imaging artefacts, motion blur, and floating debris. Most\nexisting polyp segmentation algorithms are developed on curated still image\ndatasets that do not represent real-world colonoscopy. Their performance often\ndegrades on video data. We propose a video polyp segmentation method that\nperforms self-supervised learning as an auxiliary task and a spatial-temporal\nself-attention mechanism for improved representation learning. Our end-to-end\nconfiguration and joint optimisation of losses enable the network to learn more\ndiscriminative contextual features in videos. Our experimental results\ndemonstrate an improvement with respect to several state-of-the-art (SOTA)\nmethods. Our ablation study also confirms that the choice of the proposed joint\nend-to-end training improves network accuracy by over 3% and nearly 10% on both\nthe Dice similarity coefficient and intersection-over-union compared to the\nrecently proposed method PNS+ and Polyp-PVT, respectively. Results on\npreviously unseen video data indicate that the proposed method generalises.\n","authors":["Ziang Xu","Jens Rittscher","Sharib Ali"],"pdf_url":"https://arxiv.org/pdf/2406.10200v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2406.09998v1","updated":"2024-06-14T13:15:18Z","published":"2024-06-14T13:15:18Z","title":"Understanding Pedestrian Movement Using Urban Sensing Technologies: The\n  Promise of Audio-based Sensors","summary":"  While various sensors have been deployed to monitor vehicular flows, sensing\npedestrian movement is still nascent. Yet walking is a significant mode of\ntravel in many cities, especially those in Europe, Africa, and Asia.\nUnderstanding pedestrian volumes and flows is essential for designing safer and\nmore attractive pedestrian infrastructure and for controlling periodic\novercrowding. This study discusses a new approach to scale up urban sensing of\npeople with the help of novel audio-based technology. It assesses the benefits\nand limitations of microphone-based sensors as compared to other forms of\npedestrian sensing. A large-scale dataset called ASPED is presented, which\nincludes high-quality audio recordings along with video recordings used for\nlabeling the pedestrian count data. The baseline analyses highlight the promise\nof using audio sensors for pedestrian tracking, although algorithmic and\ntechnological improvements to make the sensors practically usable continue.\nThis study also demonstrates how the data can be leveraged to predict\npedestrian trajectories. Finally, it discusses the use cases and scenarios\nwhere audio-based pedestrian sensing can support better urban and\ntransportation planning.\n","authors":["Chaeyeon Han","Pavan Seshadri","Yiwei Ding","Noah Posner","Bon Woo Koo","Animesh Agrawal","Alexander Lerch","Subhrajit Guhathakurta"],"pdf_url":"https://arxiv.org/pdf/2406.09998v1.pdf","comment":"submitted to Urban Informatics"},{"id":"http://arxiv.org/abs/2406.06048v2","updated":"2024-06-14T12:29:19Z","published":"2024-06-10T06:29:00Z","title":"Robust Latent Representation Tuning for Image-text Classification","summary":"  Large models have demonstrated exceptional generalization capabilities in\ncomputer vision and natural language processing. Recent efforts have focused on\nenhancing these models with multimodal processing abilities. However,\naddressing the challenges posed by scenarios where one modality is absent\nremains a significant hurdle. In response to this issue, we propose a robust\nlatent representation tuning method for large models. Specifically, our\napproach introduces a modality latent translation module to maximize the\ncorrelation between modalities, resulting in a robust representation. Following\nthis, a newly designed fusion module is employed to facilitate information\ninteraction between the modalities. Within this framework, common semantics are\nrefined during training, and robust performance is achieved even in the absence\nof one modality. Importantly, our method maintains the frozen state of the\nimage and text foundation models to preserve their capabilities acquired\nthrough large-scale pretraining. We conduct experiments on several public\ndatasets, and the results underscore the effectiveness of our proposed method.\n","authors":["Hao Sun","Yu Song"],"pdf_url":"https://arxiv.org/pdf/2406.06048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09884v1","updated":"2024-06-14T09:55:54Z","published":"2024-06-14T09:55:54Z","title":"Enhancing Fake News Detection in Social Media via Label Propagation on\n  Cross-modal Tweet Graph","summary":"  Fake news detection in social media has become increasingly important due to\nthe rapid proliferation of personal media channels and the consequential\ndissemination of misleading information. Existing methods, which primarily rely\non multimodal features and graph-based techniques, have shown promising\nperformance in detecting fake news. However, they still face a limitation,\ni.e., sparsity in graph connections, which hinders capturing possible\ninteractions among tweets. This challenge has motivated us to explore a novel\nmethod that densifies the graph's connectivity to capture denser interaction\nbetter. Our method constructs a cross-modal tweet graph using CLIP, which\nencodes images and text into a unified space, allowing us to extract potential\nconnections based on similarities in text and images. We then design a Feature\nContextualization Network with Label Propagation (FCN-LP) to model the\ninteraction among tweets as well as positive or negative correlations between\npredicted labels of connected tweets. The propagated labels from the graph are\nweighted and aggregated for the final detection. To enhance the model's\ngeneralization ability to unseen events, we introduce a domain generalization\nloss that ensures consistent features between tweets on seen and unseen events.\nWe use three publicly available fake news datasets, Twitter, PHEME, and Weibo,\nfor evaluation. Our method consistently improves the performance over the\nstate-of-the-art methods on all benchmark datasets and effectively demonstrates\nits aptitude for generalizing fake news detection in social media.\n","authors":["Wanqing Zhao","Yuta Nakashima","Haiyuan Chen","Noboru Babaguchi"],"pdf_url":"https://arxiv.org/pdf/2406.09884v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.09833v1","updated":"2024-06-14T08:43:31Z","published":"2024-06-14T08:43:31Z","title":"SHMamba: Structured Hyperbolic State Space Model for Audio-Visual\n  Question Answering","summary":"  The Audio-Visual Question Answering (AVQA) task holds significant potential\nfor applications. Compared to traditional unimodal approaches, the multi-modal\ninput of AVQA makes feature extraction and fusion processes more challenging.\nEuclidean space is difficult to effectively represent multi-dimensional\nrelationships of data. Especially when extracting and processing data with a\ntree structure or hierarchical structure, Euclidean space is not suitable as an\nembedding space. Additionally, the self-attention mechanism in Transformers is\neffective in capturing the dynamic relationships between elements in a\nsequence. However, the self-attention mechanism's limitations in window\nmodeling and quadratic computational complexity reduce its effectiveness in\nmodeling long sequences. To address these limitations, we propose SHMamba:\nStructured Hyperbolic State Space Model to integrate the advantages of\nhyperbolic geometry and state space models. Specifically, SHMamba leverages the\nintrinsic properties of hyperbolic space to represent hierarchical structures\nand complex relationships in audio-visual data. Meanwhile, the state space\nmodel captures dynamic changes over time by globally modeling the entire\nsequence. Furthermore, we introduce an adaptive curvature hyperbolic alignment\nmodule and a cross fusion block to enhance the understanding of hierarchical\nstructures and the dynamic exchange of cross-modal information, respectively.\nExtensive experiments demonstrate that SHMamba outperforms previous methods\nwith fewer parameters and computational costs. Our learnable parameters are\nreduced by 78.12\\%, while the average performance improves by 2.53\\%.\nExperiments show that our method demonstrates superiority among all current\nmajor methods and is more suitable for practical application scenarios.\n","authors":["Zhe Yang","Wenrui Li","Guanghui Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.09833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09762v1","updated":"2024-06-14T06:59:54Z","published":"2024-06-14T06:59:54Z","title":"Full-reference Point Cloud Quality Assessment Using Spectral Graph\n  Wavelets","summary":"  Point clouds in 3D applications frequently experience quality degradation\nduring processing, e.g., scanning and compression. Reliable point cloud quality\nassessment (PCQA) is important for developing compression algorithms with good\nbitrate-quality trade-offs and techniques for quality improvement (e.g.,\ndenoising). This paper introduces a full-reference (FR) PCQA method utilizing\nspectral graph wavelets (SGWs). First, we propose novel SGW-based PCQA metrics\nthat compare SGW coefficients of coordinate and color signals between reference\nand distorted point clouds. Second, we achieve accurate PCQA by integrating\nseveral conventional FR metrics and our SGW-based metrics using support vector\nregression. To our knowledge, this is the first study to introduce SGWs for\nPCQA. Experimental results demonstrate the proposed PCQA metric is more\naccurately correlated with subjective quality scores compared to conventional\nPCQA metrics.\n","authors":["Ryosuke Watanabe","Keisuke Nonaka","Eduardo Pavez","Tatsuya Kobayashi","Antonio Ortega"],"pdf_url":"https://arxiv.org/pdf/2406.09762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06255v4","updated":"2024-06-14T03:37:46Z","published":"2023-09-12T14:16:34Z","title":"Enhancing multimodal cooperation via sample-level modality valuation","summary":"  One primary topic of multimodal learning is to jointly incorporate\nheterogeneous information from different modalities. However most models often\nsuffer from unsatisfactory multimodal cooperation which cannot jointly utilize\nall modalities well. Some methods are proposed to identify and enhance the\nworse learnt modality but they are often hard to provide the fine-grained\nobservation of multimodal cooperation at sample-level with theoretical support.\nHence it is essential to reasonably observe and improve the fine-grained\ncooperation between modalities especially when facing realistic scenarios where\nthe modality discrepancy could vary across different samples. To this end we\nintroduce a sample-level modality valuation metric to evaluate the contribution\nof each modality for each sample. Via modality valuation we observe that\nmodality discrepancy indeed could be different at sample-level beyond the\nglobal contribution discrepancy at dataset-level. We further analyze this issue\nand improve cooperation between modalities at sample-level by enhancing the\ndiscriminative ability of low-contributing modalities in a targeted manner.\nOverall our methods reasonably observe the fine-grained uni-modal contribution\nand achieve considerable improvement. The source code and dataset are available\nat https://github.com/GeWu-Lab/Valuate-and-Enhance-Multimodal-Cooperation.\n","authors":["Yake Wei","Ruoxuan Feng","Zihe Wang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2309.06255v4.pdf","comment":"Accepted by CVPR 2024"}]}}